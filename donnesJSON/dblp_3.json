{"authors": ["Francine Berman", "Vinton G. Cerf"], "n_citation": 0, "references": [], "title": "Social and ethical behavior in the internet of things", "venue": "Communications of The ACM", "year": 2017, "id": "00f77fa9-ae49-4935-9166-2f5f9cdb3d6b"}
{"abstract": "This paper describes the design of a distributed architecture to support an interactive, interoperable, and collaborative forest fire fighting simulation using High Level Architecture (HLA). Based on the Run-time Infrastructure (RTI) services which are specified in HLA and C++ application programming interface (API) of the RTI, The distributed virtual fire fighting environment provides a practical foundation to enhance interactivity, interoperability for distributed simulation.. Users can build new federation application by the way of the goal system reorganization. The key techniques, such as FOM/SOM design, system structure, running mechanism of simulation system are discussed.", "authors": ["Chongcheng Chen", "Liyu Tang", "Xiaogang Feng", "Kaihui Lin"], "n_citation": 0, "title": "A distributed forest fire fighting simulation system based on HLA", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "06a987df-b067-413f-9555-405885651245"}
{"abstract": "We present a statistical generative model for unsupervised learning of verb argument structures. The model was used to automatically induce the argument structures for the 1,500 most frequent verbs of English. In an evaluation carried out for a representative sample of verbs, more than 90% of the induced argument structures were judged correct by human subjects. The induced structures also overlap significantly with those in PropBank, exhibiting some correct patterns of usage that are not present in this manually developed semantic resource.", "authors": ["Thiago Alexandre Salgueiro Pardo", "Daniel Marcu", "Maria Das Volpe Nunes"], "n_citation": 0, "title": "Unsupervised learning of verb argument structures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0764585b-fec1-4086-9751-643ef3902d52"}
{"abstract": "A fundamental problem in a pure Peer-to-Peer (P2P) file sharing system is how to protect the anonymity of nodes when providing efficient data access services. By seamlessly combining the technologies of multi-proxy and IP multicast together, we propose a multicast-based protocol for efficient file sharing with mutual anonymity in this paper. Furthermore, the proposed protocol can adaptively adjust file distribution and reduce the multicast cost simultaneously. The simulations show that Mapper possesses the merits of scalability, reliability, and high adaptability with high performance.", "authors": ["Baoliu Ye", "Jingyang Zhou", "Yang Zhang", "Jiannong Cao", "Daoxu Chen"], "n_citation": 0, "title": "An efficient protocol for Peer-to-Peer file sharing with mutual anonymity", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0c54ebf7-02e0-4099-b018-5777f08d1708"}
{"abstract": "Scientific gait (walking) analysis provides valuable information about an individual's locomotion function, in turn, to assist clinical diagnosis and prevention, such as assessing treatment for patients with impaired postural control and detecting risk of falls in elderly population. While several artificial intelligence (AI) paradigms are addressed for gait analysis, they usually utilize supervised techniques where subject groups are defined a priori. In this paper, we explore to investigate gait pattern mining with clustering-based approaches, in which k-means and hierarchical clustering algorithms are employed to derive gait pattern. After feature selection and data preparation, we conduct clustering on the constructed gait data model to build up pattern-based clusters. The centroids of clusters are then treated as the subject profiles to model the various kinds of gait pattern, e.g. normal or pathological. Experiments are undertaken to visualize the derived subject clusters, evaluate the quality of clustering paradigm in terms of silhouette and mean square error and compare the results with the discovery derived from hierarchy tree analysis. In addition, analysis conducted on test data demonstrates the usability of the proposed paradigm in clinical applications.", "authors": ["G. F. Xu", "Yanchun Zhang", "Rezaul Begg"], "n_citation": 0, "title": "Mining Gait Pattern for Clinical Locomotion Diagnosis Based on Clustering Techniques", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0f2f1974-a986-45ac-a187-25821dbb50b0"}
{"abstract": "Advanced personalized web applications require a carefully dealing with their users' wishes and preferences. Since such preferences do not always hold in general, personalized applications also have to consider the user's current situation. In this paper we present a novel framework for modeling situations and situated preferences. Our approach consists of a general meta model for situations, which can be applied as foundation for situation models in a wide range of applications. Furthermore, an XML-based preference repository for the storage and management of situated preferences is developed. Long-term and situated preferences can easily be accessed with the preference repository interface. Particularly, preferences best-matching to a given situation can be queried. This approach allows web applications to react flexibly and personalized to the changing situations of their users.", "authors": ["Stefan Holland", "W. Kiessling"], "n_citation": 0, "title": "Situated preferences and preference repositories for personalized database applications", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "146feca7-deba-4472-9f3e-1772ffbcd100"}
{"abstract": "This paper studies how to adjoin probability to event structures, leading to the model of probabilistic event structures. In their simplest form probabilistic choice is localised to cells, where conflict arises; in which case probabilistic independence coincides with causal independence. An application to the semantics of a probabilistic CCS is sketched. An event structure is associated with a domain-that of its configurations ordered by inclusion. In domain theory probabilistic processes are denoted by continuous valuations on a domain. A key result of this paper is a representation theorem showing how continuous valuations on the domain of a confusion-free event structure correspond to the probabilistic event structures it supports. We explore how to extend probability to event structures which are not confusion-free via two notions of probabilistic runs of a general event structure. Finally, we show how probabilistic correlation and probabilistic event structures with confusion can arise from event structures which are originally confusion-free by using morphisms to rename and hide events.", "authors": ["Daniele Varacca", "Hagen V\u00f6lzer", "Glynn Winskel"], "n_citation": 50, "title": "Probabilistic event structures and domains", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "14c7fdc4-4c54-4424-96fd-bac291f034fd"}
{"abstract": "There has been considerable interest recently on developing a system to track items like pharmaceutical drugs or food products. Such a system can help prevent counterfeits, aid product recall, and improve general logistics. In this paper, we present such system based on radio frequency identity (RFID) technology. Our solution provides the means of storing the entire movement of the item from original manufacturer to final consumer on the RFID tag itself, and also makes it more difficult to introduce large numbers of counterfeits. The solution also allows the end user to easily verify the authenticity of the item.", "authors": ["Chiu Chiang Tan", "Qun Li"], "n_citation": 0, "title": "A robust and secure RFID-based pedigree system (short paper)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "16912645-20d3-49c6-8c01-e200dfff1546"}
{"abstract": "We define an interpretation of the Isabelle/HOL logic in HOL Light and its metalanguage, OCaml. Some aspects of the Isabelle logic are not representable directly in the HOL Light object logic. The interpretation thus takes the form of a set of elaboration rules, where features of the Isabelle logic that cannot be represented directly are elaborated to functors in OCaml. We demonstrate the effectiveness of the interpretation via an implementation, translating a significant part of the Isabelle standard library into HOL Light.", "authors": ["Sean McLaughlin"], "n_citation": 50, "title": "An interpretation of isabelle/HOL in HOL light", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "17854d98-495d-4a48-a228-3d3335afed3d"}
{"abstract": "Computer and related information and communication technologies have profoundly affected the shape of modern society. Shepherding the creation and utilization of effective and efficient computational technologies are the joint tasks of Computer Science and Information Systems researchers. Their realm is to understand and explicate the nature of those technologies and how and why they come into existence. This knowledge forms the foundation of theories to explain and, hopefully, to predict their impacts on individuals, groups, organizations, and society as a whole. The very creation of an innovative technology focused on a specific problem in a specific context can have far reaching effects, completely unpredicted and unintended by the innovator. We argue that researchers in Computer Science and Information Systems must be cognizant of the broader implications of their work and encourage their interaction with practitioners and researchers in a variety of disciplines to identify fruitful areas of scientific inquiry.", "authors": ["Salvatore T. March"], "n_citation": 0, "title": "Reflections on Computer Science and Information Systems research", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "1a7b825a-fd84-4522-aa2c-ada41e722d21"}
{"abstract": "We propose an improved approach that stores and queries a large volume of XML documents in a relational database, while removing the redundancy of path information and using an inverted index on the reduced path information. In order to store XML documents in a relational database, the XML document is decomposed into nodes based on its tree structure, and stored in relational tables with path information from the root node to each node. The existing XML storage methods which use relational data model, usually store path information for every node. Thus, they can increase storage overhead and decrease query processing performance with the increased data volume. Our approach stores only leaf node path information in XML tree structure while finding out internal node path information from the leaf node path information. In this manner, our approach can reduce data volume for a large amount of XML documents to a degree and also reduce the size of inverted index for the path information with the smaller number of posting lists by key words. We show the effectiveness of this approach through several experiments that compare XPath query performance with the existing methods.", "authors": ["Byeong-Soo Jeong", "Young-Koo Lee"], "n_citation": 50, "title": "Storing and Querying of XML Documents Without Redundant Path Information", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1fc47acc-cbc7-4173-80a6-0c76785ea9ed"}
{"authors": ["Clk Wong", "Pengcheng Shi"], "n_citation": 0, "title": "Finite deformation guided nonlinear filtering for multiframe cardiac motion analysis", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "20abc9cd-3312-46fb-a8a2-21b496a6a1c7"}
{"abstract": "The Model Evolution (ME) Calculus is a proper lifting to first-order logic of the DPLL procedure, a backtracking search procedure for propositional satisfiability. Like DPLL, the ME calculus is based on the idea of incrementally building a model of the input formula by alternating constraint propagation steps with non-deterministic decision steps. One of the major conceptual improvements over basic DPLL is lemma learning, a mechanism for generating new formulae that prevent later in the search combinations of decision steps guaranteed to lead to failure. We introduce two lemma generation methods for ME proof procedures, with various degrees of power, effectiveness in reducing search, and computational overhead. Even if formally correct, each of these methods presents complications that do not exist at the propositional level but need to be addressed for learning to be effective in practice for ME. We discuss some of these issues and present initial experimental results on the performance of an implementation of the two learning procedures within our ME prover Darwin.", "authors": ["Peter Baumgartner", "Alexander Fuchs", "Cesare Tinelli"], "n_citation": 50, "title": "Lemma learning in the model evolution calculus", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "25e6d39e-a87a-43f6-8943-ad9f81e4e3d6"}
{"abstract": "This paper presents a physically-based approach for real-time music fountain simulation based on fluid dynamics and particle systems. A special case of Navier-Stokes equations is explored and particles are employed to model fountain geometry. Music intensity is used to synchronize the motion of fountain particles, and simple dynamics equations of particles are used to simulate realistic fountain behavior. The proposed algorithm can be easily integrated into interactive applications for real-time fountain simulation.", "authors": ["Huagen Wan", "Yujuan Cao", "Xiaoxia Han", "Xiaogang Jin"], "n_citation": 0, "title": "Physically-based real-time music fountain simulation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "27336253-3341-47ac-8fb9-4cbd4d3c52a8"}
{"abstract": "Building upon the known generalized-quantifier-based first-order characterization of LOGCFL, we lay the groundwork for a deeper investigation. Specifically, we examine subclasses of LOGCFL arising from varying the arity and nesting of groupoidal quantifiers. Our work extends the elaborate theory relating monoidal quantifiers to NC and its subclasses. In the absence of the BIT predicate, we resolve the main issues: we show in particular that no single outermost unary groupoidal quantifier with FO can capture all the context-free languages, and we obtain the surprising result that a variant of Greibach's hardest context-free language is LOGCFL-complete under quantifier-free BIT-free interpretations. We then prove that FO with unary groupoidal quantifiers is strictly more expressive with the BIT predicate than without. Considering a particular groupoidal quantifier, we prove that first-order logic with majority of pairs is strictly more expressive than first-order with majority of individuals. As a technical tool of independent interest, we define the notion of an aperiodic nondeterministic finite automaton and prove that FO translations are precisely the mappings computed by single-valued aperiodic nondeterministic finite transducers.", "authors": ["Clemens Lautemann", "Pierre McKenzie", "Thomas Schwentick", "Heribert Vollmer"], "n_citation": 50, "title": "The descriptive complexity approach to LOGCFL", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "27f9813a-9f75-467c-b812-d9f17074de36"}
{"abstract": "Workflow is a critical part of the emerging Grid Computing Environments and captures the linkage of constituent services together in a hierarchical fashion to build larger composite services. As an indispensable element in workflow systems for Grid services, Grid workflow language (and its enactor) serves as the basis for specifying and executing collaborative interactive workflows in Grids. Since Open Grid Services Architecture (OGSA) is built on Web services technology, it is natural to expect that the existing orchestration standards designed for Web services can be leveraged in OGSI based Grids. In this paper we propose a Grid workflow language called Grid Services Workflow Execution Language (GSWEL), which is based on OGSA and provides an extension to Business Process Execution Language for Web Services (BPEL4WS) in order to better support Grid characteristics.", "authors": ["Yan Yang", "Shengqun Tang", "Wentao Zhang", "Lina Fang"], "n_citation": 0, "title": "A Workflow Language for Grid Services in OGSI-based grids", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2ad13ef2-35d9-4e53-9a5b-e7ca1f95e1a7"}
{"abstract": "Large database schemata can be drastically simplified if techniques of modular and structural modeling are used. Applications and thus schemata have an inner meta-structure. The explicit treatment of the inner structuring may be used during development of large database schemata and may ease the development to a large extent. It is surprising that dimensions and internal separations have not been considered yet also they are very natural and easy to capture. This paper develops an approach to explicit treatment of the inherent many-dimensional structuring on the basis of dimensions such as the kernel dimension, the association dimension, the log dimension, the meta-characterization dimension, and the lifespan dimension.", "authors": ["Thomas Feyer", "Bernhard Thalheim"], "n_citation": 0, "title": "Many-dimensional schema modeling", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2c099c60-f2c7-47fd-b84d-2cd67537538e"}
{"abstract": "Scientific workflows are becoming increasingly important as a unifying mechanism for interlinking scientific data management, analysis, simulation, and visualization tasks. Scientific workflow systems are problem-solving environments, supporting scientists in the creation and execution of scientific workflows. While current systems permit the creation of executable workflows, conceptual modeling and design of scientific workflows has largely been neglected. Unlike business workflows, scientific workflows are typically highly data-centric naturally leading to dataflow-oriented modeling approaches. We first develop a formal model for scientific workflows based on an actor-oriented modeling and design approach, originally developed for studying models of complex concurrent systems. Actor-oriented modeling separates two modeling concerns: component communication (dataflow) and overall workflow coordination (orchestration). We then extend our framework by introducing a novel hybrid type system, separating further the concerns of conventional data modeling (structural data type) and conceptual modeling (semantic type). In our approach, semantic and structural mismatches can be handled independently or simultaneously, and via different types of adapters, giving rise to new methods of scientific workflow design.", "authors": ["Shawn Bowers", "Bertram Lud\u00e4scher"], "n_citation": 0, "title": "Actor-oriented design of scientific workflows", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2c34f261-ae25-4295-ad98-47ddaee45826"}
{"abstract": "In this paper we argue that people sometimes construct analogical representations of the information that they extract from simple graphs and that these representations are subject to the same nomic constraints as the original graphical representations. We briefly review behavioural and neuropsychological findings across a range of tasks related to graph comprehension, which suggest that people spontaneously construct analogical representations with a spatial quality. We describe two experiments demonstrating that the representations constructed by people reasoning about graphs may also possess this spatial quality. We contrast our results with the predictions of current models of graph comprehension and outline some further questions for research.", "authors": ["Aidan Feeney", "Lara Webber"], "n_citation": 0, "title": "Analogical representation and graph comprehension", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2f846521-a155-42cf-bcc5-de69dfc32b9f"}
{"authors": ["Raimondo Schettini", "Francesca Gasparini", "Simone Bianco"], "n_citation": 0, "title": "Region-based Illuminant Estimation for Effective Color Correction", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "300c7f3d-5350-4ed7-81f9-e15763b3a72d"}
{"authors": ["Massimo Marchi", "Alessandra Mileo", "Alessandro Provetti"], "n_citation": 0, "title": "Grid Service Selection with PPDL", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3422ae0d-71e7-4b14-8b3c-384f9f86c3ec"}
{"abstract": "XML documents, and other forms of semi-structured data, may be roughly described as edge labeled trees; it is therefore natural to use tree automata to reason on them. This idea has already been successfully applied in the context of Document Type Definition (DTD), the simplest standard for defining XML documents validity, but additional work is needed to take into account XML Schema, a more advanced standard, for which regular tree automata are not satisfactory. In this paper, we define a tree logic that directly embeds XML Schema as a plain subset as well as a new class of automata for unranked trees, used to decide this logic, which is well-suited to the processing of XML documents and schemas.", "authors": ["Silvano Dal Zilio", "Denis Lugiez"], "n_citation": 0, "title": "XML Schema, tree logic and sheaves automata", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "34e9d376-5475-4346-a714-53f92703615c"}
{"abstract": "We present a survey of recent results concerning the theoretical and empirical performance of algorithms for learning regularized least-squares classifiers. The behavior of these family of learning algorithms is analyzed in both the statistical and the worst-case (individual sequence) data-generating models.", "authors": ["Nicol\u00f2 Cesa-Bianchi"], "n_citation": 0, "title": "Applications of regularized least squares to classification problems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "37bc0482-ce38-4992-8c69-e423fe2f0394"}
{"abstract": "Data mining is a newly developed and emerging area of computational intelligence that offers new theories, techniques, and tools for analysis of large data sets. It is expected to offer more and more support to modem organizations which face a serious challenge of how to make decision from massively increased information so that they can better understand their markets, customers, suppliers, operations and internal business processes. This paper discusses fuzzy decision-making using the Grey Related Analysis method. Fuzzy models are expected to better reflect decision maker uncertainty, at some cost in accuracy relative to crisp models. Monte Carlo Simulation, a data mining technique, is used to measure the impact of fuzzy models relative to crisp models. Fuzzy models were found to provide fits as good as crisp models in the data analyzed.", "authors": ["David L. Olson", "Desheng Wu"], "n_citation": 0, "title": "Decision making with uncertainty and data mining", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "38b18209-b6cb-4ac5-a1e4-cbaa49472f06"}
{"abstract": "Model Driven Architecture (MDA) is being adopted as a new development strategy. MDA is based on both the definition of models at different levels of abstraction and the application of consecutive transformations in order to obtain code from these models. However, little methodological support is provided to both define and apply model-to-model transformations. In this work, we introduce a strategy based on graph transformations that allow us to automate the derivation of the navigational model of the OOWS method from early requirements specifications, by following an MDA-based development process. In order to define and apply the graph transformations the Attributed Graph Grammar tool is used. In addition, due to the OOWS PIM-to-Code transformation capabilities, we show how this strategy allows us to obtain prototypes of web applications from early requirements specifications.", "authors": ["Pedro Valderas", "Joan Fons", "Vicente Pelechano"], "n_citation": 50, "title": "Transforming web requirements into navigational models : AN MDA based approach", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3ab8c369-8fc4-47d6-850e-e10596fe4127"}
{"abstract": "In this paper we consider the problem of finding maximum weighted matchings in bipartite graphs with nonnegative integer weights. The presented algorithm for this problem work in 0(Wn \u03c9 ) 1  time, where \u03c9 is the matrix multiplication exponent, and W is the highest edge weight in the graph. As a consequence of this result we obtain O(Wn \u03c9 ) time algorithms for computing: minimum weight bipartite vertex cover, single source shortest paths and minimum weight vertex disjoint s-t paths.", "authors": ["Piotr Sankowski"], "n_citation": 50, "title": "Weighted Bipartite Matching in Matrix Multiplication Time", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "408b5367-1f11-46ee-98f6-595400431808"}
{"abstract": "Scalability and flexibility are key issues in the design of modeling and simulation infrastructure. In this paper, an infrastructure (called DMSE) which is dedicated to support various scalability and flexibility of modeling and simulation applications. An approach which would establish various virtual networks on the DMSE is presented. The purpose of the DMSE is to support various target networks in a common infrastructure. With help of the DBMS, the monitoring and management of the infrastructure create dynamic data in the node and view points. Thus, the infrastructure helps users to flexibly model and simulate various networks, dynamically deploy service and protocol to the node, and arbitrarily monitor and manage data from nodes.", "authors": ["Yueming Lu", "Hui Li", "Aibo Liu", "Yuefeng Ji"], "n_citation": 0, "title": "Towards a distributed modeling and simulation environment for networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "44e0764b-8d64-4402-8637-db754f70e576"}
{"abstract": "An (X,Y)-random system takes inputs X 1 , X 2 ,\u2026 \u2208 X and generates, for each new input X i , an output Yi \u2208 Y, depending probabilistically on X 1 ,. X i  and Y 1 ,\u2026, Y i-1 . Many cryptographic systems like block ciphers, MAC-schemes, pseudo-random functions, etc., can be modeled as random systems, where in fact Y i often depends only on X i , i.e., the system is stateless. The security proof of such a system (e.g. a block cipher) amounts to showing that it is indistinguishable from a certain perfect system (e.g. a random permutation). We propose a general framework for proving the indistinguishability of two random systems, based on the concept of the equivalence of two systems, conditioned on certain events. This abstraction demonstrates the common denominator among many security proofs in the literature, allows to unify, simplify, generalize, and in some cases strengthen them, and opens the door to proving new indistinguishability results. We also propose the previously implicit concept of quasi-randomness and give an efficient construction of a quasi-random function which can be used as a building block in cryptographic systems based on pseudo-random functions.", "authors": ["Ueli Maurer"], "n_citation": 0, "title": "Indistinguishability of random systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "46f5641c-2997-43a0-90e5-c0cbec090e88"}
{"abstract": "The computational grid built on wide-area distributed computing systems is a more variable and unreliable computing environment, hence it is undoubtedly necessary to predict the reliability of the resources before allocating them to a grid application in order to ensure certain availability for the application. In this paper, we present an algorithm for predicting the reliability of resources based on time series of performance data and status data generated by the Grid Monitoring Service. The predicted reliability information can augment Grid Information Services. Simulation results show that with the reliability information, the grid resource allocation service can guarantee the high availability of the applications in the computational grid.", "authors": ["Chunjiang Li", "Nong Xiao", "Xuejun Yang"], "n_citation": 0, "title": "Predicting the reliability of resources in computational Grid", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4c6e601f-2055-4cf5-875a-457970ec3238"}
{"abstract": "From operating systems and web browsers to spacecraft, many software systems maintain a log of events that provides a partial history of execution, supporting post-mortem (or post-reboot) analysis. Unfortunately, bandwidth, storage limitations, and privacy concerns limit the information content of logs, making it difficult to fully reconstruct execution from these traces. This paper presents a technique for modifying a program such that it can produce exactly those executions consistent with a given (partial) trace of events, enabling efficient analysis of the reduced program. Our method requires no additional history variables to track log events, and it can slice away code that does not execute in a given trace. We describe initial experiences with implementing our ideas by extending the CBMC bounded model checker for C programs. Applying our technique to a small, 400-line file system written in C, we get more than three orders of magnitude improvement in running time over a naive approach based on adding history variables, along with fifty- to eighty-fold reductions in the sizes of the SAT problems solved.", "authors": ["Alex Groce", "Rajeev Joshi"], "n_citation": 50, "title": "Exploiting traces in program analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4d2b30ed-98d1-4fff-89b8-489d6519eec2"}
{"authors": ["Florian Mendel", "Christian Rechberger", "Martin Schl\u00e4ffer", "S\u00f8ren S. Thomsen"], "n_citation": 0, "title": "Rebound Attacks on the Reduced Gr\u00f8stl Hash Function", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "520467ae-a04b-4558-8a09-5bbd5f111539"}
{"abstract": "The tool TORPA (Termination of Rewriting Proved Automatically) can be used to prove termination of string rewriting systems (SRSs) fully automatically. The underlying techniques include semantic labelling, polynomial interpretations, recursive path order, the dependency pair method and match bounds of right hand sides of forward closures.", "authors": ["Hans Zantema"], "n_citation": 0, "title": "TORPA: Termination of Rewriting Proved automatically", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5d7b83de-e0b0-4775-ac0c-1569d522571e"}
{"abstract": "We study random instances of a general graph partitioning problem: the vertex set of the random input graph G consists of k classes V 1 ,...,V k , and V i -V j -edges are present with probabilities Pij independently. The main result is that with high probability a partition S 1 ,..., S k  of G that coincides with V 1 ,..., V k  on a huge subgraph core(G) can be computed in polynomial time via spectral techniques. The result covers the case of sparse graphs (average degree 0(1)) as well as the massive case (average degree #V(G) - 0(1)). Furthermore, the spectral algorithm is adaptive in the sense that it does not require any information about the desired partition beyond the number k of classes.", "authors": ["Amin Coja-Oghlan"], "n_citation": 0, "title": "An Adaptive Spectral Heuristic for Partitioning Random Graphs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5d9c47e9-0b87-476f-9239-02fc0d5a23c3"}
{"abstract": "Sequential pattern mining is to find out all the frequent subsequences in a sequence database. In order to have more accurate results, constraints in addition to the support threshold need to be specified in the mining. Time-constraints cannot be managed by retrieving patterns because the support computation of patterns must validate the time attributes for every data sequence in the mining process. In this paper, we propose a memory time-indexing approach (called METISP) to discover sequential patterns with time constraints including minimum/maximum/exact gaps, sliding window, and duration. METISP scans the database into memory and constructs time-index sets for effective processing. Utilizing the index sets and the pattern-growth strategy, METISP efficiently mines the desired patterns without generating any candidate or sub-database. The comprehensive experiments show that METISP outperforms GSP and DELISP in the discovery of time-constrained sequential patterns, even with low support thresholds and very large databases.", "authors": ["Ming-Yen Lin", "Sue-Chen Hsueh", "Chia-Wen Chang"], "n_citation": 0, "title": "Fast Discovery of Time-Constrained Sequential Patterns Using Time-Indexes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5dd1fa2d-5151-4a24-9816-fc2b7d5883f3"}
{"abstract": "The research of transaction processing in Web Services and Grid services is very active in academic and engineering areas now. However, the formal method about transactions is not fully investigated in the literature. We propose a preliminary theoretical model called Membrane Calculus based on Membrane Computing and Petri Nets to formalize Grid transactions. We introduce five kinds of transition rules in Membrane Calculus (including object rules and membrane rules) and the operational semantics of transition rules is defined. Finally, a typical long-running transaction example is presented to demonstrate the usage of Membrane Calculus.", "authors": ["Zhengwei Qi", "Cheng Fu", "Dongyu Shi", "Jinyuan You", "Li Ming-lu"], "n_citation": 0, "title": "Membrane calculus: A formal method for Grid transactions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5faccb5c-07e1-4478-9a52-7a01e346c6d8"}
{"abstract": "We show new lower bounds for collision-free transmissions in Radio Networks. Our main result is a tight lower bound of \u03a9(log n log(1/\u2208)) on the time required by a uniform randomized protocol to achieve a clear transmission with success probability 1 - \u2208 in a one-hop setting. This result is extended to non-uniform protocols as well. A new lower bound is proved for the important multi-hop setting of nodes distributed as a connected Random Geometric Graph. Our main result is tight for a variety of problems.", "authors": ["Martin Farach-Colton", "Rohan J. Fernandes", "Miguel A. Mosteiro"], "n_citation": 50, "title": "Lower bounds for clear transmissions in radio networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6c03f3b1-2987-4e9e-81dd-b73b3b905a4a"}
{"abstract": "Maximal frequent itemsets mining is a fundamental and important problem in many data mining applications. Since the MaxMiner algorithm introduced the enumeration trees for MFI mining in 1998, there have been several methods proposed to use depth-first search to improve performance. This paper presents FIMfi, a new depth-first algorithm based on FP-tree and MFI-tree for mining MFI. FIMfi adopts a novel item ordering policy for efficient lookaheads pruning, and a simple method for fast superset checking. It uses a variety of old and new pruning techniques to prune the search space. Experimental comparison with previous work reveals that FIMfi reduces the number of FP-trees created greatly and is more than 40% superior to the similar algorithms on average.", "authors": ["Yuejin Yan", "Zhoujun Li", "Huowang Chen"], "n_citation": 0, "title": "Fast mining maximal frequent ItemSets based on FP-tree", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6d01d104-1811-4475-92c4-b5391cbf258d"}
{"abstract": "Probabilistic inductive logic programming, sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed. In the present paper, we start from inductive logic programming and sketch how it can be extended with probabilistic methods. More precisely, we outline three classical settings for inductive logic programming, namely learning from entailment, learning from interpretations, and learning from proofs or traces, and show how they can be used to learn different types of probabilistic representations.", "authors": ["Luc De Raedt", "Kristian Kersting"], "n_citation": 301, "title": "Probabilistic inductive logic programming", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6ecbb682-9d4c-4910-b96f-7e4e19b41c7e"}
{"abstract": "This paper presents a competitive Particle Swarm Optimization algorithm for the Traveling Salesman Problem, where the velocity operator is based upon local search and path-relinking procedures. The paper proposes two versions of the algorithm, each of them utilizing a distinct local search method. The proposed heuristics are compared with other Particle Swarm Optimization algorithms presented previously for the same problem. The results are also compared with three effective algorithms for the TSP. A computational experiment with benchmark instances is reported. The results show that the method proposed in this paper finds high quality solutions and is comparable with the effective approaches presented for the TSP.", "authors": ["Elizabeth Ferreira Gouvea Goldbarg", "Givanaldo R. de Souza", "Marco C\u00e9sar Goldbarg"], "n_citation": 11, "title": "Particle swarm for the traveling salesman problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "75e685ae-f971-4f96-945d-ba07c600cf9f"}
{"abstract": "Metric Temporal Logic (MTL) is a widely-studied real-time extension of Linear Temporal Logic. In this paper we consider a fragment of MTL, called Safety MTL, capable of expressing properties such as invariance and time-bounded response. Our main result is that the satisfiability problem for Safety MTL is decidable. This is the first positive decidability result for MTL over timed \u03c9-words that does not involve restricting the precision of the timing constraints, or the granularity of the semantics; the proof heavily uses the techniques of infinite-state verification. Combining this result with some of our previous work, we conclude that Safety MTL is fully decidable in that its satisfiability, model checking, and refinement problems are all decidable.", "authors": ["Jo\u00ebl Ouaknine", "James Worrell"], "n_citation": 50, "title": "Safety metric temporal logic is fully decidable", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "76d345db-f9cb-4e8f-9eac-0c255ef06e72"}
{"abstract": "We provide precise high-level models for eight fundamental service interaction patterns, together with schemes for their composition into complex service-based business process interconnections and interaction flows, supporting software-engineered business process management in multi-party collaborative environments. The mathematical nature of our models provides a basis for a rigorous execution-platform-independent analysis, in particular for benchmarking web services functionality. The models can also serve as accurate standard specifications, subject to further design leading by stepwise refinement to implementations.", "authors": ["Alistair P. Barros", "Egon B\u00f6rger"], "n_citation": 52, "title": "A compositional framework for service interaction patterns and interaction flows", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7c2470e5-f940-471d-9000-1b963dae626d"}
{"abstract": "In a recent experiment, Foster and Wilson [1] have observed reverse replay of behavioral sequences in rodents' hippocampal place cells during non-running awake state in coincidence with sharp waves. In this paper, to elucidate this reverse replay mechanism, a theta phase precession computational model is assumed in one time trial learning experiment of a behavioral sequence. Our simulations demonstrate that reverse replay can occur during sharp waves states under the assumption that place cells' excitability is elevated by reward. This reward induced reverse replay in the hippocampus might serve as a basis for reinforcement learning. \u00a9 Springer-Verlag Berlin Heidelberg 2006.", "authors": ["Colin Molter", "Naoyuki Sato", "Utku Salihoglu", "Yoko Yamaguchi"], "n_citation": 0, "title": "How reward can induce reverse replay of behavioral sequences in the hippocampus", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8c607006-be47-47e7-a589-0a71b0c13fb8"}
{"authors": ["V. de Boer", "P. De Leenheer", "Anna Bon", "Nana Baah Gyan", "Christophe Gu\u00e9ret", "J. Ralyt", "Xavier Franch"], "n_citation": 0, "title": "Radiomarch\u00e9: Distributed voice- and web-interfaced market information systems under rural conditions", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "8d438822-8c6f-407f-b331-a83ba5277d80"}
{"abstract": "We motivate and study the robustness of fairness notions under refinement of transitions and places in Petri nets. We show that the classical notions of weak and strong fairness are not robust and we propose a hierarchy of increasingly strong, refinement-robust fairness notions. That hierarchy is based on the conflict structure of transitions, which characterizes the interplay between choice and synchronization in a fairness notion. Our fairness notions are defined on non-sequential runs, but we show that the most important notions can be easily expressed on sequential runs as well. The hierarchy is further motivated by a brief discussion on the computational power of the fairness notions.", "authors": ["Hagen V\u00f6lzer"], "n_citation": 0, "title": "Refinement-robust fairness", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8d747b14-6308-441c-99a5-42743026af20"}
{"abstract": "The development status about digitalized protection of cultural relic and virtual museum construction has been introduced. The key technology of digitalized protection for cultural relic and virtual museum construction has been expatiated. The application of stereo photography and virtual reality technology in digitalized protection of cultural relic and virtual museum construction has been shown. The principle of stereo display has been explained in detail. The key technical problem of virtual reality used in digitalized protection of cultural relic has been expounded, including several important aspects such as 3D scanning and modeling for cultural relic, texture mapping of virtual cultural relic, technology of human-machine interaction to the virtual museum scene, etc. The problem about the mode of management, maintenance and working on virtual museum has been also introduced in this paper.", "authors": ["Zili Li", "Hui Li", "Yuanchu Li"], "n_citation": 0, "title": "The technology of stereo photography and virtual reality in research of virtual museum", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "96c271b4-5e29-4283-84f6-6a4f8c28ff78"}
{"abstract": "In this paper, we address the problem of estimating the motion of fluid flows that are visualized through a Schlieren system. Such a system is well known in fluid mechanics as it enables the visualization of unseeded flows. As the resulting images exhibit very low photometric contrasts, classical motion estimation methods based on the brightness consistency assumption (correlation-based approaches, optical flow methods) are completely inefficient. This work aims at proposing a sound energy based estimator dedicated to these particular images. The energy function to be minimized is composed of (a) a novel data term describing the fact that the observed luminance is linked to the gradient of the fluid density and (b) a specific div curl regularization term. The relevance of our estimator is demonstrated on real-world sequences.", "authors": ["Elise Arnaud", "Etienne M\u00e9min", "Roberto Sosa", "Guillermo Artana"], "n_citation": 0, "title": "A Fluid Motion Estimator for Schlieren Image Velocimetry", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "976034f5-29a4-46cb-8426-d83473ad3d00"}
{"abstract": "Data mining has become of great importance owing to ever-increasing amounts of data collected by large organizations. This paper propose an data mining algorithm called Ant-Miner(I),which is based on an improvement of Ant Colony System(ACS) algorithm. Experimental results show that Ant-Miner(I) has a higher predictive accuracy and much smaller rule list than the original Ant-Miner algorithm.", "authors": ["Weijin Jiang", "Yusheng Xu", "Yu-Hui Xu"], "n_citation": 0, "title": "A novel data mining method based on Ant colony algorithm", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9ba911e7-6b53-4113-82fd-1357d5ff8cbf"}
{"abstract": "A narrow-band-based adaptive level set method is proposed for mesh evolution in this paper. In the preprocessing step, the portions of the mesh with high curvature are detected and clustered as fine geometric details. Then the fine grids with optimal orientations are defined for each detected portions, while the coarse grid is chosen for global evolution. The solutions on different scale grids are synthesized as the final results. Compared with the level set solution on the uniform grid, our approach can achieve a good compromise between computational costs and mesh evolution quality. The implementation results also show that it is an efficient and robust method.", "authors": ["Guoxian Zheng", "Jieqing Feng", "Xiaogang Jin", "Qunsheng Peng"], "n_citation": 0, "title": "Adaptive level set method for mesh evolution", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9e10fd5a-bbe4-409d-bd5e-1d15429b341b"}
{"abstract": "In this paper we define Bohm-like trees for term rewriting systems (TRSs). The definition is based on the similarities between the Bohm trees, the Levy-Longo trees, and the Berarducci trees. That is, the similarities between the Bohm-like trees of the A-calculus. Given a term t a tree partially represents the root-stable part of t as created in each maximal fair reduction of t. In addition to defining Bohm-like trees for TRSs we define a subclass of Bohm-like trees whose members are monotone and continuous.", "authors": ["Jeroen Ketema"], "n_citation": 0, "title": "B\u00f6hm-like trees for term rewriting systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a7b16624-0e1b-4f91-8dfc-b374f950f17a"}
{"abstract": "We derive algorithms for approximating a set S of n points in the plane by an x-monotone rectilinear polyline with k horizontal segments. The quality of the approximation is measured by the maximum distance from a point in S to the segment above or below it. We consider two types of problems: min-\u2208, where the goal is to minimize the error for k horizontal segments and min-#, where the goal is to minimize the number of segments for error \u2208. After 0(n) preprocessing time, we solve the latter in O(min{k log n,n}) time per instance. We then solve the former in O(min {n 2 , nk log n}) time. We also describe an approximation algorithm for the min -e problem that computes a solution within a factor of 3 of the optimal error for k segments, or with at most the same error as the k-optimal but using 2k - 1 segments. Both approximations run in O(nlogn) time.", "authors": ["Yan Mayster", "Mario A. Lopez"], "n_citation": 0, "title": "Rectilinear approximation of a set of points in the plane", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ae2270db-6fb4-4be5-b524-680d838b6938"}
{"abstract": "Real-time behavior of a multi-tasking program running on a pre-emptive priority-based operating system is analyzed. The operating system and a collection of application tasks are modelled in Z. Real-time is represented by an ordinary Z state variable. The model is adapted to a particular application by defining a state machine for each task and associating execution times with each state. The model is analyzed by exhaustive simulation with the SMV model checker. The state transitions described by Z operation schemas are implemented in the SMV programming language. Invariants, preconditions, and postconditions from the Z are translated to formulas in CTL, the SMV specification language. The SMV program is verified by checking these formulas. This detects coding errors in the SMV program and also reveals inconsistencies in the original Z where operation schemas are inconsistent with state invariants. The errors were corrected. Additional CTL formulas describe temporal properties that cannot he expressed directly in Z. The Z model is validated by checking an example SMV program with CTL formulas that confirm scheduling results from rate-monotonic analysis (RMA). Another application that does not satisfy the assumptions of RMA is analyzed, establishing that high-priority tasks cannot indefinitely delay low-priority tasks and real-time deadlines can be met.", "authors": ["Jonathan Jacky"], "n_citation": 0, "title": "Analyzing a real-time program with Z", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "af014125-ba98-4305-b112-8c0f00dd6e09"}
{"abstract": "Teachable characters can enhance entertainment technology by providing new interactions, becoming more competent at game play, and simply being fun to teach. It is important to understand how human players try to teach virtual agents in order to design agents that learn effectively from this instruction. We present results of a user study where people teach a virtual agent a novel task within a reinforcement-based learning framework. Analysis yields lessons of how human players approach the task of teaching a virtual agent: 1) they want to direct the agent's attention; 2) they communicate both instrumental and motivational intentions; 3) they tailor their instruction to their understanding of the agent; and 4) they use negative communication as both feedback and as a suggestion for the next action. Based on these findings we modify the agent's learning algorithm and show improvements to the learning interaction in follow-up studies. This work informs the design of real-time learning agents that better match human teaching behavior to leam more effectively and be more enjoyable to teach.", "authors": ["Andrea Lockerd Thomaz", "Cynthia Breazeal"], "n_citation": 0, "title": "Teachable characters : User studies, design principles, and learning performance", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b1ed95e2-9c95-493c-a1a3-2e719dedda7b"}
{"authors": ["Colette Rolland"], "n_citation": 50, "title": "From Conceptual Modeling to Requirements Engineering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b302350a-77ac-4978-b605-ef4abf9bc28f"}
{"abstract": "There is a trend in the automation domain to create common conceptual models of an industrial plant, which serve different applications, like online displays, simulators, control systems, diagnostic tools and others. To master the complexity of such common models, modeling rules have to be enforced. Current solutions to this problem either rely on an informal definition of semantics or the semantics of one dedicated tool or language. In the current paper, we introduce a meta-modeling approach to provide and enforce such modeling rules. Our approach concisely defines consistency of conceptual models with respect to a meta model. We show how the approach is mapped to widely accepted general-purpose IT standards (like UML and XML) and tools.", "authors": ["Peter Fr\u00f6hlich", "Zaijun Hu", "Manfred Schoelzke"], "n_citation": 0, "title": "Imposing modeling rules on industrial applications through meta-modeling", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b34f05c2-0813-4a9c-898d-f4e69db83ae4"}
{"abstract": "In this paper, an Artificial Immune System (AIS) for the multi-mode  resource-constrained project scheduling problem (MRCPSP), in which  multiple execution modes are available for each of the activities of the  project, is presented. The AIS algorithm makes use of mechanisms which are inspired on the vertebrate immune system performed on an initial  population set. This population set is generated with a controlled  search method, based on experimental results which revealed a link  between predefined profit values of a mode assignment and its makespan. #R##N#The impact of the algorithmic parameters and the initial population generation method is observed and detailed comparative computational results for the MRCPSP are presented.\"", "authors": ["Vincent Van Peteghem", "Mario Vanhoucke"], "n_citation": 50, "title": "An artificial immune system for the multi-mode resource-constrained project scheduling problem", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "b3593262-a39b-4b79-908e-ee2451cb624f"}
{"abstract": "We present Semantic Property Grammars, designed to extract concepts and relations from biomedical texts. The implementation adapts a CHRG parser we designed for Property Grammars [1], which views linguistic constraints as properties between sets of categories and solves them by constraint satisfaction, can handle incomplete or erroneous text, and extract phrases of interest selectively. We endow it with concept and relation extraction abilities as well.", "authors": ["Veronica Dahl", "Baohua Gu"], "n_citation": 50, "title": "Semantic property grammars for knowledge extraction from biomedical text", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b676192c-fba5-4625-b9e8-5f3ae973efe0"}
{"abstract": "We work with fuzzy Turing machines (FTMs) and we study the relationship between this computational model and classical recursion concepts such as computable functions, r.e. sets and universality. FTMS are first regarded as acceptors. It has recently been shown in [23] that these machines have more computational power than classical Turing machines. Still, the context in which this formulation is valid has an unnatural implicit assumption. We settle necessary and sufficient conditions for a language to be r.e., by embedding it in a fuzzy language recognized by a FTM and we do the same thing for difference r.e. sets, a class of harder sets in terms of computability. It is also shown that there is no universal FTM. We also argue for a definition of computable fuzzy function, when FTMS are understood as transducers. It is shown that, in this case, our notion of computable fuzzy function coincides with the classical one.", "authors": ["Benjam\u00edn R. C. Bedregal", "Santiago Figueira"], "n_citation": 50, "title": "Classical computability and fuzzy turing machines", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b77219c8-8f80-4915-817f-839a9806d18f"}
{"abstract": "We introduce a novel representation for associative-commutative (AC) terms which, for certain important classes of rewrite rules, allows both the AC matching and the AC renormalization steps to be accomplished using time and space that is logarithmic in the size of the flattened AC argument lists involved. This novel representation can be cumbersome for other, more general algorithms and manipulations. Hence, we describe machine efficient techniques for converting to and from a more conventional representation together with a heuristic for deciding at runtime when to convert a term to the new representation. We sketch how our approach can be generalized to order-sorted AC rewriting and to other equational theories. We also present some experimental results using the Maude 2 interpreter.", "authors": ["Steven Eker"], "n_citation": 0, "title": "Associative-commutative rewriting on large terms", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b8a18259-ba76-40ca-b106-9e5abd7da57f"}
{"authors": ["Majid Pooyandeh", "Mohammad Saadi Mesgari", "Abbas Alimohammadi", "Rouzbeh Shad"], "n_citation": 0, "title": "A Comparison Between Complexity and Temporal GIS Models for Spatio-temporal Urban Applications", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "c1069366-21d7-47a1-8f18-bff85e2e98c6"}
{"abstract": "Among the largest impact research themes at the time of world-wide recession, the key subject is how to cope with mega company formations and e-government (digital government) projects that depend on the successes of information system integration. The current information system integration approaches such as wrapper approaches basically create combinatorial interfacing and/or combinatorial data conversion making the integration practically impossible because of interfacing explosion and/or computational explosion. A linear approach to overcome the combinatorial explosion is presented and discussed.", "authors": ["Tosiyasu L. Kunii"], "n_citation": 0, "title": "What's wrong with wrapper approaches in modeling information system integration and interoperability?", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c921f384-1fb6-4158-9fe5-8f06234fc237"}
{"abstract": "Netcharts have been introduced recently by Mukund et al. in [17]. This new appealing approach to the specification of collections of message sequence charts (MSCs) benefits from a graphical description, a formal semantics based on Petri nets, and an appropriate expressive power. As opposed to high-level MSCs, any regular MSC language is the language of some netchart. Motivated by two open problems raised in [17], we establish in this paper that the questions (i) whether a given high-level MSC describes some netchart language (ii) whether a given netchart is equivalent to some high-level MSC (iii) whether a given netchart describes a regular MSC language are undecidable. These facts are closely related to our first positive result: We prove that netchart languages are exactly the MSC languages that are implementable by message passing automata up to refinement of message contents. Next we focus on FIFO netcharts: The latter are defined as the netcharts whose executions correspond to all firing sequences of their low-level Petri net. We show that the questions (i) whether a netchart is a FIFO netchart (ii) whether a FIFO netchart describes a regular MSC language (iii) whether a regular netchart is equivalent to some high-level MSC are decidable.", "authors": ["Nicolas Baudru", "R\u00e9mi Morin"], "n_citation": 0, "title": "The pros and cons of netcharts", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ca5d1ded-408f-423a-b828-42b02516a3c3"}
{"abstract": "We present the FocusCheck model-checking tool for the verification and easy debugging of assertion violations in sequential C programs. The main functionalities of the tool are the ability to: (a) identify all minimum-recursion, loop-free counter-examples in a C program using on-the-fly abstraction techniques; (b) extract focus-statement sequences (FSSs) from counter-examples, where a focus statement is one whose execution directly or indirectly causes the violation underlying a counter-example; (c) detect and discard infeasible counter-examples via feasibility analysis of the corresponding FSSs; and (d) isolate program segments that are most likely to harbor the erroneous statements causing the counter-examples. FocusCheck is equipped with a smart graphical user interface that provides various views of counter-examples in terms of their FSSs, thereby enhancing usability and readability of model-checking results.", "authors": ["Curtis W. Keller", "Diptikalyan Saha", "Samik Basu", "Scott A. Smolka"], "n_citation": 0, "title": "Focuscheck : A tool for model checking and debugging sequential C programs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cea5cfea-83a1-4ebd-8135-5d6c5df42b4a"}
{"abstract": "Ad hoc wireless networks can be deployed quickly anywhere and, unlike mobile switching, do not rely on a fixed infrastructure, such as base stations. Hierarchical techniques have long been known to afford scalability in networks. In this paper, we describe a cluster-routing protocol based on a multi-layer scheme in ad hoc networks. This paper assesses the scalability, with respect to increasing node count, of hierarchical routing in mobile ad hoc networks. This work provides scalable in ad hoc networks. We present detailed cluster head selection algorithm of ad hoc routing protocols. Our proposed protocol, called 'Control cluster head Algorithm based on Hierarchical Cluster' (CAHC) for ad hoc networks. The CAHC strategy takes advantage of a hierarchical architecture that is designed for scalable cluster head (CH) using new concept of control cluster head (CCH) scheme.", "authors": ["Keun-Ho Lee", "Chong-Sun Hwang"], "n_citation": 0, "title": "Hierarchical Cluster Configuration Scheme for Scalable Ad Hoc Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d40c7ce9-ee67-4fa0-9a32-18a0c00a080c"}
{"abstract": "In this paper we describe our experiences in specifying and verifying a complex cryptographic protocol actually used in industry that has been developed for the area of chipcard based biometric identification systems. The main emphasis was placed on authenticity, integrity and confidentiality properties. The formal analysis even led to several simplifying modifications of the protocol that facilitate the implementation, yet maintaining the protocol security properties we considered. The formal analysis is based on an inductive approach performed with the help of VSE (Verification Support Environment). The heuristic based proof automation techniques realized in VSE result in an average grade of automation of 80 percent. Thus, VSE provides substantial support for the specification and verification of cryptographic protocols.", "authors": ["Lassaad Cheikhrouhou", "Georg Rock", "Werner Stephan", "Matthias Schwan", "Gunter Lassmann"], "n_citation": 50, "title": "Verifying a Chipcard-Based Biometric Identification Protocol in VSE", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "da90775e-e1c3-4571-95e3-bb39a5ebf246"}
{"abstract": "Composed of many classes or modules, big software can be represented with network model. By extracting the topology of UML metamodel from the UML metamodel specification, the scale-free, small-world networks properties are revealed. Based on this observation, we come up with our algorithms that can classify all classes in UML metamodel into three kinds: core, general and leaf. Our algorithm can categorize all classes into several subgroups by three factors, i.e., degree, betweenness and weak link. It is illustrated through case study that this algorithm is effective at mining community structure in large software systems.", "authors": ["Bin Liu", "Deyi Li", "Jin Liu", "Fei He"], "n_citation": 0, "title": "Classifying class and finding community in UML metamodel network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "dabe1af6-a332-4d2b-8ecf-74e2a218e58c"}
{"abstract": "Ontology learning today ranges from simple frequency counting methods to advanced linguistic analyses of sentence structure and word semantics. For ontologies in information retrieval systems, class concepts and hierarchical relationships at the appropriate level of detail are crucial to the quality of retrieval. In this paper, we present an unsupervised keyphrase extraction system and evaluate its ability to support the construction of ontologies for search applications. In spite of its limitations, such a system is well suited to constantly changing domains and captures some interesting domain features that are important in search ontologies. The approach is evaluated against the project management documentation of a Norwegian petroleum company.", "authors": ["Jon Atle Gulla", "Hans Olaf Borch", "Jon Espen Ingvaldsen"], "n_citation": 0, "title": "Unsupervised keyphrase extraction for search ontologies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "df71fc6f-e482-41cb-9b7a-7ec426dac64b"}
{"abstract": "We consider the problem of drawing a directed graph in two dimensions with a minimum number of crossings such that for every node the incoming edges appear consecutively in the cyclic adjacency lists. We show how to adapt the planarization method and the recently devised exact crossing minimization approach in a simple way. We report experimental results on the increase in the number of crossings involved by this additional restriction on the set of feasible drawings. It turns out that this increase is negligible for most practical instances.", "authors": ["Christoph Buchheim", "Michael J\u00fcnger", "Annette Menze", "Merijam Percan"], "n_citation": 0, "title": "Bimodal Crossing Minimization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e14f91d9-087c-4085-b6e0-5152e3f4e836"}
{"abstract": "The Dolev-Yao model of security protocol analysis may beformalized using a notation based on multi-set rewriting with existential quantification. This presentation describes the multiset rewriting approach to security protocol analysis, algorithmic upper and lower bounds on specific forms of protocol analysis, and some of the ways this model is useful for formalizing sublte properties of specific protocols.", "authors": ["John C. Mitchell"], "n_citation": 0, "title": "Multiset rewriting and security protocol analysis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e4ba5684-43b3-4251-860f-d7e54ba3091c"}
{"abstract": "Model learning emerges as an effective method for black-box state machine models of hardware and software components.", "authors": ["Frits W. Vaandrager"], "n_citation": 50, "references": ["00cd344e-924d-4ba4-abb4-a74169684886", "047fc525-3392-4484-8fd9-c3f8d17b3a93", "050f3d56-45c0-4c32-86fb-db2fe9b5fb88", "117e1a35-8d13-4304-995b-22349986ec89", "1375d034-cffe-4de0-a082-a9ec42f76b64", "1e0d6931-3210-4a3e-999c-dff8cc75e9f6", "1e20a3c8-0d0f-42a1-bdbf-f677aeeba128", "1fedc902-b23b-4d45-8bb7-c3411f5f58a7", "237dc368-0f3e-46d4-bba0-bee57c3a2808", "2b98d66f-c979-489a-b74b-b67632f31d4e", "32518a4c-fbc3-470f-8024-cb7a65f1fe3f", "37d51478-1bd4-4b0e-ac5d-9f33c0e5497e", "4070f260-b4a5-439e-bc9c-893d50b13448", "4a4df749-e5ce-4e81-be80-fc1cd04c5c4b", "4e09f13f-541e-47f5-8e3d-0c0a0e2bb764", "4e85397c-5e73-460c-9610-f03c8c24d2da", "505f493b-e09d-444d-9ee2-5e5db6a5b8ac", "5451565e-d98b-4a84-b521-6fa30167e594", "5b7dbc85-bff1-46a3-bc1a-fa5db8d57fcb", "62b0f667-369d-46e3-9483-ce5ebdda6f6e", "65e45c4e-2407-4810-a0f2-4a8041ad824a", "66eb940c-eb9e-416b-990c-6eb6436d2d14", "6967e8df-bab4-4bdb-9ce9-bea9b64fdb67", "6bd3f7b7-0c2a-4dca-811b-3b2b0765cf94", "6df92a4c-0939-4cf7-895b-cca53196712f", "6f1624eb-ad5d-4d16-8f66-65b81f654f73", "6f6e1cf1-e3b2-406e-bcc5-736f5636eff4", "6f8ef0dc-3846-4f77-b210-5b4401730de5", "7e645996-0aa3-4316-b506-03e013d0989f", "80082195-b0da-475a-872d-6a7fb2769c99", "80d5fae0-9cb4-4dc3-a544-d3a4c8309052", "825d71a7-3f21-4ef8-aa4c-16109812fb4c", "86b846d4-9e69-4d71-8a04-86327287a15a", "88ce4013-25cd-45c2-ab63-bda1f3d8044c", "91877231-41b1-4ef3-bcb4-bd36067aaa85", "94ef215f-19cc-479b-bf9d-8714d29d7f99", "95849374-de27-4b31-9ff4-7bc682ea5de1", "9f548f77-6336-487b-9e9d-d51234277dd6", "a13f319b-b4f1-4596-b8d4-51713dc0653a", "a246e432-612a-4481-95b5-29ba3db6369b", "a7951eb9-5979-4e2c-99e6-ffcf99015b1c", "ad8fbf8a-53ea-48c5-9ec6-17f4b0266edb", "b1d86f27-64e9-4866-a68a-a2cf49829f38", "b3acc122-2f0a-4859-b1e2-9884ee305e07", "b90915cb-bd75-4fe1-a50e-75d7c1e9d355", "ba9080fa-ec5a-4848-8526-710c764c09ab", "bedffcd4-e9a7-4a88-af10-81e012c61063", "bf30e94e-d8b8-4441-b651-a4527f8f41b9", "c00bbb49-6e29-4103-8883-55acd23c248b", "c17bbc16-30dd-4a3e-bbed-74f1cc91da98", "ca275245-0b33-4329-8838-bff0b05b56b5", "cf66ddc1-4596-483e-93ae-fcfad3d8934a", "cfdab149-75c3-4aec-b87e-c72dc72e48f2", "cfea40f7-7c61-497c-8be7-284aa479552d", "d298d5df-d8fc-46eb-b60d-7db5d043b56f", "d9e52882-c0f6-4c6c-b5b0-68dde6a2507c", "deaf0b7a-940b-42fe-be59-f7209f68a05b", "df0f3411-a928-4709-b7ab-a4f62ac0aaff", "dff16c1f-e3ce-4302-a591-64cccf6167be", "e1cae81c-9a2e-4d40-a9f0-2d24b9fb0bc9", "e3e14fa9-63b1-49df-ad25-460759dc4db0", "e837cbb4-c3f9-4046-86fd-b7d5593f4406", "e8c3e5b6-290a-49d3-a772-671a7ff3d1ad", "e90ca26c-db59-470f-96b3-154b036c461a", "ea07125c-1936-4509-b450-611cac3394b6", "eb89e176-75d3-4cf7-95a3-fe82826b37f9", "f4a696c2-5a47-4844-8758-d6845b1ba7d9", "f71f2be5-9caa-47c8-a085-672143d01436", "fbbe3725-f139-483c-80d3-bd015b133551", "fbe8059e-e302-4776-b722-2aa2525d4448"], "title": "Model learning", "venue": "Communications of The ACM", "year": 2017, "id": "e7d9127a-4593-4b8f-a650-7c4d42078d28"}
{"abstract": "Government-sanctioned and market-based anti-piracy measures can both mitigate economic harm from piracy.", "authors": ["Brett Danaher", "Michael D. Smith", "Rahul Telang"], "n_citation": 0, "references": ["2ee490aa-d655-4738-961c-3d722913fe92", "cf9f2dca-f48a-4417-b723-c4b589efa9d0"], "title": "Copyright enforcement in the digital age: empirical evidence and policy implications", "venue": "Communications of The ACM", "year": 2017, "id": "e8312666-457c-4897-a0f3-5a929d0e66d8"}
{"abstract": "This paper presents an approach to increasing the adaptability and flexibility of service-oriented applications. The focuses are on the modeling of user context information and its uses for enabling personalized service selection, composition and activation. The resulting models and mechanisms are an essential part of the novel language VINCA, which is targeted at facilitating service composition on business level in an intuitive and just-in-time manner.", "authors": ["Hao Liu", "Yanbo Han", "Gang Li", "Cheng Zhang"], "n_citation": 0, "title": "Achieving context sensitivity of service-oriented applications with the business-end programming language VINCA", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ea6d126f-6c16-4f3a-8277-a7a76fb5e62b"}
{"abstract": "Knowledge about multi-dimensional frequent patterns is interesting and useful. The classic frequent pattern mining algorithms based on a uniform minimum support, such as Apriori and FP-growth, either miss interesting patterns of low support or suffer from the bottleneck of itemset generation. Other frequent pattern mining algorithms, such as Adaptive Apriori, though taking various supports, focus mining at a single abstraction level. Furthermore, as an Apriori-based algorithm, the efficiency of Adaptive Apriori suffers from the multiple database scans. In this paper, we extend FP-growth to attack the problem of multidimensional frequent pattern mining. The algorithm Ada-FP, which stands for Adaptive FP-growth. The efficiency of the Ada-FP is guaranteed by the high scalability of FP-growth. To increase the effectiveness, the Ada-FP pushes various support constraints into the mining process. We show that the Ada-FP is more flexible at capturing desired knowledge than previous Algorithm.", "authors": ["S. Vijayalakshmi", "S. Suresh Raja"], "n_citation": 0, "title": "Multidimensional frequent pattern mining using association rule based constraints", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "eb519e2e-40f2-43d4-b2e8-12047ba0a86c"}
{"abstract": "We investigate further improvement of boosting in the case that the target concept belongs to the class of r-of-k threshold Boolean functions, which answer +1 if at least r of k relevant variables are positive, and answer -1 otherwise. Given m examples of a r-of-k function and literals as base hypotheses, popular boosting algorithms (e.g., AdaBoost) construct a consistent final hypothesis by using O(k 2  log m) base hypotheses. While this convergence speed is tight in general, we show that a modification of AdaBoost (confidence-rated AdaBoost [SS99] or InfoBoost [As100]) can make use of the property of r-of-k functions that make less error on one-side to find a consistent final hypothesis by using O(kr log m) hypotheses. Our result extends the previous investigation by Hatano and Warmuth [HW04] and gives more general examples where confidence-rated AdaBoost or InfoBoost has an advantage over AdaBoost.", "authors": ["Kohei Hatano", "Osamu Watanabe"], "n_citation": 0, "title": "Learning r-of-k functions by Boosting", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ee4b4393-31b2-4c6b-87f6-39e3eb7b26a4"}
{"abstract": "Given the range space (P,R), where P is a set of n points in R 2  and R is the family of subsets of P induced by all axis-parallel rectangles, the conflict-free coloring problem asks for a coloring of P with the minimum number of colors such that (P, R) is conflict-free. We study the following question: Given P, is it possible to add a small set of points Q such that (P U Q, R) can be colored with fewer colors than (P,R)? Our main result is the following: given P, and any e > 0, one can always add a set Q of O(n 1- \u2208) points such that P U Q can be conflict-free colored using O(n 3/8(1+e) ) 1  colors. Moreover, the set Q and the conflict-free coloring can be computed in polynomial time, with high probability. Our result is obtained by introducing a general probabilistic re-coloring technique, which we call quasi-conflict-free coloring, and which may be of independent interest. A further application of this technique is also given.", "authors": ["Khaled M. Elbassioni", "Nabil H. Mustafa"], "n_citation": 0, "title": "Conflict-free colorings of rectangles ranges", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f03f7c5e-0a49-46be-9ec4-4b3703d258d4"}
{"abstract": "We explore an alternative for metric limits in the context of infinitary lambda calculus with transfinite reduction sequences. We will show how to use the new approach to get calculi that correspond to the 111, 101 and 001 infinitary lambda calculi of Kennaway et al, which have been proved to correspond to Berarducci Trees, Levy-Longo Trees and Bohm Trees respectively. We will identify subsets of the sets of meaningless terms of the metric calculi and prove that the approximation based calculi are equivalent to their metric counterparts up to these subsets.", "authors": ["Stefan Blom"], "n_citation": 0, "title": "An approximation based approach to infinitary lambda calculi", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f1ae1caa-9eb1-4e01-93e3-010bd3a270b0"}
{"abstract": "Constraint automata are a semantic model for Reo modeling language. Testing correctness of mapping black-box components in Reo to constraint automata is an important problem in analyzing the semantic model of Reo. This testing requires a suite of test cases that cover the automaton states and transitions and also examine different paths. In this paper, Genetic Algorithm (GA) is employed to generate such suite of test cases. This test data generation is improved by Genetic Symbiosis Algorithm (GSA). The results show that GSA approach brings us a suite of test cases with full coverage of automata states and transitions and also diversity of examined paths.", "authors": ["Samira Tasharofi", "Sepand Ansari", "Marjan Sirjani"], "n_citation": 0, "title": "Generating Test Cases for Constraint Automata by Genetic Symbiosis Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f3c1a0ed-fd2e-4d6a-bf9e-238d3f6a6e75"}
{"abstract": "Organizations typically have multiple security policies operating together in the same system. The integration of multiple policies might be needed to achieve the desired security requirements. Validating this integrated policy is a non-trivial process. This paper addresses the problem of composing, modeling and validating the security policies. We show how the various approaches for composing security policies can be modeled and verified using Alloy, a lightweight modeling system with automatic semantic analysis capability.", "authors": ["Manachai Toahchoodee", "Indrakshi Ray"], "n_citation": 0, "title": "Validation of policy integration using alloy", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fac08cf2-dbb3-4b53-a7e0-9fe4d400ed83"}
{"abstract": "This paper presents motion simulation/evaluation system for factory workers in the framework of Info-Ergonomics. One of the key technologies is CG simulation based on the precise human body mockup called Bone-Based Human Model. Using BBHM, real motions of workers can be mapped for precise simulation. Another important issue is data and knowledge integration. For the purpose of schematizing such data and providing retrieval functions are discuss in an extended database system, Real World Database.", "authors": ["Hiroshi Arisawa", "Takako Sato", "Takashi Tomii"], "n_citation": 0, "title": "Human-Body Motion Simulation Using Bone-Based Human Model and Construction of Motion Database", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fed860d8-5940-495a-8a47-30247faf6068"}
{"abstract": "Many organizations and industries continuously struggle to achieve a foundation of a sound verification and validation process. Because verification and validation is only marginally addressed in software process improvement models like CMMI, a separate verification and validation improvement model has been developed. The framework of the improvement model presented in this paper is based on the Testing Maturity Model (TMM). However considerable enhancements are provided, such as a V&V assessment procedure and a detailed metrics program to determine effectiveness and efficiency of the improvements. This paper addresses the development approach of MB-V2M2, an outline of the model, early validation experiments, current status and future outlook. The resulting model is dubbed MB-V2M2 (Metrics Based Verification@Validation Maturity Model).", "authors": ["Jef Jacobs", "Jos J. M. Trienekens"], "n_citation": 0, "title": "Towards a metrics based verification and validation maturity Model", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0221c9ba-cd79-41a8-bbf0-13668077de58"}
{"abstract": "A single logical entity can be referred to by several different names over a large text corpus. We present our algorithm for finding all such co-reference sets in a large corpus. Our algorithm involves three steps: morphological similarity detection, contextual similarity analysis, and clustering. Finally, we present experimental results on over large corpus of real news text to analyze the performance our techniques.", "authors": ["Levon Lloyd", "Andrew Mehler", "Steven Skiena"], "n_citation": 50, "title": "Identifying co-referential names across large corpora", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "046845a9-e880-44e4-a708-d4be4daf1c28"}
{"abstract": "Model checking specifications with complex data and behaviour descriptions often fails due to the large state space to be processed. In this paper we propose a technique for reducing such specifications (with respect to certain properties under interest) before verification. The method is an adaption of the slicing technique from program analysis to the area of integrated formal notations and temporal logic properties. It solely operates on the syntactic structure of the specification which is usually significantly smaller than its state space. We show how to build a reduced specification via the construction of a so called program dependence graph, and prove correctness of the technique with respect to a projection relationship between full and reduced specification. The reduction thus preserves all properties formulated in temporal logics which are invariant under stuttering, as for instance LTL_ X .", "authors": ["Ingo Br\u00fcckner", "Heike Wehrheim"], "n_citation": 0, "title": "Slicing an integrated formal method for verification", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "08bf0e8c-4515-49a8-adb9-cedc1f79cb98"}
{"abstract": "We introduce a model for cooperative coevolutionary algorithms (CCEAs) using partial mixing, which allows us to compute the expected long-run convergence of such algorithms when individuals' fitness is based on the maximum payoff of some N evaluations with partners chosen at random from the other population. Using this model, we devise novel visualization mechanisms to attempt to qualitatively explain a difficult-to-conceptualize pathology in CCEAs: the tendency for them to converge to suboptimal Nash equilibria. We further demonstrate visually how increasing the size of N, or biasing the fitness to include an ideal-collaboration factor, both improve the likelihood of optimal convergence, and under which initial population configurations they are not much help.", "authors": ["Liviu Panait", "R. Paul Wiegand", "Sean Luke"], "n_citation": 0, "title": "A visual demonstration of convergence properties of cooperative coevolution", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0b22484a-556b-4cc4-a0e1-ebe7a9ec946b"}
{"authors": ["Bev Littlewood"], "n_citation": 0, "title": "On diversity, and the elusiveness of independence", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0be57a11-9a55-4937-85f4-1df5d6d56bfb"}
{"abstract": "We propose a fast approach to 3 D object detection and pose estimation that owes its robustness to a training phase during which the target object slowly moves with respect to the camera. No additional information is provided to the system, save a very rough initialization in the first frame of the training sequence. It can be used to detect the target object in each video frame independently. Our approach relies on a Randomized Tree-based approach to wide-baseline feature matching. Unlike previous classification-based approaches to 3-D pose estimation, we do not require an a priori 3-D model. Instead, our algorithm learns both geometry and appearance. In the process, it collects, or harvests, a list of features that can be reliably recognized even when large motions and aspect changes cause complex variations of feature appearances. This is made possible by the great flexibility of Randomized Trees, which lets us add and remove feature points to our list as needed with a minimum amount of extra computation.", "authors": ["Mustafa \u00d6zuysal", "Vincent Lepetit", "F. Fleuret", "Pascal Fua"], "n_citation": 112, "title": "Feature Harvesting for Tracking-by-Detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0dc49ec0-2651-41ee-9acf-836070d71a8a"}
{"abstract": "We show the fastest implementation result of RSA on Itanium 2. For realizing the fast implementation, we improved the implementation algorithm of Montgomery multiplication proposed by Itoh et al. By using our implementation algorithm, pilepine delay is decreased than previous one on Itanium 2. And we implemented this algorithm with highly optimized for parallel processing. Our code can execute 4 instructions per cycle (At maximum, 6 instructions are executed per cycle on Itanium 2), and its probability of pipeline stalling is just only 5%. Our RSA implementation using this code performs 32 times per second of 4096-bit RSA decryption with CRT on Itanium 2 at 900MHz. As a result, our implementation of RSA is the fastest on Itanium2. This is 3.1 times faster than IPP, a software library developed by Intel, in the best case.", "authors": ["Kazuyoshi Furukawa", "Masahiko Takenaka", "Kouichi Itoh"], "n_citation": 0, "title": "A fast RSA implementation on itanium 2 processor", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0feaed88-2f31-473a-80a9-4b21b5e2a840"}
{"abstract": "In this paper, we describe how formal specification is adopted to improve the commonly used verification and validation technique known as program inspection, in order to establish a more rigorous, repeatable, and efficient inspection process than the conventional practice. We present a systematic approach to inspecting program code on the basis of the relation between functional scenarios defined in a specification and execution paths implemented in its program. We report a prototype tool for the approach to support both forward and backward inspection strategies, and a case study of inspecting an Automatic Teller Machine system to evaluate the performance of the approach and the tool.", "authors": ["Shaoying Liu", "Fumiko Nagoya", "Yuting Chen", "Masashi Goya", "John A. McDermid"], "n_citation": 0, "title": "An automated approach to specification-based program inspection", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "0ffc13d7-b77c-4882-a15c-44e2b94a86d4"}
{"abstract": "In this paper, we introduce UTBot, a virtual agent platform for teaching agent systems' design. UTBot implements a client for the Unreal Tournament game server and Gamebots system. It provides students with the basic functionality required to start developing their own intelligent virtual agents to play autonomously UT games. UTBot includes a generic agent architecture, CAA (Context-sensitive Agent Architecture), a domain-specific world model, a visualization tool, several basic strategies (represented by internal modes and internal behaviors), and skills (represented by external behaviors). The CAA architecture can support complex long-term behaviors as well as reactive short-term behaviors. It also realizes high context-sensitivity of behaviors. We also discuss our experience using UTBot as a pedagogical tool for teaching agent systems' design in undergraduate Artificial Intelligence course.", "authors": ["In-Cheol Kim"], "n_citation": 0, "title": "Teaching agent systems' design using 3D interactive computer games", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "10531ba2-dff6-4170-a3a4-390f97e437ba"}
{"abstract": "In today's new economy, characterized by industrial revolutions, globalization and information technology changes traditional classroom and teaching methods. Calculus is the basic course for the fresher at university. Mathematics has a negative development at universities and colleges. The number of students taking mathematics courses and those successfully passing the exams are rapidly decreasing. To overturn this negative development, it may be necessary to teach in a different way. This is where new technology could play an important role. We create a web browser environment by using the webMathematica, which is a new technology based on Java Server Page, Tomcat and Mathematica, in learning calculus. The modules consist of topic modules linked with case study modules, to help learn calculus and see it in context. The aim of our project is to use new technology to present mathematics on the web, and create a pilot course for students and teachers available free on the web. The heart of our approach is the development of mathematical power: understanding, using, and appreciating mathematics. Students can reach any documents for calculus and solve their problems with webMathematica to check whether their results are correct or not. It is possible to reach our page at the following URL address: http://galois.iyte.edu.tr/calculus.", "authors": ["\u00dcnal Ufuktepe", "G\u00fcnnur Ufuktepe", "Asli Deniz", "Veli D\u00fcndar"], "n_citation": 0, "title": "e-Calculus at IZTECH", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "125edd71-47ef-4bea-a233-21a484f90ab1"}
{"abstract": "To extract source signals with certain temporal structures, such as periodicity, we propose a two-stage extraction algorithm. Its first stage uses the autocorrelation property of the desired source signal, and the second stage exploits the independence assumption. The algorithm is suitable to extract periodic or quasi-periodic source signals, without requiring that they have distinct periods. It outperforms many existing algorithms in many aspects, confirmed by simulations. Finally, we use the proposed algorithm to extract the components of visual event-related potentials evoked by three geometrical figure stimuli, and the classification accuracy based on the extracted components achieves 93.2%.", "authors": ["Zhilin Zhang", "Liqing Zhang", "Xiu-Ling Wu", "Jie Li", "Qibin Zhao"], "n_citation": 0, "title": "Two-Stage Temporally Correlated Source Extraction Algorithm with Its Application in Extraction of Event-Related Potentials", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "12ccdb40-09bd-48f5-80e3-4a0bb925084f"}
{"authors": ["Francisco C. Santos", "Hugues Bersini", "Tom Lenaerts"], "n_citation": 0, "title": "Networks regulating networks: The effects of constraints on topological evolution", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "13a46094-f7c4-4d81-9c35-55e303701319"}
{"abstract": "Language identification an important task for web information retrieval. This paper presents the implementation of a tool for language identification in mono- and multi-lingual documents. The tool implements four algorithms for language identification. Furthermore, we present a n-gram approach for the identification of languages in multi-lingual documents. An evaluation for monolingual texts of varied length is presented. Results for eight languages including Ukrainian and Russian are shown. It could be shown that n-gram-based approaches outperform word-based algorithms for short texts. For longer texts, the performance is comparable. The evaluation for multi-lingual documents is based on both short synthetic documents and real world web documents. Our tool is able to recognize the languages present as well as the location of the language change with reasonable accuracy.", "authors": ["Thomas Mandl", "Margaryta Shramko", "Olga Tartakovski", "Christa Wornser-Hacker"], "n_citation": 50, "title": "Language identification in multi-lingual web-documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "13a6de66-60b8-439b-8cdc-3113c509751b"}
{"abstract": "In an earlier paper we gave efficient algorithms for partitioning chordal graphs into k independent sets and   cliques. This is a natural generalization of the problem of recognizing split graphs, and is NP-complete for graphs in general, unless k     = 1.) In this paper we expand our focus and consider general M-partitions, also known as trigraph homomorphisms, for the class of chordal graphs. For each symmetric matrix M over 0,1, *, the M-partition problem seeks a partition of the input graph into independent sets, cliques, or arbitrary sets, with some pairs of sets being required to have no edges, or to have all edges joining them, as encoded in the matrix M. Such partitions generalize graph colorings and homomorphisms, and arise frequently in the theory of graph perfection. We show that many M-partition problems that are NP-complete in general become solvable in polynomial time for chordal graphs, even in the presence of lists. On the other hand, we show that there are M-partition problems that remain NP-complete even for chordal graphs. We also discuss forbidden subgraph characterizations for the existence of M-partitions.", "authors": ["Tom\u00e1s Feder", "Pavol Hell", "Sulamita Klein", "Loana Tito Nogueira", "F\u00e1bio Protti"], "n_citation": 0, "title": "List partitions of chordal graphs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1494f078-c1cd-4895-a58c-8a75cad0f1e8"}
{"abstract": "As the Internet is widely used, high performance networks and systems prompts appearance of various applications and requests higher capacity resources. Recently, studies on grid are actively conducted to reorganize each computing resource, which used to be dispersed for each application requesting these high capacity calculation resources, into a virtual high capacity resource by connecting to networks. The network needed in the grid environment has been progressed along with the WDM-base Lambda Network that can build a high-bandwidth network for relatively low cost. This paper presents two ways that improve network performance on grid environment. First way is approach to reorganize the existing network into L2/OPN (Optical Private Network) to avoid traffic congestion. And second way is manage TCP windows buffer size to present a suitable TCP size for the Grid environment system.", "authors": ["Min-Ki Noh", "Joon-Min Gil", "KiSung Yoo", "Seongjin Ahn"], "n_citation": 0, "title": "A Study on L2/OPN Design for Grid High Performance Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "15252dc7-6417-4eeb-a711-499018d62b90"}
{"abstract": "We introduce a new notion of weak factor recognition that is the foundation of new data structures and on-line string matching algorithms. We define a new automaton built on a string p = p 1 p 2  \u2026 pm that acts like an oracle on the set of factors p i  \u2026 p j . If a string is recognized by this automaton, it may be a factor of p. But, if it is rejected, it is surely not a factor. We call it factor oracle. More precisely, this automaton is acyclic, recognizes at least the factors of p, has m + 1 states and a linear number of transitions. We give a very simple sequential construction algorithm to build it. Using this automaton, we design an efficient experimental on-line string matching algorithm (we conjecture its optimality in regard to the experimental results) that is really simple to implement. We also extend the factor oracle to predict that a string could be a suffix (i.e. in the set p i  \u2026 p m ) of p. We obtain the suffix oracle, that enables in some cases a tricky improvement of the previous string matching algorithm.", "authors": ["Cyril Allauzen", "Maxime Crochemore", "Mathieu Raffinot"], "n_citation": 62, "title": "Efficient experimental string matching by weak factor recognition", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "16c46da4-dcc3-44be-ad2b-e39bcb2c071e"}
{"abstract": "Estimating a meaningful average or mean shape from a set of shapes represented by unlabeled point-sets is a challenging problem since, usually this involves solving for point correspondence under a non-rigid motion setting. In this paper, we propose a novel and robust algorithm that is capable of simultaneously computing the mean shape from multiple unlabeled point-sets (represented by finite mixtures) and registering them nonrigidly to this emerging mean shape. This algorithm avoids the correspondence problem by minimizing the Jensen-Shannon (JS) divergence between the point sets represented as finite mixtures. We derive the analytic gradient of the cost function namely, the JS-divergence, in order to efficiently achieve the optimal solution. The cost function is fully symmetric with no bias toward any of the given shapes to be registered and whose mean is being sought. Our algorithm can be especially useful for creating atlases of various shapes present in images as well as for simultaneously (rigidly or non-rigidly) registering 3D range data sets without having to establish any correspondence. We present experimental results on non-rigidly registering 2D as well as 3D real data (point sets).", "authors": ["Fei Wang", "Baba C. Vemuri", "Anand Rangarajan", "Ilona M. Schmalfuss", "Stephan Eisenschenk"], "n_citation": 0, "title": "Simultaneous Nonrigid Registration of Multiple Point Sets and Atlas Construction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "18530d63-0ee5-40ca-89ec-76c817db4b7f"}
{"abstract": "Many requirements for a business process depend on the workflow execution data that includes common data for all the population of processes, state of resources, state of processes, etc. The natural way to specify and implement such requirements is to put them into the process definition. In order to do it, we need: (1) a generalised workflow metamodel that includes data on the workflow environment, process definitions, and process execution; (2) a powerful and flexible query language addressing the metamodel; (3) integration of a query language with a business process definition language. In this paper the mentioned workflow metamodel together with the business process query language BPQL is presented. BPQL is integrated with the XML Process Definition Language (XPDL) increasing significantly its expressiveness and flexibility. We also present practical results for application of the proposed language in the OfficeObjects\u00ae WorkFlow system.", "authors": ["Mariusz Momotko", "Kazimierz Subieta"], "n_citation": 0, "title": "Process Query language: A way to make workflow processes more flexible", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1a4bf99f-8b09-4eec-9698-2a08c61cbeb5"}
{"abstract": "Performance evaluation is a central issue in the design of complex real-time systems. In this work, we propose an extension of so-called Max-Plus algebraic techniques to handle more realistic types of real-time systems. In particular, our framework encompasses graph or partial order automata, and more generally abstract models of real-time computations (including synchronous programs running over distributed architectures). To achieve this, we introduce a new dioid of partially commutative power series (transductions), whose elements encode timed behaviors. This formalism extends the traditional representation of timed event graphs by (rational) commutative transfer series with coefficients in the Max-Plus semiring. We sketch how this framework can be used to symbolically solve several problems of interest, related to real-time systems. Then we illustrate the use of this framework to encode a nontrivial mixed formalism of dataflow diagrams and automata.", "authors": ["Albert Benveniste", "Claude Jard", "S. Gaubert"], "n_citation": 50, "title": "Algebraic techniques for timed systems", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "1a8a590e-8298-4f5b-a101-c8f36a4c157f"}
{"abstract": "We present and evaluate an approach for the run-time conformance checking of Java classes against property-driven algebraic specifications. Our proposal consists in determining, at run-time, whether the classes subject to analysis behave as required by the specification. The key idea is to reduce the conformance checking problem to the runtime monitoring of contract-annotated classes, a process supported today by several runtime assertion-checking tools. Our approach comprises a rather conventional specification language, a simple language to map specifications into Java types, and a method to automatically generate monitorable classes from specifications, allowing for a simple, but effective, runtime monitoring of both the specified classes and their clients.", "authors": ["Isabel Nunes", "Ant\u00f3nia Lopes", "Vasco Thudichum Vasconcelos", "Jo\u00e3o Abreu", "L. Reis"], "n_citation": 0, "title": "Checking the Conformance of Java Classes Against Algebraic Specifications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1d924aba-68eb-4622-8300-f8ade2bfd364"}
{"abstract": "Separation performance of directionally constrained filterbank ICA is evaluated in presence of noise with different spectral properties. Stationarity of mixing channels is exploited to introduce directional constraint on the adaptive subband separation networks of filterbank-based blind source separation approach. Directional constraints on demixing network improves separation of source signals from noisy convolved mixtures, when significant spectral overlap exists between the noise and the convolved mixtures. Observations corrupted with low frequency noises exhibit slight improvement, in the separation performance as there is less spectral overlap. Initialization and constraining of subband demixing network in accordance to the spatial location of source signals results in faster convergence and effective permutation correction, irrespective, of the nature of additive noise.", "authors": ["Chandra Shekhar Dhir", "Hyung-Min Park", "Soo-Young Lee"], "n_citation": 0, "title": "Performance Evaluation of Directionally Constrained Filterbank ICA on Blind Source Separation of Noisy Observations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1da18a72-1a59-4ce8-8602-2d787a3c4231"}
{"abstract": "Searching the state space of a system using enumerative and on-the-fly depth-first traversal is an established technique for model checking finite-state systems. In this paper, we propose algorithms for on-the-fly exploration of recursive state machines, or equivalently pushdown systems, which are suited for modeling the behavior of procedural programs. We present algorithms for reachability (is a bad state reachable?) as well as for fair cycle detection (is there a reachable cycle with progress?). We also report on an implementation of these algorithms to check safety and liveness properties of recursive boolean programs, and its performance on existing benchmarks.", "authors": ["Rajeev Alur", "Swarat Chaudhuri", "Kousha Etessami", "P. Madhusudan"], "n_citation": 0, "title": "On-the-fly reachability and cycle detection for recursive state machines", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1dddfec2-eca5-4ed6-8319-5bea5f4ba4ea"}
{"authors": ["Klaus Ambos-Spies", "Sabastiaan A. Terwijn", "Xishong Zheng", "X.S Zhang", "D.Z. Du"], "n_citation": 50, "title": "Resource bounded randomness and weakly complete problems", "venue": "Lecture Notes in Computer Science", "year": 1994, "id": "1ef2d6e3-9585-4f0d-ac3e-a17eb9e4baa4"}
{"abstract": "Repairing a database means making the database consistent by applying changes that are as small as possible. Nearly all approaches to repairing have assumed deletions and insertions of entire tuples as basic repair primitives. A negative effect of deletions is that when a tuple is deleted because it contains an error, the correct values contained in that tuple are also lost. It can be semantically more meaningful to update erroneous values in place, called update-based repairing. We prove that a previously proposed approach to update-based repairing leads to intractability. Nevertheless, we also show that the complexity decreases under the rather plausible assumption that database errors are mutually independent. An inconsistent database can generally be repaired in many ways. The consistent answer to a query on a database is usually defined as the intersection of the answers to the query on all repaired versions of the database. We propose an alternative semantics, defining the consistent answer as being maximal homomorphic to the answers on all repairs. This new semantics always produces more informative answers and ensures closure of conjunctive queries under composition.", "authors": ["Jef Wijsen"], "n_citation": 0, "title": "Making more out of an inconsistent database", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2163b86f-1368-44ac-aecc-9c34bc0f5e0a"}
{"abstract": "In passive network measurement, the quality of an observed traffic stream is obviously crucial to the quality of the results. Some sources of error (e.g., packet loss at a capture device) are well understood, others less so. In this work, we describe the inline data integrity measurement provided by the QoF TCP-aware flow meter. By instrumenting the data structures QoF uses for detecting lost and retransmitted TCP segments, we can provide an in-band, per-flow estimate of observation loss: segments which were received by the recipient but not observed by the flow meter. We evaluate this mechanism against controlled, induced error, and apply it to two data sets used in previous work.", "authors": ["Brian Trammell"], "n_citation": 0, "title": "Inline Data Integrity Signals for Passive Measurement", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "2429a503-dd4c-4cd4-a158-b4012849b7ea"}
{"abstract": "Multi-signatures allow multiple signers to jointly authenticate a message using a single compact signature. Many applications however require the public keys of the signers to be sent along with the signature, partly defeating the effect of the compact signature. Since identity strings are likely to be much shorter than randomly generated public keys, the identity-based paradigm is particularly appealing for the case of multi-signatures. In this paper, we present and prove secure an identity-based multi-signature (IBMS) scheme based on RSA, which in particular does not rely on (the rather new and untested) assumptions related to bilinear maps. We define an appropriate security notion for interactive IBMS schemes and prove the security of our scheme under the one-wayness of RSA in the random oracle model.", "authors": ["Mihir Bellare", "Gregory Neven"], "n_citation": 50, "title": "Identity-based multi-signatures from RSA", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "25fe2655-aea1-48bd-ac9a-fd85c44915a3"}
{"abstract": "How to build an efficient and scalable p2p file sharing system is still an open question. Structured systems obtain O(log(N)) lookup upper bound by associating content with node, But they can not supporting complex queries. On the other hand, Gnutella-like unstructured systems support complex queries, but because of its random-graph topology and its flooding content discovery mechanism, it can not scale to large network systems. In this paper, we present Janus, which build unstructured file sharing system over structured overlay. Different from previous approaches, Janus keeps bidirectional links in its routing table. And with one-hop replication and biased random walk Janus make it possible to implement complex queries in the scalable manner. The experimental results indicate that, when the system running over a network of 10,000 peers, it only needs 100 hops to search half of the total system.", "authors": ["Haitao Dong", "Weimin Zheng", "Dongsheng Wang"], "n_citation": 0, "title": "Janus: Build gnutella-like file sharing system over structured overlay", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "268d0436-725c-4cc5-a5f5-c65abee06e5b"}
{"abstract": "An analysis based on wavelet modulation scales feature extraction is proposed. Considering human auditory perception and varieties of disturbances, instead of the frequency differences, wavelet modulation scales are adopted to reflect the dynamic features of speech in ASR. Experiments for the Chinese digit-string recognition show extracting the wavelet modulation scales as the dynamic features have good performance both in additional noises and convolutional noises environment.", "authors": ["Xin Ma", "Weidong Zhou", "Fang Ju", "Qi Jiang"], "n_citation": 0, "title": "Speech Feature Extraction Based on Wavelet Modulation Scale for Robust Speech Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "27d453a1-57f7-42d9-93fb-3ce02c28464e"}
{"abstract": "Automated design of circuits is a vital task, which becomes more and more challenging due to the conflict of ever-growing scales and complexities of circuits and slow acquisition of relevant knowledge. Evolutionary design of circuit (EDC) combined with data mining is a promising way to solve the problem. To improve EDC in the aspects of efficiency, scalability and capability of optimization, a novel technique is developed. It features an adaptive multi-objective genetic algorithm and interactions between EDC and data mining. The proposed method is validated by the experiments on arithmetic circuits, showing many exciting results especially some novel knowledge discovered from the EDC data.", "authors": ["Shuguang Zhao", "Mingying Zhao", "Jin Li", "Change Wang"], "n_citation": 0, "title": "CBR-Based Knowledge Discovery on Results of Evolutionary Design of Logic Circuits", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2a6b28ba-54e2-4924-8d37-517e3e3e7d7e"}
{"abstract": "In this paper we study a few important tree optimization problems with applications to computational biology. These problems ask for trees that are consistent with an as large part of the given data as possible. We show that the maximum homeomorphic agreement subtree problem cannot be approximated within a factor of N\u2208, where N is the input size, for any 0 \u2264 \u2208 \u2264 1/18) in polynomial time unless P=NP, even if all the given trees are of height 2. On the other hand, we present an O(N log N)-time heuristic for the restriction of this problem to instances with 0(1) trees of height 0(1) yielding solutions within a constant factor of the optimum. We prove that the maximum inferred consensus tree problem is NP-complete, and we provide a simple fast heuristic for it yielding solutions within one third of the optimum. We also present a more specialized polynomial-time heuristic for the maximum inferred local consensus tree problem.", "authors": ["Leszek Gasieniec", "Jesper Jansson", "Andrzej Lingas", "Anna \u00d6stlin"], "n_citation": 11, "title": "On the complexity of computing evolutionary trees", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "2b4e73da-89a3-4177-8c0c-f87e22c10261"}
{"abstract": "Till now, few desynchronization methods for video fingerprint have been presented, which are implemented in raw data. In this paper, a compression compliant video desynchronization method for collusion resilient fingerprint is proposed. The technique can simultaneously apply random space/time desynchronization and compression to videos. In our experiments, with little visual degradation, the joint process costs no more time and bandwidth than those of MPEG2 encoding/decoding. By this method, the video quality degrades dramatically for colluded copies. Besides evaluating the method by compression quality, compression time, and visual quality, we also discussed the system security. Two attacks are considered for the security evaluation: re-synchronization attack (including Most Similar Frame Collusion and Random Similar Frame Replacement) and re-desynchronization attack. Schemes for robustness to these attacks are also shown. Two theorems are presented to point out the security limit of single time desynchronization and single space desynchronization and the influence of related parameters to security.", "authors": ["Zhongxuan Liu", "Shiguo Lian", "Ronggang Wang", "Zhen Ren"], "n_citation": 50, "title": "Desynchronization in compression process for collusion resilient video fingerprint", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2b8bf108-96f4-4aa0-8a7e-e26c19b8db16"}
{"abstract": "A digital signature scheme that achieves an optimal bandwidth (generating signatures as short as possible) is called an optimal signature scheme. The previous optimal signature schemes all need the random permutations (or the ideal ciphers) with large block size as building blocks. However, the practical cipher with large block size such as Halevi and Rogaway's CMC-mode should call the underlying secure block cipher with small block size many times each time. This makes the previous optimal signature schemes which use the large domain permutation (or the ideal cipher) less efficient in the real world, even if there exist the methods that can encipher the messages with larger domain. On the other hand, all the practical signature schemes are not optimal in bandwidth including PSS-R, FDH, DSA, etc. Hence, the problem on how to design a practical, efficient and optimal signature scheme remains open. This paper uses two random oracles and an ideal cipher with a smaller block size to design an optimal padding for signature schemes. The ideal cipher in our scheme can be implemented with a truly real block cipher (e.g. AES). Therefore, we provide a perfect solution to the open problem. More precisely, we design a practical, efficient and optimal signature scheme. Particularly, in the case of RSA, the padding leads the signature scheme to achieve not only optimality in bandwidth but also a tight security.", "authors": ["Haifeng Qian", "Zhibin Li", "Zhijie Chen", "Siman Yang"], "n_citation": 50, "title": "A practical optimal padding for signature schemes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d877946-917e-4426-bd87-995c61fa9d4f"}
{"abstract": "We investigated the possibility of detecting affect from natural language dialogue in an attempt to endow an intelligent tutoring system, AutoTutor, with the ability to incorporate the learner's affect into its pedagogical strategies. Training and validation data were collected in a study in which college students completed a learning session with AutoTutor and subsequently affective states of the learner were identified by the learner, a peer, and two trained judges. We analyzed each of these 4 data sets with the judges' affect decisions, along with several dialogue features that were mined from AutoTutor's log files. Multiple regression analyses confirmed that dialogue features could significantly predict particular affective states (boredom, confusion, flow, and frustration). A variety of standard classifiers were applied to the dialogue features in order to assess the accuracy of discriminating between the individual affective states compared with the baseline state of neutral.", "authors": ["Sidney K. D'Mello", "Arthur C. Graesser"], "n_citation": 50, "title": "Affect detection from human-computer dialogue with an intelligent tutoring system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2dbc0b2c-2ca6-4770-95be-5be84a754866"}
{"abstract": "We prove a tight lower bound for generic protocols for secure multicast key distribution where the messages sent by the group manager for rekeying the group are obtained by arbitrarily nested application of a symmetric-key encryption scheme, with random or pseudorandom keys. Our lower bound shows that the amortized cost of updating the group key for a secure multicast protocol (measured as the number of messages transmitted per membership change) is log 2 (n) + o(1). This lower bound matches (up to a small additive constant) the upper bound of Canetti, Garay, Itkis, Micciancio, Naor and Pinkas (Infocomm 1999), and is essentially optimal.", "authors": ["Daniele Micciancio", "Saurabh Panjwani"], "n_citation": 0, "title": "Optimal communication complexity of generic multicast key distribution", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2e95f015-80a4-4236-be14-9ff485395c7b"}
{"abstract": "Web services promise to become a key enabling technology for B2B e-commerce. Several languages have been proposed to compose Web services into workflows. The QoS of the Web services-based workflows may play an essential role in choosing constituent Web services and determining service level agreement with their users. In this paper, we identify a set of QoS metrics in the context of Web services and propose a unified probabilistic model for describing QoS values of (atomic/composite) Web services. In our model, each QoS measure of a Web service is regarded as a discrete random variable with probability mass function (PMF). We describe a computation framework to derive QoS values of a Web services-based workflow. Two algorithms are proposed to reduce the sample space size when combining PMFs. The experimental results show that our computation framework is efficient and results in PMFs that are very close to the real model.", "authors": ["San-Yih Hwang", "Haojun Wang", "Jaideep Srivastava", "Raymond A. Paul"], "n_citation": 0, "title": "A probabilistic QoS model and computation framework for Web services-based workflows", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2f26806a-ec21-4d89-aa78-aa926c8f5249"}
{"abstract": "We consider the problem of computing broadcast schedules for a speed-controlled channel minimizing overall energy consumption. Each request defines a strict deadline and we assume that sending at some speed s for t time units consumes energy t . s \u03b1 . For the case that the server holds a single message and the speed of a broadcast needs to be fixed when it is started, we present an O(2 \u03b1 )-competitive deterministic online algorithm and prove that this is asymptotically best possible even allowing randomization. For the multi-message case we prove that an extension of our algorithm is (4c- 1) \u03b1 -competitive if the lengths of requests vary by at most a factor of c. Allowing the speed of running broadcasts to be changed, we give lower bounds that are still exponential in a.", "authors": ["Patrick Briest", "Christian Gunia"], "n_citation": 0, "title": "Energy-Efficient Broadcast Scheduling for Speed-Controlled Transmission Channels", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2f3a5bb9-4648-40be-a258-80844e74f328"}
{"abstract": "In this work we study weighted network congestion games with player-specific latency functions where selfish players wish to route their traffic through a shared network. We consider both the case of splittable and unsplittable traffic. Our main findings are as follows: - For routing games on parallel links with linear latency functions without a constant term we introduce two new potential functions for unsplittable and for splittable traffic respectively. We use these functions to derive results on the convergence to pure Nash equilibria and the computation of equilibria. We also show for several generalizations of these routing games that such potential functions do not exist. - We prove upper and lower bounds on the price of anarchy for games with linear latency functions. For the case of unsplittable traffic the upper and lower bound are asymptotically tight.", "authors": ["Martin Gairing", "Burkhard Monien", "Karsten Tiemann"], "n_citation": 50, "title": "Routing (Un-) Splittable Flow in Games with Player-Specific Linear Latency Functions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "30a7d632-f08c-4c22-8d2f-3cfc52a0e381"}
{"abstract": "In recent years there has been growing interest in recognition models using local image features for applications ranging from long range motion matching to object class recognition systems. Currently, many state-of-the-art approaches have models involving very restrictive priors in terms of the number of local features and their spatial relations. The adoption of such priors in those models are necessary for simplifying both the learning and inference tasks. Also, most of the state-of-the-art learning approaches are semi-supervised batch processes, which considerably reduce their suitability in dynamic environments, where unannotated new images are continuously presented to the learning system. In this work we propose: 1) a new model representation that has a less restrictive prior on the geometry and number of local features, where the geometry of each local feature is influenced by its k closest neighbors and models may contain hundreds of features; and 2) a novel unsupervised on-line learning algorithm that is capable of estimating the model parameters efficiently and accurately. We implement a visual class recognition system using the new model and learning method proposed here, and demonstrate that our system produces competitive classification and localization results compared to state-of-the-art methods. Moreover, we show that the learning algorithm is able to model not only classes with consistent texture (e.g., faces), but also classes with shape only (e.g., leaves), classes with a common shape but with a great variability in terms of internal texture (e.g., cups), and classes of flexible objects (e.g., snake).", "authors": ["Gustavo Carneiro", "David G. Lowe"], "n_citation": 74, "title": "Sparse Flexible Models of Local Features", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "30d7069a-d7f3-4d52-8e4f-08d15d0f6501"}
{"abstract": "A module of a graph is a non-empty subset of vertices such that every non-module vertex is either connected to all or none of the module vertices. An indecomposable graph contains no non-trivial module (modules of cardinality 1 and |V| are trivial). We present an algorithm to compute indecomposability preserving elimination sequence, which is faster by a factor of |V| compared to the algorithms based on earlier published work. The algorithm is based on a constructive proof of Ille's theorem [9]. The proof uses the properties of X-critical graphs, a generalization of critical indecomposable graphs.", "authors": ["Chandan K. Dubey", "Shashank K. Mehta"], "n_citation": 0, "title": "On Indecomposability Preserving Elimination Sequences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "31a52d36-9735-4b7b-ace4-1bcb779f5d1e"}
{"authors": ["Anne Condon"], "n_citation": 0, "title": "RNA molecules : Glimpses through an algorithmic lens", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "322628fb-1e42-4ec9-93cc-c467bae85fb9"}
{"abstract": "This paper proposes a hybrid machine learning model for electricity demand forecasting, based on Bayesian Clustering by Dynamics (BCD) and Support Vector Machine (SVM). In the proposed model, a BCD classifier is firstly applied to cluster the input data set into several subsets by the dynamics of load series in an unsupervised manner, and then, groups of 24 SVMs for the next day's electricity demand curve are used to fit the training data of each subset. In the numerical experiment, the proposed model has been trained and tested on the data of the historical load from New York City.", "authors": ["Shu Fan", "Chengxiong Mao", "Jiadong Zhang", "Luonan Chen"], "n_citation": 0, "title": "Forecasting Electricity Demand by Hybrid Machine Learning Model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3288f205-e283-4c9f-a9c0-3124bf92dcff"}
{"abstract": "We show how logic programs may be used to protect deductive databases from the unauthorized retrieval of positive and negative information, and from unauthorized insert and delete requests. To achieve this protection, a deductive database is expressed in a form that is guaranteed to permit only authorized access requests to be performed. The protection of the positive information that may be retrieved from a database and the information that may be inserted are treated in a uniform way as is the protection of the negative information in the database, and the information that may be deleted.", "authors": ["Steve Barker"], "n_citation": 50, "title": "Access control for deductive databases by logic programming", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "367a5a08-a038-4c28-8a75-77d92a2d734c"}
{"authors": ["Aleksandrs Belovs", "Juris Smotrovs"], "n_citation": 50, "references": ["03bf166b-57ca-4167-b7ed-ebdc3f83dc39", "0bb89205-597a-4864-ad75-ac463123322e", "4be8646e-1c6c-497e-a446-ee6475cfba48", "52faeaa5-91af-46c2-9229-689fc480ceb3", "6fa29d22-7ebe-46fc-93d0-86121eebe7ff", "8f5fb858-fca8-4af0-96f0-042d544f4601", "93f910ae-223b-4e31-99b4-4f8ee3218510", "a3004cff-f9a2-4701-8930-d7bf99f4b5c5", "a6d15a2d-e7bb-4b21-854b-f7e631059d9d", "ab417a01-e567-4674-a0f1-0568de22de0d", "c42ccf9f-b1a5-4e1f-a01f-18f2739e5883"], "title": "A Criterion for Attaining the Welch Bounds with Applications for Mutually Unbiased Bases", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "3691908e-73a7-4fe6-a391-1a4dfe9cb485"}
{"abstract": "In the IPv6 transition process, coexistence with IPv4 is required, in which methods are invoked, where a user can use IPv6 easily, in a transition phase. In addition, methods also exist to manage an IPv6 network, in order to improve the activation and efficiency of IPv6 usage. In this paper, a system is proposed, with a function to measure network performance and obtain automatic connection using tunneling, where even an amateur can connect to the IPv6 system, along with an index. High performance of IPv6 can be manually verified, in order to promote the supply of IPv6, increasing the efficiency of network management.", "authors": ["Jae-Wook Lee", "Jahwan Koo", "Jinwook Chung", "Youngsong Mun", "Young-Hwan Lim", "Seung-Jung Shin", "Seongjin Ahn"], "n_citation": 0, "title": "Automatic Configuration of IPv6 Tunneling in a Dual Stack Host", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "373226db-7ee2-4ed7-aca8-c994c214b04b"}
{"abstract": "Federated database systems provide a homogeneous interface to possibly heterogeneous local database systems. This homogeneous interface consists of a global schema which is the result of a logical integration of the schemata of the corresponding local database systems and file systems. In this paper, we sketch the integration process and a set of tools for supporting the design process. Besides the classical database schema integration, the design process for federated information systems requires the integration of other aspects like integrity rules, authorization policies and transactional processes. This paper reports on an integrated approach to tool support of several of these integration aspects. The different integration facets are linked via the database integration method GIM allowing a high degree of automatic integration steps.", "authors": ["Kerstin Schwarz", "Ingo Schmitt", "Can T\u00fcrker", "Michael H\u00f6ding", "Eyk Hildebrandt", "S\u00f6ren Balko", "Stefan Conrad", "Gunter Saake"], "n_citation": 0, "title": "Design support for database federations", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "37ac939a-d222-4bed-ba77-93365f2e688b"}
{"abstract": "In this paper, we propose an adaptive incremental nonlinear dimensionality reduction algorithm for data stream in adaptive Self-organizing Isometric Embedding [1][3] framework. Assuming that each sampling point of underlying manifold and its adaptive neighbors [3] can preserve the principal directions of the regions that they reside on, our algorithm need only update the geodesic distances between anchors and all the other points, as well as distances between neighbors of incremental points and all the other points when a new point arrives. Under the above assumption, our algorithms can realize an approximate linear time complexity embedding of incremental points and effectively tradeoff embedding precision and time cost.", "authors": ["Hou Yuexian", "Gong Kefei", "He Pi-lian"], "n_citation": 0, "title": "Adaptively Incremental Self-organizing Isometric Embedding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37b85392-edf4-4d93-9aa6-c29d69814a0f"}
{"abstract": "Since the precise modeling of reflection is a difficult task, most feature points trackers assume that objects are lambertian and that no lighting change occurs. To some extent, a few approaches answer these issues by computing an affine photometric model or by achieving a photometric normalization. Through a study based on specular reflection models, we explain explicitly the assumptions on which these techniques are based. Then we propose a tracker that compensates for specular highlights and lighting variations more efficiently when small windows of interest are considered. Experimental results on image sequences prove the robustness and the accuracy of this technique in comparison with the existing trackers. Moreover, the computation time of the tracking is not significantly increased.", "authors": ["Mich\u00e8le Gouiff\u00e8s", "Christophe Collewet", "Christine Fernandez-Maloigne", "Alain Tremeau"], "n_citation": 0, "title": "Feature Points Tracking : Robustness to Specular Highlights and Lighting Changes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "39044a06-900c-4148-bada-91eec8533d67"}
{"abstract": "We introduce a novel representation for the graph colouring problem, called the Integer Merge Model, which aims to reduce the time complexity of an algorithm. Moreover, our model provides useful information for guiding heuristics as well as a compact description for algorithms. To verify the potential of the model, we use it in DSATUR, in an evolutionary algorithm, and in the same evolutionary algorithm extended with heuristics. An empiricial investigation is performed to show an increase in efficiency on two problem suites , a set of practical problem instances and a set of hard problem instances from the phase transition.", "authors": ["Istv\u00e1n Juhos", "Jano van Hemert"], "n_citation": 0, "title": "Improving graph colouring algorithms and heuristics using a novel representation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "39fbeb2a-108c-4a92-bb4d-1c5b637abcb2"}
{"abstract": "A lambda term is k-duplicating if every occurrence of a lambda abstractor binds at most k variable occurrences. We prove that the problem of higher order matching where solutions are required to be k-duplicating (but with no constraints on the problem instance itself) is decidable. We also show that the problem of higher order matching in the affine lambda calculus (where both the problem instance and the solutions are constrained to be 1-duplicating) is in NP, generalizing de Groote's result for the linear lambda calculus [4].", "authors": ["Daniel J. Dougherty", "Tomasz Wierzbicki"], "n_citation": 50, "title": "A decidable variant of higher order matching", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3b62bc09-da5c-4756-9c52-84761d5f43bb"}
{"abstract": "Many random key predistribution schemes have been proposed for pairwise key establishment in sensor networks recently. A general model of security under which these key predistribution techniques can be formally analyzed for correctness is required. In this paper, we have made such an attempt. We use the well known computational model of probabilistic turn based 21  -player games to model the key predistribution schemes and have shown how this model can be translated in formally specifying a property that these schemes should have. To the best of our knowledge this is the first work where we show the significance of probabilistic turn based 21  -player games in modelling security requirement of key predistribution schemes.", "authors": ["Debapriyay Mukhopadhyay", "Suman Roy"], "n_citation": 0, "title": "A game based model of security for key predistribution schemes in wireless sensor network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4058391c-9a9c-4d08-91d0-e9a579f44d5a"}
{"abstract": "The Bifurcating Neuron (BN) is an integrate-and-fire neuron that exhibits crisis-mediated transitions between multiple, symmetrical chaotic attractors. The Bifurcating Neuron Network 3 (BNN-3), a class of BN networks, was reported to be a natural model for solving coloring problems due to the multi-stability of BN. An important question left behind unanswered by the preliminary report was the scalability of BNN-3 as a coloring problem solver. Another question was the possibility of BNN-3 playing an N-ary associative memory as other multi-state neuron network models do. We carried out an extended study and were able to reach positive conclusions for both questions.", "authors": ["Jinhyuk Choi", "Geehyuk Lee"], "n_citation": 0, "title": "The Bifurcating Neuron Network 3 as Coloring Problem Solver and N-Ary Associative Memory", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "41fa1f35-458e-4f37-80fc-8c9e56616fbc"}
{"abstract": "A large number of text files, including HTML documents and XML documents, can be organized as tree structures. One objective of data mining is to discover frequent patterns in them. In this paper, first, we introduce a canonical form of free tree, which is based on the breadth-first canonical string; secondly, we present some properties of a closed frequent subtree and a maximal frequent subtree as well as their relationships; thirdly, we study a pruning technique of frequent free subtree and improvement on the mining of the nonclosed frequent free subtree; finally, we present an algorithm that mines all closed and maximal frequent free trees and prove validity of this algorithm.", "authors": ["Ping Guo", "Yang Zhou", "Jun Zhuang", "Ting Chen", "Yan-Rong Kang"], "n_citation": 0, "title": "An efficient algorithm for mining both closed and maximal frequent free subtrees using canonical forms", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "42a082ac-865c-4cee-8e30-ee4749a98776"}
{"abstract": "Efficient fault monitoring in the grid environment is a must that enables the stable provision of distributed computing resources without the interruption even in the case of grid node faults and dynamic reconfiguration. This paper proposes a mobile agent based approach for the fault detection of grid systems. In this approach, mobile agents embedding an extended Adaptive and Distributed System level Diagnosis (ADSD) algorithm are deployed to the nodes on a grid, and detect and diagnose the node failures in an adaptive and distributed way. We also present the experiment results.", "authors": ["Kyungkoo Jun", "Seokhoon Kang"], "n_citation": 0, "title": "Adaptive and distributed fault monitoring of grid by mobile agents", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "445fdd2e-f467-47ae-b6fc-f24e56dd86e7"}
{"abstract": "Handoff latency affects the service quality of real-time applications. In this paper we develop an analytical model to analyze the Mobile IP Fast Authentication protocol (MIFA) and compare it to Hierarchical Mobile IP (HMIP). The study compares the signaling costs of the protocols as well as the overall load for packet forwarding. Our study shows that MIFA minimizes the packet delivery cost compared to HMIP. Additionally, MIFA is more efficient when the arrival rate of the packets increases. Thus MIFA outperforms HMIP with respect to signaling cost. From the performance point of view MIFA performs similar to HMIP when the domain consists of two hierarchy levels only, and outperform HMIP otherwise. However, MIFA does not require a hierarchical network architecture as HMIP does.", "authors": ["Ali Diab", "Andreas Mitschele-Thiel", "Rend B\u00f6ringer"], "n_citation": 0, "title": "Comparison of signaling and packet forwarding overhead for HMIP and MIFA", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4b8dfc95-dbce-4ad8-81b0-20031b3f5fe6"}
{"abstract": "Many reliability prediction techniques require an estimate for the number of residual faults. In this paper, a new theory is developed for using test coverage to estimate the number of residual faults. This theory is applied to a specific example with known faults and the results agree well with the theory. The theory is used to justify the use of linear extrapolation to estimate residual faults. It is also shown that it is important to establish the amount of unreachable code in order to make a realistic residual fault estimate.", "authors": ["Peter G. Bishop"], "n_citation": 50, "title": "Estimating residual faults from code coverage", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4d08fcf6-a586-4389-b281-0d669cdc6ed2"}
{"abstract": "Clustering is defining inter relationship of unordered data and grouping data systematically. The systems using clustering provide the grouped information to the users. The complex metadata description generated on the basis of pre-defined ontologies serve as perfect input data for clustering. In this paper, we propose an approach for clustering ontology-based metadata. Main contribution of this paper are the definition of evaluation function to measure ontology-based metadata and study using this evaluation function within hierarchical clustering algorithm.", "authors": ["Je-Min Kim", "Young-Tack Park"], "n_citation": 0, "title": "A Study of the Evaluation Function and the Clustering Algorithm for Semantic Web Environment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4db2e019-b26e-4d49-91e9-a53c2a3161d3"}
{"abstract": "In this paper we provide a uniform framework, based on extraction calculi, where to study the complexity of the problem to decide the disjunction and the explicit definability properties for Intuitionistic Logic and some Superintuitionistic Logics. Unlike the previous approaches, our framework is independent of structural properties of the proof systems and it can be applied to Natural Deduction systems, Hilbert style systems and Gentzen sequent systems.", "authors": ["Mauro Ferrari", "Camillo Fiorentini", "Guido Fiorino"], "n_citation": 0, "title": "On the complexity of disjunction and explicit definability properties in some intermediate logics", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4f0fab50-2538-4307-bfee-31cf7a899db2"}
{"abstract": "This paper proposes an approach to representing and querying semistructured Web data. The proposed approach is based on nested tables, which may have internal nested structural variations to accommodate semistructured data. Our motivation is to reduce the complexity found in typical query languages for semistructured data and to provide users with an alternative for quickly querying data obtained from multiple-record Web pages. We show the feasibility of our proposal by developing a prototype for a graphical query interface called QSByE (Querying Semistructured data By Example). For QSByE, we define a particular variation of nested tables and propose a set of QBE-like operations that extends typical nested-relational-algebra operations to handle semistructured data. We show examples of how users can pose interesting queries using QSByE.", "authors": ["Altigran Soares da Silva", "Irna M. R. Evangelista Filha", "Alberto H. F. Laender", "David W. Embley"], "n_citation": 0, "title": "Representing and querying Semistructured Web data using nested tables with structural variants", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "50df452d-235a-4dcd-accd-1b251a16ba89"}
{"abstract": "We study the expressiveness of finite message-passing automata with a priori unbounded FIFO channels and show them to capture exactly the class of MSC languages that are definable in existential monadic second-order logic interpreted over MSCs. Moreover, we prove the monadic quantifier-alternation hierarchy over MSCs to be infinite and conclude that the class of MSC languages accepted by message-passing automata is not closed under complement. Furthermore, we show that satisfiability for (existential) monadic seconder-order logic over MSCs is undecidable.", "authors": ["Benedikt Bollig", "Martin Leucker"], "n_citation": 50, "title": "Message-passing automata are expressively equivalent to EMSO logic", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5231027f-422e-4cd0-8fcf-43d1ba916aa9"}
{"abstract": "Managers of systems of shared resources typically have many separate goals. Examples are efficient utilization of the resources among its users and ensuring no user's satisfaction in the system falls below a preset minimal level. Since such goals will usually conflict with one another, either implicitly or explicitly the manager must determine the relative importance of the goals, encapsulating that into an overall utility function rating the possible behaviors of the entire system. Here we demonstrate a distributed, robust, and adaptive way to optimize that overall function. Our approach is to interpose adaptive agents between each user and the system, where each such agent is working to maximize its own private utility function. In turn, each such agent's function should be both relatively easy for the agent to learn to optimize, and aligned with the overall utility function of the system manager - an overall function that is based on but in general different from the satisfaction functions of the individual users. To ensure this we enhance the Collective INtelligence (COIN) framework to incorporate user satisfaction functions in the overall utility function of the system manager and accordingly in the associated private utility functions assigned to the users' agents. We present experimental evaluations of different COIN-based private utility functions and demonstrate that those COIN-based functions outperform some natural alternatives.", "authors": ["St\u00e9phane Airiau", "Sandip Sen", "David H. Wolpert", "Kagan Tumer"], "n_citation": 0, "title": "Providing effective access to shared resources: A COIN approach", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5328d611-e6b7-48aa-a3db-6d2743ed0354"}
{"abstract": "We report on an initial success obtained in investigating the Feature Interaction problem (FI) via proof planning. FIs arise as an unwanted/unexpected behaviour in large telephone networks and have recently attracted interest not only from the Computer Science community but also from the industrial world. So far, FIs have been solved mainly via approximation plus finite-state methods (model checking being the most popular); in our work we attack the problem via proof planning in First-Order Linear Temporal Logic (FOLTL), therefore making use of no finite-state approximation or restricting assumption about quantification. We have integrated the proof planner \u03bbCLAM with an object-level FOLTL theorem prover called FTL, and have so far re-discovered a feature interaction in a basic (but far from trivial) example.", "authors": ["Claudio Castellini", "Alan Smaill"], "n_citation": 50, "title": "Proof planning for feature interactions: A preliminary report", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "53bbb7b6-d1ed-4884-96f5-00316edf613a"}
{"abstract": "A sequence diagram in UML is used to model interactions among objects that participate in a use case. Developing a sequence diagram is complex; our experience shows that novice developers have significant difficulty. In earlier work, we presented a ten-step heuristic method for developing sequence diagrams. This paper presents a tabular analysis method (TAM) which improves on the ten-step heuristic method. TAM analyzes the message requirements of the use case, while documenting the resulting analysis in a tabular format. The resulting table is referenced to build the sequence diagram. This process aids novice modelers by separating the problem analysis from the learning curve of a modeling tool. Building sequence diagrams with the systematic approach of TAM facilitates consistency with the use case model and the class model. We found that developers effectively developed sequence diagrams using TAM.", "authors": ["Margaret Hilsbos", "Il-Yeol Song"], "n_citation": 0, "title": "Use of tabular analysis method to construct UML sequence diagrams", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "596b75af-6efd-4c04-b8b6-bf13307a5c3d"}
{"abstract": "In the past, we talked about single information systems. In the future, we expect an ever increasing number of information systems and data sources, reaching from traditional databases and large document collections, information sources contained in web pages, down to information systems in mobile devices as they will occur in a pervasive computing environment. Therefore not only the immense amount of information demands new thoughts but also the number of different information sources. Essentially, their coordination poses a great challenge for the development of future tools that will be suitable to access, process, and maintain information. We talk about the continuous, infinite information, shortly called the information space. Information in this space is distributed, heterogeneous and undergoes continuous changes. So, the infrastructure for information spaces must provide convenient tools for accessing information, for developing applications for analyzing, mining, classifying, and processing information, and for transactional processes that ensure consistent propagation of information changes and simultaneous invocations of several (web) services within a transactional workflow. As far as possible, the infrastructure should avoid global components. Rather, a peer-to-peer decentralized coordination middleware must be provided that has some self-configuration and adaptation features. In this paper we will elaborate some of the aspects related to process-based coordination within the information space and report on research from our hyperdatabase research framework and from experiences in ETHWorld, an ETH wide project that will establish the ETH information space. Nevertheless, this paper is rather visionary and is intended to stimulate new research in this wide area.", "authors": ["Hans-J\u00f6rg Schek", "Heiko Schuldt", "Christoph Schuler", "Roger Weber"], "n_citation": 50, "title": "Infrastructure for information spaces", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "59f7eb92-e80a-4661-8271-0d397910ee9a"}
{"abstract": "Knowledge Sharing is a key enabler of development of the ru- ral poor. ICTs can play a critical role, providing for instance market data or weather information to sustenance farmers, or education to children in remote areas. While advanced knowledge technology has proven its use in many applications in the so-called developed world most of the tools cannot be easily applied in developing countries, because of restricted infrastructure, unsuitable modes of communication or ignorance of the local context. In the K4D tutorial at EKAW 2014 we argued that a new of kind of research in Knowledge Engineering is needed in order to make knowledge technology useful outside privileged developed countries. This research will have to include existing social and economic structures as fundamental requirements in order to be successful. Finally, we claim that this holds for a broader spectrum of subdisciplines of Computer Science, and not just for Knowledge Engineering, which lets us advocate Development Informatics: a joint forum for CS researchers who try to make their research relevant for the developing world as well. 1", "authors": ["Stefan Schlobach", "Victor de Boer", "Christophe Gu\u00e9ret", "Stephane Boyera", "Philippe Cudr\u00e9-Mauroux", "Patrick Lambrix", "Eero Hyv\u00f6nen", "Eva Blomqvist", "Valentina Presutti", "Guilin Qi", "Uli Sattler", "Ying Ding", "Chiara Ghidini"], "n_citation": 0, "title": "From Knowledge Engineering for Development to Development Informatics", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "5c4d6b56-97b5-47b2-bfdd-dad785b7077f"}
{"abstract": "The even cycle problem for both undirected [Tho88] and directed [RST99] graphs has been the topic of intense research in the last decade. In this paper, we study the computational complexity of cycle length modularity problems. Roughly speaking, in a cycle length modularity problem, given an input (undirected or directed) graph, one has to determine whether the graph has a cycle C of a specific length (or one of several different lengths), modulo a fixed integer. We denote the two families (one for undirected graphs and one for directed graphs) of problems by (S, m)-UC and (S, rn)-DC, where m \u2208 N and S C {0, 1,..., m - 1}. (S, m)-UC (respectively, (S, m)-DC) is defined as follows: Given an undirected (respectively, directed) graph G, is there a cycle in G whose length, modulo m, is a member of S? In this paper, we fully classify (i.e., as either polynomial-time solvable or as NP-complete) each problem (S, m)-UC such that 0 \u2208 S and each problem (S, m)-DC such that 0 \u00ac\u2208 S. We also give a sufficient condition on S and m for the following problem to be polynomial-time computable: (S, m)-UC such that 0 \u00ac\u2208 S.", "authors": ["Edith Hemaspaandra", "Holger Spakowski", "Mayur Thakur"], "n_citation": 0, "title": "Complexity of cycle length modularity problems in graphs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5c8724cf-ad3d-4cba-9c26-bf1b07e77819"}
{"abstract": "We introduce a new higher-order rewriting formalism, called Expression Reduction Systems with Patterns (ERSP), where abstraction is not only allowed on variables but also on nested patterns. These patterns are built by combining standard algebraic patterns with choice constructors used to denote different possible structures allowed for an ed argument. In other words, the non deterministic choice between different rewriting rules which is inherent to classical rewriting formalisms can be lifted here to the level of patterns. We show that confluence holds for a reasonable class of systems and terms.", "authors": ["Julien Forest", "Delia Kesner"], "n_citation": 0, "title": "Expression Reduction Systems with Patterns", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5fae1e1b-d170-4b7a-b478-ab70d5389c4e"}
{"abstract": "We consider the problem of protein folding in the HP model on the 3D square lattice. This problem is combinatorially equivalent to folding a string of 0's and 1's so that the string forms a self-avoiding walk on the lattice and the number of adjacent pairs of 1's is maximized. The previously best-known approximation algorithm for this problem has a guarantee of 3/8 = 375 [HI95]. In this paper, we first present a new 3/8 approximation algorithm for the 3D folding problem that improves on the absolute approximation guarantee of the previous algorithm. We then show a connection between the 3D folding problem and a basic combinatorial problem on binary strings, which may be of independent interest. Given a binary string in {a,b}*, we want to find a long subsequence of the string in which every sequence of consecutive a's is followed by at least as many consecutive b's. We show a non-trivial lower-bound on the existence of such subsequences. Using this result, we obtain an algorithm with a slightly improved approximation ratio of at least .37501 for the 3D folding problem. All of our algorithms run in linear time.", "authors": ["Alantha Newman", "Matthias Ruhl"], "n_citation": 0, "title": "Combinatorial problems on strings with applications to protein folding", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "601c4a60-b27c-4f5d-b6e3-c231ed10e48e"}
{"authors": ["Jan Treur", "Muhammad Umair", "Vu1010095", "Faculteit der Exacte Wetenschappen"], "n_citation": 0, "title": "A Cognitive Agent Model Using Inverse Mirroring for False Attribution of Own Actions to Other Agents", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "618153a7-4bd6-4c8c-8143-d4cd9fc80472"}
{"abstract": "The text classification problem, which is the task of assigning natural language texts to predefined categories based on their content, has been widely studied. Traditional text classification use VSM (Vector Space Model), which views documents as vectors in high dimensional spaces, to represent documents. In this paper, we propose a non-VSM kNN algorithm for text classification. Based on correlations between categories and features, the algorithms first get k F-C tuples, which are the first k tuples in term of correlation value, from an unlabeled document. Then the algorithm predicts the category of the unlabeled documents via these tuples. We have evaluated the algorithm on two document collections and compared it against traditional kNN. Experimental results show that our algorithm outperforms traditional kNN in both efficiency and effectivity.", "authors": ["Zhi-Hong Deng", "Shiwei Tang"], "n_citation": 0, "title": "A non-VSM kNN algorithm for text classification", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "61e8c7d1-71ad-46e1-83f3-cf366493490a"}
{"authors": ["Jan De Belder", "Marie-Francine Moens"], "n_citation": 0, "title": "Integer linear programming for Dutch sentence compression", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "65067952-26d3-4553-84a9-45f395fe5b1a"}
{"abstract": "This paper describes guess-and-determine attacks on the stream cipher SNOW. The first attack has a data complexity of O(2 64 ) and a process complexity of O(2 256 ). The second attack has process complexity of O(2 224 ), and a data complexity of O(2 95 ).", "authors": ["Philip Michael Hawkes", "Gregory G. Rose"], "n_citation": 0, "title": "Guess-and-determine attacks on SNOW", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "65abdb7f-d74c-459f-b34b-ee2f58e7a432"}
{"abstract": "A project named Virtual Museum Net of Magna Graecia is presented, whose objective is to provide an unitary image of the archaeological heritage of Calabria (a Southern Italian region), through Computer Technology, Multimedia Designing and Virtual Reconstructions. The project aims to encourage the technological transfer of the most advanced researches in the exploitation and conservation sector of the Cultural Heritage. The Virtual Museum Net of Magna Graecia project is included in the context of Knowledge Media Design for Museums. This project links the museum scenery and multimedia, in order to use technology as system of representation.", "authors": ["Pier Augusto Bertacchini", "Eleonora Bilotta", "Elvira Di Bianco", "Gianpiero Di Blasi", "Pietro Pantano"], "n_citation": 0, "title": "Virtual museum net", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "661be5e7-8dd5-463a-b58c-b7ba19f5a765"}
{"abstract": "In program development it is useful to know that a call to a Prolog program will not inadvertently leave a choice-point on the stack. Determinacy inference has been proposed for solving this problem yet the analysis was found to be wanting in that it could not infer determinacy conditions for programs that contained cuts or applied certain tests to select a clause. This paper shows how to remedy these serious deficiencies. It also addresses the problem of identifying those predicates which can be rewritten in a more deterministic fashion. To this end, a radically new form of determinacy inference is introduced, which is founded on ideas in ccp, that is capable of reasoning about the way bindings imposed by a rightmost goal can make a leftmost goal deterministic.", "authors": ["Andy King", "Lunjin Lu", "Samir Genaim"], "n_citation": 0, "title": "Detecting determinacy in prolog programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "69b9f82c-ccc4-46e0-86f6-474173bde0ab"}
{"abstract": "In this paper, we propose a multiple-level image semantics classification method. The multiple-level image semantics classifier is constructed according to a hierarchical semantics tree. A semantics tree is defined according to the individual user's habit of managing files. So it is personalized. The classification features are selected by calculating information entropy of images. The hierarchical classifier is constructed according to a class correlation measure. This measure considers both the relation of the classifiers between different hierarchical levels and the relation between the classifiers at the same level. The unlabelled pictures can be classified top-down and assigned to corresponding class and semantic labels. In our experiment binary SVM is used. The hierarchical classifier is built by selecting meta-classifiers with the combinations that have better performance. The result shows that the hierarchical classifier is more effective than a flat method.", "authors": ["Hongli Xu", "De Xu", "Fangshi Wang"], "n_citation": 0, "title": "User-Centered Image Semantics Classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6b4c0451-1313-4568-b9d2-d94e00059d5f"}
{"abstract": "A rich dense-time logic, called Interval Duration Logic (IDL), is useful for specifying quantitative properties of timed systems. The logic is undecidable in general. However, several approaches can be used for checking validity (and model checking) of IDL formulae in practice. In this paper, we propose bounded validity checking of IDL formulae by polynomially reducing this to checking unsatisfiability of lin-sat formulae. We implement this technique and give performance results obtained by checking the unsatisfiability of the resulting lin-sat formulae using the ICS solver. We also perform experimental comparisons of several approaches for checking validity of IDL formulae, including (a) digitization followed by automata-theoretic analysis, (b) digitization followed by pure propositional SAT solving, and (c) lin-sat solving as proposed in this paper. Our experiments use a rich set of examples drawn from the Duration Calculus literature.", "authors": ["Babita Sharma", "Paritosh K. Pandya", "Supratik Chakraborty"], "n_citation": 0, "title": "Bounded validity checking of interval duration logic", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6cc7047d-33f0-4250-8a3a-a4eb4cbc2cb6"}
{"abstract": "This paper conducts an empirical analysis of a conceptual model quality framework for evaluating the quality of process models. 194 participants were trained in the concepts of the quality framework, and then used it to evaluate models represented in a workflow modelling language. A randomised, double-blind design was used, and the results evaluated using a combination of quantitative and qualitative techniques. An analysis was also conducted of the framework's likelihood of adoption in practice, which is an issue rarely addressed in IS design research. The study provides strong support for the validity of the framework and suggests that it is likely to be adopted in practice, but raises questions about its reliability. The research findings provide clear direction for further research to improve the framework.", "authors": ["Daniel L. Moody", "Guttorm Sindre", "Terje Brasethvik", "Arne S\u00f8lvberg"], "n_citation": 0, "title": "Evaluating the quality of process models: Empirical testing of a quality framework", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6e697ff1-77bf-42ca-b0a4-dde79d6ee501"}
{"abstract": "We focus on the problem of user revocation in secure adhoc networks. The current approach to achieve security in adhoc networks is to use a secret instantiation protocol in which, each user is given a subset of secrets from a common secret pool. To communicate securely, a pair of users use the secrets that are common to both of them. However, when users are compromised, some of these secrets are also compromised. Hence, to revoke the compromised users, the secrets known to these users need to be updated. Many group key management solutions exist for revocation of users from a group. However, due to the limitations in adhoc networks, i.e., lack of efficient broadcast mechanisms and lossy links, revocation of users is a challenging problem. In this paper, we propose a revocation algorithm that combines the secret instantiation protocols with group key management protocols. Depending on the combination of protocols used, our revocation algorithm provides deterministic or probabilistic guarantees for revocation. We illustrate our revocation algorithm by combining the square grid protocol and the logical key hierarchy protocol.", "authors": ["Bezawada Bruhadeshwar", "Sandeep S. Kulkarni"], "n_citation": 0, "title": "User revocation in secure adhoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "701848cd-026b-4e82-9d71-d7bd8187f73b"}
{"abstract": "Cellular Genetic Algorithms (cGA) are spatially distributed Genetic Algorithms that, because of their high level of diversity, are superior to regular GAs on several optimization functions. Also, since these distributed algorithms only require communication between few closely arranged individuals, they are very suitable for a parallel implementation. We propose a new kind of cGA, called hierarchical cGA (H-cGA), where the population structure is augmented with a hierarchy according to the current fitness of the individuals. Better individuals are moved towards the center of the grid, so that high quality solutions are exploited quickly, while at the same time new solutions are provided by individuals at the outside that keep exploring the search space. This algorithmic variant is expected to increase the convergence speed of the cGA algorithm and maintain the diversity given by the distributed layout. We examine the effect of the introduced hierarchy by observing the variable takeover rates at different hierarchy levels and we compare the H-cGA to the cGA algorithm on a set of benchmark problems and show that the new approach performs promising.", "authors": ["Stefan Janson", "Enrique Alba", "Bernab\u00e9 Dorronsoro", "Martin Middendorf"], "n_citation": 0, "title": "Hierarchical cellular genetic algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "708f65eb-4be4-4bb2-b297-bf61499fd015"}
{"abstract": "Query rewriting method is proposed for the heterogeneous information integration infrastructure formed by the subject mediator environment. Local as View (LAV) approach treating schemas exported by sources as materialized views over virtual classes of the mediator is considered as the basis for the subject mediation infrastructure. In spite of significant progress of query rewriting with views, it remains unclear how to rewrite queries in the typed, object-oriented mediator environment. This paper embeds conjunctive views and queries into an advanced canonical object model of the mediator. The selection' projection-join (SPJ) conjunctive query semantics based on type specification calculus is introduced. The paper demonstrates how the existing query rewriting approaches can be extended to be applicable in such typed environment. The paper shows that refinement of the mediator class instance types by the source class instance types is the basic relationship required for establishing query containment in the object environment.", "authors": ["Leonid A. Kalinichenko", "Dmitry O. Martynov", "Sergey A. Stupnikov"], "n_citation": 0, "title": "Query rewriting using views in a typed mediator environment", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "71cb3752-1a1f-4347-baab-b0e766022649"}
{"abstract": "To search a large dictionary for a collocation expressing a desired meaning, the human reader needs some kind of hierarchical structure that would facilitate such search. In this paper, fragments of semantic classification of modifiers are elaborated based on several highly modifier-productive nouns, namely, common nouns person, action, look, corporation, and price, as well the terms coating, medium, and check. By modifiers meant are adjectives, participles, or preposition phrases syntactically dependent on the nouns. The classification rubrics proved to be heavily dependent on the modified headword noun and are representative fragments of a Roget-like thesaurus. It is shown that the modifiers under consideration are rather selective in their use, similarly to standard lexical functions (LFs) by Mel'cuk, while for many nouns LFs can be absent. The obtained classification rubrics can be used for other English nouns and for other languages. Some deficiencies of the proposed rubrics are discussed.", "authors": ["Igor A. Bolshakov", "Alexander F. Gelbukh"], "n_citation": 0, "title": "On semantic classification of modifiers", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "74f3f392-042e-4dbd-b131-a897ffd54837"}
{"abstract": "Proxy encryption schemes transform cipher-text from one key to another without revealing the plain-text. Agents that execute such transformations are therefore minimally trusted in distributed systems leading to their usefulness in many applications. However, till date no application of proxy encryption has been deployed and used in practice. In this work we describe our efforts in developing a deployable secure mailing list solution based on proxy encryption techniques. Securing emails exchanged on mailing lists requires that confidentiality, integrity, and authentication of the emails be provided. This includes ensuring their confidentiality while in transit at the list server; a functionality that is uniquely supported by proxy encryption. In developing this solution we addressed the challenges of identifying requirements for deployability, defining a component architecture that maximizes the use of COTS components to help in deployment, developing the proxy encryption protocol to satisfy requirements and to fit within the component architecture, implementing and testing the solution, and packaging the release. As evidence of its deployability, the resulting secure mailing list solution is compatible with common email clients including Outlook, Thunderbird, Mac Mail, Emacs, and Mutt.", "authors": ["Himanshu Khurana", "Jin Heo", "Meenal Pant"], "n_citation": 0, "title": "From proxy encryption primitives to a deployable secure-mailing-list solution", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7719c2f5-e847-4800-9eb9-4ee8a5911f2f"}
{"abstract": "We describe an architecture requiring very few changes to any standard von Neumann machine that provably withstands coalitions between a malicious operating system and other users, in the sense that: 1. If the operating system permits a program to run, then the program produces the same outputs as it would produce if it were running on an ideal, single-user machine; moreover, even if the operating system behaves according to expectations only most of the time, the programs get executed. 2. The only information leaked by a program to the malicious coalition is the time and space requirements of the program. 3. If the malicious operating system is dynamically replaced by a good operating system, then the latter can quickly and correctly determine what memory resources are available for future programs, as well as how much time is left for each of the currently executing programs, and can distribute these resources without any restrictions. This can be accomplished without restarting currently executing programs. To our knowledge, ours is the first attempt to provide provable guarantees along these lines, and the first treatment of any kind, provable or otherwise, for the third property.", "authors": ["Mikl\u00f3s Ajtai", "Cynthia Dwork", "Larry J. Stockmeyer"], "n_citation": 0, "title": "An architecture for provably secure computation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7811f347-6440-46f2-9046-42ca9839366f"}
{"abstract": "Exact exponential-time algorithms for NP-hard problems is an emerging field, and an increasing number of new results are being added continuously. Two important NP-hard problems that have been studied for decades are the treewidth and the minimum fill problems. Recently, an exact algorithm was presented by Fomin, Kratsch, and Todinca to solve both of these problems in time O*(1.9601). Their algorithm uses the notion of potential maximal cliques, and is able to list these in time O*(1.9601 n ), which gives the running time for the above mentioned problems. We show that the number of potential maximal cliques for an arbitrary graph G on n vertices is O*(1.8135 n ), and that all potential maximal cliques can be listed in O*(1.8899 n ) time. As a consequence of this results, treewidth and minimum fill-in can be computed in O*(1.8899 n ) time.", "authors": ["Yngve Villanger"], "n_citation": 0, "title": "Improved exponential-time algorithms for treewidth and minimum fill-in", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "785b4301-b795-46e4-97fe-306fcfa80d89"}
{"abstract": "Verifying the safety property of a transition system given by a term rewriting system is an undecidable problem. In this paper, we give an abstraction for the problem which is automatically generated from a given TRS by using abstract interpretation. Then we show that there are some cases in which the problem can be decided. Also we show a new decidable subclass of term rewriting systems which effectively preserves recognizability.", "authors": ["Toshinori Takai"], "n_citation": 0, "title": "A verification technique using term rewriting systems and abstract interpretation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "78abc8f5-a80b-47bb-a58e-4aff9d88acc0"}
{"abstract": "In high bit rate optical fiber communication systems, Polarization mode dispersion (PMD) is one of the main factors to signal distortion and needs to be compensated. Because PMD possesses the time-varying and the statistical properties, to establish an effective control algorithm for adaptive or automatic PMD compensation is a challenging task. Widely used control algorithms are the gradient-based peak search methods, whose main drawbacks are easy being locked into local sub-optima for compensation and no ability to resist noise. In this paper, we introduce particle swarm optimization (PSO), which is an evolutionary approach, into automatic PMD compensation as feedback control algorithm. The experiment results showed that PSO-based control algorithm has unique features of rapid convergence to the global optimum without being trapped in local sub-optima and good robustness to noise in the transmission line that had never been achieved in PMD compensation before.", "authors": ["Xiaoguang Zhang", "Lixia Xi", "Gaoyan Duan", "Li Yu", "Zhongyuan Yu", "Bojun Yang"], "n_citation": 0, "title": "An Intelligent PSO-Based Control Algorithm for Adaptive Compensation Polarization Mode Dispersion in Optical Fiber Communication Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7a6989c3-d42c-4360-bab1-cf1950f4c83b"}
{"authors": ["Richard Martin", "Edward L. Robertson"], "n_citation": 8, "title": "A comparison of frameworks for enterprise architecture modeling", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7aace8f1-1693-4335-9a18-5ac29874dc52"}
{"abstract": "The book Data model patterns : conventions of thought(1) describes a set of standard data models that can be applied to standard business situations. These patterns, it turns out, occur on several levels. At the basic level are models of the things seen in business. The patterns in the book are a bit more abstract than conventionally seen, but they do describe things that are easily recognizable to anyone : people and organizations, products, contracts, and so forth.", "authors": ["David C. Hay"], "n_citation": 0, "title": "Advanced data model patterns", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "7cc9efe5-9489-4f84-81cc-8a1963340b76"}
{"abstract": "Recently, a number of relational-based approaches for detecting the changes to XML data have been proposed to address the scalability problem of main memory-based approaches (e.g., X-Diff, XyDiff). These approaches store the XML documents in the relational database and issue SQL queries (whenever appropriate) to detect the changes. In this paper, we propose a relational-based ordered XML change detection technique (called OXONE) that uses a schema-conscious approach as the underlying storage strategy for XML data. Previous efforts have focused on detecting changes to ordered XML in an schema-oblivious storage environment. Although the schema-oblivious approach produces better result quality compared to XyDiff (a main memory-based ordered XML change detection approach), its performance degrade with increase in data size and is slower than XyDiff for smaller data set. We propose a technique to overcome these limitations. Our experimental results show that OXONE is up to 22 times faster and more scalable than the relational-based schema-oblivious approach. The performances of OXONE and XyDiff (C version) are comparable. However, more importantly, our approach is more scalable compared to XyDiff for larger datasets and has much superior the result quality of deltas than XyDiff.", "authors": ["Erwin Leonardi", "Sourav S. Bhowmick"], "n_citation": 0, "title": "OXONE: A Scalable Solution for Detecting Superior Quality Deltas on Ordered Large XML Documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7cce45c3-c4cc-4a69-a15f-46ab3c25e462"}
{"abstract": "Protein structure analysis is one of the most important research issues in the post-genomic era, and faster and more accurate query data structures for such 3-D structures are highly desired for research on proteins. This paper proposes a new data structure for indexing protein 3-D structures. For strings, there are many efficient indexing structures such as suffix trees, but it has been considered very difficult to design such sophisticated data structures against 3-D structures like proteins. Our index structure is based on the suffix trees and is called the geometric suffix tree. By using the geometric suffix tree for a set of protein structures, we can search for all of their substructures whose RMSDs (root mean square deviations) or URMSDs (unit-vector root mean square deviations) to a given query 3-D structure are not larger than a given bound. Though there are O(N 2 ) substructures, our data structure requires only O(N) space where N is the sum of lengths of the set of proteins. We propose an O(N 2 ) construction algorithm for it, while a naive algorithm would require O(N 3 ) time to construct it. Moreover we propose an efficient search algorithm. We also show computational experiments to demonstrate the practicality of our data structure. The experiments show that the construction time of the geometric suffix tree is practically almost linear to the size of the database, when applied to a protein structure database.", "authors": ["Tetsuo Shibuya"], "n_citation": 0, "title": "Geometric suffix tree : A new index structure for protein 3-D structures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7d21161c-f0b9-4aef-b075-9a4dd2247e02"}
{"abstract": "In this paper, a protocol is proposed that provides the advantages of lazy approaches, forestalling their traditionally found disadvantages. Thus, our approach reduces the abortion rates, and improves the performance of the system. It can also use a dynamic computation of the protocol threshold, approximating its results to the optimal ones. In addition, fault tolerance has been included in the algorithm, using a pseudo-optimistic approach, and avoiding to block any local activity, and minimizing the interference over any node in the system. A complete description of these algorithms is presented here. Finally, and empirical validation is also discused.", "authors": ["Luis Ir\u00fan-Briz", "Francesc D. Mu\u00f1oz-Esco\u00ed", "Josep M. Bernab\u00e9u-Aub\u00e1n"], "n_citation": 0, "title": "An improved optimistic and fault-tolerant replication protocol", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7e8bb012-edb1-435f-b259-be334aa85ef5"}
{"abstract": "Thermal deformation is a nonlinear dynamic phenomenon and is one of the significant factors for the accuracy of machine tools. In this study, a dynamic feed-forward neural network model is built to predict the thermal deformation of machine tool. The temperatures and thermal deformations data at present and past sampling time interval are used train the proposed neural model. Thus, it can model dynamic and the nonlinear relationship between input and output data pairs. According to the comparison results, the proposed neural model can obtain better predictive accuracy than that of some other neural model.", "authors": ["Chuan-Wei Chang", "Yuan Kang", "Yi-Wei Chen", "Ming-Hui Chu", "Yea-Ping Wang"], "n_citation": 0, "title": "Thermal Deformation Prediction in Machine Tools by Using Neural Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ef37316-531f-4eab-9709-4dbaf22d49ad"}
{"abstract": "Although Korean shipyards have accumulated a great amount of data, they do not have appropriate tools to utilize the data in practical works. Engineering data contains the experiences and know-how of experts. Data mining technique is useful to extract knowledge or information from the accumulated existing data. This paper presents a machine learning method based on genetic programming (GP), which can be one of the components for the realization of data mining. The paper deals with linear models of GP for regression or approximation problems when the given learning samples are not sufficient.", "authors": ["Kyung Ho Lee", "Yun Seog Yeun", "Young Soon Yang", "Jang Hyun Lee", "June Oh"], "n_citation": 0, "title": "Data Analysis and Utilization Method Based on Genetic Programming in Ship Design", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7f44ce95-9519-4fcc-9360-5bba56c32991"}
{"abstract": "In this paper, two approaches to improve the illumination robustness of the face recognition algorithms are presented, that is, Symmetrical Image Correction (SIC) and Bit-Plan Feature Fusion (BPFF). SIC can reduce bright speckles and shadows caused by over lighting. BPFF constructs a new virtual face with Bit-Plan information of face images. Generalized PCA is then applied to the virtual faces to achieve face recognition. Experiments show that, the proposed combined method can reduce the sensitivity of face recognition to illuminations using fewer projection vectors than the compared approaches.", "authors": ["Huiyuan Wang", "Yan Leng", "Z. G. Wang", "Xiaojuan Wu"], "n_citation": 50, "title": "Generalized PCA Face Recognition by Image Correction and Bit Feature Fusion", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7f79d77c-3e52-4662-9adb-bcbf1d68574f"}
{"abstract": "Given a long string of characters from a constant size (w.l.o.g. binary) alphabet we present an algorithm to determine whether its characters have been generated by a single i.i.d. random source. More specifically, consider all possible k-coin models for generating a binary string S, where each bit of S is generated via an independent toss of one of the k coins in the model. The choice of which coin to toss is decided by a random walk on the set of coins where the probability of a coin change is much lower than the probability of using the same coin repeatedly. We present a statistical test procedure which, for any given S, determines whether the a posteriori probability for k = 1 is higher than for any other k > 1. Our algorithm runs in time O(l 4  log l), where l is the length of S, through a dynamic programming approach which exploits the convexity of the a posteriori probability for k. The problem we consider arises from two critical applications in analyzing long alignments between pairs of genomic sequences. A high alignment score between two DNA sequences usually indicates an evolutionary relationship, i.e. that the sequences have been generated as a result of one or more copy events followed by random point mutations. Such sequences may include functional regions (e.g. exons) as well as non-functional ones (e.g. introns). Functional regions with critical importance exhibit much lower mutation rates than non-functional DNA (or DNA with non-critical functionality) due to selective pressures for conserving such regions. As a result, given an alignment between two highly similar genome sequences, it may be possible to distinguish functional regions from non-functional ones using variations in the mutation rate. Our test provides means for determining variations in the mutation rate and thus checking the existence of DNA regions of varying degrees of functionality. A second application for our test is in determining whether two highly similar, thus evolutionarily related, genome segments are the result of a single copy event or of a complex series of copies. This is particularly an issue in evolutionary studies of genome regions rich with repeat segments (especially non-functional tandemly repeated DNA). Our approach can be used to distinguish simple copies from complex repeats again by exploiting variations in mutation rates.", "authors": ["S. Cenk Sahinalp", "Evan E. Eichler", "Paul W. Goldberg", "Petra Berenbrink", "Tom Friedetzky", "Funda Erg\u00fcn"], "n_citation": 0, "title": "Statistical identification of uniformly mutated segments within repeats", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8162d89b-8d93-42bc-8387-0f8d93630c3f"}
{"abstract": "The transition to eOperations in the Norwegian oil and gas industry is expected to yield up to 30% reduction in costs and 10% increase in production. But new information security risks are introduced by substituting traditional offshore operations like drilling, production, delivery, etc, mostly locally operated at the offshore platforms with increasing remote onshore operation via computer networks. In eOperations, security incidents can have serious safety and performance implications. Using a generic risk matrix from case studies and a conceptual system dynamics model we explore policies for resource allocation to production and to security/safety. The simulation model allows studying the resilience of the system depending on management policies and incidents as represented in the risk matrix. We show that there is a region where the system behaviour is very sensitive to changes in resource allocation and to incidents.", "authors": ["Felicjan Rydzak", "Lars S. Breistrand", "Finn Olav Sveen", "Ying Qian", "Jose J. Gonzalez"], "n_citation": 0, "title": "Exploring Resilience Towards Risks in eOperations in the Oil and Gas Industry", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "84356019-28af-43d5-bd88-2062cff0392f"}
{"abstract": "We present a probability-based unified search framework composed of semi-supervised semantic clustering and then a constraint-based shape matching. Given a query, we propose to use an ensemble of classifiers to estimate the likelihood of the query belonging to each category by exploring the strengths from individual classifiers. Three descriptors driven by Multilevel-Detail shape descriptions have been used to generate the classifier independently. A weighted linear combination rule, called MCE (Minimum Classification Error), is adapted to support high-quality downstream application of the unified search. Experiments are conducted to evaluate the proposed framework using the Engineering Shape Benchmark database. The results have shown that search effectiveness is significantly improved by enforcing the probability-based semantic constraints to shape-based similarity retrieval.", "authors": ["Suyu Hou", "Karthik Ramani"], "n_citation": 50, "title": "A probability-based unified 3D shape search", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "85498569-52ec-4eb7-b114-af8b3e496c61"}
{"abstract": "We propose a computational model consisting of mutually connected V1, V2, and PP modules, to realize the effect of attention to the determination of border-ownership (BO) that tells on which side of a contour owns the border. The V2 module determines BO from surrounding contrast extracted by the V1 module that could be affected by top-down spatial attention from the PP module. The simulation results show that the spatial attention modifies the direction of figure, and that the direction of figure is even flipped in ambiguous figures such as the Rubin's vase, although the attention is applied only to enhance local contrast in VI. These results show that the activities of BO selective cells in V2 are modified significantly when spatial attention functions in early visual area, V1.", "authors": ["Nobuhiko Wagatsuma", "Ko Sakai"], "n_citation": 50, "title": "Spatial Attention in Early Vision Alternates Direction-of-Figure", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "866e2108-c975-4a65-81ad-18ad623cd317"}
{"abstract": "We present a new method for proving liveness and termination properties for fair concurrent programs, which does not rely on finding a ranking function or on computing the transitive closure of the transition relation. The set of states from which termination or some liveness property is guaranteed is computed by a backwards reachability analysis. A central technique for handling concurrency is a check for certain commutativity properties. The method is not complete. However, it can be seen as a complement to other methods for proving termination, in that it transforms a termination problem into a simpler one with a larger set of terminated states. We show the usefulness of our method by applying it to existing programs from the literature. We have also implemented it in the framework of Regular Model Checking, and used it to automatically verify non-starvation for parameterized algorithms.", "authors": ["Parosh Aziz Abdulla", "Bengt Jonsson", "Ahmed Rezine", "Mayank Saksena"], "n_citation": 0, "title": "Proving liveness by backwards reachability", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8ac57084-42b5-4d0d-bb38-af38ba544263"}
{"abstract": "Routing and wavelength assignment (RWA) is an important issue in WDM optical transport networks. The objective is to find lightpaths for given demands with minimal use of wavelengths. It is typically solved using combination of linear programming and graph coloring, or heuristic path selection algorithms. Such methods are complex or yield sub-optimal paths. In this paper, we propose an efficient algorithm, RWA based on Priorities of Shortest Path (RPSP). The proposed algorithm utilizes the lengths of the shortest paths for the source-destination demand pairs and assigns disjoint lightpaths to the ordered demands. The computer simulation shows that our proposed algorithm indeed requires up to 34% fewer wavelengths with almost identical running time than the previously proposed effective BGAforEDP algorithm.", "authors": ["Soon-Bin Yim", "Min Young Chung", "Hyunseung Choo", "Tae-Jin Lee"], "n_citation": 0, "title": "A Simple and Efficient RWA Algorithm Based on Priority of Edge Disjoint Paths", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8b6014bc-1ef0-407c-bc01-e0683a4b5154"}
{"abstract": "We investigate a class of set constraints defined as atomic set constraints augmented with projection. This class subsumes some already studied classes such as atomic set constraints with left-hand side projection and INES constraints. All these classes enjoy the nice property that satisfiability can be tested in cubic time. This is in contrast to several other classes of set constraints, such as definite set constraints and positive set constraints, for which satisfiability ranges from DEXPTIME-complete to NEXPTIME-complete. However, these latter classes allow set operators such as intersection or union which is not the case for the class studied here. In the case of atomic set constraints with projection one might expect that satisfiability remains polynomial. Unfortunately, we show that that the satisfiability problem for this class is no longer polynomial, but CoNP-hard. Furthermore, we devise a PSPACE algorithm to solve this satisfiability problem.", "authors": ["Witold Charatonik", "Jean-Marc Talbot"], "n_citation": 50, "title": "Atomic set constraints with projection", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8c676cee-c3d8-45a6-a538-8173dcb19b72"}
{"abstract": "Recently, the 802.11e Working Group (WG) has proposed the Hybrid Coordination Function (HCF), which has a HCF Controlled Channel Access (HCCA) and an Enhanced Distributed Coordination Access (EDCA), in order to provide QoS in WLANs. In this paper an innovative HCCA-based algorithm, which will be referred to as Power Save Feedback Based Dynamic Scheduler (PS FBDS) providing bounded delays while ensuring energy saving, has been developed. The performance of PS FBDS has been extensively investigated using ns-2 simulations; results show that the proposed algorithm is able to provide a good trade-off between QoS and power saving at both low and high network loads.", "authors": ["Gennaro Boggia", "Pietro Camarda", "Fara Favia", "Luigi Alfredo Grieco", "Saverio Mascolo"], "n_citation": 50, "title": "Providing delay guarantees and power saving in IEEE 802.11e network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8c7bcd4e-adc4-4b65-8f5e-77bde83261bb"}
{"abstract": "So far, model-based testing approaches have mostly been used in testing through various kinds of APIs. In practice, however, testing through a GUI is another equally important application area, which introduces new challenges. In this paper, we introduce a new methodology for model-based GUI testing. This includes using Labeled Transition Systems (LTSs) in conjunction with action word and keyword techniques for test modeling. We have also conducted an industrial case study where we tested a mobile device and were able to find previously unreported defects. The test environment included a standard MS Windows GUI testing tool as well as components implementing our approach. Assessment of the results from an industrial point of view suggests directions for future development.", "authors": ["Antti Kervinen", "Mika Maunumaa", "Tuula P\u00e4\u00e4kk\u00f6nen", "Mika Katara"], "n_citation": 50, "title": "Model-Based Testing Through a GUI", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8cb74144-71d4-4496-be09-e9fe68cf6219"}
{"abstract": "A precondition of existing ensemble-based distributed data mining techniques is the assumption that contributing data are identically and independently distributed. However, this assumption is not valid in many virtual organization contexts because contextual heterogeneity exists. Focusing on regression tasks, this paper proposes a context-based meta-learning technique for horizontally partitioned data with contextual heterogeneity. The predictive performance of our new approach and the state of the art techniques are evaluated and compared on both simulated and real-world data sets.", "authors": ["Yan Xing", "Michael G. Madden", "Jim Duggan", "Gerard J. Lyons"], "n_citation": 50, "title": "Context-sensitive regression analysis for distributed data", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8e046f93-a0c7-42b7-896b-61de9f8c03b4"}
{"abstract": "We generalize on-line learning to handle delays in receiving labels for instances. After receiving an instance x, the algorithm may need to make predictions on several new instances before the label for x is returned by the environment. We give two simple techniques for converting a traditional on-line algorithm into an algorithm for solving a delayed on-line problem. One technique is for instances generated by an adversary; the other is for instances generated by a distribution. We show how these techniques effect the original on-line mistake bounds by giving upper-bounds and restricted lower-bounds on the number of mistakes.", "authors": ["Chris Mesterharm"], "n_citation": 50, "title": "On-line learning with delayed label feedback", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9192cb08-c902-4826-a3b2-d8c539da4cfb"}
{"abstract": "Even with the presence of active research communities that study information system design, the term information system (IS) still lacks precise formal underpinnings. Unlike for databases, there is no agreement on what constitutes IS principles. Any significantly advanced IS contains some kind of a database system. On the other hand, any useful database system is actually an IS, providing additional services beyond simply maintaining data and running queries and updates. As a result, the distinction between issues related to databases and to ISs tends to get blurred, and it is not clear that the principles underlining the study of ISs should be different than these for databases. In this paper we argue that the interactive aspect of ISs necessitates a fundamentally different set of IS design principles, as compared to conventional database design. We provide some promising directions for a formal study of IS models, based on the observation that interactive behaviors cannot be reduced to algorithmic behaviors.", "authors": ["Dina Q. Goldin", "Srinath Srinivasa", "Bernhard Thalheim"], "n_citation": 50, "title": "IS=DBS+Interaction : Towards principles of information system design", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "92c4cd86-d0f2-4293-8c2f-421a1c9bb6ce"}
{"abstract": "One way to broadcast a popular video is to partition the video into segments, which are broadcasted on several streams periodically. The approach lets multiple users share streams; thus, the stress on the scarce bandwidth can be alleviated without sacrificing viewers' waiting time. One representative approach is the Fast Broadcasting (FB) scheme. The scheme does not obtain shortest waiting time but it can offer a more reliable video transmission on wireless networks. However, the scheme mainly supports transmission of CBR-encoded videos. In this paper, we propose a FB-based scheme for VBR-encoded videos. The scheme can smooth required bandwidth. From the simulation results, the SFB scheme has smaller required bandwidth, buffers, and disk transfer rate than the FB scheme. For a video, the maximum difference of its required bandwidth is less or equal to max(B i max - B i min), where B i max and B i min represent the maximum and minimum required bandwidth on stream i.", "authors": ["Hsiang-Fu Yu", "Hung-Chang Yang", "Yi-Ming Chen", "Li-Ming Tseng", "Chen-Yi Kuo"], "n_citation": 0, "title": "Smooth Fast Broadcasting (SFB) for compressed videos", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "931943b5-7eb5-43cf-8321-b8811d662381"}
{"authors": ["Joerg Endrullis", "R.C. de Vrijer", "Vu1012417", "Faculteit der Exacte Wetenschappen"], "n_citation": 2, "title": "Reduction Under Substitution", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "967ff950-a622-4659-822a-dac131c09a2e"}
{"abstract": "Fitness based selection procedures leave majority of population individuals idle, that is, they don't take place in any recombination operation although some of them have above average fitness values. Based on this observation, a two-phase two-strategy genetic algorithm using a conventional strategy with multiple mutation operators in the first phase is proposed. In the second phase, those individuals that are not sufficiently recombined in the first phase are reconsidered within a second strategy and recombined using multiple mutation operators only. In the second strategy, mutation operator probabilities are adaptively determined based on the cumulative fitness-gain achieved by each mutation operator over a number of generations. The proposed genetic algorithm paradigm is used for the solution of hard numerical and combinatorial optimization problems. The results demonstrate that the proposed approach performs much better than the conventional implementations in terms of solution quality and the convergence speed.", "authors": ["Adnan Acan"], "n_citation": 0, "title": "Mutation multiplicity in a panmictic two-strategy genetic algorithm", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "98dac9e0-fc0f-4412-9491-e99e3a298685"}
{"abstract": "In this we paper we consider the version of the classical Towers of Hanoi games where the game-board contains more than three pegs. For k pegs we give a 2 C k n1/(k-2)  lower bound on the number of steps necessary for transferring n disks from one peg to another. Apart from the value of the constants C k  this bound is tight.", "authors": ["Mario Szegedy"], "n_citation": 0, "title": "In how many steps the k peg version of the towers of Hanoi game can be solved", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "99c6f013-2d6f-4338-9bb1-65e993a7f07e"}
{"abstract": "This paper deals with a class of second order formulae where the only predicate is joinability modulo a conditional term rewrite system, first order variables range over ground terms and second order variables are interpreted as relations on ground terms (i.e. sets of tuples of ground terms). We define a generic algorithm that decides the satisfiability of positive second order joinability formulae when an algorithm is known to finitely represent solutions of first order formulae. When the answer is positive, the algorithm computes one particular instance for the second order variables. We apply this technique to the class of positive second order pseudo-regular formulae. The result is then a logic program that represents the instance of the second order variables. We define a transformation to translate this instance into a CTRS. This result can be used to automatically synthesize a program that defines a relation from its specification.", "authors": ["S\u00e9bastien Limet", "P. Pillot"], "n_citation": 0, "title": "Deciding satisfiability of positive second order joinability formulae", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9b261b49-a41b-4731-9328-eefc686eb3e8"}
{"abstract": "Web information coupling refers to an association of topically related web documents. This coupling is initiated explicitly by a user in a web warehouse specially designed for web information. Web information coupling provides the means to derive additional, useful information from the WWW. In this paper, we discuss and show how two web operators, i.e., global web coupling and local web coupling, are used to associate related web information from the WWW and also from multiple web tables in a web warehouse. This paper discusses various issues in web coupling such as coupling semantics, coupling-compability, and coupling evaluation.", "authors": ["Sourav S. Bhowmick", "Wee Keong Ng", "Ee Peng Lim"], "n_citation": 0, "title": "Information coupling in web databases", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "9d0a5938-c38f-4768-b584-c30b2f8e973d"}
{"abstract": "We present an approach of how to extract automatically an XML document structure from a conceptual data model that describes the content of a document. We use UML class diagrams as the conceptual model that can be represented in XML syntax (XMI). The algorithm we present in the paper is implemented as a set of rules that transform the UML class diagram into an adequate document type definition (DTD). The generation of the DTD from the semantic model corresponds with the logical XML database design with the DTD as the database schema description. Therefore, we consider many semantic issues, such as the dealing with relationships, how to express them in a DTD in order to minimize the loss of semantics. Since our algorithm is based on XSLT stylesheets, its transformation rules can be modified in a very flexible manner in order to consider different mapping strategies and requirements.", "authors": ["Thomas Kudrass", "Tobias Krumbein"], "n_citation": 0, "title": "Rule-based generation of XML DTDs from UML class diagrams", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9e1567bd-8dd7-4577-86c6-0ffb65057231"}
{"abstract": "In this paper, the application of minimal resource allocation network (MRAN) trained with Unscented Kalman Filter (UKF) to the nonlinear channel equalization problems was discussed. Using novel criterion and prune strategy, the algorithm uses online learning, and has the ability to grow and prune the hidden neurons to realize a minimal network structure. Simulation results show that the equalizer is well suited for nonlinear channel equalization problems and the proposed equalizer required short training data to attain good performance.", "authors": ["Ye Zhang", "Jianhua Wu", "Guojin Wan", "Yiqiang Wu"], "n_citation": 0, "title": "Unscented Kalman Filter-Trained MRAN Equalizer for Nonlinear Channels", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9e87ab93-bc5a-4667-ba5d-04c7973f275a"}
{"abstract": "In the operation of safety-critical systems, the sequences by which failures can lead to accidents can be many and complex. This is particularly true for the emerging class of systems known as systems of systems, as they are composed of many distributed, heterogenous and autonomous components. Performing hazard analysis on such systems is challenging, in part because it is difficult to know in advance which of the many observable or measurable features of the system are important for maintaining system safety. Hence there is a need for effective techniques to find causal relationships within these systems. This paper explores the use of machine learning techniques to extract potential causal relationships from simulation models. This is illustrated with a case study of a military system of systems.", "authors": ["Robert Alexander", "Dimitar Kazakov", "Tim Kelly"], "n_citation": 0, "title": "System of Systems Hazard Analysis Using Simulation and Machine Learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9fb63124-f627-4d93-9ef0-0e746b231fca"}
{"abstract": "This paper examines the relation between the degree of experimentally induced focal ischemia in the left-brain of 24 experimental rats and Higher Order Statistics (HOS) such as the bispectrum and the bicoherence index of scalp EEG recorded at the time of the ischemic event. The aim is to propose the assessment of HOS in non-invasive scalp EEG to facilitate identification and even classification of focal ischemic events in terms of the degree of tissue damage. The latter is achieved by a supervised, multilayer, feed-forward Artificial Neural Network (ANN). The ANN utilizes a back propagation algorithm to classify ischemic states of the brain. The target values used during the training session of the network are the degree of ischemic tissue damage (graded as serious, middle and slight) as assessed by histological and immunhistochemical methods in the brain slice of the experimental animals. The results show that the ANN can correctly identify and classify ischemic events with high precision 91.67% based on HOS measures of scalp EEG obtained during ischemia. These findings may potentially be of great scientific merit, especially due to their possibly very important medical implications: a potential non-invasive method that reliably identifies the presence and the degree of ischemia at the time of its occurrence.", "authors": ["Liyu Huang", "Weirong Wang", "Sekou Singare"], "n_citation": 0, "title": "Bispectrum Quantification Analysis of EEG and Artificial Neural Network May Classify Ischemic States", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a0aad9d1-d61b-4d0a-b5ef-0f0466d35f8e"}
{"abstract": "We present a method to generate glyphs which convey complex information in graphical form. A glyph has a linear geometry which is specified using geometric operations, each represented by characters nested in a string. This format allows several glyph strings to be concatenated, resulting in more complex geometries. We explore automatic generation of a large number of glyphs using a genetic algorithm. To measure the visual distinctness between two glyph geometries, we use the iterative closest point algorithm. We apply these methods to create two different types of representations for biological proteins, transforming the rich data describing their various characteristics into graphical form. The representations are automatically built from a finite set of glyphs, which have been created manually or using the genetic algorithm.", "authors": ["G. Pintilie", "Brigitte Tuekam", "Christopher W. V. Hogue"], "n_citation": 50, "title": "Generation of glyphs for conveying complex information, with application to protein representations", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a40b2cb3-96cc-4797-899a-d55ea02b5a20"}
{"abstract": "We propose a transmission rate prediction method of video data. The proposed method uses a Kalman filter for predicting transmission rate. It used algorithm to detect shot transition information by high speed in compressed domain in order to grasp precise shot transition of video data and classified into abrupt shot transition type and gradual shot transition type. Classified information is used as factors of Kalman filter and to predict transmission rate of video data. It predicted transmission rate with 96.2 \u223c 97.6% in the experiment.", "authors": ["Won Kim", "Hyo-Jong Jang", "Gye-Young Kim"], "n_citation": 0, "title": "Transmission Rate Prediction of VBR Motion Image Using the Kalman Filter", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a5537a07-2ae5-4959-adde-fd6a27afe1ff"}
{"abstract": "Structural recursion is a graph traversing and restructuring operation in UnQL [7], [8], a query language for semistructured data. In this paper we consider satisfiability questions mainly in the presence of schema graphs [2], [9], which are used for describing the structure of semistructured data. We introduce a new kind of simulation between schema graphs, with which the relationships can be represented in more subtle ways. By means of operational graphs we also develop a new way for defining the semantics of structural recursions. Our results give us algorithms for checking whether a given query will satisfy the restrictions imposed by schema graphs and techniques with which these can be involved in queries. Query optimizing methods are also developed.", "authors": ["Andras Benczur", "Balazs K"], "n_citation": 0, "title": "Static analysis of structural recursion in semistructured databases and its consequences", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a721ca75-ea27-48e4-900a-335a9b75dbd9"}
{"abstract": "Wavelet Trees have been introduced in [Grossi, Gupta and Vitter, SODA '03] and have been rapidly recognized as a very flexible tool for the design of compressed full-text indexes and data compressors. Although several papers have investigated the beauty and usefulness of this data structure in the full-text indexing scenario, its impact on data compression has not been fully explored. In this paper we provide a complete theoretical analysis of a wide class of compression algorithms based on Wavelet Trees. We also show how to improve their asymptotic performance by introducing a novel framework, called Generalized Wavelet Trees, that aims for the best combination of binary compressors (like, Run-Length encoders) versus non-binary: compressors (like. Huff-man and Arithmetic encoders) and Wavelet Trees of properly-designed shapes. As a corollary, we prove high-order entropy bounds for the challenging combination of Burrows-Wheeler Transform and Wavelet Trees.", "authors": ["Paolo Ferragina", "Raffaele Giancarlo", "Giovanni Manzini"], "n_citation": 0, "title": "The Myriad Virtues of Wavelet Trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a807de55-bcaf-4432-88d7-2e2a5e0fcd0a"}
{"abstract": "Over the last few years, the increase in spatial data has led to more research on spatial indexing. Most studies, however, are based on adding or changing various options in R-tree, and few studies have focused on increasing search performance via minimum bounding rectangle (MBR) compression. In a spatial index, a greater number of node entries lowers tree heights and decreases the number of node accesses, thereby shrinking disk I/O. This study proposes a new MBR compression scheme using semi-approximation (SA) and SAR-tree, which indexes spatial data using R-tree. Since SA decreases the size of MBR keys, halves QMBR enlargement, and increases node utilization, it improves the overall search performance. This study mathematically analyzes the number of node accesses and evaluates the performance of SAR-tree using real location data. The results show that the proposed index performs better than existing MBR compression schemes.", "authors": ["Jongwan Kim", "Seokjin Im", "Sang-Won Kang", "Chong-Sun Hwang"], "n_citation": 0, "title": "Spatial Indexing Based on the Semi-approximation Scheme of MBR", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a880c91e-915d-44f3-8c7a-464b6dee9ca1"}
{"abstract": "In this paper, we propose a cooperative inter-domain security management to protect access of legitimate users from the DDoS attacks exploiting randomly spoofed source IP addresses. We assume that Internet is divided into multiple domains and there exists one or more domain security manager in each domain, which is responsible for identifying hosts within the domain. The security management cooperation is achieved in two steps. First, a domain security manager forwards information regarding identified suspicious attack flows to neighboring managers. Secondly, the domain security manager verifies the attack upon receiving return messages from the neighboring managers. The management method proposed in this paper is designed not only to prevent network resources from being exhausted by the attacks but also to increase the possibility that legitimate users can fairly access the target services. Through the experiment on a test-bed, the proposed method was verified to be able to maintain high detection accuracy and to enhance the normal packet survival rate.", "authors": ["Sung Ki Kim", "Byoung Joon Min"], "n_citation": 0, "title": "Inter-domain Security Management to Protect Legitimate User Access from DDoS Attacks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a995fe33-1622-4ea9-8c9c-0df3e5246779"}
{"abstract": "This paper describes a computational approach to style conversion of cartoon animation. Our system works in two stages: the internal color shading phase, in which characters painted by the conventional method are segmented into their constituent regions and edges first, textures painted using different media are then added to the identified regions, and the line rendering phase, in which curvature of edges detected during the first phase is calculated and used to pick up turning points from edges, strokes with defined shapes and textures are then placed in the positions determined by turning points along the contour as well as internal lines of characters. Our system obviates the tedium of creating more expressive rendering by hand, and keeps frame-to-frame coherence in the resultant animations.", "authors": ["Tianzhou Chen", "Jinhui Yu", "Qunsheng Peng"], "n_citation": 0, "title": "Style conversion of cartoon animation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a9e90531-3140-41d5-bea5-081bf7ea9f8d"}
{"abstract": "Since three-dimensional computer graphics (3D-CG) technology and interaction technology should be applied to e-learning as well as games, it is important that people are able to easily create interactive contents based on 3D-CG even if they are not 3D-CG professionals. In this paper, we propose a concept for a support system for creating interactive contents. The system runs on MS Windows and uses Direct X as the file format. A content creator, by describing a script using two kinds of script files prepared by the system, can easily create 3D-CG scenes and can also control the interactions between a user and the system. As an example of content creation, we present and explain an interactive content in which a user can enter the virtual Todaiji Temple, built more than one thousand years ago, and through the interactions with computer characters can learn about the many historical events of that time.", "authors": ["Kojzi Miyazaki", "Yurika Nagai", "Ryohei Nakatsu"], "n_citation": 50, "title": "Architecture of an authoring system to support interactive contents creation for games/E-learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa657cd0-3223-4847-aa8c-9a88ec55276d"}
{"abstract": "Given a graph with edges colored RED and BLUE, we wish to sample and approximately count the number of perfect matchings with exactly k RED edges. We study a Markov chain on the space of all matchings of a graph that favors matchings with k RED edges. We show that it is rapidly mixing using non-traditional canonical paths that can backtrack, based on an algorithm for a simple combinatorial problem. We show that this chain can be used to sample dimer configurations on a 2-dimensional toroidal region with k RED edges.", "authors": ["Nayantara Bhatnagar", "Dana Randall", "Vijay V. Vazirani", "Eric Vigoda"], "n_citation": 0, "title": "Random bichromatic matchings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ac9d4a90-97ff-4108-880b-8717b20cdfea"}
{"abstract": "Constraints play an important role in conceptual modeling. In general, the specification of constraints, both static and transition, must be done in some logic-based language. Unfortunately, the resulting formulas may be complex, error-prone and difficult to read. This explain why almost all conceptual modeling languages have developed a special, easy-to-use syntax (language features) to state the most common constraints. Most features (often with graphical symbols) developed so far are concerned with static constraints (like keys, partitions or cardinalities), and very little work has been done for transition constraints. In this paper, we identify six temporal features, three related to class populations and three to attributes. The corresponding transition integrity constraints appear in almost any conceptual model and their specification is necessary and important. We believe that our temporal features make their specification simple and practical. We have named each feature, and provide a declarative and procedural formalization for them.", "authors": ["Dolors Costal", "Antoni Oliv\u00e9", "M.-R. Sancho"], "n_citation": 50, "title": "Temporal features of class populations and attributes in conceptual models", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "acb86578-8ba1-4403-a221-8c13d84667a2"}
{"abstract": "Determining the worst-case execution times (WCETs) of tasks in safety-critical hard real-time systems is a difficult problem. A combination of automatic analysis techniques with a few user annotations yields precise WCET estimates.", "authors": ["Christian Ferdinand", "Reinhold Heckmann", "Reinhard Wilhelm"], "n_citation": 0, "title": "Analyzing the Worst-Case Execution Time by Abstract Interpretation of Executable Code", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "acc4bcb7-e96b-4831-bac9-f6b46c1f955f"}
{"abstract": "To facilitate managing access control in a system, security officers increasingly write access control policies in specification languages such as XACML, and use a dedicated software component called a Policy Decision Point (PDP). To increase confidence on written policies, certain types of policy testing (often in an ad hoc way) are usually conducted, which probe the PDP with some typical requests and check PDP's responses against expected ones. This paper develops a first step toward systematic policy testing by defining and measuring policy coverage when testing policies. We have developed a coverage-measurement tool to measure policy coverage given a set of XACML policies and a set of requests. We have developed a tool for request generation, which randomly generates requests for a given set of policies, and a tool for request reduction, which greedily selects a nearly minimal set of requests for achieving the same coverage as the originally generated requests. To evaluate coverage-based request reduction and its effect on fault detection, we have conducted an experiment with mutation testing on a set of real policies. Our experimental results show that the coverage-based test reduction can substantially reduce the size of generated requests and incur only relatively low loss on fault detection. We also conduct a study on the policy coverage achieved by manually generated requests.", "authors": ["Evan Martin", "Tao Xie", "Ting Yu"], "n_citation": 0, "title": "Defining and measuring policy coverage in testing access control policies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "af377803-cee4-49a7-84cb-6dffa3d3808b"}
{"abstract": "We establish a relationship between the online mistake-bound model of learning and resource-bounded dimension. This connection is combined with the Winnow algorithm to obtain new results about the density of hard sets under adaptive reductions. This improves previous work of Fu (1995) and Lutz and Zhao (2000), and solves one of Lutz and Mayordomo's Twelve Problems in Resource-Bounded Measure (1999).", "authors": ["John M. Hitchcock"], "n_citation": 50, "title": "Online learning and resource-bounded dimension : Winnow yields new lower bounds for hard sets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "afcacbf8-3c81-436b-bdc1-6f14fb94a152"}
{"abstract": "An n-node tree has to be explored by k mobile agents (robots), starting in its root. Every edge of the tree must be traversed by at least one robot, and exploration must be completed as fast as possible. Even when the tree is known in advance, scheduling optimal collective exploration turns out to be NP-hard. We investigate the problem of distributed collective exploration of unknown trees. Not surprisingly, communication between robots influences the time of exploration. Our main communication scenario is the following: robots can communicate by writing at the currently visited node previously acquired information, and reading information available at this node. We construct an exploration algorithm whose running time for any tree is only O(k/ log k) larger than optimal exploration time with full knowledge of the tree. (We say that the algorithm has overhead O(k/ log k)). On the other hand we show that, in order to get overhead sublinear in the number of robots, some communication is necessary. Indeed, we prove that if robots cannot communicate at all, then every distributed exploration algorithm works in time Q(k) larger than optimal exploration time with full knowledge, for some trees.", "authors": ["Pierre Fraigniaud", "Leszek Gasieniec", "Dariusz R. Kowalski", "Andrzej Pelc"], "n_citation": 0, "title": "Collective tree exploration", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b5c92937-0b4e-4363-b1da-8e4b11921def"}
{"abstract": "We consider the problem of clustering data into k > 2 clusters given complex relations - going beyond pairwise - between the data points. The complex n-wise relations are modeled by an n-way array where each entry corresponds to an affinity measure over an n-tuple of data points. We show that a probabilistic assignment of data points to clusters is equivalent, under mild conditional independence assumptions, to a super-symmetric non-negative factorization of the closest hyper-stochastic version of the input n-way affinity array. We derive an algorithm for finding a local minimum solution to the factorization problem whose computational complexity is proportional to the number of n-tuple samples drawn from the data. We apply the algorithm to a number of visual interpretation problems including 3D multi-body segmentation and illumination-based clustering of human faces.", "authors": ["Amnon Shashua", "Ron Zass", "Tamir Hazan"], "n_citation": 0, "title": "Multi-way Clustering Using Super-Symmetric Non-negative Tensor Factorization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b9b31bed-1556-4086-82c9-be53786785b2"}
{"abstract": "Domain-specific modelling has proved its worth for improving development speed and dependability of applications. By raising the level of abstraction away from the code and towards the structure and behaviour of an application, it also offers good possibilities for generating the same application from the same models, but for a wide variety of client platforms. This paper examines one example of domain-specific modelling for an embedded application, and how that was extended to a mobile platform.", "authors": ["Steven Kelly", "Risto Pohjonen"], "n_citation": 0, "title": "Domain-specific modelling for cross-platform product families", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b9cdb670-0db3-4cfd-b7c7-c30901164bb2"}
{"abstract": "Event detection is a very important area of research that discovers new events reported in a stream of text documents. Previous research in event detection has largely focused on finding the first story and tracking the events of a specific topic. A topic is simply a set of related events defined by user supplied keywords with no associated semantics and little domain knowledge. We therefore introduce the Anticipatory Event Detection (AED) problem: given some user preferred event transition in a topic, detect the occurence of the transition for the stream of news covering the topic. We confine the events to come from the same application domain, in particular, mergers and acquisitions. Our experiments showed that classical cosine similarity method fails for the AED task, whereas our conceptual model-based approach, through the use of domain knowledge and named entity type assignments, seems promising. We show experimentally that an AED voting classifier operating on a vector representation with name entities replaced by types performed AED successfully.", "authors": ["Qi He", "Kuiyu Chang", "Ee-Peng Lim"], "n_citation": 50, "title": "A Model for Anticipatory Event Detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bc4c4499-c41d-406a-b88e-3752dbbe480f"}
{"abstract": "Why is deciding simulation preorder (and simulation equivalence) computationally harder than deciding bisimulation equivalence on almost all known classes of processes? We try to answer this question by describing two general methods that can be used to construct direct one-to-one polynomial-time reductions from bisimulation equivalence to simulation preorder (and simulation equivalence). These methods can be applied to many classes of finitely generated transition systems, provided that they satisfy certain abstractly formulated conditions. Roughly speaking, our first method works for all classes of systems that can test for 'non-enabledness' of actions, while our second method works for all classes of systems that are closed under synchronization.", "authors": ["Anton\u00edn Ku\u010dera", "Richard Mayr"], "n_citation": 50, "title": "Why is simulation harder than bisimulation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "bd0356d8-7a81-4574-8d9b-4518ec438737"}
{"abstract": "We start from a simple asymptotic result for the problem of on-line regression with the quadratic loss function: the class of continuous limited-memory prediction strategies admits a leading prediction strategy, which not only asymptotically performs at least as well as any continuous limited-memory strategy but also satisfies the property that the excess loss of any continuous limited-memory strategy is determined by how closely it imitates the leading strategy. More specifically, for any class of prediction strategies constituting a reproducing kernel Hilbert space we construct a leading strategy, in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy. This result is extended to the loss functions given by Bregman divergences and by strictly proper scoring rules.", "authors": ["Vladimir Vovk"], "n_citation": 8, "title": "Leading strategies in competitive on-line prediction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "be04ab3a-7372-4adc-9115-77612ebcf8a5"}
{"abstract": "The determination of the player's gestures and actions in sports video is a key task in automating the analysis of the video material at a high level. In many sports views, the camera covers a large part of the sports arena, so that the resolution of player's region is low. This makes the determination of the player's gestures and actions a challenging task, especially if there is large camera motion. To overcome these problems, we propose a method based on curvature scale space templates of the player's silhouette. The use of curvature scale space makes the method robust to noise and our method is robust to significant shape corruption of a part of player's silhouette. We also propose a new recognition method which is robust to noisy sequences of data and needs only a small amount of training data.", "authors": ["Myung-Cheol Roh", "Bill Christmas", "Joseph Kittler", "Seong-Whan Lee"], "n_citation": 50, "title": "Robust Player Gesture Spotting and Recognition in Low-Resolution Sports Video", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bf530327-fffa-4939-9202-e5b38c62e8d7"}
{"abstract": "Taking Dunhuang MoGao Frescoes as research background, a new method to generate the sketch for Dunhuang frescoes is proposed. Learning from the traditional manual imitation procedure, we first get the contour line of a Dunhuang fresco which is composed of many connected curves, and then render the strokes by learning styles from examples. For several example strokes, the style of example strokes are captured and expressed as style function. Given a target curve, a new stroke can be generated with the style of one example and moreover, the style of the target stroke can be further customized into different styles similar to the examples by interpolation of styles. This research has the potential to provide a computer aided tool for art historians to do imitation work, and improve the efficiency of imitation.", "authors": ["Jianming Liu", "Dongming Lu", "Xifan Shi"], "n_citation": 0, "title": "Interactive sketch generation for dunhuang frescoes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c02da73a-ec03-4849-bace-7ac861150028"}
{"abstract": "We investigate the problem of model checking Interval-valued Discrete-time Markov Chains (IDTMC). IDTMCs are discrete-time finite Markov Chains for which the exact transition probabilities are not known. Instead in IDTMCs, each transition is associated with an interval in which the actual transition probability must lie. We consider two semantic interpretations for the uncertainty in the transition probabilities of an IDTMC. In the first interpretation, we think of an IDTMC as representing a (possibly uncountable) family of (classical) discrete-time Markov Chains, where each member of the family is a Markov Chain whose transition probabilities lie within the interval range given in the IDTMC. This semantic interpretation we call Uncertain Markov Chains (UMC). In the second semantics for an IDTMC, which we call Interval Markov Decision Process (IMDP), we view the uncertainty as being resolved through non-determinism. In other words, each time a state is visited, we adversarially pick a transition distribution that respects the interval constraints, and take a probabilistic step according to the chosen distribution. We show that the PCTL model checking problem for both Uncertain Markov Chain semantics and Interval Markov Decision Process semantics is decidable in PSPACE. We also prove lower bounds for these model checking problems.", "authors": ["Koushik Sen", "Mahesh Viswanathan", "Gul Agha"], "n_citation": 0, "title": "Model-checking markov chains in the presence of uncertainties", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c2ce4740-a17f-48e5-a27f-3adb16004844"}
{"abstract": "In many languages, the English word computer is often literally translated to the counting machine. Counting is apparently the most elementary operation that a computer can do, and thus it should be trivial to a computer to count. This, however, is a misconception. The apparently simple operation of enumeration and counting is actually computationally hard. It is also one of the most important elementary operation for many data mining tasks. We show how capital counting is for a variety of data mining applications and how this complex task can be achieved with acceptable efficiency.", "authors": ["Osmar R. Za\u00efane"], "n_citation": 0, "title": "Relevance of counting in data mining tasks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c371a305-ed1c-4622-bcb8-7a3954833f89"}
{"abstract": "Replays are key cues for events detection in sport videos since they are the immediate consequence of highlights or important events happened in sports. In many sports videos, replays are usually sandwiched with two identical logo transitions, prompt the beginning and end of a replay. A logo transition is a kind of special digital video effects, usually contains 12-35 consecutive frames, describe a flying or variable object. In this paper, a novel automatic logo detection approach is proposed. It contains two main stages: a logo transition template is automatically learned by dynamic programming and unsupervised clustering, a key frame is also extracted; then the extracted key frame and the learned logo template are used jointly to detect logos in sports videos. The optical flow features are used to depict the motion characteristics of the logo transitions. Experiments on different types of sports videos show that the proposed approach can reliably detect logos in sports videos efficiently.", "authors": ["Hongliang Bai", "Wei Hu", "Tao Wang", "Xiaofeng Tong", "Changping Liu", "Yimin Zhang"], "n_citation": 0, "title": "A Novel Sports Video Logo Detector Based on Motion Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c4c1d3db-6256-4d5d-b439-a0b1af728b0a"}
{"abstract": "We consider a general class of forecasting protocols, called linear protocols, and discuss several important special cases, including multi-class forecasting. Forecasting is formalized as a game between three players: Reality, whose role is to generate objects and their labels; Forecaster, whose goal is to predict the labels; and Skeptic, who tries to make money on any lack of agreement between Forecaster's predictions and the actual labels. Our main mathematical result is that for any continuous strategy for Skeptic in a linear protocol there exists a strategy for Forecaster that does not allow Skeptic's capital to grow. This result is a meta-theorem that allows one to transform any constructive law of probability in a linear protocol into a forecasting strategy whose predictions are guaranteed to satisfy this law. We apply this meta-theorem to a weak law of large numbers in inner product spaces to obtain a version of the K29 prediction algorithm for linear protocols and show that this version also satisfies the attractive properties of proper calibration and resolution under a suitable choice of its kernel parameter, with no assumptions about the way the data is generated.", "authors": ["Vladimir Vovk", "Ilia Nouretdinov", "Akimichi Takemura", "Glenn Shafer"], "n_citation": 50, "title": "Defensive forecasting for linear protocols", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c4dce642-73db-4384-bbcb-7e4ff9393602"}
{"abstract": "We examine how to induce selfish heterogeneous users in a multicommodity network to reach an equilibrium that minimizes the social cost. In the absence of centralized coordination, we use the classical method of imposing appropriate taxes (tolls) on the edges of the network. We significantly generalize previous work [20,13,9] by allowing user demands to be elastic. In this setting the demand of a user is not fixed a priori but it is a function of the routing cost experienced, a most natural assumption in traffic and data networks.", "authors": ["George Karakostas", "Stavros G. Kolliopoulos"], "n_citation": 50, "title": "Edge Pricing of Multicommodity Networks for Selfish Users with Elastic Demands", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c7b15712-c6c6-4101-8de2-642008671cff"}
{"abstract": "We show that the marked version of the Post Correspondence Problem where the words on a list are required to differ in the first letter, is decidable. On the other hand, PCP remains undecidable if we only require the words to differ in the first two letters. Thus we locate the decidability/undecidability-boundary between marked and 2-marked PCP.", "authors": ["Vesa Halava", "Mika Hirvensalo", "R.M. de Wolf"], "n_citation": 50, "references": ["c8c6546e-15a9-4b61-91cb-793139bd6426"], "title": "Decidability and undecidability of marked PCP", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "c8a67bfd-e2cf-4831-adf8-a4fefbb05e80"}
{"abstract": "The objective of this work is the detection of object classes, such as airplanes or horses. Instead of using a model based on salient image fragments, we show that object class detection is also possible using only the object's boundary. To this end, we develop a novel learning technique to extract class-discriminative boundary fragments. In addition to their shape, these ''codebook entries also determine the object's centroid (in the manner of Leibe et al. [19]). Boosting is used to select discriminative combinations of boundary fragments (weak detectors) to form a strong Boundary-Fragment-Model (BFM) detector. The generative aspect of the model is used to determine an approximate segmentation. We demonstrate the following results: (i) the BFM detector is able to represent and detect object classes principally defined by their shape, rather than their appearance; and (ii) in comparison with other published results on several object classes (airplanes, cars-rear, cows) the BFM detector is able to exceed previous performances, and to achieve this with less supervision (such as the number of training images).", "authors": ["Andreas Opelt", "Axel Pinz", "Andrew Zisserman"], "n_citation": 0, "title": "A Boundary-Fragment-Model for Object Detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c8de4f5b-f2d7-4ef2-94dd-8960d0e12fa8"}
{"abstract": "This paper addresses the problem of generating symbolic test cases for testing the conformance of a black-box implementation with respect to a specification, in the context of reactive systems. The challenge we consider is the selection of test cases according to a test purpose, which is here a set of scenarios of interest that one wants to observe during test execution. Because of the interactions that occur between the test case and the implementation, test execution can be seen as a game involving two players, in which the test case attempts to satisfy the test purpose. Efficient solutions to this problem have been proposed in the context of finite-state models, based on the use of fixpoint computations. We extend them in the context of infinite-state symbolic models, by showing how approximate fixpoint computations can be used in a conservative way. The second contribution we provide is the formalization of a quality criterium for test cases, and a result relating the quality of a generated test case to the approximations used in the selection algorithm.", "authors": ["Bertrand Jeannet", "Thierry J\u00e9ron", "Vlad Rusu", "Elena Zinovieva"], "n_citation": 0, "title": "Symbolic test selection based on approximate analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c9ba9742-6ebb-4c07-aa32-4bff01093e7d"}
{"abstract": "We propose a homography estimation method from the contours of planar regions. Standard projective invariants such as cross ratios or canonical frames based on hot points obtained from local differential properties are extremely unstable in real images suffering from pixelization, thresholding artifacts, and other noise sources. We explore alternative constructions based on global convexity properties of the contour such as discrete tangents and concavities. We show that a projective frame can be robustly extracted from arbitrary shapes with at least one appreciable concavity. Algorithmic complexity and stability are theoretically discussed and experimentally evaluated in a number of real applications including projective shape matching, alignment and pose estimation. We conclude that the procedure is computationally efficient and notably robust given the ill-conditioned nature of the problem.", "authors": ["Alberto Ruiz", "Pedro E. L\u00f3pez de Teruel", "Lorenzo Fern\u00e1ndez"], "n_citation": 0, "title": "Robust Homography Estimation from Planar Contours Based on Convexity", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ca96a8d6-0f50-4253-b535-c0ef0d49c2bb"}
{"abstract": "We study learning in a modified EXACT model, where the oracles are corrupt and only few of the presented attributes are relevant. Both modifications were already studied in the literature, and efficient solutions were found to most of their variants. Nonetheless, their reasonable combination is yet to be studied, and combining the existing solutions either fails or works with complexity that can be significantly improved. In this paper we prove equivalence of EXACT learning attribute-efficiently with and without corrupt oracles. For each of the possible scenarios we describe a generic scheme that enables learning in these cases using modifications of the standard learning algorithms. We also generalize and improve previous non attribute-efficient algorithms for learning with corrupt oracles.", "authors": ["Rotem Bennet", "Nader H. Bshouty"], "n_citation": 0, "title": "Learning attribute-efficiently with corrupt oracles", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cc988f3e-ad76-4f1f-b749-5ab0b4e6cbed"}
{"abstract": "In the 1.5-dimensional terrain guarding problem we are given as input an x-monotone chain (the terrain) and asked for the minimum set of guards (points on the terrain) such that every point on the terrain is seen by at least one guard. It has recently been shown that the 1.5-dimensional terrain guarding problem is approximable to within a constant factor [3, 7], though no attempt has been made to minimize the approximation factor. We give a 4-approximation algorithm for the 1.5D terrain guarding problem that runs in quadratic time. Our algorithm is faster, simpler, and has a better worst-case approximation factor than previous algorithms.", "authors": ["James King"], "n_citation": 0, "title": "A 4-approximation algorithm for guarding 1.5-dimensional terrains", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cdac533b-f263-4c5e-a44e-718a66ae4330"}
{"abstract": "Given a set S of n pre-placed radio-stations and a source station s*, we consider two variations of minimum cost homogeneous range assignment problem for the 2-hops broadcast from s* to all the members in S, where the range assigned to a radio-station is either zero or a fixed value r. Thus, the cost of range assignment is proportional to the number of radio-stations having range r. The variations we study are (i) find the value of r and identify the radio-stations with range r such that the total cost is minimum, and (ii) given a real number r, check whether homogeneous 2-hops broadcast from s* to the members in S is possible with range r, and if so, then identify the smallest subset of S whom range r is to be assigned such that 2-hops broadcast from s* is possible. The first problem is solved in O(n 3 ) time and space. But, the second one seems to be hard. We present a 3-factor approximation algorithm for this problem, which runs in O(n 2 ) time. An O(n 2 ) time heuristic algorithm for the second problem is also presented. Most of the times, it produces optimum result for randomly placed radio-stations.", "authors": ["Gautam Das", "Sandip Das", "Subhas C. Nandy"], "n_citation": 0, "title": "Homogeneous 2-Hops Broadcast in 2D", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d197ba92-bbfd-4001-95f8-26f0a3aad5a9"}
{"abstract": "We give a dichotomy theorem for the problem of counting homomorphisms to directed acyclic graphs. H is a fixed directed acyclic graph. The problem is, given an input digraph G, to determine how many homomorphisms there are from G to H. We give a graph-theoretic classification, showing that for some digraphs H, the problem is in P and for the rest of the digraphs H the problem is #P-complete. An interesting feature of the dichotomy, absent from related dichotomy results, is the rich supply of tractable graphs H with complex structure.", "authors": ["Martin E. Dyer", "Leslie Ann Goldberg", "Mike Paterson"], "n_citation": 50, "title": "On Counting Homomorphisms to Directed Acyclic Graphs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d56234a7-2356-4383-b0d0-22858aa3a7f8"}
{"abstract": "Folksonomies are classification schemes that emerge from the collective actions of users who tag resources with an unrestricted set of key terms. There has been a flurry of activity in this domain recently with a number of high profile web sites and search engines adopting the practice. They have sparked a great deal of excitement and debate in the popular and technical literature, accompanied by a number of analyses of the statistical properties of tagging behavior. However, none has addressed the deep nature of folksonomies. What is the nature of a tag? Where does it come from? How is it related to a resource? In this paper we present a study in which the linguistic properties of folksonomies reveal them to contain, on the one hand, tags that are similar to standard categories in taxonomies. But on the other hand, they contain additional tags to describe class properties. The implications of the findings for the relationship between folksonomy and ontology are discussed.", "authors": ["Csaba Veres"], "n_citation": 64, "title": "The language of folksonomies : What tags reveal about user classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "db5797ab-1124-4d69-8bc9-35ed37b6b04e"}
{"abstract": "The use of firewalls and network intrusion detection systems (NIDSs) is the dominant method to survey and guarantee the security policy in current corporate networks. On the one hand, firewalls are traditional security components which provide means to filter traffic within corporate networks, as well as to police the incoming and outcoming interaction with the Internet. On the other hand, NIDSs are complementary security components used to enhance the visibility level of the network, pointing to malicious or anomalous traffic. To properly configure both firewalls and NIDSs, it is necessary the use of a set of configuration rules, i.e., a set of filtering or alerting rules. Nevertheless, the existence of anomalies within the set of configuration rules of both firewalls and NIDSs is very likely to degrade the network security policy. The discovering and removal of these anomalies is a serious and complex problem to solve. In this paper, we present a set of mechanisms for such a management.", "authors": ["Joaquin Garcia-Alfaro", "Fr\u00e9d\u00e9ric Cuppens", "Nora Cuppens-Boulahia"], "n_citation": 0, "title": "Towards Filtering and Alerting Rule Rewriting on Single-Component Policies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dc87c034-b98f-4316-887e-2216a02b9ddd"}
{"abstract": "Defending against distributed denial-of-service (DDoS) attack is one of the hardest security problems on the internet today. In this paper, we investigate a fast search algorithm for IP trace back, which is similar to the Viterbi algorithm and it has simple implementation. The approach is capable of tracking back attacks as quickly as possible. Our research can feature low network and router overhead, and support incremental deployment.", "authors": ["Jia Hou", "Moon Ho Lee"], "n_citation": 0, "title": "A fast search and advanced marking scheme for network IP traceback model", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "dd2b9500-7aad-4d85-80ed-97a858b24c6c"}
{"abstract": "A stochastic graph game is played by two players on a game graph with probabilistic transitions. We consider stochastic graph games with \u03c9-regular winning conditions specified as Rabin or Streett objectives. These games are NP-complete and coNP-complete, respectively. The value of the game for a player at a state s given an objective \u03a6 is the maximal probability with which the player can guarantee the satisfaction of \u03a6 from s. We present a strategy-improvement algorithm to compute values in stochastic Rabin games, where an improvement step involves solving Markov decision processes (MDPs) and nonstochastic Rabin games. The algorithm also computes values for stochastic Streett games but does not directly yield an optimal strategy for Streett objectives. We then show how to obtain an optimal strategy for Streett objectives by solving certain nonstochastic Streett games.", "authors": ["Krishnendu Chatterjee", "Thomas A. Henzinger"], "n_citation": 0, "title": "Strategy improvement for stochastic rabin and streett games", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ded5e98c-28a8-4ba4-a28e-92790a0114d7"}
{"abstract": "A non-decreasing sequence of n integers is the degree sequence of a 1-tree (i.e., an ordinary tree) on n vertices if and only if there are least two 1's in the sequence, and the sum of the elements is 2(n - 1). We generalize this result in the following ways. First, a natural generalization of this statement is a necessary condition for k-trees, and we show that it is not sufficient for any k > 1. Second, we identify non-trivial sufficient conditions for the degree sequences of 2-trees. We also show that these sufficient conditions are almost necessary using bounds on the partition function p(n) and probabilistic methods. Third, we generalize the characterization of degrees of 1-trees in an elegant and counter-intuitive way to yield integer sequences that characterize k-trees, for all k.", "authors": ["Zvi Lotker", "Debapriyo Majumdar", "N. S. Narayanaswamy", "Ingmar Weber"], "n_citation": 0, "title": "Sequences Characterizing k-Trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "def60c6e-2594-452a-820c-26b64d155b94"}
{"abstract": "This paper presents a novel method to solve multi-view face detection problem by Error Correcting Output Codes (ECOC). The motivation is that face patterns can be divided into separated classes across views, and ECOC multi-class method can improve the robustness of multi-view face detection compared with the view-based methods because of its inherent error-tolerant ability. One key issue with ECOC-based multi-class classifier is how to construct effective binary classifiers. Besides applying ECOC to multi-view face detection, this paper emphasizes on designing efficient binary classifiers by learning informative features through minimizing the error rate of the ensemble ECOC multi-class classifier. Aiming at designing efficient binary classifiers, we employ spatial histograms as the representation, which provide an over-complete set of optional features that can be efficiently computed from the original images. In addition, the binary classifier is constructed as a coarse to fine procedure using fast histogram matching followed by accurate Support Vector Machine (SVM). The experimental results show that the proposed method is robust to multi-view faces, and achieves performance comparable to that of state-of-the-art approaches to multi-view face detection.", "authors": ["Hongming Zhang", "Wen Gao", "Xilin Chen", "Shiguang Shan", "Debin Zhao"], "n_citation": 50, "title": "Robust Multi-view Face Detection Using Error Correcting Output Codes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e00d2bf0-fbdc-4083-9530-50956601341e"}
{"abstract": "The purpose of distributed system-level diagnosis is to have each fault-free nodes determine the state of all nodes of system. The paper presents a Multi-level distributed system-level diagnosis, which considers the problem of achieving scalability and performance tuning for distributed diagnosis. Existing work is aimed to reduce either diagnosis latency or network utilization but scales poorly. A diagnosis algorithm, called Multi-level DSD, is presented to provide scalability, which controls both latency and network utilization in fully connected networks. The algorithm is scalable in the sense that it is possible to diagnose system with large number of processing elements (nodes) by tuning diagnosis parameters. The diagnosis algorithm allows tuning of diagnosis performance to lever latency message cost trade-off. Multi-level DSD divides system in clusters of nodes, where each cluster is either a single node or a group of clusters. Cluster diagnoses itself by running a cluster diagnosis algorithm between its sub clusters. Clusters at each level runs same cluster diagnosis algorithm.", "authors": ["Paritosh Chandrapal", "P. Kumar"], "n_citation": 0, "title": "A scalable multi-level distributed system-level diagnosis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e1265cea-eeab-4824-9c26-d0b520f3bd60"}
{"abstract": "A novel machine learning based approach was proposed recently as a complementary technique to the acceleration based methods for verifying infinite state systems. In this method, the set of states satisfying a fixpoint property is learnt as opposed to being iteratively computed. We extend the machine learning based approach to verifying general \u03c9-regular properties that include both safety and liveness. To achieve this, we first develop a new fixpoint based characterization for the verification of w-regular properties. Using this characterization, we present a general framework for verifying infinite state systems. We then instantiate our approach to the context of regular model checking where states are represented as strings over a finite alphabet and the transition relation of the system is given as a finite state transducer; unlike previous learning based algorithms, we make no assumption about the transducer being length-preserving. Using Angluin's L* algorithm for learning regular languages, we develop an algorithm for verification of w-regular properties of such infinite state systems. The algorithm is a complete verification procedure for systems for whom the fixpoint can be represented as a regular set. We have implemented the technique in a tool called LEVER and use it to analyze some examples.", "authors": ["Abhay Vardhan", "Koushik Sen", "Mahesh Viswanathan", "Gul Agha"], "n_citation": 0, "title": "Using language inference to verify omega-regular properties", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e20b5baf-3d7d-4fec-8e9b-537f75463514"}
{"abstract": "We present a novel approach to gait recognition that considers gait sequences as cyclostationary processes on a shape space of simple closed curves. Consequently, gait analysis reduces to quantifying differences between statistics underlying these stochastic processes. The main steps in the proposed approach are: (i) off-line extraction of human silhouettes from IR video data, (ii) use of piecewise-geodesic paths, connecting the observed shapes, to smoothly interpolate between them, (iii) computation of an average gait cycle within class (i.e. associated with a person) using average shapes, (iv) registration of average cycles using linear and nonlinear time scaling, (iv) comparisons of average cycles using geodesic lengths between the corresponding registered shapes. We illustrate this approach on infrared video clips involving 26 subjects.", "authors": ["David Kaziska", "Anuj Srivastava"], "n_citation": 0, "title": "Cyclostationary Processes on Shape Spaces for Gait-Based Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e30ba859-5c42-4b28-830d-d8fdc609976d"}
{"abstract": "A novel adaptive and patch-based approach is proposed for image regularization and representation. The method is unsupervised and based on a pointwise selection of small image patches of fixed size in the variable neighborhood of each pixel. The main idea is to associate with each pixel the weighted sum of data points within an adaptive neighborhood and to use image patches to take into account complex spatial interactions in images. In this paper, we consider the problem of the adaptive neighborhood selection in a manner that it balances the accuracy of the estimator and the stochastic error, at each spatial position. Moreover, we propose a practical algorithm with no hidden parameter for image regularization that uses no library of image patches and no training algorithm. The method is applied to both artificially corrupted and real images and the performance is very close, and in some cases even surpasses, to that of the best published denoising methods.", "authors": ["Charles Kervrann", "J\u00e9r\u00f4me Boulanger"], "n_citation": 0, "title": "Unsupervised Patch-Based Image Regularization and Representation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e3e325c0-616f-4edc-a7ba-93394da6336c"}
{"abstract": "Determining the causal relation among attributes in a domain is a key task in the data mining and knowledge discovery. In this paper, we applied a causal discovery algorithm to the business traveler expenditure survey data [1]. A general class of causal models is adopted in this paper to discover the causal relationship among continuous and discrete variables. All those factors which have direct effect on the expense pattern of travelers could be detected. Our discovery results reinforced some conclusions of the rough set analysis and found some new conclusions which might significantly improve the understanding of expenditure behaviors of the business traveler.", "authors": ["Gang Li"], "n_citation": 0, "title": "A causal analysis for the expenditure data of business travelers", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "e65b90ea-5c8d-4bc8-8798-97798957681e"}
{"abstract": "We relate the expressive power of Datalog and constraint satisfaction with infinite templates. The relationship is twofold: On the one hand, we prove that every non-empty problem that is closed under disjoint unions and has Datalog width one can be formulated as a constraint satisfaction problem (CSP) with a countable template that is \u03c9-categorical. Structures with this property are of central interest in classical model theory. On the other hand, we identify classes of CSPs that can be solved in polynomial time with a Datalog program. For that, we generalise the notion of the canonical Datalog program of a CSP, which was previously defined only for CSPs with finite templates by Feder and Vardi. We show that if the template F is \u03c9-categorical, then CSP(F) can be solved by an (l, k)-Datalog program if and only if the problem is solved by the canonical (l, k)-Datalog program for F. Finally, we prove algebraic characterisations for those w-categorical templates whose CSP has Datalog width (1, k), and for those whose CSP has strict Datalog width l.", "authors": ["Manuel Bodirsky", "V\u00edctor Dalmau"], "n_citation": 0, "title": "Datalog and constraint satisfaction with infinite templates", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e6c87da7-a085-4853-9597-2abbb34fbb38"}
{"abstract": "This paper presents results of the plausibility evaluation of computer-generated emotions and moods. They are generated by ALMA (A Layered Model of Affect), a real-time computational model of affect, designed to serve as a modular extension for virtual humans. By a unique integration of psychological models of affect, it provides three major affect types: emotions, moods and personality that cover short, medium, and long term affect. The evaluation of computer-generated affect is based on textual dialog situations in which at least two characters are interacting with each other. In this setup, elicited emotions or the change of mood are defined as consequences of dialog contributions from the involved characters. The results indicate that ALMA provides authentic believable emotions and moods. They can be used for modules that control cognitive processes and physical behavior of virtual humans in order to improve their lifelikeness and their believable qualities.", "authors": ["Patrick Gebhard", "Kerstin H. Kipp"], "n_citation": 50, "title": "Are computer-generated emotions and moods plausible to humans?", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e9065d50-c35e-4a0e-a18b-bac16ee5a828"}
{"abstract": "Linear arithmetic decision procedures form an important part of theorem provers for program verification. In most verification benchmarks, the linear arithmetic constraints are dominated by simple difference constraints of the form x \u2264 y + c. Sparse linear arithmetic (SLA) denotes a set of linear arithmetic constraints with a very few non-difference constraints. In this paper, we propose an efficient decision procedure for SLA constraints, by combining a solver for difference constraints with a solver for general linear constraints. For SLA constraints, the space and time complexity of the resulting algorithm is dominated solely by the complexity for solving the difference constraints. The decision procedure generates models for satisfiable formulas. We show how this combination can be extended to generate implied equalities. We instantiate this framework with an equality generating Simplex as the linear arithmetic solver, and present preliminary experimental evaluation of our implementation on a set of linear arithmetic benchmarks.", "authors": ["Shuvendu K. Lahiri", "Madanlal Musuvathi"], "n_citation": 0, "title": "Solving sparse linear constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e9a330f0-6e71-4c3b-ab35-26e860fdf201"}
{"abstract": "This paper describes a grid-based deployment algorithm to maximize the network coverage in mobile sensor networks. The algorithm divides target areas with scattered mobile sensors into some grids and selects a grid header in each grid. The grid header is responsible for collecting the information of nodes in the grid. If the number of nodes in the grid is less than the average number of nodes, the grid header requests the node movement to neighboring grids. After the node movement from the neighboring grids, if the number of nodes in the grid is below the average, it requests the node movement to the two-hop grids apart through the neighboring grids. After the node movement from the two-hop grids, the grid header performs to relocate its members in the grid. By performing three procedures, the algorithm is able to deploy uniformly the given nodes in the target area. This paper presents the operation of a balanced deployment algorithm and shows performance evaluation of the deployment algorithm.", "authors": ["Kil-Woong Jang", "Byung-Soon Kim"], "n_citation": 0, "title": "A Balanced Deployment Algorithm for Mobile Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ebc99374-f837-4e55-af5c-21d3d1baf81c"}
{"abstract": "Though new technological trends and paradigms arise for developing complex software systems, systematic reuse continues to be an elusive goal. In this context, the adoption of Commercial Off-The-Shelf (COTS) technologies introduces many challenges that still have not been fully overcome, such as the lack of comprehensive mechanisms to record and manage the required information for supporting COTS components selection. In this paper we present a domain analysis approach for gathering the information needed to describe COTS market segments as required for effective COTS components selection. Due to the diversity of the information to capture, we propose different dimensions of interest for COTS components selection that are covered by different domain models. These models are articulated by means of a single framework based on a widespread software quality standard.", "authors": ["Claudia P. Ayala", "Xavier Franch"], "n_citation": 0, "title": "Domain Analysis for Supporting Commercial Off-the-Shelf Components Selection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "eca1f0a0-f535-4725-a096-23f2920f8084"}
{"abstract": "Artifacts in engineering design are structurally complex and may be represented in software as recursively composite objects. Due to the evolutionary nature of the design process each artifact and its components may evolve through several versions. This paper describes enhanced database system facilities that are used to group mutually consistent component versions together into useful configurations. The versioning system includes integrity management facilities that allow evolving design constraints to be captured flexibly at individual component/object level. In order to permit evolution, integrity constraints are represented within versionable objects, so-called constraint version objects (CVOs). Inter-dependency constraints can be modelled to express the consistency semantics necessary to combine component artifact versions into useful configurations. The evolution of these configurations can be captured in the database, as configurations are also represented as versioned objects.", "authors": ["Tom W. Carnduff", "Jeevani Samantha Goonetillake"], "n_citation": 0, "title": "Managing configuration with evolving constraints in design databases", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "ecdcd3ca-2311-4a25-a4be-779809b0fc5a"}
{"abstract": "We present three perspectives of the use of formalism in the construction of High-Integrity Embedded Real-time Systems. In the first, we describe the long-term research aims. The scope is the entire system, the goal is to demonstrate intentional correctness, and the emphasis is on scientific certainty. In the second, we present medium-term research aims. The scope is more on the software in the system, and the emphasis shifts to the notion of engineering confidence. Following on from the medium-term view we propose a set of challenges for formal engineering methods research, based on our perception of the technical issues surrounding the provision of viable engineering solutions. In the third perspective we discuss the short term. In particular, we describe how our recent research is attempting to meet some of the proposed challenges, as a first step towards our medium and long-term aspirations.", "authors": ["John A. McDermid", "Andy Galloway"], "n_citation": 0, "title": "Three Perspectives in Formal Engineering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ed7dd706-7994-4b19-aa4b-1af9c909d515"}
{"abstract": "LDAP (Lightweight Directory Access Protocol) directories are being widely used on the Web, for white pages information, user profiles, etc. The advantages LDAP offers are (i) the support for highly distributed data on the Web while still keeping a uniform data model; (ii) the flexibility of a semi-structured data model, i.e. a flexible data type definition enabling the presentation and manipulation of heterogeneous data entries in a natural manner. Although many implementations of the LDAP protocol exist, the still lacking logical formalization prohibits a formal analysis and makes it difficult to make use of the numerous results developed for relational databases. In this paper, we give a first-order logic semantics of LDAP and discuss the expressive power of LDAP. In particular, schema typing constraints are interpreted as semantic integrity constraints. We apply our framework to the containment problem of LDAP queries with schema constraints; we reduce this problem to the containment problem of Datalog in the presence of integrity constraints.", "authors": ["Fang Wei", "Georg Lausen"], "n_citation": 0, "title": "A formal analysis of the lightweight directory access protocol", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ef68397c-d127-4796-8f97-ddabf6557cc8"}
{"abstract": "We describe in this paper an experimental setting for browsing and reading simultaneously multiple digital documents. An hemispherical visualization device is used to immerge the reader into 3D representations of digital collections.", "authors": ["Rodrigo Andrade de Almeida", "Pierre Cubaud", "J\u00e9r\u00f4me Dupire", "St\u00e9phane Natkin", "Alexandre Topol"], "n_citation": 0, "title": "Experiments towards 3D immersive interaction for digital libraries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f013e45c-8b84-4244-aff5-6a28437e8247"}
{"abstract": "In this work, a new phonemes recognition system is proposed. The base of decision of the proposed system is the tongue position and roundedness of the lips. The features of the speech are the coefficients of Wavelet Packet Transform with sub-bands selected through the Mel scale. The SVM (Support Vector Machine) is used as classifier in the structure of a Hierarchical Committee Machine. The database used for the recognition was a set of oral vocalic phonemes of the Portuguese language. The experimental results show success rates of 97.50% for the user-dependent case and 91.01% for the user-independent case. This new proposal increased 3.5% the success rate in relation to the one vs. all decision strategy.", "authors": ["Adriano de Andrade Bresolin", "Adri\u00e3o Duarte D\u00f3ria Neto", "Pablo J. Alsina"], "n_citation": 50, "title": "A New Hierarchical Decision Structure Using Wavelet Packet and SVM for Brazilian Phonemes Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f113616f-822f-41ee-be62-d428a405f9c9"}
{"abstract": "The paper presents a new automated pattern classification method. At first original data points are partitioned by unsupervised self-organizing map network (SOM). Then from the above clustering results, some labelled points nearer to each clustering center are chosen to train supervised generalization regression neural network model (GRNN). Then utilizing the decided GRNN model, we reclassify these original data points and gain new clustering results. At last from new clustering results, we choose some labelled points nearer to new clustering center to train and classify again, and so repeat until clustering center no longer changes. Experimental results for Iris data, Wine data and remote sensing data verify the validity of our method.", "authors": ["Chao-Feng Li", "Jun-ben Zhang", "Zhengyou Wang", "Shitong Wang"], "n_citation": 50, "title": "A Fully Automated Pattern Classification Method of Combining Self-Organizing Map with Generalization Regression Neural Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f3988419-63d8-4e71-825b-09095e374bcb"}
{"abstract": "Given any positive integer r, our objective is to develop a strategy for grouping the states of a n-node network into r \u2264 n distinct non-overlapping groups. The criterion for this partitioning is defined as follows. First, a LQR controller is defined for the original n-node network. Then, a r-dimensional reduced-order network is created by imposing a projection matrix P on the n-node open-loop network, and a reduced-order r-dimensional LQR controller is constructed. The resulting controller is, thereafter, projected back to its original coordinates, and implemented in the n-node network. The problem, therefore, is to find a grouping strategy or P that will minimize the difference between the closed-loop transfer matrix of the original network with the full-order controller and that with the projected controller, in the sense of \u210b2 norm. We derive an upper bound on this difference in terms of P, and, thereby propose a design for P using K-means that tightens the bound while guaranteeing numerical feasibility.", "authors": ["Nan Xue", "Aranya Chakrabortty"], "n_citation": 0, "title": "\u210b2-clustering of closed-loop consensus networks under a class of LQR design", "venue": "advances in computing and communications", "year": 2016, "id": "f3b79597-652d-46eb-b1ba-0fef644a9d43"}
{"authors": ["Vito Trianni", "Christos Ampatzis", "Anders Lyhne Christensen", "Elio Tuci", "Marco Dorigo", "Stefano Nolfi"], "n_citation": 9, "title": "From solitary to collective behaviours: Decision making and cooperation", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "f3fe2b23-66a9-47cf-b352-4f1a53371513"}
{"abstract": "This paper presents two approaches to symbolic interval time series forecasting. The first approach is based on the autoregressive moving average (ARMA) model and the second is based on a hybrid methodology that combines both ARMA and artificial neural network (ANN) models. In the proposed approaches, two models are respectively fitted to the mid-point and range of the interval values assumed by the symbolic interval time series in the learning set. The forecast of the lower and upper bounds of the interval value of the time series is accomplished through the combination of forecasts from the mid-point and range of the interval values. The evaluation of the proposed models is based on the estimation of the average behaviour of the mean absolute error and mean square error in the framework of a Monte Carlo experiment.", "authors": ["Andr\u00e9 Luis Santiago Maia", "Francisco de A. T. de Carvalho", "Teresa Bernarda Ludermir"], "n_citation": 0, "title": "A Hybrid Model for Symbolic Interval Time Series Forecasting", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f577f904-f9e9-4b34-bf17-e6930f9b3c12"}
{"abstract": "We prove that if there is a polynomial time algorithm which computes the permanent of a matrix of order n for any inverse polynomial fraction of all inputs, then there is a BPP algorithm computing the permanent for every matrix. It follows that this hypothesis implies P# P  = BPP. Our algorithm works over any sufficiently large finite field (polynomially larger than the inverse of the assumed success ratio), or any interval of integers of similar range. The assumed algorithm can also be a probabilistic polynomial time algorithm. Our result is essentially the best possible based on any black box assumption of permanent solvers, and is a simultaneous improvement of the results of Gemmell and Sudan [GS92], Feige and Lund [FL92] as well as Cai and Hemachandra [CH91], and Toda (see [ABG90]).", "authors": ["Jin-Yi Cai", "Aduri Pavan", "D. Sivakumar"], "n_citation": 50, "title": "On the hardness of permanent", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "f61cf496-1ea6-4dcd-bb79-615c272cc106"}
{"abstract": "It is very difficult to select useful measures and to generate patterns detecting attacks from network. Patterns to detect intrusions are usually generated by expert's experiences that need a lot of man-power, management expense and time. This paper proposes the statistical methods for detecting attacks without expert's experiences. The methods are to select the detection measures from features of network connections and to detect attacks. We extracted normal and each attack data from network connections, and selected the measures for detecting attacks by relative entropy. Also we made probability patterns and detected attacks by likelihood ratio. The detection rates and the false positive rates were controlled by the different threshold in the method. We used KDD CUP 99 dataset to evaluate the performance of the proposed methods.", "authors": ["Gil-Jong Mun", "Yongmin Kim", "DongKook Kim", "Bong-Nam Noh"], "n_citation": 50, "title": "Network Intrusion Detection Using Statistical Probability Distribution", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f623c176-e3fc-412b-a32f-2fcbba093cf5"}
{"abstract": "A fast and simple solution was suggested to reduce the inter-pixels correlations in natural images, of which the power spectra roughly fell off with the increasing spatial frequency f according to a power law; but the 1/f exponent, a, was different from image to image. The essential of the proposed method was to flatten the decreasing power spectrum of each image by using an adaptive low-pass and whitening filter. The act of low-pass filtering was just to reduce the effects of noise usually took place in the high frequencies. The act of whitening filtering was a special processing, which was to attenuate the low frequencies and boost the high frequencies so as to yield a roughly flat power spectrum across all spatial frequencies. The suggested method was computationally more economical than the geometric covariance matrix based PCA method. Meanwhile, the performance degradations accompanied with the computational economy improvement were fairly insignificant.", "authors": ["Ling-Zhi Liao", "Siwei Luo", "Mei Tian", "Lianwei Zhao"], "n_citation": 50, "title": "Fast and Adaptive Low-Pass Whitening Filters for Natural Images", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f6f0dfde-5618-4e2b-99e4-10d9c186b9bb"}
{"abstract": "Workflow systems execute workflows and assign work items to the work list of participants. As work lists usually hold multiple work items, participants have to decide which work item to handle next. When selecting a specific work item other work items must be postponed, which will in succession delay their appendant workflows. This may lead to disproportionately increased execution durations and turn around times if fixed-date constraints are defined on succeeding tasks. We propose a probabilistic method which assists the participant when deciding which work item to handle next, with the intention to decrease turnaround times and to avoid time-related escalations, by providing information about the delay to expect when postponing tasks.", "authors": ["Martin Bierbaumer", "Johann Eder", "Horst Pichler"], "n_citation": 0, "title": "Accelerating workflows with fixed date constraints", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f703d468-ab23-4252-8311-a03352b7764e"}
{"abstract": "This paper presents a symbolic model checking algorithm for continuous-time Markov chains for an extension of the continuous stochastic logic CSL of Aziz et al [1]. The considered logic contains a time-bounded until-operator and a novel operator to express steady-state probabilities. We show that the model checking problem for this logic reduces to a system of linear equations (for unbounded until and the steady state-operator) and a Volterra integral equation system for time-bounded until. We propose a symbolic approximate method for solving the integrals using MTDDs (multi-terminal decision diagrams), a generalisation of MTBDDs. These new structures are suitable for numerical integration using quadrature formulas based on equally-spaced abscissas. like trapezoidal, Simpson and Romberg integration schemes.", "authors": ["Christel Baier", "J.P. Katoen", "Holger Hermanns"], "n_citation": 276, "title": "Approximate symbolic model checking of continuous-time Markov chains", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "f8fa19a0-166e-4b9e-9b1f-48ada14db1ed"}
{"abstract": "Multi-Processor Systems-on-Chips (MPSoCs) gather multiple processors and hardware accelerators in a single chip to meet the performance and energy consumption requirements of mobile devices. To follow the rapid evolution of such applications, the MPSoC community need flexible and programmable platforms intended to be diverted to many use cases, and hence consider definitely the software as one of the main aspects of the system design. To deal with an ever growing complexity when designing for such heterogeneous and evolving platforms, software developers have to adopt a novel software design methodology that encourages the software customization through modularity, reuse and module assembly to build systems and applications. Component-based Software Engineering (CBSE), enabling software customization by adding, removing and substituting components seems to be adequate to reach that goal. We investigate this area while developing Think, a lightweight implementation of the Fractal component model, which applies CBSE principles down to the lowest software layer: the operating system. Think allows various kinds of communication semantics from simple method invocations to RPC, recursive component composition, and comes with retargetable configuration and specification tools. In this paper, we show how Think can make flexible and customizable the operating system and application design for MPSoC a reality.", "authors": ["Ali Erdem \u00d6zcan", "S\u00e9bastien Jean", "Jean-Bernard Stefani"], "n_citation": 50, "title": "Bringing Ease and Adaptability to MPSoC Software Design: A Component-Based Approach.", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f9b7e323-b121-4982-ba97-6744bf2e83b8"}
{"abstract": "In this paper, we introduce a modified fuzzy min-max(FMM) neural network model for pattern classification, and present a real-time face detection method using the proposed model. The learning process of the FMM model consists of three sub-processes: hyperbox creation, expansion and contraction processes. During the learning process, the feature distribution and frequency data are utilized to compensate the hyperbox distortion which may be caused by eliminating the overlapping area of hyperboxes in the contraction process. We present a multi-stage face detection method which is composed of two stages: feature extraction stage and classification stage. The feature extraction module employs a convolutional neural network (CNN) with a Gabor transform layer to extract successively larger features in a hierarchical set of layers. The proposed FMM model is used for the pattern classification stage. Moreover, the model is utilized to select effective feature sets for the skin-color filter of the system.", "authors": ["Ho-Joon Kim", "Juho Lee", "Hyunseung Yang"], "n_citation": 50, "title": "A Weighted FMM Neural Network and Its Application to Face Detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fbab175a-d7b1-4196-b10e-6ed803c83f58"}
{"abstract": "Partitioning a permutation into a minimum number of monotone subsequences is NP-hard. We extend this complexity result to minimum partitioning into k-modal subsequences, that is, subsequences having at most k internal extrema. Based on a network flow interpretation we formulate both, the monotone and the k-modal version, as mixed integer programs. This is the first proposal to obtain provably optimal partitions of permutations. From these models we derive an LP rounding algorithm which is a 2-approximation for minimum monotone partitions and a (k + 1)-approximation for minimum (upper) k-modal partitions in general; this is the first approximation algorithm for this problem. In computational experiments we see that the rounding algorithm performs even better in practice. For the associated online problem, in which the permutation becomes known to an algorithm sequentially, we derive a logarithmic lower bound on the competitive ratio for minimum monotone partitions, and we analyze two (bin packing) online algorithms. These findings immediately apply to online cocoloring of permutation graphs; they are the first results concerning online algorithms for this graph theoretical interpretation.", "authors": ["Gabriele Di Stefano", "Stefan Krause", "Marco E. L\u00fcbbecke", "Uwe Zimmermann"], "n_citation": 0, "title": "On Minimum k-Modal Partitions of Permutations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ff13a374-94c2-4ccb-b241-afe515f5ee28"}
{"authors": ["Gunnar R\u00e4tsch"], "n_citation": 0, "title": "Solving semi-infinite linear programs using boosting-like methods", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ff1fd2bf-b19c-4dab-a2b1-981df66e8944"}
{"abstract": "SFLASH [Spec] is a fast asymmetric signature scheme intended for low cost smart cards without cryptoprocessor. It belongs to the family of multivariate asymmetric schemes. It was submitted to the call for cryptographic primitives organised by the European project NESSIE, and successfully passed the first phase of the NESSIE selection process in September 2001. In this paper, we present a cryptanalysis of SFLASH which allows an adversary provided with an SFLASH public key to derive a valid signature of any message. The complexity of the attack is equivalent to less than 2 38  computations of the public function used for signature verification. The attack does not appear to be applicable to the FLASH companion algorithm of SFLASH and to the modified (more conservative) version of SFLASH proposed in October 2001 to the NESSIE project by the authors of SFLASH in replacement of [Spec].", "authors": ["Henri Gilbert", "Marine Minier"], "n_citation": 0, "title": "Cryptanalysis of SFLASH", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "03db659d-6575-4cc3-97a9-61aa60cbe2fe"}
{"abstract": "As Internet technologies develop, XML is becoming widely used as a standard data/document format. Although the use of XML documents has attracted public attention, the application of IR technologies in XML document retrieval is still in its premature stage. We foresee that typical XML queries for end-users will be very terse, like those used with current Web search engines. Therefore, an XML search engine should be able to search appropriate retrieval results using only a few keywords. In this paper, we introduce a notion of context nodes. Context nodes are used to automatically extract coherent partial documents without the knowledge of XML document structures. This method is useful because it does not require domain analysts to analyze DTDs and specify candidate partial documents beforehand. We use the term context search to represent search methods which employ the notion of context node. As an instantiation of context search methods, we have developed algorithms to identify result partial documents in the vector space model. We made a performance evaluation to verify the effectiveness of our method.", "authors": ["Kenji Hatano", "Hiroko Kinutani", "Masatoshi Yoshikawa", "Shunsuke Uemura"], "n_citation": 0, "title": "Extraction of partial XML documents using IR-based structure and contents analysis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "08733511-e9f3-40ca-a680-abb6311ddfce"}
{"abstract": "We study duality between input and output in the \u03c0-calculus. In dualisable versions of \u03c0, including \u03c0I and fusions, duality breaks with the addition of ordinary input/output types. We introduce $\\overline\\pi$, intuitively the minimal symmetrical conservative extension of \u03c0 with input/output types. We prove some duality properties for $\\overline\\pi$ and we study embeddings between $\\overline\\pi$ and \u03c0 in both directions. As an example of application of the dualities, we exploit the dualities of $\\overline\\pi$ and its theory to relate two encodings of call-by-name \u03bb-calculus, by Milner and by van Bakel and Vigliotti, syntactically quite different from each other.", "authors": ["Daniel Hirschkoff", "Jean-Marie Madiot", "Davide Sangiorgi"], "n_citation": 0, "title": "Duality and i/o-Types in the \u03c0-Calculus", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "0a688590-d520-486e-9233-a7738a72084d"}
{"authors": ["Nao Hirokawa", "Aart Middeldorp", "Harald Zankl"], "n_citation": 0, "title": "Uncurrying for Termination", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "14e56936-cb8e-4f24-bdb3-394247c84f7c"}
{"abstract": "Scientific datasets often consist of complex data types such as images. Mining such data presents interesting issues related to semantics. In this paper, we explore the research issues in mining data from the field of nanotechnology. More specifically, we focus on a problem that relates to image comparison of material nanostructures. A significant challenge here relates to the notion of similarity between the images. Features such as size and height of nano-particles and inter-particle distance are important in image similarity as conveyed by domain experts. However, there are no precise notions of similarity defined apriori. Hence there is a need for learning similarity measures. In this paper, we describe our proposed approach to learn similarity measures for graphical data. We discuss this with reference to nanostructure images. Other challenges in image comparison are also outlined. The use of this research is discussed with respect to targeted applications.", "authors": ["Aparna S. Varde", "Jianyu Liang", "Elke A. Rundensteiner", "Richard D. Sisson"], "n_citation": 0, "title": "Mining images of material nanostructure data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "17bd7312-41f8-4470-9a71-908ad06e3a22"}
{"abstract": "This paper presents an abstract framework and multiple diagram-based methods for proving meaning preservation, i.e., that all rewrite steps of a rewriting system preserve the meaning given by an operational semantics based on a rewriting strategy. While previous rewriting-based methods have generally needed the treated rewriting system as a whole to have such properties as, e.g., confluence, standardization, and/or termination or boundedness of developments, our methods can work when all of these conditions fail, and thus can handle more rewriting systems. We isolate the new lift/project with termination diagram as the key proof idea and show that previous rewriting-based methods (Plotkin's method based on confluence and standardization and Machkasova and Turbak's method based on distinct lift and project properties) implicitly use this diagram. Furthermore, our framework and proof methods help reduce the proof burden substantially by, e.g., supporting separate treatment of partitions of the rewrite steps, needing only elementary diagrams for rewrite step interactions, excluding many rewrite step interactions from consideration, needing weaker termination properties, and providing generic support for using developments in combination with any method.", "authors": ["J. B. Wells", "Detlef Plump", "Fairouz Kamareddine"], "n_citation": 0, "title": "Diagrams for meaning preservation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "18b3380b-5c21-4234-a0f6-c7e3b9aa9097"}
{"abstract": "Regularly spiking neurons are classified into two categories, Class I and Class II, by their firing properties for constant inputs. To investigate how the firing properties of single neurons affect to ensemble rhythmic activities in neural networks, we constructed different types of neural networks whose excitatory neurons are the Class I neurons or the Class II neurons. The networks were driven by random inputs and developed with STDP learning. As a result, the Class I and the Class II neural networks generate different types of rhythmic activities: the Class I neural network generates slow rhythmic activities, and the Class II neural network generates fast rhythmic activities.", "authors": ["Ryosuke Hosaka", "Tohru Ikeguchi", "Kazuyuki Aihara"], "n_citation": 0, "title": "Self-organizing Rhythmic Patterns with Spatio-temporal Spikes in Class I and Class II Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "19293ca4-7b59-46f4-87ab-d48b8319776a"}
{"abstract": "The openness of the Web allows any user to access almost any type of information. However, some information, such as adult content, is not appropriate for all users, notably children. Additionally for adults, some contents included in abnormal porn sites can do ordinary people's mental health harm. In this paper, we propose an efficient 2-way text filter for blocking harmful web documents and also present a new criterion for clear classification. It filters off 0-grade web texts containing no harmful words using pattern matching with harmful words dictionaries, and classifies 1-grade,2-grade and 3-grade web texts using a machine learning algorithm.", "authors": ["Youngsoo Kim", "Taekyong Nam", "Dongho Won"], "n_citation": 0, "title": "2-Way Text Classification for Harmful Web Documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1cbe2edd-ef23-45c7-9695-b4431f1ce5e2"}
{"abstract": "In this paper we discuss variability modelling for hypermedia applications. Inspired by domain engineering, we propose a domain engineering based method for hypermedia development. Since several adaptive hypermedia become more and more popular to incorporate different information views for different audience or environments, we believe that it is important to move variability capturing to modelling phases. Several established modelling views of hypermedia application are discussed from the variability point of view. We also explain modelling techniques by means of examples for the application domain view, the navigation view, the presentation view and discuss importance of the user/environment view for parametrisation of components.", "authors": ["Peter Dolog", "M\u00e1ria Bielikov\u00e1"], "n_citation": 0, "title": "Towards variability modelling for reuse in hypermedia engineering", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1cda85ad-2c6c-4e8f-ae90-7937bdb5dd9d"}
{"abstract": "Bin packing is a well studied problem which has many applications. In this paper we design a robust APTAS for the problem. The robust APTAS receives a single input item to be added to the packing at each step. It maintains an approximate solution throughout this process, by slightly adjusting the solution for each new item. At each step, the total size of items which may migrate between bins must be bounded by a constant factor times the size of the new item. We show that such a property cannot be maintained with respect to optimal solutions.", "authors": ["Leah Epstein", "Asaf Levin"], "n_citation": 50, "title": "A Robust APTAS for the Classical Bin Packing Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1d4ea783-8dc6-4ed0-a87b-3e61f6b22bc0"}
{"abstract": "Modular techniques for automatic verification attempt to overcome the state-explosion problem by exploiting the modular structure naturally present in many system designs. Unlike other tasks in the verification of finite-state systems, current modular techniques rely heavily on user guidance. In particular, the user is typically required to construct module abstractions that are neither too detailed as to render insufficient benefits in state exploration, nor too coarse as to invalidate the desired system properties. In this paper, we construct abstract modules automatically, using reachability and controllability information about the concrete modules. This allows us to leverage automatic verification techniques by applying them in layers: first we compute on the state spaces of system components, then we use the results for constructing abstractions, and finally we compute on the abstract state space of the system. Our experimental results indicate that if reachability and controllability information is used in the construction of ions, the resulting abstract modules are often significantly smaller than the concrete modules and can drastically reduce the space and time requirements for verification.", "authors": ["Rajeev Alur", "L. de Alfaro", "Thomas A. Henzinger", "Freddy Y. C. Mang"], "n_citation": 75, "title": "Automating modular verification", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "1dc59882-baf4-454c-a665-6fc985b821d4"}
{"abstract": "Ontologies play an indispensable role in the Semantic Web by specifying the definitions of concepts and individual objects. However, most of the existing methods for constructing ontologies can only specify concepts as crisp sets. However, we cannot avoid encountering concepts that are without clear boundaries, or even vague in meanings. Therefore, existing ontology models are unable to cope with many real cases effectively. With respect to a certain category, certain objects are considered as more representative or typical. Cognitive psychologists explain this by the prototype theory of concepts. This notion should also be taken into account to improve conceptual modeling. While there has been different research attempting to handle vague concepts with fuzzy set theory, formal methods for measuring typicality of objects are still insufficient. We propose a cognitive model of concepts for ontologies, which handles both likeliness (fuzzy membership grade) and typicality of individuals. We also discuss the nature and differences between likeliness and typicality. This model not only enhances the effectiveness of conceptual modeling, but also brings the results of reasoning closer to human thinking. We believe that this research is beneficial to future research on ontological engineering in the Semantic Web.", "authors": ["Ching-man Au Yeung", "Ho-fung Leung"], "n_citation": 0, "title": "Ontology with Likeliness and Typicality of Objects in Concepts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1ee00f41-c70d-4e91-b64e-d501147a680f"}
{"abstract": "In this work, we address the problem of constraint conflicts while integrating the conceptual schemas of multiple autonomous databases modeled using the Entity-Relationship (ER) approach. This paper presents a detailed framework to resolve three types of constraint conflicts, domain constraint conflicts, attribute constraint conflicts and relationship constraint conflicts. There are two types of domain constraint conflict, convertible and inconvertible. We distinguish two types of convertible domain constraints conflict, reversible and irreversible, and present an algorithm to resolve domain constraint conflicts. We identify six factors that can contribute to conflict in attribute constraints: imprecise constraint design, domain mismatch, incomplete information, imprecise semantics, value inconsistency and set relation between object types. In relationship constraint conflict resolution, we examine the set relation between equivalent relationship sets and the functional dependencies that hold in these relationship sets. Our conflict resolution approach does not assume that equivalent entity types or relationship sets in two schemas model exactly the same set of instances in the real world. Furthermore, our approach enforces the most precise constraints and enables the retrieval of all the data in the local databases via the integrated schema.", "authors": ["Min-Dar Lee", "Tok Wang Ling"], "n_citation": 0, "title": "Resolving constraint conflicts in the integration of entity-relationship schemas", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "1f2d1689-da63-4504-8d55-481254568a5f"}
{"abstract": "Since programming languages are Turing complete, it is impossible to decide for all programs whether a given non-trivial semantic property is valid or not. The way-out chosen by abstract interpretation is to provide approximate methods which may fail to certify a program property on some programs. Precision of the analysis can be measured by providing classes of programs for which the analysis is complete, i.e., decides the property in question. Here, we consider analyses of polynomial identities between integer variables such as x 1 . x 2  -2x 3  = 0. We describe current approaches and clarify their completeness properties. We also present an extension of our approach based on weakest precondition computations to programs with procedures and equality guards.", "authors": ["Markus M\u00fcller-Olm", "Michael Petter", "Helmut Seidl"], "n_citation": 0, "title": "Interprocedurally analyzing polynomial identities", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1f44165c-3544-433e-9207-edcd239b865b"}
{"abstract": "Traditionally, the Conceptual Modelling (CM) community has been interested in defining methods to model Information Systems by specifying their data and behaviour, disregarding user interaction. On the other hand, the Human-Computer Interaction (HCI) community has defined techniques oriented to the modelling of the interaction between the user and the system, proposing a user-centred software construction, but leaving out details on system data and behaviour. This paper aspires to reconcile both visions by integrating task modelling techniques using a sound, conceptual model-based software development process in a HCI context. The system is considered on its three axis (data, functionality and interaction), as a whole. The use of CTT (Concurrent Task Trees) embedded in a model-based approach makes it possible to establish mapping rules between task structure patterns that describe interaction and the elements of the abstract interface model. By defining such structural patterns, the CTT notation is much more manageable and productive; therefore, this HCI technique can be easily integrated in a well-established conceptual modelling approach. This proposal is underpinned by the MDA-based technology OlivaNova Method Execution, which allows real automatic software generation, while still taking user interface into account at an early requirements elicitation stage.", "authors": ["Sergio Espa\u00f1a", "Jose Ignacio Panach", "In\u00e9s Pederiva", "Oscar Pastor"], "n_citation": 0, "title": "Towards a Holistic Conceptual Modelling-Based Software Development Process", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2250ed18-68a6-4316-a817-e68742ad2b23"}
{"abstract": "In this paper, research on exploring the potential of several popular equalization techniques while overcoming their disadvantages has been conducted. First, extensive literature survey on equalization is conducted. The focus has been placed on several popular linear equalization algorithm such as the conventional least-mean-square (LMS) algorithm, the recursive least squares (RLS) algorithm, the fi1tered-X LMS algorithm and their development. The approach in analysing the performance of the filtered-X LMS Algorithm, a heuristic method based on linear time-invariant operator theory is provided to analyse the robust perfonnance of the filtered-X structure. It indicates that the extra filter could enhance the stability margin of the corresponding non filtered X structure. To overcome the slow convergence problem while keeping the simplicity of the LMS based algorithms, an H2 optimal initialization is proposed.", "authors": ["Wanlei Zhou", "Hua Ye", "Lin Ye"], "n_citation": 0, "title": "Improving the performance of equalization in communication systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "225dc19f-5f47-427b-8b97-bb76729204bc"}
{"abstract": "Recurrent Backpropagation schemes for fixed point learning in continuous-time dynamic neural networks can be formalized through a differential-algebraic model, which in turn leads to singularly perturbed training techniques. Such models clarify the relative time-scaling between the network evolution and the adaptation dynamics, and allow for rigorous local convergence proofs. The present contribution addresses some related issues in a discrete-time context: fixed point problems can be analyzed in terms of iterations with different evolution rates, whereas periodic trajectory learning can be reduced to a multiple fixed point learning problem via Poincare maps.", "authors": ["Ricardo Riaza", "Pedro J. Zufiria"], "n_citation": 0, "title": "Time-scaling in recurrent neural learning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "240c6527-0cce-4563-b026-bf7ba88e127e"}
{"abstract": "Providing virtual characters with natural gestures is a complex task. Even if the range of gestures is limited, deciding when to play which gesture may be considered both an engineering or an artistic task. We want to strike a balance by presenting a system where gesture selection and timing can be human authored in a script, leaving full artistic freedom to the author. However, to make authoring faster we offer a rule system that generates gestures on the basis of human authored rules. To push automation further, we show how machine learning can be utilized to suggest further rules on the basis of previously annotated scripts. Our system thus offers different degrees of automation for the author, allowing for creativity and automation to join forces.", "authors": ["Michael Kipp"], "n_citation": 0, "title": "Creativity meets automation : Combining nonverbal action authoring with rules and machine learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "24b5799e-0560-4a76-99b2-0767d5a8e0f3"}
{"abstract": "Support for the development of fault-tolerant applications has been identified as one of the major technical challenges to address for the successful deployment of computational grids. Reflective systems provide a very attarctive solution for the problem. But in the traditional systems, all the binding relationships between the function part and the non-function part of the application are fixed. A new reflective architecture which uses events to achieve dynamic binding is proposed in the article. The two best known fault-tolerance algorithms are mapped into the architecture to set forth the dynamic binding protocol.", "authors": ["Lingxia Liu", "Quanyuan Wu", "Bin Zhou"], "n_citation": 0, "title": "A fault-tolerant architecture for grid system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2759fc94-9ee0-4ed6-a00b-41d4f7fe2c48"}
{"abstract": "It is well-known that n players, connected only by pairwise secure channels, can achieve unconditional broadcast if and only if the number t of cheaters satisfies t < n/3. In this paper, we show that this bound can be improved - at the sole price that the adversary can prevent successful completion of the protocol, but in which case all players will have agreement about this fact. Moreover, a first time slot during which the adversary forgets to cheat can be reliably detected and exploited in order to allow for future broadcasts with t < n/2. This even allows for secure multi-party computation with t < n/2 after the first detection of such a time slot.", "authors": ["Matthias Fitzi", "Nicolas Gisin", "Ueli Maurer", "Oliver von Rotz"], "n_citation": 0, "title": "Unconditional byzantine agreement and multi-party computation secure against dishonest minorities from scratch", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "283506aa-6e1e-4759-b1b8-77b719803d7b"}
{"abstract": "The need for scalable and efficient frequent pattern mining has driven the development for parallel algorithms. High cost performance platforms like PC cluster are also becoming widely available. Modern algorithms for frequent pattern mining employs complicated tree structure. Here we report the development of the tree based parallel mining algorithms on PC cluster: Parallel FP-growth and an extension to mine web access patterns called Parallel WAP-mine.", "authors": ["Masaru Kitsuregawa", "Iko Pramudiono"], "n_citation": 0, "title": "PC cluster based Parallel frequent pattern mining and Parallel web access pattern mining", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2844d3b4-dc0d-4787-a10e-67b4aa9900fd"}
{"abstract": "In Data Warehouse (DW) scenarios, ETL (Extraction, Transformation, Loading) processes are responsible for the extraction of data from heterogeneous operational data sources, their transformation (conversion, cleaning, normalization, etc.) and their loading into the DW. In this paper, we present a framework for the design of the DW back-stage (and the respective ETL processes) based on the key observation that this task fundamentally involves dealing with the specificities of information at very low levels of granularity including transformation rules at the attribute level. Specifically, we present a disciplined framework for the modeling of the relationships between sources and targets in different levels of granularity (including coarse mappings at the database and table levels to detailed inter-attribute mappings at the attribute level). In order to accomplish this goal, we extend UML (Unified Modeling Language) to model attributes as first-class citizens. In our attempt to provide complementary views of the design artifacts in different levels of detail, our framework is based on a principled approach in the usage of UML packages, to allow zooming in and out the design of a scenario.", "authors": ["Sergio Luj\u00e1n-Mora", "Panos Vassiliadis", "Juan Tkujillo"], "n_citation": 0, "title": "Data mapping diagrams for Data Warehouse design with UML", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "28d072d9-d701-4df1-a50c-74f83d2ec8ec"}
{"abstract": "Today it is widely recognized that optimization based on rewriting leads to faster query execution. The role of a query rewriting grows significantly when a query defined in terms of some view is processed. Using views is a good idea for building flexible virtual data integration systems with declarative query support. At present time such systems tend to be based on the XML data model and use XML as the internal data representation for processing query over heterogeneous data. Hence an elaborated algorithm of query rewriting is of great importance for efficient processing of XML declarative queries. This paper describes the query rewriting techniques for the XQuery language that is implemented as part of the BizQuery virtual data integration system. The goals of XQuery rewriting are stated. Query rewriting rules for FLWR expressions and for recursive XQuery functions are presented. Also the role of the XML schema in query rewriting is discussed.", "authors": ["Maxim N. Grinev", "Sergey Kuznetsov"], "n_citation": 50, "title": "Towards an exhaustive set of rewriting rules for XQuery optimization: BizQuery experience", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2a4ea2d3-8387-4069-b23b-1150c8cce201"}
{"abstract": "This paper studies parallel learning of growing self-organizing maps ( GSOMs ) and its application to traveling sales person problems ( TSPs ). Input space of city positions are divided into subspaces automatically through adaptive resonance theory ( ART ) map. One GSOM is allocated to each subspace and grows following input data. After all the GSOMs grow sufficiently they are fused and we obtain a tour. The algorithm performance can be controlled by four parameters: the number of subspaces, insertion interval, learning coefficient and final number of cells. In basic experiments for a data-set of 929 cities we can find semi-optimal solution much faster than serial methods although there exist trade-off between tour length and execution time.", "authors": ["Tetsunari Oshime", "Toshimichi Saito", "Hiroyuki Torikai"], "n_citation": 50, "title": "ART-Based Parallel Learning of Growing SOMs and Its Application to TSP", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d056c5f-3a9c-47b1-9d12-36b13bb872bf"}
{"abstract": "The Service Availability Forum is specifying high availability interfaces for carrier grade applications. Along with the direct support for applications an implementation of these interfaces implies that it can itself be highly available. To ensure this availability an implementation must be secure, but these security mechanisms must themselves not reduce the availability of the overall system [1,2]. The security of high availability interfaces (and their middleware implementations) therefore requires a careful design to address potential cross influences. In this paper, we first discuss the general security scope for SA Forum systems, do a threat analysis and list a number of assumption of the execution environment. Then, we present a strawman architecture for the SA Forum Security service (SEC). Rather than presenting a detailed design, with this architecture we attempt to provide guidance, expose issues to be addressed and offer solution ideas for those issues.", "authors": ["Peter Richard Badovinatz", "Santosh Balakrishnan", "Makan Pourzandi", "Manfred Reitenspiess", "Chad Tindel"], "n_citation": 0, "title": "The Service Availability Forum Security Service (SEC) : Status and Future Directions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d2b5c85-2fde-40bc-b785-00720c7fa086"}
{"abstract": "This paper discusses timing and pacing issues in Interactive Storytelling applications at the conceptual level. Herein, aspects of both, the authoring process and the experiencing of interactive stories are considered. Undoubtedly, interactive stories and Interactive Storytelling applications provide a huge potential as basis for any kind of dialogue based, game based, or 'serious' edutainment application. On the other hand - and in contrast to linear, pre-scripted and less interactive applications, such as films, books or life performances - the challenge of developing applications based on interactive stories lies in the interactivity and possible 'free scenes'. By free scenes we refer to interactive elements such as chatting with a virtual character, 'playing' with an interactive installation in a museum, or performing on-site rallies ('un-guided' tours), enabling users to interactively explore the content space and acquire knowledge in their individual style and pace. Hereby, these free scenes might disturb the fluent continuation of the underlying story plot, and harmfully influence the dramaturgy and suspense of the story model. Chapter 1 further introduces and motivates the topic, while Chapter 2 provides a comprehensive overview of related research work along with some examples from the edutainment domain. Chapter 3 and 4 introduce a new approach conceptualized by the authors of this paper in the context of the EU funded project INSCAPE 1 . Finally the main aspects are summarized and an outlook indicates open issues to be addressed in future research.", "authors": ["Stefan G\u00f6bel", "Rainer Malkewitz", "Felicitas Becker"], "n_citation": 0, "title": "Story pacing in interactive storytelling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d88ad91-ea71-46ef-8e0a-817a8727fd26"}
{"abstract": "Structural damage detection is very important for identifying and diagnosing the nature of the damage in an early stage so as to reduce catastrophic failures and prolong the service life of structures. In this paper, a novel approach is presented that integrates independent component analysis (ICA) and support vector machine (SVM). The procedure involves extracting independent components from measured sensor data through ICA and then using these signals as input data for a SVM classifier. The experiment presented employs the benchmark data from the University of British Columbia to examine the effectiveness of the method. Results showed that the accuracy of damage detection using the proposed method is significantly better than the approach by integrating ICA and ANN. Furthermore, the prediction output can be used to identify different types and levels of structure damages.", "authors": ["Huazhu Song", "Luo Zhong", "Bo Han"], "n_citation": 0, "title": "Structural damage detection by integrating independent component analysis and support vector machine", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2e22961a-707a-4b7c-a130-b68b52b67deb"}
{"abstract": "We study an on-line broadcast scheduling problem in which requests have deadlines, and the objective is to maximize the weighted throughput, i.e., the weighted total length of the satisfied requests. For the case where all requested pages have the same length, we present an online deterministic algorithm named BAR and prove that it is 4.56-competitive. This improves the previous algorithm of Kim and Chwa [11] which is shown to be 5-competitive by Chan et al. [4]. In the case that pages may have different lengths, we prove a lower bound of \u03a9(\u0394/log A) on the competitive ratio where A is the ratio of maximum to minimum page lengths. This improves upon the previous \u221a\u0394 lower bound in [11,4] and is much closer to the current upper bound of (A + 2\u221a\u0394 + 2) in [7]. Furthermore, for small values of A we give better lower bounds.", "authors": ["Feifeng Zheng", "Stanley P. Y. Fung", "Wun-Tat Chan", "Francis Y. L. Chin", "Chung Keung Poon", "Prudence W. H. Wong"], "n_citation": 0, "title": "Improved On-Line Broadcast Scheduling with Deadlines", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2e6bf736-af4f-4512-8d38-7637772dfb1a"}
{"abstract": "Due to shortage of time and resources in the early phases of mobile work support development processes, it is necessary to develop a 'quick and clean' method for user requirements modelling. The method consists of three easy steps that anyone with an interest in human factors should be able to complete: (1) The explication of a common vision, (2) observations in the field, and (3) problem oriented focus group processes. The method is designed to generate results that may serve as a foundation for more comprehensive later stage system modelling. As an illustration of the method, a case of use is presented: The development of an electronic delivery guide (EDG) for the newspaper deliverers of a major Norwegian newspaper distribution.", "authors": ["Asbj\u00f8rn F\u00f8lstad", "Odd-Wiking Rahlff"], "n_citation": 0, "title": "Basic user requirements for mobile work support systems - three easy steps", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "336abc09-fd07-4106-bc4d-0266eafe7748"}
{"abstract": "In this article we describe an improved concept for second-order differential-power analysis (DPA) attacks on masked smart card implementations of block ciphers. Our concept allows to mount second-order DPA attacks in a rather simple way: a second-order DPA attack consists of a pre-processing step and a DPA step. Therefore, our way of performing second-order DPA attacks allows to easily assess the number of traces that are needed for a successful attack. We give evidence on the effectiveness of our methodology by showing practical attacks on a masked AES smart card implementation. In these attacks we target inputs and outputs of the SubBytes operation in the first encryption round.", "authors": ["Elisabeth Oswald", "Stefan Mangard", "Christoph Herbst", "Stefan Tillich"], "n_citation": 0, "title": "Practical second-order DPA attacks for masked smart card implementations of block ciphers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "35c1eeed-aec7-4141-9ec1-560aa08822c4"}
{"abstract": "We describe an immersive music visualization application which enables interaction between a live musician and a responsive virtual character. The character reacts to live performance in such a way that it appears to be experiencing an emotional response to the music it 'hears.' We modify an existing tonal music encoding strategy in order to define how the character perceives and organizes musical information. We reference existing research correlating musical structures and composers' emotional intention in order to simulate cognitive processes capable of inferring emotional meaning from music. The ANIMUS framework is used to define a synthetic character who visualizes its perception and cognition of musical input by exhibiting responsive behaviour expressed through animation.", "authors": ["Robyn Taylor", "Pierre Boulanger", "Daniel Torres"], "n_citation": 0, "title": "Visualizing emotion in musical performance using a virtual character", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "35e82b39-b60d-4752-8da1-f33464cff036"}
{"abstract": "We show an alternative implementation of the gap amplification step in Dinur's [4] recent proof of the PCP theorem. We construct a product G t of a constraint graph G, so that if every assignment in G leaves an e-fraction of the edges unsatisfied, then in G t  every assignment leaves an \u03a9(te)-fraction of the edges unsatisfied, that is, it amplifies the gap by a factor \u03a9(t). The corresponding result in [4] showed that one could amplify the gap by a factor \u03a9\u221at). More than this small quantitative improvement, the main contribution of this work is in the analysis. Our construction uses random walks on expander graphs with exponentially distributed length. By this we ensure that some random variables arising in the proof are automatically independent, and avoid some technical difficulties.", "authors": ["Jaikumar Radhakrishnan"], "n_citation": 50, "title": "Gap Amplification in PCPs Using Lazy Random Walks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "36a173db-2928-4c68-a9e5-76dc836745be"}
{"abstract": "Symmetry reduction is a technique to combat the state explosion problem in temporal logic model checking. Its use with symbolic representation has suffered from the prohibitively large BDD for the orbit relation. One suggested solution is to pre-compute a mapping from states to possibly multiple representatives of symmetry equivalence classes. In this paper, we propose a more efficient method that determines representatives dynamically during fixpoint iterations. Our scheme preserves the uniqueness of representatives. Another alternative to using the orbit relation is counter abstraction. It proved efficient for the special case of full symmetry, provided a conducive program structure. In contrast, our solution applies also to systems with less than full symmetry, and to systems where a translation into counters is not feasible. We support these claims with experimental results.", "authors": ["E. Allen Emerson", "Thomas Wahl"], "n_citation": 0, "title": "Dynamic symmetry reduction", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "36a8e10f-689f-4000-b22a-7efe7d184894"}
{"abstract": "In electronic commerce, two fundamental types of models are business models and process models. While a business model is concerned with value exchanges between actors, a process model describes the procedural realization of business requirements. There is a need for methodological guidelines and tool support to move from a business model to a process model, which enables design decisions to be based on requirements captured in the business model. This paper addresses a systematic transformation of business models to process models. We propose a designer assistant that systematically aids a designer in generating a process model in an executable process modeling language.", "authors": ["Prasad Jayaweera", "Paul Johannesson", "Petia Wohed"], "n_citation": 0, "title": "Process patterns to generate E-commerce systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "36c6069d-0a40-441a-a176-8bd34c767f6b"}
{"abstract": "In this paper, we propose an effort prediction model in which data including missing values is complemented by using the collaborative filtering[1,2,3] and the effort of projects is derived from a multiple regression analysis[4,5] using the data. Because companies, recently, focus on methods to predict effort of projects, which prevent project failures such as exceeding deadline and cost, due to more complex embedded software, which brings the evolution of the performance and function enhancement [6, 7,8]. Moreover, we conduct the evaluation experiment that compared the accuracy of our method with other two methods according to five criteria to confirm their accuracy. The results of the experiment shows that our method gives predictions the best in the five evaluat ion criteria.", "authors": ["Kazunori Iwata", "Yoshiyuki Anan", "Toyoshiro Nakashima", "Naohiro Ishii"], "n_citation": 0, "title": "Effort Prediction Model Using Similarity for Embedded Software Development", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c2fa17a-6256-4642-9621-46e7cb4880b7"}
{"abstract": "In order to search similar moving object trajectories, the previously used methods focused on Euclidean distance and considered only spatial similarity. Euclidean distance is not appropriate for road network space, where the distance is limited to the space adjacent to the roads. In this paper, we consider the properties of moving objects in road network space and define temporal similarity as well as spatio-temporal similarity between trajectories based on POI (Points of Interest) and TOI (Times of Interest) on road networks. Based on these definitions, we propose methods for searching for similar trajectories in road network space. Experimental results show the accuracy of our methods and the average search time in query processing.", "authors": ["Jung-Rae Hwang", "Hye-Young Kang", "Li Ki-Joune"], "n_citation": 50, "title": "Searching for similar trajectories on road networks using spatio-temporal similarity", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c9cf0a6-00b8-4be4-a8e4-323bf68f2123"}
{"abstract": "Independent Component Analysis (ICA) is a useful statistical method for separating mixed data sources into statistically independent patterns. In this paper, we apply ICA to transform multivariate time series data into independent components (ICs), and then propose a clustering algorithm called ICACLUS to group underlying data series according to the ICs found. This clustering algorithm can be used to identify stocks with similar stock price movement. The experiments show that this method is effective and efficient, which also outperforms other comparable clustering methods, such as K-means.", "authors": ["Edmond HaoCun Wu", "Philip L. H. Yu"], "n_citation": 0, "title": "Independent component analysis for clustering multivariate time series data", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3d8ad742-af99-4144-a1d4-8cb0607300bf"}
{"abstract": "Grid is a new type of resource sharing and distributed infrastructure. Because of limitations of software and hardware, the service that a certain grid can offer is finite, and so is the number of users. If the number of users is too small, much of the planned resources could turn out to be wasteful; on the contrary, if there are too many users, the excessive loading could substantially reduce the benefit of each user and also the efficiency of the grid service. Therefore, the following are two central problems for grid design: 1. How many users should the grid serve so that each user can receive the maximum benefit? 2. To a certain group of users, how much resources should be invested so that the construction and maintenance the grid become viable? Based on the economic theory of clubs, this paper makes a quantitative analysis of the quasi-optimal size of the grid by regarding grid services and resources as club goods. The results are two important conclusions. We use an experiment involving GridFTP to verify the theory.", "authors": ["Yao Shi", "Francis C. M. Lau", "Du Zhi-hui", "Rui-Chun Tang", "Sanli Li"], "n_citation": 0, "title": "Club theory of the grid", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3e3a11e1-4146-4a8f-a548-fe9fab9af4b3"}
{"authors": ["Pascal Fontaine", "Pascal Gribomont"], "n_citation": 0, "title": "Decidability of invariant validation for paramaterized systems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "40ddd021-a6d9-41ea-993b-1efdfadbff44"}
{"abstract": "A chordal ring, denoted by CR(N, d), is a graph G = (V, E) with V = {0,1,...,N -1} and E = {(u,v) | [v - u] N  = 1 or d}, where 2 < N < N/2 and [r] N denotes r modulo N. We show that for 2 < d < N/2, CR(N,d) has 4 independent spanning trees rooted at the same vertex, and for 2 < d = N/2, CR(N,d) has 3 independent spanning trees rooted at the same vertex. We can design a fault-tolerant broadcasting scheme for CR(N, d) using independent spanning trees.", "authors": ["Yukihiro Iwasaki", "Yuka Kajiwara", "Koji Obokata", "Yoshihide Igarashi"], "n_citation": 50, "title": "Independent spanning trees of chordal rings", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "4545ee03-70f0-485d-a036-c35156c7a915"}
{"abstract": "Shadow maps provide a fast and convenient method of generating real-time shadows in computer games. Unfortunately, shadow maps suffer from aliasing problems. Percentage Closer Filtering (PCF) is one of the solutions, but in cases of severe aliasing it can only mask the aliasing by unnaturally blurring. In this paper we present two improved antialiased shadow algorithms based on PCF, named as Modified PCF I and Modified PCF II. The primary idea of our algorithms is to make sawtooth shadow edges straighter just by data modification before bilinear interpolation in PCF. We respectively implement the algorithms with graphic hardware in Pixel Shader 2.0 and 3.0, several examples are also provided to show that most of aliasing artifacts have been removed compared with PCF. The time complexity and hardware requirement of Modified PCF I are almost as same as that of PCF. The resource requirement of Modified PCF II is more rigorous than that of Modified PCF I, but it can still accomodate to real-time game rendering with obvious antialiasing effect.", "authors": ["Nailiang Zhao", "Yanjun Chen", "Zhigeng Pan"], "n_citation": 0, "title": "Antialiased shadow algorithms for game rendering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "46855b21-18e9-47eb-9200-3d626897e085"}
{"abstract": "In volume data visualization, the classification is used to determine voxel visibility and is usually carried out by transfer functions that define a mapping between voxel value and color/opacity. The design of transfer functions is a key process in volume visualization applications. However, one transfer function that is suitable for a data set usually dose not suit others, so it is difficult and time-consuming for users to design new proper transfer function when the types of the studied data sets are changed. By introducing neural networks into the transfer function design, a general adaptive transfer function (GATF) is proposed in this paper. Experimental results showed that by using neural networks to guide the transfer function design, the robustness of volume rendering is promoted and the corresponding classification process is optimized.", "authors": ["Liansheng Wang", "Xucan Chen", "Sikun Li", "Xun Cai"], "n_citation": 0, "title": "General Adaptive Transfer Functions Design for Volume Rendering by Using Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "46eaf25d-0b68-4654-98bb-3e1456d9f5cf"}
{"abstract": "To quantify the regularity of state updates, the concept of Irregular Degree was introduced. Two experiments corresponding to four different filtering policies were designed. In the first experiment three filtering policies based on Irregular Degree were compared. In the second experiment, the filtering policy achieving the best task performance in the first experiment was selected to compare with other filtering policies. The results of the first experiment indicate that the stability of state updates per second is a more important impact factor than Irregular Degree on the task performance. The results of the second experiment indicate that corresponding to different state updates per second, the filtering policies can achieve different task performance.", "authors": ["Ling Chen", "Wei Liu", "Gencai Chen"], "n_citation": 0, "title": "Effects of filtering policies on task performance in a desktop CVE system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "472d16b2-534c-44a6-9e3d-99a23d956045"}
{"abstract": "In this paper a theory of dialogue acts analysis in problem-solving tasks-oriented conversations is presented. The theory postulates that in practical dialogues every transaction has a component in the obligations and the common ground planes of expression, and contributions made by dialogue acts making a charge in the transaction should be balanced by contributions making the corresponding credit, and a complete transaction is balanced in both of these planes. In addition, transactions have a structure which constraints strongly the realization of dialogue acts. A dialogue act tagging methodology based on the theory is also presented. The theory and its related methodology have been applied to the analysis of a multimodal corpus in a design task, and the figures of the agreement reached in the preliminary experiments are presented.", "authors": ["Luis Albreto Pineda Cones", "Hayde Castellanos", "Sergio Rafael Coria Olguin", "Varinia Estrada", "Fernanda L\u00f3pez", "Isabel L\u00f3pez", "Ivan Meza", "Iv\u00e1n Moreno", "Patricia Troncoso P\u00e9rez", "Carlos G\u00f3mez Rodr\u00edguez"], "n_citation": 50, "title": "Balancing transactions in practical dialogues", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "476d5097-8d9b-4702-a6a6-5590610152f9"}
{"abstract": "An ordering for terms with sequence variables and flexible arity symbols is presented. The ordering coincides with the lexicographic extension of multiset path ordering on terms without sequence variables. It is shown that the classical strict superposition calculus with ordering and equality constraints can be used as a refutationally complete proving method for well-constrained sets of clauses with sequence variables and flexible arity symbols.", "authors": ["Temur Kutsia"], "n_citation": 0, "title": "Theorem proving with sequence variables and flexible arity symbols", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "49058505-3682-4a68-ad48-ec01d5b99384"}
{"abstract": "Scoring matrices are widely used in sequence comparisons. A scoring matrix \u03b3 is indexed by symbols of an alphabet. The entry in \u03b3 in row a and column b measures the cost of the edit operation of replacing symbol a by symbol b. For a given scoring matrix and sequences s and t, we consider two kinds of induced scoring functions. The first function, known as weighted edit distance, is defined as the sum of costs of the edit operations required to transform s into t. The second, known as normalized edit distance, is defined as the minimum quotient between the sum of costs of edit operations to transform s into t and the number of the corresponding edit operations. In this work we characterize the class of scoring matrices for which the induced weighted edit distance is actually a metric. We do the same for the normalized edit distance.", "authors": ["Eloi Araujo", "Jos\u00e9 Soares"], "n_citation": 0, "title": "Scoring matrices that induce metrics on sequences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "497e72b9-89d7-4d09-a4e3-c2a91ab2a2b8"}
{"abstract": "In this paper, we propose a schema versioning mechanism to manage the schema evolution in temporal object-oriented databases. The schema evolution management uses an object-oriented data model that supports temporal features and versions definition - the Temporal Versions Model - TVM. One interesting feature of our proposal is that TVM is used to control not only the schema versioning, but also the storage of extensional database and propagation of the changes performed on the objects. The extensional data level supports integration with the existing database, allowing the maintenance of conventional and temporal versioned objects. The instance propagation approach is proposed through the specification of propagation and conversion functions. These functions assure the correct instance propagation and allow the user to handle all instances consistently in both backward and forward schema versions. Finally, the initial requirements concerning data management in the temporal versioning environment, during schema evolution, are presented.", "authors": ["Renata de Matos Galante", "Nina Edelweiss", "Clesio Saraiva dos Santos"], "n_citation": 50, "title": "Change management for a temporal versioned object-oriented database", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "49af3963-e215-4623-84e0-fabe092786b6"}
{"abstract": "Mining user profiles is a crucial task for Web usage mining, and can be accomplished by mining frequent patterns. However, in the Web usage domain, sessions tend to be very sparse, and mining the right user profiles tends to be difficult. Either too few or too many profiles tend to be mined, partly because of problems in fixing support thresholds and intolerant matching. Also, in the Web usage mining domain, there is often a need for post-processing and validation of the results of mining. In this paper, we use criterion guided optimization to mine localized and error-tolerant transaction patterns, instead of using exact counting based method, and explore the effect of different post-processing options on their quality. Experiments with real Web transaction data are presented.", "authors": ["Olfa Nasraoui", "Suchandra Goswami"], "n_citation": 0, "title": "Mining and Validation of Localized Frequent Web Access Patterns with Dynamic Tolerance", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4be00cb0-7a3f-49d9-9a55-81b46f404cd2"}
{"abstract": "This paper discusses the formal specification of properties that determine the behavior of component based BDI agents, i.e. classical BDI agents in which the mental attitudes are conditional and represented by interconnected components. Some properties, such as realism and commitment strategies, have already been discussed in the BDI literature and can be formally specified by for example Rao and Georgeff's BDI CTL  formalism. Other properties are specific to component based cognitive agents and cannot be specified by existing BDI CTL  formalisms. We focus here on the so-called functional dependencies between mental attitudes where a mental attitude is considered to be a function of one or more other mental attitudes. To formally specify the properties of functional dependencies we extend Rao and Georgeff's BDI CTL  formalism. In particular, for functional dependencies we introduce 'only belief', 'only desire' and 'only intend' operators in the tradition of Levesque's 'all I know' operator, and for components we distinguish between 'belief in' and 'belief out', 'desire in' and 'desire out', and 'intention in' and 'intention out' operators. We show how our extended formalism can be used to specify functionality properties such as conservativity, monotonicity, and self-boundedness, as well as properties related to the connections between and control of the components.", "authors": ["Mehdi Dastani", "Leendert W. N. van der Torre"], "n_citation": 0, "title": "An extension of BDICTL with functional dependencies and components", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4f08e93f-c729-43d6-9646-32f59f44372d"}
{"abstract": "Active Appearance Models (AAM) are compact representations of the shape and appearance of objects. Fitting AAMs to images is a difficult, non-linear optimization task. Traditional approaches minimize the L2 norm error between the model instance and the input image warped onto the model coordinate frame. While this works well for high resolution data, the fitting accuracy degrades quickly at lower resolutions. In this paper, we show that a careful design of the fitting criterion can overcome many of the low resolution challenges. In our resolution-aware formulation (RAF), we explicitly account for the finite size sensing elements of digital cameras, and simultaneously model the processes of object appearance variation, geometric deformation, and image formation. As such, our Gauss-Newton gradient descent algorithm not only synthesizes model instances as a function of estimated parameters, but also simulates the formation of low resolution images in a digital camera. We compare the RAF algorithm against a state-of-the-art tracker across a variety of resolution and model complexity levels. Experimental results show that RAF considerably improves the estimation accuracy of both shape and appearance parameters when fitting to low resolution data.", "authors": ["Goksel Dedeoglu", "Simon Baker", "Takeo Kanade"], "n_citation": 50, "title": "Resolution-Aware Fitting of Active Appearance Models to Low Resolution Images", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "50938c82-c849-47cd-a475-eb6f77252d42"}
{"abstract": "We extend the notion of general dimension, a combinatorial characterization of learning complexity for arbitrary query protocols, to encompass approximate learning. This immediately yields a characterization of the learning complexity in the statistical query model. As a further application, we consider approximate learning of DNF formulas and we derive close upper and lower bounds on the number of statistical queries needed. In particular, we show that with respect to the uniform distribution, and for any constant error parameter e < 1/2, the number of statistical queries needed to approximately learn DNF formulas (over n variables and s terms) with tolerance r = \u0398(1/s) is n \u0398(log s) .", "authors": ["Johannes K\u00f6bler", "Wolfgang Lindner"], "n_citation": 50, "title": "A general dimension for approximately learning boolean functions", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "51184e7e-0ee9-434c-8fa8-e89e90b7c4d0"}
{"abstract": "Tremendous advancement and more readily availability of Grid technologies encourages organizations to establish in-house Grids and make use of their available desktop resources to solve computing intensive problems. These mini Grids, because of their limited scope and resource availability, may not serve the real world problem in every aspect and, hence, lead them to collaborate on demand with other grids while keeping themselves autonomous and independent. The specific problem that underlies in such collaborative Grids is resource scheduling among autonomously administrated Grids. In this paper*, we propose a distributed, scalable and reconfigurable inter-Grid resource sharing framework where Grids can dynamically join or resign the framework on a need base. This framework, based on peer-to-peer communication paradigm, enables resource sharing among autonomous Grid systems.", "authors": ["Imran Rao", "Eui-Nam Huh", "Sungyoung Lee", "TaeChoong Chung"], "n_citation": 0, "title": "Distributed, Scalable and Reconfigurable Inter-grid Resource Sharing Framework", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "51aa87bc-55a6-43b8-a12c-2caf84a3af44"}
{"abstract": "In this article we discuss different types of template attacks on masked implementations. All template attacks that we describe are applied in practice to a masked AES software implementation on an 8-bit microcontroller. They all break this implementation. However, they all require quite a different number of traces. It turns out that a template-based DPA attack leads to the best results. In fact, a template-based DPA attack is the most natural way to apply a template attack to a masked implementation. It can recover the key from about 15 traces. All other attacks that we present perform worse. They require between about 30 and 1800 traces. There is no difference between the application of a template-based DPA attack to an unmasked and to a masked implementation. Hence, we conclude that in the scenario of template attacks, masking does not improve the security of an implementation.", "authors": ["Elisabeth Oswald", "Stefan Mangard"], "n_citation": 75, "title": "Template attacks on masking-resistance is futile", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "55eae259-0491-4590-a1e8-f5a69796cb17"}
{"abstract": "The paper describes an evolutionary algorithm for the general nonlinear programming problem using a surrogate model. Surrogate models are used in optimization when model evaluation is expensive. Two surrogate models are implemented, one for the objective function and another for a penalty function based on the constraint violations. The proposed method uses a sequential technique for updating these models. The quality of the surrogate models is determined by their consistency in ranking the population rather than their statistical accuracy. The technique is evaluated on a number of standard test problems.", "authors": ["Thomas Philip Runarsson"], "n_citation": 0, "title": "Constrained evolutionary optimization by approximate ranking and surrogate models", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "564005bc-b324-4ec4-8e19-cc8a52115bf8"}
{"abstract": "We investigate a timed extension of the class of Basic Parallel Processes (BPP), in which actions are durational and urgent and parallel components have independent local clocks. The main result is decidability of strong bisimilarity, known also as performance equivalence, in this class. This extends the earlier decidability result for plain BPP [8] as well as decidability for timed BPP with strictly positive durations of actions [3]. Both ill-timed and well-timed semantics are treated. Our decision procedure is based on decidability of the validity problem for Presburger arithmetic.", "authors": ["S\u0142awomir Lasota"], "n_citation": 0, "title": "Decidability of strong bisimilarity for timed BPP", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "569e0eb4-6965-420a-8ce4-23512fbf1667"}
{"abstract": "The development of wireless sensor networks (WSNs) software today is tackled by a code-and-fix process that relies solely on the primitive constructs provided by the operating system and the skills of developers. For WSNs to emerge from research labs and make a true impact on society at large, we need methodologies, techniques, and abstractions that improve the development process, foster the designer's confidence about the WSN behavior, and whose effectiveness is demonstrated in the real world.   How do we achieve these goals? The aforementioned challenges are germane to the techniques and expertise matured by software engineering (SE). Unfortunately, the WSN and SE research communities have been mostly impermeable to each other. In this paper we elaborate on this state of affairs, by arguing that a principled approach to development is inevitable as WSNs become more and more pervasive, and by identifying and discussing specific areas where a synergy between the SE and WSN communities could provide immediate, much-needed results.", "authors": ["Gian Pietro Picco"], "n_citation": 50, "references": ["06fe72e7-0852-43e6-abc7-c79e70650e88", "105314df-68a1-40e0-b793-af225567b50c", "4809cab5-9e71-4ea0-a010-964a809a8f97", "b176ff61-a66e-4d8c-9bc7-d94bb409edb8", "ea4a28ce-8cb5-4a60-82f6-f79789f8d1e4", "ffaa723e-7d23-4230-9040-91ba11ba4921"], "title": "Software engineering and wireless sensor networks: happy marriage or consensual divorce?", "venue": "international conference on software engineering", "year": 2010, "id": "59822256-99ba-46af-b399-ee21a10ed2a2"}
{"abstract": "The development of the wireless Internet market and its structure is driven by differing industry fundamentals and the revenue derived from content and content-related services is expected to increase significantly for all actors within the wireless industry. Providing flexible and scalable accounting and billing systems will be essential for success when offering wireless services to end customers. The wireless service providers have difficulties in billing their customers due to their inability to associate customer transactions with network usage, correlate data from multiple sources and flexibly support the emerging billing models. In this paper an accounting and billing model for the two wireless services is presented. The evaluation of the services proved that the number of roles and partners in the wireless services is huge and that solutions like accounting agents and billing mediation servers are needed for tracking customer transactions and directing the accounting and billing between the partners.", "authors": ["P\u00e4ivi Kallio", "Giovanni Cortese", "Roberto Tiella", "Alessandro Zorer"], "n_citation": 0, "title": "Accounting and billing of wireless Internet services in the third generation networks", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5b8cb470-fe4c-456b-b10d-979b6be02057"}
{"abstract": "Particle swarm optimization is widely applied for training neural network. Since in many applications the number of weights of NN is huge, when PSO algorithms are applied for NN training, the dimension of search space is so large that PSOs always converge prematurely. In this paper an improved stochastic PSO (SPSO) is presented, to which a random velocity is added to improve particles' exploration ability. Since SPSO explores much thoroughly to collect information of solution space, it is able to find the global best solution with high opportunity. Hence SPSO is suitable for optimization about high dimension problems, especially for NN training.", "authors": ["Xin Chen", "Yangmin Li"], "n_citation": 0, "title": "Neural Network Training Using Stochastic PSO", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5c9b3804-8d8a-4104-95f0-48f89a2d5052"}
{"abstract": "We study the problem of placing symbols of an alphabet onto the minimum number of keys on a small keyboard so that any word of a given dictionary can be recognized univoquely only by looking at the corresponding sequence of pressed keys. This problem is motivated by the design of small keyboards for mobile devices. We show that the problem is hard in general, and NP-complete even if we only wish to decide whether two keys are sufficient. We also consider two variants of the problem. In the first one, symbols on a same key must be contiguous in an ordered alphabet. The second variant is a fixed-parameter version of the previous one that minimizes a well-chosen measure of ambiguity in the recognition of the words for a given number of keys. Hardness and approximability results are given.", "authors": ["Jean Cardinal", "Stefan Langerman"], "n_citation": 0, "title": "Designing small keyboards is hard", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5d05ac76-c4ee-420e-b5df-a5ed121c6ab6"}
{"abstract": "We give an algorithm computing the branchwidth of interval graphs in time O(n 3 log n). This method generalizes to permutation graphs and, more generaly, to trapezoid graphs. In contrast, we show that computing branchwidth is NP-complete for splitgraphs and bipartite graphs.", "authors": ["Ton Kloks", "Jan Kratochv\u00edl", "Haiko M\u00fcller"], "n_citation": 0, "title": "New branchwidth territories", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "5e2b2cab-bc12-4f29-85ca-fce4c316f12f"}
{"abstract": "The development of Internet commercial applications and corporate Intranets around the world, that often use Java as programming language, is a significant topic in modem Software Engineering. In this context, more than ever, well-defined methodologies and high-level tools are essential for developing quality software in a way that should be as independent as possible ofthe changes in technology. In this article, we present an OO method based on a formal object-oriented model. The main feature of this method is that developers' efforts are focused on the conceptual modeling step, where analysts capture system requirements, and the full implementation can automatically be obtained following an execution model (including structure and behaviour). The final result is a web application with a three-tiered architecture, which is implemented in Java with a relational DBMS as object repository.", "authors": ["Oscar Pastor", "Vicente Pelechano", "Emilio Insfran", "Jaime G\u00f3mez"], "n_citation": 0, "title": "From object oriented conceptual modeling to automated programming in Java", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "5ea5f2b3-431d-491c-b3f3-b7947c4decae"}
{"abstract": "We present a new scheme to robustly detect a type of human attentive behavior, which we call frequent change in focus of attention (FCFA), from video sequences. FCFA behavior can be easily perceived by people as temporal changes of human head pose (normally the pan angle). For recognition of this behavior by computer, we propose an algorithm to estimate the head pan angle in each frame of the sequence within a normal range of the head tilt angles. Developed from the ISOMAP, we learn a non-linear head pose embedding space in 2-D, which is suitable as a feature space for person-independent head pose estimation. These features are used in a mapping system to map the high dimensional head images into the 2-D feature space from which the head pan angle is calculated very simply. The non-linear person-independent mapping system is composed of two parts: 1) Radial Basis Function (RBF) interpolation, and 2) an adaptive local fitting technique. The results show that head orientation can be estimated robustly. Following the head pan angle estimation, an entropy-based classifier is used to characterize the attentive behaviors. The experimental results show that entropy of the head pan angle is a good measure, which is quite distinct for FCFA and focused attention behavior. Thus by setting an experimental threshold on the entropy value we can successfully and robustly detect FCFA behavior.", "authors": ["Nan Hu", "Weimin Huang", "Surendra Ranganath"], "n_citation": 0, "title": "Robust Attentive Behavior Detection by Non-linear Head Pose Embedding and Estimation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5f464d01-7e3e-43e9-ac37-b14617a83077"}
{"abstract": "This paper introduces a new evaluation function, called \u03b4, for the Bandwidth Minimization Problem for Graphs (BMPG). Compared with the classical \u03b2 evaluation function used, our \u03b4 function is much more discriminating and leads to smoother landscapes. The main characteristics of \u03b4 are analyzed and its practical usefulness is assessed within a Simulated Annealing algorithm. Experiments show that thanks to the use of the \u03b4 function, we are able to improve on some previous best results of a set of well-known benchmarks.", "authors": ["Eduardo Rodriguez-Tello", "Jin-Kao Hao", "Jose Torres-Jimenez"], "n_citation": 0, "title": "An improved evaluation function for the Bandwidth Minimization Problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5f66f010-136b-4912-b3df-22066578dfe2"}
{"abstract": "It is well-accepted that there is a need to incorporate domain knowledge into system development tools of various kinds. Most of these are case tools that have been quite successful in supporting design on a syntactical basis. However, in general, they are not capable of representing and using information about the semantics of an application domain. This research presents a framework for supporting the generation and analysis of conceptual database designs through the use of ontologies. The framework is implemented in a database design assistant prototype that illustrates the research results.", "authors": ["Vijayan Sugumaran", "Veda C. Storey"], "n_citation": 0, "title": "An ontology-based framework for generating and improving database design", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "604fc195-faf4-4cdb-aa81-a3cc1cc34935"}
{"abstract": "Nowadays, almost all mobile devices in the market have Wireless Netwqrk Interface Cards (WNICs) attached that enable them to construct Ad Hoc network and communicate flexibly with each other. Most emerging mobile applications, which are usually computation and data access intensive, have different power consumption on different devices. Hence, with aim at lower power consumption, we can apply the on-demand grid computing to mobile devices, migrating task among mobile devices or, if available, stationary computers. Unavoidably, the task migration introduces additional communication activities, which result in significant power consumption on WNIC. Most existing papers use oversimplified communication energy model, and none of them fully exploits the important features of wireless Ad Hoc network to aggressively reduce communication energy consumption. Taking a cross-layer approach, we present a general modeling framework for joint task placement, routing and power control in the context of low power mobile grid computing. The modeling framework can serve as the basis of various network aware optimizations targeting on energy consumption, energy dissipation rate, network life time, etc.. Moreover, we propose an algorithm for minimizing global energy consumption, and study how various factors, such as WNIC, device density, communication penalty, etc., affect the potential of energy saving and the communication pattern in the mobile grid computing.", "authors": ["Min Li", "Xiaobo Wu", "Menglian Zhao", "Hui Wang", "Ping Li", "Xiaolang Yan"], "n_citation": 0, "title": "Joint task placement, routing and power control for low power mobile grid computing in Ad Hoc network", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "61851ba5-e739-45d4-b7f5-0e9e3f93e17a"}
{"abstract": "In this paper we present approximation results for the class constrained bin packing problem that has applications to Video-on-Demand Systems. In this problem we are given bins of capacity B with C compartments, and n items of Q different classes. The problem is to pack the items into the minimum number of bins, where each bin contains items of at most C different classes and has total items size at most B. We present several approximation algorithms for off-line and online versions of the problem.", "authors": ["Eduardo C. Xavier", "Fl\u00e1vio Keidi Miyazawa"], "n_citation": 0, "title": "The Class Constrained Bin Packing Problem with Applications to Video-on-Demand", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "63b28dfe-321f-4094-bd28-5d9d2319956a"}
{"abstract": "We provide methods for transforming an encryption scheme susceptible to decryption errors into one that is immune to these errors. Immunity to decryption errors is vital when constructing non-malleable and chosen ciphertext secure encryption schemes via current techniques; in addition, it may help defend against certain cryptanalytic techniques, such as the attack of Proos [33] on the NTRU scheme. When decryption errors are very infrequent, our transformation is extremely simple and efficient, almost free. To deal with significant error probabilities, we apply amplification techniques translated from a related information theoretic setting. These techniques allow us to correct even very weak encryption schemes where in addition to decryption errors, an adversary has substantial probability of breaking the scheme by decrypting random messages (without knowledge of the secret key). In other words, under these weak encryption schemes, the only guaranteed difference between the legitimate recipient and the adversary is in the frequency of decryption errors. All the above transformations work in a standard cryptographic model; specifically, they do not rely on a random oracle. We also consider the random oracle model, where we give a simple transformation from a one-way encryption scheme which is error-prone into one that is immune to errors. We conclude that error-prone cryptosystems can be used in order to create more secure cryptosystems.", "authors": ["Cynthia Dwork", "Moni Naor", "Omer Reingold"], "n_citation": 0, "title": "Immunizing encryption schemes from decryption errors", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "63eaae9f-0533-488b-8e9f-7f7a9d036890"}
{"abstract": "We develop new methods to statically bound the resources needed for the execution of systems of concurrent, interactive threads. Our study is concerned with a synchronous model of interaction based on cooperative threads whose execution proceeds in synchronous rounds called instants. Our contribution is a system of compositional static analyses to guarantee that each instant terminates and to bound the size of the values computed by the system as a function of the size of its parameters at the beginning of the instant. Our method generalises an approach designed for first-order functional languages that relies on a combination of standard termination techniques for term rewriting systems and an analysis of the size of the computed values based on the notion of quasi-interpretation. These two methods can be combined to obtain an explicit polynomial bound on the resources needed for the execution of the system during an instant.", "authors": ["Roberto M. Amadio", "Silvano Dal Zilio"], "n_citation": 50, "title": "Resource control for synchronous cooperative threads", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "649e3d72-6bd9-4d80-bc9a-4412a6b298cb"}
{"abstract": "We report our experiences on application development with the open source OpenAIS framework, which is an implementation of the standard Application Interface Specification (AIS) issued by the Service Availability Forum. Our focus is put on integrating existing (legacy) applications or services into the AIS framework (where the source code of these services is not available) in order to make such services highly available. This is achieved by using Proxy components, which are responsible for managing the High Availability (HA) lifecycle of legacy services (called proxied components). We estimate the availability of legacy services as provided by using redundant proxy and proxied components in the OpenAIS framework on a benchmark service architecture. Furthermore, as the AIS standard does not contain any recommendation on business-related communication, in the paper, we propose to use communication mediation to forward requests to service provider components and responses back to the service consumers.", "authors": ["Andr\u00e1s K\u00f6vi", "Danel Varro", "Zolt\u00e1n N\u00e9meth"], "n_citation": 0, "title": "Making Legacy Services Highly Available with OpenAIS : An Experience Report", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "664b9b9e-0263-42b2-8f78-80022e140dc8"}
{"abstract": "Several authors devised type-based termination criteria for ML-like languages allowing non-structural recursive calls. We extend these works to general rewriting and dependent types, hence providing a powerful termination criterion for the combination of rewriting and \u03b2-reduction in the Calculus of Constructions.", "authors": ["Fr\u00e9d\u00e9ric Blanqui"], "n_citation": 36, "title": "A type-based termination criterion for dependently-typed higher-order rewrite systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "669d3f4b-310a-4f6f-9995-e507063583dc"}
{"abstract": "We consider the rendezvous problem for identical mobile agents (i.e., running the same deterministic algorithm) with tokens in a synchronous torus with a sense of direction and show that there is a striking computational difference between one and more tokens. More specifically, we show that 1) two agents with a constant number of unmovable tokens, or with one movable token, each cannot rendezvous if they have o(logn) memory, while they can perform rendezvous with detection as long as they have one unmovable token and O(log n) memory; in contrast, 2) when two agents have two movable tokens each then rendezvous (respectively, rendezvous with detection) is possible with constant memory in an arbitrary n x m (respectively, n x n) torus; and finally, 3) two agents with three movable tokens each and constant memory can perform rendezvous with detection in a n x m torus. This is the first publication in the literature that studies tradeoffs between the number of tokens, memory and knowledge the agents need in order to meet in such a network.", "authors": ["Evangelos Kranakis", "Danny Krizanc", "Euripides Markou"], "n_citation": 0, "title": "Mobile agent rendezvous in a synchronous torus", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "674741e1-fb99-42ef-94d3-e2a19c458c02"}
{"abstract": "A ranking criterion based on the posterior probability is proposed for feature selection on support vector machines (SVM). This criterion has the advantage that it is directly related to the importance of the features. Four approximations are proposed for the evaluation of this criterion. The performances of these approximations, used in the recursive feature elimination (RFE) approach, are evaluated on various artificial and real-world problems. Three of the proposed approximations show good performances consistently, with one having a slight edge over the other two. Their performances compare favorably with feature selection methods in the literature.", "authors": ["Kai Quan Shen", "Chong Jin Ong", "Xiao Ping Li", "Hui Zheng", "Einar Wilder-Smith"], "n_citation": 50, "title": "Feature Selection Using SVM Probabilistic Outputs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "68c4773d-459a-4216-9ffc-11473677aa78"}
{"abstract": "We propose a generalized equation to represent a continuum of surface reconstruction solutions of a given non-integrable gradient field. We show that common approaches such as Poisson solver and Frankot-Chellappa algorithm are special cases of this generalized equation. For a N x N pixel grid, the subspace of all integrable gradient fields is of dimension N 2  - 1. Our framework can be applied to derive a range of meaningful surface reconstructions from this high dimensional space. The key observation is that the range of solutions is related to the degree of anisotropy in applying weights to the gradients in the integration process. While common approaches use isotropic weights, we show that by using a progression of spatially varying anisotropic weights, we can achieve significant improvement in reconstructions. We propose (a) \u03b1-surfaces using binary weights, where the parameter a allows trade off between smoothness and robustness, (b) M-estimators and edge preserving regularization using continuous weights and (c) Diffusion using affine transformation of gradients. We provide results on photometric stereo, compare with previous approaches and show that anisotropic treatment discounts noise while recovering salient features in reconstructions.", "authors": ["Amit K. Agrawal", "Ramesh Raskar", "Rama Chellappa"], "n_citation": 0, "title": "What Is the Range of Surface Reconstructions from a Gradient Field", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "71b1bc03-b504-4e2c-b865-e1008abb7d5b"}
{"abstract": "Functional logic languages extend purely functional languages with two features: operations defined by overlapping rules and logic variables in both defining rules and expressions to evaluate. In this paper, we show that only one of these features is sufficient in a core language. On the one hand, overlapping rules can be eliminated by introducing logic variables in rules. On the other hand, logic variables can be eliminated by introducing operations defined by overlapping rules. The proposed transformations between different classes of programs not only give a better understanding of the features of functional logic programs but also may simplify implementations of functional logic languages.", "authors": ["Sergio Antoy", "Michael Hanus"], "n_citation": 106, "title": "Overlapping rules and logic variables in functional logic programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7206f933-ef25-46e9-92c1-a13d44165727"}
{"abstract": "We study route selection for packet switching in the competitive throughput model. In contrast to previous papers which considered competitive algorithms for packet scheduling, we consider the packet routing problem (output port selection in a node). We model the node routing problem as follows: a node has an arbitrary number of input ports and an arbitrary number of output queues. At each time unit, an arbitrary number of new packets may arrive, each packet is associated with a subset of the output ports (which correspond to the next edges on the allowed paths for the packet). Each output queue transmits packets in some arbitrary manner. Arrival and transmission are arbitrary and controlled by an adversary. The node routing algorithm has to route each packet to one of the allowed output ports, without exceeding the size of the queues. The goal is to maximize the number of the transmitted packets. In this paper, we show that all non-refusal algorithms are 2-competitive. Our main result is an almost optimal e e-1 \u2243 1.58-competitive algorithm, for a large enough queue size. For packets with arbitrary values (allowing preemption) we present a 2-competitive algorithm for any queue size.", "authors": ["Yossi Azar", "Yoel Chaiutin"], "n_citation": 0, "title": "Optimal node routing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "722bd518-22f3-4cf5-923c-3baebed74313"}
{"abstract": "Metadata Integration over distributed digital libraries is regarded one of the difficulties to realize, since it is not clear how to describe arbitrary functionality such that other components can inspect the description and 'decide' automatically that this functionality is appropriate for a given task. This paper makes an approach in extending the Open Archive Initiative for Metadata Havesting Protocol (OAI-PMH) into the Peer-to-Peer network, with the goal of integrating and interoperating between or among heterogeneous but relevant metadata schemas.", "authors": ["Hao Ding"], "n_citation": 0, "title": "Towards the Metadata Integration issues in Peer-to-Peer based digital libraries", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "728e2eb7-5264-4a2d-bd34-e90cd1510928"}
{"abstract": "We describe in this paper a general algorithm for solving first-order constraints in the theory T of the evaluated trees which is a combination of the theory of finite or infinite trees and the theory of the rational numbers with addition, subtraction and a linear dense order relation. It transforms a first-order formula \u03c6, which can possibly contain free variables, into a disjunction \u03a6 of solved formulas which is equivalent in T, without new free variables and such that \u03a6 is either true or false or a formula having at least one free variable and being equivalent neither to true nor to false in T.", "authors": ["Thi-Bich-Hanh Dao", "Khalil Djelloul"], "n_citation": 0, "title": "Solving first-order constraints in the theory of the evaluated trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "72adbb70-cc0b-40ff-aaa8-6b46d928356f"}
{"abstract": "We characterize the expressive power of EX, EF and EX+EF logics. These are the fragments of CTL built using the respective operators. We give a forbidden pattern characterization of the tree languages definable in these logics. The characterizations give optimal algorithms for deciding if a given tree language is expressible in one of the three logics.", "authors": ["Miko\u0142aj Boja\u0144czyk", "Igor Walukiewicz"], "n_citation": 0, "title": "Characterizing EF and EX tree logics", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "73a29ba1-b56a-4c08-8a71-527ffea57b1a"}
{"abstract": "This paper presents a method for regression problem based on divide- and-conquer approach to the selection of a set of prototypes from the training set for the nearest neighbor rule. This method aims at detecting and eliminating redundancies in a given data set while preserving the significant data. A reduced prototype set contains Pairwise Opposite Class-Nearest Neighbor (POC-NN) prototypes which are used instead of the whole given data. Before finding POC-NN prototypes, all sampling data have to be separated into two classes by using the criteria through odd and even sampling number of data, then POC-NN prototypes are obtained by iterative separation and analysis of the training data into two regions until each region is correctly grouped and classified. The separability is determined by the POC-NN prototypes essential to define the function approximator for local sampling data locating near these POC-NN prototypes. Experiments and results reported showed the effectiveness of this technique and its performance in both accuracy and prototype rate to those obtained by classical nearest neighbor techniques.", "authors": ["Thanapant Raicharoen", "Chidchanok Lursinsap", "Frank Lin"], "n_citation": 0, "title": "A Divide-and-Conquer Approach to the Pairwise Opposite Class-Nearest Neighbor (POC-NN) Algorithm for Regression Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "776c7a07-0307-44ab-b02a-8dec1c311637"}
{"abstract": "Document clustering is classifying a data set of documents into groups of closely related documents, so that its resulting clusters can be used in browsing and searching the documents of a specific topic. In most cases of such as application, a set of new documents are incrementally added to the data set and there can be a large variation in the number of words in each document. This paper proposes an incremental document clustering method for an incrementally increasing data set of documents. The normalized inverse document frequency of a word in the data set is introduced to cope with the variation of the number of words in each document. Furthermore, an average link method for document clustering instead of using one similarity measure used in two similarity measures: a cluster cohesion rate and a cluster participation rate. Furthermore, a category tree for a set of identified clusters is introduced to assist the incremental document clustering of newly added documents. In this paper, the performance of the proposed method is analyzed by a series of experiments to identify their various characteristics.", "authors": ["Kil Hong Joo", "Soojung Lee"], "n_citation": 0, "title": "An incremental document clustering algorithm based on a hierarchical agglomerative approach", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7807e3d4-e156-45a7-a60b-33e926ed2e89"}
{"abstract": "Dirty paper coding are relevant for wireless networks, multiuser channels, and digital watermarking. We show that the problem of dirty paper is essentially equivalent to some classes of constrained memories, and we explore the binary so-called nested codes, which are used for efficient coding and error-correction on such channels and memories.", "authors": ["Hans Georg Schaathun", "G\u00e9rard D. Cohen"], "n_citation": 0, "title": "Nested Codes for Constrained Memory and for Dirty Paper", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "78fa7fef-3633-49fa-95f1-56e16660bfc6"}
{"abstract": "This paper discusses theoretical limitations of classification systems that are based on feature maps and use a separating hyper-plane in the feature space. In particular, we study the embeddability of a given concept class into a class of Euclidean half spaces of low dimension, or of arbitrarily large dimension but realizing a large margin. New bounds on the smallest possible dimension or on the largest possible margin are presented. In addition, we present new results on the rigidity of matrices and briefly mention applications in complexity and learning theory.", "authors": ["J\u00fcrgen Forster", "Hans Ulrich Simon"], "n_citation": 50, "title": "On the smallest possible dimension and the largest possible margin of linear arrangements representing given concept classes uniform distribution", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7907877c-362e-4c2f-b43c-120e33af841b"}
{"abstract": "We consider the problems of finding k-connected spanning subgraphs with minimum average weight. We show that the problems are NP-hard for k > 1. Approximation algorithms are given for four versions of the minimum average edge weight problem: 1. 3-approximation for k-edge-connectivity, 2. O(logk) approximation for k-node-connectivity 3. 2 + e approximation for k-node-connectivity in Euclidian graphs, for any constant \u2208 > 0, 4. 5.8-approximation for k-node-connectivity in graphs satisfying the triangle inequality.", "authors": ["Prabhakar Gubbala", "Balaji Raghavachari"], "n_citation": 0, "title": "Finding k-connected subgraphs with minimum average weight", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "79302032-9792-41be-b6dd-2fe1d5910907"}
{"abstract": "We introduce an algorithm for extracting all longest repeats with k don't cares from a given sequence. Such repeats are composed of two parts separated by a block of k don't care symbols. The algorithm uses suffix trees to fulfill this task and relies on the ability to answer the lowest common ancestor queries in constant time. It requires O(n log n) time in the worst-case.", "authors": ["Maxime Crochemore", "Costas S. Iliopoulos", "Manal Mohamed", "Marie-France Sagot"], "n_citation": 0, "title": "Longest repeats with a block of don't cares", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "798cab1b-b20e-462a-a14f-a17c00a8fa49"}
{"abstract": "With the recent development of LBS(Location Based Service) and Telematics, the use of spatio-temporal data mining which extracts useful knowledge such as movement patterns of moving objects gets increasing. However, the existing movement pattern extraction methods including STPMinel and STPMine2 create lots of candidate movement patterns when the minimum support is low. As a result of that, the performance of time and space is sharply increased as a weak point. Therefore, in this paper, we suggest the STMPE (Spatio-Temporal Movement Pattern Extraction) algorithm in order to efficiently extract movement patterns of moving objects from the large capacity of spatio-temporal data. The STMPE algorithm generalizes spatio-temporal data and minimizes the use of memory. Because it produces and maintains short-term movement patterns, the frequency of database scan can be minimized. Actually, the STMPE algorithm was improved twice to 10 times better than STPMinel and STPMine2 from the result of performance evaluation.", "authors": ["Dong-Oh Kim", "Hong-Koo Kang", "Dong-Suk Hong", "Jae-Kwan Yun", "Ki-Joon Han"], "n_citation": 50, "title": "STMPE : An Efficient Movement Pattern Extraction Algorithm for Spatio-temporal Data Mining", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7d4eef9a-1814-4dc4-ac42-e7955102ca10"}
{"abstract": "A novel high performance computational (HPC) framework by using Computational Grid technology for aero-crafts aerodynamic simulation and optimization is present. There are three components in this HPC CFD-Grid framework: Grid Service module based on IBM Service Domain, general interface standards for CFD codes and CFD codes for application. All those components in the service domain system are named as Grid Services independently. The HPC CFD-Grid framework can run on various computers with various platforms, through which various CFD codes and computer resources are provided to the designers in the form of services, no matter where those computers are located. Based on this CFD-Grid of Shanghai Jiao Tong University supported by China-Grid project, aero-crafts aerodynamic simulation and optimization can be completed. To demonstrate the ability of this CFD-Grid HPC framework, aero-craft simulation and optimization cases was described.", "authors": ["Hong Liu", "Xinhua Lin", "Yang Qi", "Xin-Da Lu", "Li Ming-lu"], "n_citation": 0, "title": "Aero-crafts aerodynamic simulation and optimization by using CFD-Grid based on Service domain", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7fe2a334-ee4d-4a4f-9f1e-f426e66a8832"}
{"abstract": "In this paper, we present a novel congestion control algorithm for the Transmission Control Protocol (TCP) for the future Internet. Our assumption of future Internet is that, with the increasing quality of service (QoS) requirements, per-flow packet scheduling (per-flow here refers to per TCP or UDP connection) will replace the current first-come-first-serve algorithm used in routers. Based on the assumption, we design a new congestion control algorithm. In our TCP-CC algorithm, each connection adjusts the size of the congestion window according to the size of its packet queue at the bottleneck router. Thus, the queue size for each connection at the bottleneck router is within a controlled range. We show that congestion loss is effectively reduced compared to the current TCP congestion algorithm.", "authors": ["Haiguang Wang", "Winston Khoon Guan Seah"], "n_citation": 0, "title": "A novel TCP congestion control (TCP-CC) algorithm for future internet applications and services", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "802dca80-33fc-4cb6-9eca-e32715c3bfac"}
{"abstract": "Computing efficiently with numbers can be crucial for some theorem proving applications. In this paper, we present a library of modular arithmetic that has been developed within the COQ proof assistant. The library proposes the usual operations that have all been proved correct. The library is purely functional but can also be used on top of some native modular arithmetic. With this library, we have been capable of certifying the primality of numbers with more than 13000 digits.", "authors": ["Benjamin Gr\u00e9goire", "Laurent Th\u00e9ry"], "n_citation": 50, "title": "A purely functional library for modular arithmetic and its application to certifying large prime numbers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "81c90b7c-517e-40ce-ad6e-eca1273e6f32"}
{"abstract": "In the domain of scientific computation, there exist a large number of legacy applications that are too valuable to give up and too complex to rewrite. Enabling them on the computational grid plays an important role for realizing the Grid potential. This paper presents a component-based approach for managing and accessing legacy applications on the computational grid. It encapsulates a collection of complementary and competitive legacies to be an independent entity, which automatically schedules these distributed legacies with domain expertise to perform computation. Computing jobs are specified in domain terms and dynamically mapped to appropriate resources. Transparently, competitive legacies are utilized to improve load balance and reliability, and complementary legacies are scheduled to meet the requirements of jobs with different characteristics. To evaluate this approach, a prototype has been implemented, and preliminary experiment results are also presented.", "authors": ["Huashan Yu", "Zhuoqun Xu", "Wenkui Ding"], "n_citation": 0, "title": "Scheduling legacy applications with domain expertise for autonomic computing", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "84ef37a3-60de-4bd9-b1d0-ebe395e2ed4b"}
{"abstract": "A novel approach is developed for Protein Secondary Structure Prediction based on Bayesian Neural Networks (BNN). BNN usually outperforms the traditional Back-Propagation Neural Networks (BPNN) due to its excellent ability to control the complexity of the model. Results indicates that BNN has an average overall three-state accuracy Q 3  increase 3.65% and 4.01% on the 4-fold cross-validation data sets and TEST data set respectively, comparing with the traditional BPNN. Meanwhile, a so-called cross-validation choice of starting values is presented, which will shorten the bum-in phase during the MCMC (Markov Chain Monte Carlo) simulation substantially.", "authors": ["Jianlin Shao", "Dong Xu", "Lanzhou Wang", "Yifei Wang"], "n_citation": 0, "title": "Bayesian neural networks for prediction of protein secondary structure", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "850cfbde-502d-4fa6-831e-8b40cb13e387"}
{"abstract": "We consider a class of infinite-state Markov decision processes generated by stateless pushdown automata. This class corresponds to 11 2-player games over graphs generated by BPA systems or (equivalently) 1-exit recursive state machines. An extended reachability objective is specified by two sets S and T of safe and terminal stack configurations, where the membership to S and T depends just on the top-of-the-stack symbol. The question is whether there is a suitable strategy such that the probability of hitting a terminal configuration by a path leading only through safe configurations is equal to (or different from) a given x \u2208 {0, 1}. We show that the qualitative extended reachability problem is decidable in polynomial time, and that the set of all configurations for which there is a winning strategy is effectively regular. More precisely, this set can be represented by a deterministic finite-state automaton with a fixed number of control states. This result is a generalization of a recent theorem by Etessami & Yannakakis which says that the qualitative termination for 1-exit RMDPs (which exactly correspond to our 11 2-player BPA games) is decidable in polynomial time. Interestingly, the properties of winning strategies for the extended reachability objectives are quite different from the ones for termination, and new observations are needed to obtain the result. As an application, we derive the EXPTIME-completeness of the model-checking problem for 11 2-player BPA games and qualitative PCTL formulae.", "authors": ["Tom\u00e1\u0161 Br\u00e1zdil", "Vaclav Brozek", "Vojtech Forejt", "Anton\u00edn Ku\u010dera"], "n_citation": 50, "title": "Reachability in recursive markov decision processes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8599892d-e754-4e1f-b777-88151952b955"}
{"abstract": "Robotic soccer remains an area of active research owing to the difficulties of dynamic team formation and hard real time constraints regarding planning. Much of the existing research relies upon a central agency for coordination. Insect societies distribute work and allocate roles without a need for such a central agency and are robust with respect to changing environments and available agent resources. This paper explores the use of insect-inspired division of labour principles to robot soccer, highlighting the flexibility of the approach and ability to adapt to a wide range of soccer playing strategies.", "authors": ["Tony White", "James P. Helferty"], "n_citation": 0, "title": "Emergent team formation : Applying division of labour principles to robot soccer", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "881bc8f5-0fad-4dfb-9ad1-2fbe9dc92f9f"}
{"abstract": "In this paper we present a model of functional access to data that, we argue, is suitable for modeling a class of data repositories characterized by functional access, such as web sites. We discuss the problem of modeling such data sources as a set of relations, of determining whether a given query expressed on these relations can be translated into a combination of functions defined by the data sources, and of finding an optimal plan to do so. We show that, if the data source is modeled as a single relation, an optimal plan can be found in a time linear in the number of functions in the source but, if the source is modeled as a number of relations that can be joined, finding the optimal plan is NP-hard.", "authors": ["Simone Santini", "Amarnath Gupta"], "n_citation": 0, "title": "Modeling functional data sources as relations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "88a1f762-a1f7-40ac-95d1-311e4d01c69f"}
{"abstract": "The S-connectivity \u03bb G  S (u, v) of (u, v) in a graph G is the maximum number of uv-paths that no two of them have an edge or a node in S - {u,v} in common. The corresponding Connectivity Augmentation (CA) problem is: given a graph Go = V, E 0 ), S \u2286V, and requirements r(u,v) on V x V, find a minimum size set F of new edges (any edge is allowed) so that \u03bb Gu  S +F.(u,v) > r(u,v) for all u,v \u2208 V. Extensively studied particular cases are the edge-CA (when S = \u03a6) and the node-CA (when S =V). A. Frank gave a polynomial algorithm for undirected edge-CA and observed that the directed case even with r(u,v) e {0,1} is at least as hard as the Set-Cover problem. Both directed and undirected node-CA have approximation threshold \u03a9(2log 1- \u2208 n ). We give an approximation algorithm that matches these approximation thresholds. For both directed and undirected CA with arbitrary requirements our approximation ratio is: O(log n) for S\u00ac= V arbitrary, and O(r max ..logn) for S = V, where r max  = max u,v \u2208 V  r(u,v).", "authors": ["Guy Kortsarz", "Zeev Nutov"], "n_citation": 0, "title": "Tight Approximation Algorithm for Connectivity Augmentation Problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "88c6d51b-b8af-44aa-8751-7dba710cef34"}
{"abstract": "A number of natural models for learning in the limit is introduced to deal with the situation when a learner is required to provide a grammar covering the input even if only a part of the target language is available. Examples of language families are exhibited that are learnable in one model and not learnable in another one. Some characterizations for learnability of algorithmically enumerable families of languages for the models in question are obtained. Since learnability of any part of the target language does not imply monotonicity of the learning process, we consider also our models under additional monotonicity constraint.", "authors": ["Sanjay Jain", "Efim B. Kinber"], "n_citation": 0, "title": "Learning and extending sublanguages", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "88fbebb6-7220-450e-b3e6-fc0efd82687e"}
{"abstract": "This paper proposes a novel approach for cryptanalysis of certain cryptographic pseudorandom sequence (keystream) generators consisting of the composition of a linear finite state machine (LFSM) and nonlinear mapping. The proposed approach includes a dedicated decimation of the sample for cryptanalysis based on the following: Suppose certain B bits of the LFSM initial state as known and identify time instances where certain arguments of the nonlinear function depend only on these B bits and are equal to zero. As opposed to previously reported methods, the proposed one also identifies and uses certain characteristics of the LFSM state-transition matrix in order to reduce the nonlinearity of the system of overdefined equations employed in an algebraic attack scenario, or to reduce the noise introduced by the linearization of the nonlinear function which corrupts the linear equations employed in a correlation attack scenario.", "authors": ["Miodrag J. Mihaljevic", "Marc P. C. Fossorier", "Hideki Imai"], "n_citation": 50, "title": "A General Formulation of Algebraic and Fast Correlation Attacks Based on Dedicated Sample Decimation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "895366b0-a291-47b4-91fb-441002f51f80"}
{"abstract": "We introduce a new surface deformation and modeling method in this paper. Referring to the swept volume generation, the surface is pulled or pushed along a trajectory curve. The key point is the sweeping function. Surface points are moved to where they should be during sweeping operations according to the global parameter, which is determined by topological distance. An index factor controls how much the surface deforms around the handle point. The proposed method is easy to extend to fit different applications such as various constraints, local deformation and animations.", "authors": ["Chengjun Li", "Wenbing Ge", "Guoping Wang"], "n_citation": 50, "title": "Dynamic surface deformation and modeling using rubber sweepers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "91cf1155-100a-4f46-ab36-9b620848750a"}
{"abstract": "The rank, r, and the dimension of the kernel, k, for binary Hadamard codes of length 2 t  were studied in [12], constructing such codes for all possible pairs (r, k). Now, we will focus on Hadamard codes of length 2 t  s, s > 1 odd. As long as there exists a Hadamard code of length 4s, constructions of Hadamard codes of length n = 2 t . s (t > 3) with any rank, r \u2208 {4s + t - 3,..., n/2}, and any possible dimension of the kernel, k \u2208 {1,..., t - 1}, are given.", "authors": ["Kevin T. Phelps", "Josep Rif\u00e0", "Merc\u00e8 Villanueva"], "n_citation": 2, "title": "Hadamard Codes of Length 2ts (s Odd). Rank and Kernel", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "97424845-072a-4d29-b8a7-6546dfe46f45"}
{"abstract": "We study network and congestion games with atomic players that can split their flow. This type of games readily applies to competition among freight companies, telecommunication network service providers, intelligent transportation systems and manufacturing with flexible machines. We analyze the worst-case inefficiency of Nash equilibria in those games and conclude that although self-interested agents will not in general achieve a fully efficient solution, the loss is not too large. We show how to compute several bounds for the worst-case inefficiency, which depend on the characteristics of cost functions and the market structure in the game. In addition, we show examples in which market aggregation can adversely impact the aggregated competitors, even though their market power increases. When the market structure is simple enough, this counter-intuitive phenomenon does not arise.", "authors": ["Roberto Cominetti", "Jos\u00e9 R. Correa", "Nicol\u00e1s E. Stier-Moses"], "n_citation": 0, "title": "Network Games with Atomic Players", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "98334e42-6223-4a35-aefd-ceae09052eb1"}
{"abstract": "This paper presents a non-conventional application of symbolic animation. We propose to assist the modeller in building an efficient formal model, by automatically detecting potential weaknesses or imprecisions in the model. We propose to detect inconsistencies within the formal models written with pre- and postconditions, and to point out unusual model properties, such as a weak invariant or unreachable effects. Our approach is based on constraint solving technologies to perform the animation and to detect the various problems.", "authors": ["Fabrice Bouquet", "Fr\u00e9d\u00e9ric Dadeau", "Bruno Legeard"], "n_citation": 0, "title": "How symbolic animation can help designing an efficient formal model", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9a15c5a6-3c54-4e82-8fe3-96aeeb41d508"}
{"abstract": "Side-channel attacks on block ciphers and public key algorithms have been discussed extensively. However, there is only sparse literature about side-cannel attacks on stream ciphers. The few existing references mainly treat timing [8] and template attacks [10], or provide a theoretical analysis [6], [7] of weaknesses of stream cipher constructions. In this paper we present attacks on two focus candidates, Trivium and Grain, of the eSTREAM stream cipher project. The attacks exploit the resynchronization phase of ciphers. A novel concept for choosing initial value vectors is introduced, which totally eliminates the algorithmic noise of the device, leaving only the pure side-channel signal. This attack allows to recover the secret key with a small number of samples and without building templates. To prove the concept we apply the attack to hardware implementations of the ciphers. For both stream ciphers we are able to reveal the complete key.", "authors": ["Wieland Fischer", "Berndt Gammel", "Oliver Kniffler", "Joachim Velten"], "n_citation": 52, "title": "Differential power analysis of stream ciphers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9bedd656-ba60-4730-9ef7-27401d0c10a1"}
{"abstract": "In recent years, Minimax Probability Machine (MPM) have demonstrated excellent performance in a variety of pattern recognition problems. At the same time various machine learning methods have been used on relevance feedback tasks in Content-based Image Retrieval (CBIR). One of the problems in typical techniques for relevance feedback is that they treat the relevant feedback and irrelevant feedback equally. In other words, the negative instances largely outnumber the positive instances. Hence, the assumption that they are balanced is incorrect. In this paper we study how MPM can be applied to image retrieval, more precisely, Biased MPM during the relevance feedback iterations. We formulate the relevance feedback based on a modified MPM called Biased Minimax Probability Machine (BMPM). Different from previous methods, this model directly controls the accuracy of classification of the future data to build up biased classifiers. Hence, it provides a rigorous treatment on imbalanced data. Mathematical formulation and explanations are provided for showing the advantages. Experiments are conducted to evaluate the performance of our proposed framework, in which encouraging and promising experimental results are obtained.", "authors": ["Xiang Peng", "Irwin King"], "n_citation": 50, "title": "Imbalanced Learning in Relevance Feedback with Biased Minimax Probability Machine for Image Retrieval Tasks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9cd6c245-650f-4ce2-b92c-17f478ef2ee3"}
{"abstract": "Many connections have been established between learning and logic, or learning and topology, or logic and topology. Still, the connections are not at the heart of these fields. Each of them is fairly independent of the others when attention is restricted to basic notions and main results. We show that connections can actually be made at a fundamental level, and result in a parametrized logic that needs topological notions for its early developments, and notions from learning theory for interpretation and applicability. One of the key properties of first-order logic is that the classical notion of logical consequence is compact. We generalize the notion of logical consequence, and we generalize compactness to \u03b2-weak compactness where \u03b2 is an ordinal. The effect is to stratify the set of generalized logical consequences of a theory into levels, and levels into layers. Deduction corresponds to the lower layer of the first level above the underlying theory, learning with less than \u03b2 mind changes to layer \u03b2 of the first level, and learning in the limit to the first layer of the second level. Refinements of Borel-like hierarchies provide the topological tools needed to develop the framework.", "authors": ["Eric Martin", "Arun Sharma", "Prank Stephan"], "n_citation": 50, "title": "Learning, logic, and topology in a common framework", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9f1ff645-9a42-4106-b539-b85141b958d2"}
{"abstract": "This paper presents two enhancements for the creation and analysis of Binary Partition Trees (BPTs). Firstly, the classic creation of BPT based on colour is expanded to include syntactic criteria derived from human perception. Secondly, a method to include semantic information in the BPT analysis is shown thanks to the definition of the BPT Semantic Neighborhood and the introduction of Semantic Trees. Both techniques aim at bridging the semantic gap between signal and semantics following a bottom-up and a top-down approach, respectively.", "authors": ["Carlos Ferran", "Xavier Gir\u00f3", "Ferran Marques", "Joan R. Casas"], "n_citation": 50, "title": "BPT enhancement based on syntactic and semantic criteria", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a0c73650-9426-4307-8966-eb1bfc20af07"}
{"abstract": "We prove that the minimum weight of a dicycle is equal to the maximum number of disjoint dicycle covers, for every weighted digraph whose underlying graph is planar and does not have K 5  - e as a minor (K 5  - e is the complete graph on five vertices, minus one edge). Equality was previously known when forbidding K 4  as a minor, while an infinite number of weighted digraphs show that planarity does not guarantee equality. The result also improves upon results known for Woodall's Conjecture and the Edmonds-Giles Conjecture for packing dijoins. Our proof uses Wagner's characterization of planar 3-connected graphs that do not have K 5  - e as a minor.", "authors": ["Orlando Lee", "Aaron Williams"], "n_citation": 0, "title": "Packing dicycle covers in planar graphs with no K5 -e minor", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a101b5cd-1d0d-466a-a4de-d6575e3bd06a"}
{"abstract": "Object-oriented unit tests consist of sequences of method invocations. Behavior of an invocation depends on the method's arguments and the state of the receiver at the beginning of the invocation. Correspondingly, generating unit tests involves two tasks: generating method sequences that build relevant receiver-object states and generating relevant method arguments. This paper proposes Symstra, a framework that achieves both test generation tasks using symbolic execution of method sequences with symbolic arguments. The paper defines symbolic states of object-oriented programs and novel comparisons of states. Given a set of methods from the class under test and a bound on the length of sequences, Symstra systematically explores the object-state space of the class and prunes this exploration based on the state comparisons. Experimental results show that Symstra generates unit tests that achieve higher branch coverage faster than the existing test-generation techniques based on concrete method arguments.", "authors": ["Tao Xie", "Darko Marinov", "Wolfram Schulte", "David Notkin"], "n_citation": 294, "title": "Symstra : A framework for generating object-oriented unit tests using symbolic execution", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a1e0776a-7bdc-4dd5-9a93-e6c99e25c439"}
{"abstract": "DHT routing algorithms have been studied from various aspects. The advantage of this kind of algorithms is that they can locate objects successfully in polylogadthmic overlay hops. Instead of getting testing results via network simulation, this paper describes experiences deploying Emergint, a DHT based peer-to-peer routing algorithm, in a real circumstance aiming to answer the question whether a DHT algorithm can get its theoretical performance in real-used systems.", "authors": ["Chong Wang", "Yafei Dai", "Hua Han", "Xiaoming Li"], "n_citation": 0, "title": "Experiences deploying peer-to-peer network for a distributed file system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a33e227a-68bb-4487-bcde-825d0b5e5439"}
{"abstract": "We introduce a method to elicit and to structure using Event B, processes interacting with ad hoc dynamic architecture. A Group Communication System is used as the investigation support. The method takes account of the evolving structure of the interacting processes; it guides the user to structure the abstract system that models his/her requirements. The method also integrates property verification using both theorem proving and model checking. A B specification of a GCS is built using the proposed approach and its stated properties are verified using a B theorem prover and a B model checker.", "authors": ["J. Christian Attiogbe"], "n_citation": 50, "title": "Multi-process Systems Analysis Using Event B : Application to Group Communication Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a38ecb1e-ba83-4e1b-8193-668fe189c754"}
{"abstract": "In this paper, we illustrate an augmented reality toolkit that can be used to develop edutainment materials with 3-dimensional graphics and video images. It is based on SEDRIS(Synthetic Environment Data Representation and Interchange Specification) and represents a complete range of real world object attributes. The toolkit manipulates and integrates 3-dimensional graphics objects and video. The virtual environment data structure allows object attributes to be visualized and interactively manipulated in the toolkit.", "authors": ["Myeong Won Lee", "Min-Gun Lee", "Sung-Gon Kim", "Kicheon Hong"], "n_citation": 0, "title": "An augmented reality toolkit based on SEDRIS", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a4afaa47-9ebb-43a7-953b-61f57b8ae8df"}
{"abstract": "The paper presents a sound and (relatively) complete deductive proof system for the verification of CTL* properties over possibly infinite-state reactive systems. The proof system is based on a set of proof rules for the verification of basic CTL* formulas, namely CTL* formulas with no embedded path quantifiers. We first show how to decompose the proof of a general (non-basic) CTL* formula into proofs of basic CTL* formulas. We then present proof rules for some of the most useful basic CTL* formulas, then present a methodology for transforming an arbitrary basic formula into one of these special cases.", "authors": ["Amir Pnueli", "Yonit Kesten"], "n_citation": 0, "title": "A deductive proof system for CTL", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a5498f44-6df4-4f74-a2fd-34036736b139"}
{"abstract": "The Grid promises improvements in the effectiveness with which global businesses are managed if it enables distributed expertise to be efficiently applied to the analysis of distributed data. We report an ESRC-funded collaboration between EPCC in Edinburgh and Curtin University of Technology in Perth, Australia, that is applying public-domain Grid technologies to secure data mining within a commercial environment. We describe this Grid infrastructure and discuss its strengths and weaknesses.", "authors": ["Alastair Hume", "Ashley Lloyd", "Terence Sloan", "Adam Carter"], "n_citation": 50, "title": "Applying Grid technologies to distributed data mining", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a6bb31d8-756d-4354-ab04-31470c2c4ae7"}
{"abstract": "In this paper, an advanced distributed control scheme that connects the control networks to the IP networks, based on LonWorks technology, which is one of the control networks, are presented. The proposed approach is implemented by using a simple programmable basic Lon node (BLN) and a IBM PS/2 (PC) compatible computer as an Internet server. BLN is a developed electric board in this research and it physically contains a transceiver, Neuron chip and some memory devices. To perform various functions as an Internet server of PC, control software of Lon on internet system (LOIS) with C-language for GNU/LINUX environment is also developed. Our approach makes system designers to easily implement their various specific applications, only with the download of a control program from serial port (RS-232) of PC.", "authors": ["Il-Joo Shim", "Kyung-Bae Chang", "Ki-Hyung Yu", "Dong-Woo Cho", "Kyoo-Dong Song", "Gwi-Tae Park"], "n_citation": 0, "title": "An advanced implementation of a distributed control scheme based on lonworks system over IP networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a989df73-4866-4a70-b62d-492ffa319a3e"}
{"abstract": "In this paper, a music information retrieval system in real mobile environment is proposed. In order to alleviate distortions due to the mobile noise, a noise reduction algorithm is applied and then a feature extraction using Multi-Feature Clustering is implemented to improve the system performance. The proposed system shows quite successful performance with real world cellular phone data.", "authors": ["Won-Jung Yoon", "Sanghun Oh", "Kyu-Sik Park"], "n_citation": 0, "title": "Robust Music Information Retrieval on Mobile Network Based on Multi-Feature Clustering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aba02fba-f73d-4df5-92f0-797055371f08"}
{"abstract": "We consider a problem that is related to the Universal Encoding Problem from information theory. The basic goal is to find rules that map partial information about a distribution X over an m-letter alphabet into a guess X for X such that the Kullback-Leibler divergence between X and X is as small as possible. The cost associated with a rule is the maximal expected Kullback-Leibler divergence between X and X. First, we show that the cost associated with the well-known add-one rule equals ln(1 + (m - 1)/(n + 1)) thereby extending a result of Forster and Warmuth [3,2] to m > 3. Second, we derive an absolute (as opposed to asymptotic) lower bound on the smallest possible cost. Technically, this is done by determining (almost exactly) the Bayes error of the add-one rule with a uniform prior (where the asymptotics for n \u2192 \u221e was known before). Third, we hint to tools from approximation theory and support the conjecture that there exists a rule whose cost asymptotically matches the theoretical barrier from the lower bound.", "authors": ["Dietrich Braess", "J\u00fcrgen Forster", "Tomas Sauer", "Hans Ulrich Simon"], "n_citation": 0, "title": "How to achieve minimax expected Kullback-Leibler distance from an unknown finite distribution", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b043b97e-f209-4277-be56-d8ae880e5053"}
{"abstract": "The home network is expected to experience significant growth over next few years, as wireless and ubiquitous networking becomes more common and accessible. However, the broadcast nature of this technology creates new security issues. To ensure the effective deployment in home environments, network security must reach a certain level which is reasonably acceptable to the research community. The security mechanism for home networks must not require heavy computations, since usually consist of low CPUs capable, limited memory and storage, and mobility concerns. This paper presents a secure authentication and session key establishment mechanism suitable for home networks. The proposed scheme is based on the Secure Remote Password (SRP) protocol. The performance evaluation demonstrates that our proposed mechanism is more secure than previous ones while maintaining the similar level of security overhead including processing time.", "authors": ["Hoseong Jeon", "Min Young Chung", "Jaehyoun Kim", "Hyunseung Choo"], "n_citation": 0, "title": "Verifier-Based Home Network Security Mechanism", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b060563b-04cd-4b22-b6a6-662e0433fdbe"}
{"abstract": "An improved 6DOF electromagnetic tracking algorithm is proposed. It utilizes the scattering parameters, which are about the transmitter coils, the receiver coils and the involved circuits, to determine the pose and orientation information of the target. Compared with the traditional algorithm which takes the system parameters as same in all the three directions, it is more accordant to the nature of the current tracking device. And it also simplifies the system hardware design. The experimental results prove the algorithm is valid and can improve the accuracy of the system.", "authors": ["Yan Zhi-gang", "Yuan Kui"], "n_citation": 0, "title": "An improved 6DOF electromagnetic tracking algorithm with anisotropic system parameters", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b6258896-a6a0-4314-8764-3f2f382b4a19"}
{"abstract": "We present a modification of Brass et al's transformation-based method for the bottom-up computation of well-founded semantics (WFS), in order to cope with explicit negation, in the sense of Alferes and Pereira's WFSX semantics. This variation consists in the simple addition of two intuitive transformations that guarantee the satisfaction of the so-called coherence principle: whenever an objective literal is founded, its explicit negation must be unfounded. The main contribution is the proof of soundness and completeness of the resulting method with respect to WFSX. Additionally, by a direct inspection on the method, we immediately obtain results that help to clarify the comparison between WFSX and regular WFS when dealing with explicit negation.", "authors": ["Pedro Cabalar"], "n_citation": 50, "title": "A rewriting method for well-founded semantics with explicit negation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b679ff21-b0a6-47db-9d70-a30898772bf3"}
{"abstract": "This paper presents a methodology for graphic pattern design and redesign applicable to tile and textile patterns. The methodology is based on a Design Information System integrated with two computer tools. One for the structural and morphologic analysis of graphic designs that uses as reference framework the scientific theory of symmetry groups, and a second interactive tool for the structural edition of patterns that exploits all the capabilities provided by the manipulation of the minimum region and fundamental parallelogram of pattern designs. We present some application examples to generate new designs as modifications from designs acquired from historic sources. The methodology and tools are oriented to bridge the gap between the historical and artistic production of graphic design and the tile and textile industries.", "authors": ["Josh Maria Gomis", "Margarita Valor", "Francisco Albert", "Manuel Contero"], "n_citation": 0, "title": "Intregated System and methodology for supporting textile and tile pattern design", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b7c2d42b-7416-4c71-9f11-868231f44e86"}
{"abstract": "Fuzzy co-clustering is a method that performs simultaneous fuzzy clustering of objects and features. In this paper, we introduce a new fuzzy co-clustering algorithm for high-dimensional datasets called Cosine-Distance-based & Dual-partitioning Fuzzy Co-clustering (CODIALING FCC). Unlike many existing fuzzy co-clustering algorithms, CODIALING FCC is a dual-partitioning algorithm. It clusters the features in the same manner as it clusters the objects, that is, by partitioning them according to their natural groupings. It is also a cosine-distance-based algorithm because it utilizes the cosine distance to capture the belongingness of objects and features in the co-clusters. Our main purpose of introducing this new algorithm is to improve the performance of some prominent existing fuzzy co-clustering algorithms in dealing with datasets with high overlaps. In our opinion, this is very crucial since most real-world datasets involve significant amount of overlaps in their inherent clustering structures. We discuss how this improvement can be made through the dual-partitioning formulation adopted. Experimental results on a toy problem and five large benchmark document datasets demonstrate the effectiveness of CODIALING FCC in handling overlaps better.", "authors": ["William-Chandra Tjhi", "Li-Hui Chen"], "n_citation": 0, "title": "A New Fuzzy Co-clustering Algorithm for Categorization of Datasets with Overlapping Clusters", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bc632f62-7702-4049-8168-55e0056d5113"}
{"abstract": "A stable proposal for extending the first-order TPTP (Thousands of Problems for Theorem Provers) language to higher-order logic, based primarily on lambda-calculus expressions, is presented. The purpose of the system is to facilitate sharing of theorem-proving problems in higher-order logic among many researchers. Design goals are discussed. BNF2, a new specification language, is presented. Unix/Linux scripts translate the specification document into a lex scanner andyacc parser.", "authors": ["Allen Van Gelder", "Geoff Sutcliffe"], "n_citation": 50, "title": "Extending the TPTP language to higher-order logic with automated parser generation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bc85f175-390a-490e-aafc-729708d16bc0"}
{"abstract": "We define a high-level model to mathematically capture the behavioural interface of abstract Virtual Providers (VP), their refinements and their composition into rich mediator structures. We show for a Virtual Internet Service Provider example how to use such a model for rigorously formulating and proving properties of interest.", "authors": ["Michael Altenhofen", "Egon B\u00f6rger", "Jens Lemcke"], "n_citation": 50, "title": "An abstract model for process mediation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bcbefed3-1b31-4898-8ece-a5ffaa7cb898"}
{"abstract": "A retrieval-aware image format (rim format) is developed for the usage in the similar-image retrieval. The format is based on PCA and ICA which can compress source images with an equivalent or often better rate-distortion than JPEG. Besides the data compression, the learned PCA/ICA bases are utilized in the similar-image retrieval since they reflect each source image's local patterns. Following the format presentation, an image search viewer for network environments (Wisvi; Waseda image search viewer) is presented. Therein, each query is an image per se. The Wisvi system based on the 'rim method successfully finds similar-images from non-uniform network environments. Experiments support that the PCA/ICA methods are viable to the joint compression and retrieval of digital images. Interested test users can download a /3-version of the tool for the joint image compression and retrieval from a web site specified in this paper.", "authors": ["Naoto Katsumata", "Yasuo Matsuyama", "Takeshi Chikagawa", "Fuminori Ohashi", "Fumiaki Horiike", "Shunichi Honma", "Tomohiro Nakamura"], "n_citation": 0, "title": "Retrieval-Aware Image Compression, Its Format and Viewer Based Upon Learned Bases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bcf768ab-1581-43ae-846b-df7bc5ef8f46"}
{"abstract": "We propose a novel neural network for classification of visual patterns. The new network, called pyramidal neural network or PyraNet, has a hierarchical structure with two types of processing layers, namely pyramidal layers and 1-D layers. The PyraNet is motivated by two concepts: the image pyramids and local receptive fields. In the new network, nonlinear 2-D are trained to perform both 2-D analysis and data reduction. In this paper, we present a fast training method for the PyraNet that is based on resilient back-propagation and weight decay, and apply the new network to classify gender from facial images.", "authors": ["Son Lam Phung", "Abdesselam Bouzerdoum"], "n_citation": 50, "title": "Gender Classification Using a New Pyramidal Neural Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c106c2b1-b93c-4d1d-8023-4812a4335e7d"}
{"abstract": "In this paper, we present our behavioral SDL model for the IEEE 802.15.3 MAC protocol. The model was derived using an object-oriented approach based on the client/server paradigm and is divided into data path and control path subsystems. Within the model, there are different abstraction layers, each layer providing a well-defined set of services to the higher layers. This modular design approach facilitated teamwork and led to a model, which is understandable, easy to extend, adapt, and test. Thus, our SDL model can serve as a basis for the following steps in the design flow, which is also presented briefly.", "authors": ["Daniel Dietterle", "Irina Babanskaja", "Kai F. Dombrowski", "Rolf Kraemer"], "n_citation": 0, "title": "High-level behavioral SDL model for the IEEE 802.15.3 MAC protocol", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c28540f5-b19b-4a63-8a0d-8a102a2d0f99"}
{"abstract": "Distributed computing on heterogeneous nodes, or grid computing, provides a substantial increase in computational power available for many applications. This paper reports our experience of calculating cryptographic hashes on a small grid test bed using a software package called BOINC. The computation power on the grid allows for searching the input space of a cryptographic hash to find a matching hash value. In particular, we show an implementation of searching possible 5 character passwords hashed with the MD4 algorithm on the grid. The resulting performance shows individual searches of sections of the password space returning a near linear decrease in calculation time based on individual participant node performance. Due to the overhead involved of scheduling these sections of the password space and processing of the results, the overall performance gain is slightly less than linear, but still reasonably good. We plan to design new scheduling algorithms and perform more testing to enhance BOINC's capability in our future research.", "authors": ["Stephen Pellicer", "Yi Pan", "Minyi Guo"], "n_citation": 0, "title": "Distributed MD4 password hashing with grid computing package BOINC", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c28ece28-1559-4773-89b4-9ae016e08abf"}
{"abstract": "The authorization problem is to decide whether, according to a security policy, some principal should be allowed access to a resource. In the trust-management system SPKI/SDSI, the security policy is given by a set of certificates, and proofs of authorization take the form of certificate chains. The certificate-chain-discovery problem is to discover a proof of authorization for a given request. Certificate-chain-discovery algorithms for SPKI/SDSI have been investigated by several researchers. We consider a variant of the certificate-chain discovery problem where the certificates are distributed over a number of servers, which then have to cooperate to identify the proof of authorization for a given request. We propose two protocols for this purpose. These protocols are based on distributed model-checking algorithms for weighted pushdown systems (WPDSs). These protocols can also handle cases where certificates are labeled with weights and where multiple certificate chains must be combined to form a proof of authorization. We have implemented these protocols in a prototype and report preliminary results of our evaluation.", "authors": ["Somesh Jha", "Stefan Schwoon", "Hao Wang", "Thomas W. Reps"], "n_citation": 50, "title": "Weighted pushdown systems and trust-management systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c295b016-8115-48a4-9fdc-c26a87d19937"}
{"abstract": "The Fujisaki-Okamoto (FO) conversion is widely known to be able to generically convert a weak public key encryption scheme, say one-way against chosen plaintext attacks (OW-CPA), to a strong one, namely, indistinguishable against adaptive chosen ciphertext attacks (IND-CCA). It is not known that if the same holds for identity-based encryption (IBE) schemes, though many IBE and variant schemes are in fact specifically using the FO conversion. In this paper, we investigate this issue and confirm that the FO conversion is generically effective also in the IBE case. However, straightforward application of the FO conversion only leads to an IBE scheme with a loose (but polynomial) reduction. We then propose a simple modification to the FO conversion, which results in considerably more efficient security reduction.", "authors": ["Peng Yang", "Takashi Kitagawa", "Goichiro Hanaoka", "Rui Zhang", "Kanta Matsuura", "Hideki Imai"], "n_citation": 0, "title": "Applying Fujisaki-Okamoto to Identity-Based Encryption", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c31bd0b5-7c49-43b2-8455-9d57f8855bc4"}
{"abstract": "One obtains in this paper a process algebra RCCS, in the style of CCS, where processes can backtrack. Backtrack, just as plain forward computation, is seen as a synchronization and incurs no additional cost on the communication structure. It is shown that, given a past, a computation step can be taken back if and only if it leads to a causally equivalent past.", "authors": ["Vincent Danos", "Jean Krivine"], "n_citation": 0, "title": "Reversible communicating systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c36eb803-3681-4888-b93a-d5f80e54d735"}
{"abstract": "We are developing a decentralized approach to trust based on referral systems, where agents adaptively give referrals to one another to find other trustworthy agents. Interestingly, referral systems provide us with a useful and intuitive model of how links may be generated: a referral corresponds to a customized link generated on demand by one agent for another. This gives us a basis for studying the processes underlying trust and authority, especially as they affect the structure of the evolving social network of agents. We explore key relationships between the policies and representations of the individual agents on the one hand and the aggregate structure of their social network on the other.", "authors": ["Pinar Yolum", "Munindar P. Singh"], "n_citation": 0, "title": "Self-organizing referral networks: A process view of trust and authority", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c74aa578-1734-4ef1-84e2-14e9b54cf32a"}
{"abstract": "We show that, unlike the case in finitary term rewriting, confluence is not a modular property of infinitary term rewriting systems, even when these are non-collapsing. We also give a positive result: two sufficient conditions for the modularity of confluence in the infinitary setting.", "authors": ["Jakob Grue Simonsen"], "n_citation": 0, "title": "On the modularity of confluence in infinitary term rewriting", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c75838d1-4506-4c58-a967-d0619cea7acc"}
{"abstract": "Several methodologies for semantic schema integration have been proposed in the literature, often using some variant of the ER model as the common data model. As part of these methodologies, various transformations have been defined that map between ER schemas which are in some sense equivalent. This paper gives a unifying formalisation of the ER schema transformation process and shows how some common schema transformations can be expressed within this single framework. Our formalism clearly identifies which transformations apply for any instance of the schema and which only for certain instances.", "authors": ["Peter McBrien", "Alex Poulovassilis"], "n_citation": 50, "title": "A formal framework for ER schema transformation", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "ca4bdc79-4575-4739-bfbe-9a09c39e1656"}
{"abstract": "In this paper, we present the complete classification of recursive relationships and the criteria that contribute to the structural validity of modeling recursive relationships within the entity-relationship (ER) diagram. Unlike typical other analyses that use only maximum cardinality constraints, we have used both maximum and minimum cardinality constraints in defining the properties and their structural validity criteria. We used the notions of role uniqueness, path connectivity, and cardinality constraints to derive a complete and comprehensive set of decision rules. Five rules and three corollaries were established to determine structural validity of recursive relationships. The contribution of this paper is to present a complete taxonomy of recursive relationships with their properties as well as the decision rules for their structural validity. These decision rules can be readily applied to real world data models regardless of their complexity. The rules can easily be incorporated into the database modeling and designing process, or extended into case tool implementations.", "authors": ["James Dullea", "Il Yeol Song"], "n_citation": 0, "title": "A taxonomy of recursive relationships and their structural validity in ER modeling", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "cc46e742-d371-48b1-929e-f23fa7dc204d"}
{"abstract": "A Jumping Genes Paradigm that combines with fuzzy rules is applied for optimizing the digital IIR filters. The criteria that govern the quality of the optimization procedure are based on two basic measures. A newly formulated performance metric for the digital IIR filter is formed for checking its performance while its system order which usually reflects upon the required computational power is also adopted as another objective function for the optimization. The proposed scheme in this paper was able to obtain frequency-selective filters for lowpass, highpass, bandpass and bandstop with better performance than those previously obtained and the filter system order was also optimized with lower possible number.", "authors": ["Sai-Ho Yeung", "Kim-Fung Man"], "n_citation": 0, "title": "A Jumping Genes Paradigm with Fuzzy Rules for Optimizing Digital IIR Filters", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ccdd1039-28e4-4a35-a7e9-d3e5c3826637"}
{"abstract": "Viola and Jones [VJ] demonstrate that cascade classification methods can successfully detect objects belonging to a single class, such as faces. Detecting and identifying objects that belong to any of a set of classes, many class detection, is a much more challenging problem. We show that objects from each class can form a cluster in a classifier space and illustrate examples of such clusters using images of real world objects. Our detection algorithm uses a decision tree classifier (whose internal nodes each correspond to a VJ classifier) to propose a class label for every sub-image W of a test image (or reject it as a negative instance). If this W reaches a leaf of this tree, we then pass W through a subsequent VJ cascade of classifiers, specific to the identified class, to determine whether W is truly an instance of the proposed class. We perform several empirical studies to compare our system for detecting objects of any of M classes, to the obvious approach of running a set of M learned VJ cascade classifiers, one for each class of objects, on the same image. We found that the detection rates are comparable, and our many-class detection system is about as fast as running a single VJ cascade, and scales up well as the number of classes increases.", "authors": ["Ramana Isukapalli", "Ahmed M. Elgammal", "Russell Greiner"], "n_citation": 50, "title": "Learning to Detect Objects of Many Classes Using Binary Classifiers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cd1f8ea5-39e2-40ee-90aa-9b923bd1e74a"}
{"abstract": "Recent experimental study reports existence of complex type of intemeurons in the primary visual cortex. The response of these inhibitory cells depends mainly upon feed-forward LGN inputs. The goal of this study is to determine the role of these cells in modulating the response of simple cells. Here we demonstrate that if the inhibitory contribution due to these cells balances the feed-forward excitatory inputs the spike response of cortical cell becomes sharply tuned. Using a single cell integrate and fire neuron model we show that the ratio of average inhibitory to excitatory conductance controls the balance between excitation and inhibition. We find that many different values of ratio can result in balanced condition. However, the response of the cell is not sharply tuned for each of these ratios. In this study we explicitly determine the best value of ratio needed to make the response of the cell sharply tuned.", "authors": ["Akhil Garg", "Basabi Bhaumik"], "n_citation": 0, "title": "Ratio of Average Inhibitory to Excitatory Conductance Modulates the Response of Simple Cell", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ce4e462e-51f1-4170-ae50-62a7ebe07139"}
{"abstract": "The process of system design often begins with the selection of an appropriate reference model. Model selection necessitates a good understanding of the system to be developed, as well as of the reference models available for that particular type of systems. In this paper, we propose a conceptual framework for comparing reference models, based on an elaboration of a linguistics-based classification approach. This framework is applied to the comparative analysis of two well known reference models for electronic commerce.", "authors": ["Vojislav B. Misic", "Jing Zhao"], "n_citation": 0, "title": "Evaluating the quality of reference models", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "cfe19029-0d82-4e14-91c6-5c3c026563a6"}
{"abstract": "Many geographical applications deal with spatial objects that cannot be adequately described by determinate, crisp concepts because of their intrinsically indeterminate and vague nature. Current geographical information systems and spatial database systems are unable to cope with this kind of data. To support such data and applications, we introduce vague spatial data types for vague points, vague lines, and vague regions. These data types cover and extend previous approaches and are part of a data model called VASA (Vague Spatial Algebra). Their formal framework is based on already existing, general exact models of crisp spatial data types, which simplifies the definition of the vague spatial model. In addition, we obtain executable specifications for the operations which can be immediately used as implementations. This paper gives a formal definition of the three vague spatial data types as well as some basic operations and predicates. A few example queries illustrate the embedding and expressiveness of these new data types in query languages.", "authors": ["Alejandro Pauly", "Markus Schneider"], "n_citation": 0, "title": "Vague spatial data types, set operations, and predicates", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d0ccf44a-dfd2-4719-8bd6-05b036388e26"}
{"abstract": "Topological relationships between spatial objects in the twodimensional space have been investigated for a long time in a number of disciplines like artificial intelligence, cognitive science, linguistics, and robotics. In the context of spatial databases and geographical information systems, as predicates they especially support the design of suitable query languages for spatial data retrieval and analysis. But so far, they have only been defined for simplified abstractions of spatial objects like continuous lines and simple regions. With the introduction of complex spatial data types in spatial data models and extensions of commercial database systems, an issue arises regarding the design, definition, and number of topological relationships operating on these complex types. This paper first introduces a formally defined, conceptual model of general and versatile spatial data types for complex lines and complex regions. Based on the well known 9-intersection model, it then formally determines the complete set of mutually exclusive topological relationships between complex lines and complex regions. Completeness and mutual exclusion are shown by a proof technique called proof-by-constraint-and-drawing.", "authors": ["Markus Schneider", "Thomas Behr"], "n_citation": 0, "title": "Topological relationships between complex lines and complex regions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d36b65bf-808b-44fc-a1ab-5c971259c555"}
{"abstract": "We present a tool for automatically proving termination of first-order rewrite systems. The tool is based on the dependency pair method of Arts and Giesl. It incorporates several new ideas that make the method more efficient. The tool produces high-quality output and has a convenient web interface.", "authors": ["Nao Hirokawa", "Aart Middeldorp"], "n_citation": 50, "title": "Tsukuba Termination Tool", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d5e9a493-8a43-434a-9ed3-087476b6c1a8"}
{"abstract": "This paper presents first results for a theoretical foundation for emergency e-Logistics based on an extended FIPA-compliant layered multi-agent system architecture. The approach is based on the holonic paradigm proposed by A. Koestler in his attempt to create a model for self-organization in biological systems and proven by the HMS Consortium to be very useful for resource management and allocation in the manufacturing domain. We integrate the mechanism of emergence into the holonic paradigm encompassing both vertical and horizontal integration of resources in a distributed organization to enable the dynamic creation, refinement and optimization of flexible ad-hoc infrastructures for emergency e-logistics management.", "authors": ["Mihaela Ulieru", "Rainer Unland"], "n_citation": 0, "title": "A holonic self-organization approach to the design of emergent e-Logistics infrastructures", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d63231a8-1568-495a-8fdc-199d586aa2ab"}
{"abstract": "In this paper we describe our responsive video performance, Deep Surrender, created using Cycling '74's Max/MSP and Jitter packages. Video parameters are manipulated in real-time, using chromakeying and colour balance modification techniques to visualize the keyboard playing and vocal timbre of a live performer. We present the musical feature extraction process used to create a control system for the production, describe the mapping between audio and visual parameters, and discuss the artistic motivations behind the piece.", "authors": ["Robyn Taylor", "Pierre Boulanger"], "n_citation": 0, "title": "Deep surrender : Musically controlled responsive video", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d83e31bf-eb1d-4b91-a64d-e496b1951c3c"}
{"abstract": "Simulation grid (SimGrid) is fit for the large-scale warfare simulation applications. Since stakeholders in warfare simulation include scenario developers, running & managing personnel and model managers, this paper firstly presents a special SimGrid framework which could be divided into three levels: application level, HLA/RTI level and model level. It then detailedly analyzes and describes how to search the model service based on warfare SimGrid architecture and its index integration mechanism. Finally, a model service location solution based on response order and access order is put forward. The service searching solution based on response order and access order is a good solution to rapidly search and locate necessary models in large-scale warfare simulation. The solution takes into whole account the access characteristics of warfare simulation models and its real-time characteristics, which greatly ensured the rapid service searching. In this solution, all kinds of parameters can be easily assigned and controlled. All buffer queues can be dynamically allocated according to requests of users. Obviously the solution can make great contribution to construct large-scale warfare simulation grid.", "authors": ["Yunxiang Ling", "Miao Zhang", "Xiaojun Lu", "Wenyuan Wang", "Songyang Lao"], "n_citation": 0, "title": "Model searching algorithm based on response order and access order in war-game simulation grid", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "db9e9f73-fef4-4da9-82ed-6736ca523d14"}
{"abstract": "This paper investigates an approach for statically preventing race conditions in an object-oriented language. The setting of this work is a variant of Gordon and Hankin's concurrent object calculus. We enrich that calculus with a form of dependent object types that enables us to verify that threads invoke and update methods only after acquiring appropriate locks. We establish that well-typed programs do not have race conditions.", "authors": ["Colin Flanagan", "M. Abadi"], "n_citation": 0, "title": "Object types against races", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "dc12c863-a677-4bab-bc55-4cc97ec42821"}
{"abstract": "For ordinary circuits with a fixed upper bound on the maximal fanin of gates it has been shown that logarithmic redundancy is necessary and sufficient to overcome random hardware faults. Here, we consider the same question for unbounded fanin circuits that in the noiseless case can compute Boolean functions in sublogarithmic depth. Now the details of the fault model become more important. The fault tolerance depends on the types of gates that are used, and whether the error probabilities are known exactly or only an upper bound for them. Concerning the first distinction the two most important models are circuits consisting of and- and or-gates with arbitrarily many inputs, and circuits built from the more general type of threshold gates. We will show that reliable computation is impossible for and/or-circuits and threshold circuits with unknown error probabilities. Gates with large fanin are of no use in this case. Circuits of arbitrary size, but fixed depth can compute only a tiny subset of all Boolean functions reliably. Only in case of threshold circuits and exactly known error probabilities redundancy is able to compensate faults. We describe a transformation from fault-free to fault-tolerant circuits that is optimal with respect to depth keeping the circuit size polynomial.", "authors": ["R. Reischuk"], "n_citation": 0, "title": "Can large fanin circuits perform reliable computations in the presence of noise", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "dc1c87de-ae5d-47b6-b1a3-b5c79736761e"}
{"abstract": "The problem of finding the maxima of a point set plays a fundamental role in computational geometry. Based on the idea of the certificates of exclusion, two algorithms are presented to solve the maxima problem under the assumption that N points are chosen from a d-dimensional hypercube uniformly and each component of a point is independent of all other components. The first algorithm runs in O(N) expected time and finds the maxima using dN + d ln N + d 2  N 1-1/d (ln N) 1/d  + O(dN 1-1/d ) expected scalar comparisons. The experiments show the second algorithm has a better expected running time than the first algorithm while a tight upper bound of the expected running time is not obtained. A third maxima-finding algorithm is presented for N points with a d-dimensional component independence distribution, which runs in O(N) expected time and uses 2dN + O(ln N(ln(ln N))) + d 2 N 1-1/d (ln N) 1/d  + O(dN 1-1/d ) expected scalar comparisons. The substantial reduction of the expected running time of all three algorithms, compared with some known linear expected-time algorithms, has been attributed to the fact that a better certificate of exclusion has been chosen and more non-maximal points have been identified and discarded.", "authors": ["Haibin Dai", "X. Zhang"], "n_citation": 0, "title": "Improved linear expected-time algorithms for computing maxima", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e0e8a2c2-be12-44ab-a9d7-737040b17f49"}
{"abstract": "The designers of virtual agents often draw on a large research literature in psychology, linguistics and human ethology to design embodied agents that can interact with people. In this paper, we consider a structural acting system developed by Francois Delsarte as a possible resource in designing the nonverbal behavior of embodied agents. Using human subjects, we evaluate one component of the system, Delsarte's Cube, that addresses the meaning of differing attitudes of the hand in gestures.", "authors": ["Stacy C. Marsella", "Sharon Marie Carnicke", "Jonathan Gratch", "Anna Okhmatovskaia", "Albert Rizzo"], "n_citation": 0, "title": "An exploration of delsarte's structural acting system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e3876f01-f712-4f1b-92bb-8c15e91e47ec"}
{"abstract": "We develop statistical measures for assessing the acceptability of a frequent class of multiword expressions. We also use the measures to estimate the degree of productivity of the expressions over semantically related nouns. We show that a linguistically-inspired measure outperforms a standard measure of collocation in its match with human judgments. The measure uses simple extraction techniques over non-marked-up web data.", "authors": ["Afsaneh Fazly", "Ryan North", "Suzanne Stevenson"], "n_citation": 0, "title": "Automatically determining allowable combinations of a class of flexible multiword expressions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e5c82c7d-ed84-4e67-833e-cab8221b1fb5"}
{"abstract": "Aiming at a deeper understanding of the essence of spatial logics for concurrency, we study a minimal spatial logic without quantifiers or any operators talking about names. The logic just includes the basic spatial operators void, composition and its adjunct, and the next step modality; for the model we consider a tiny fragment of CCS. We show that this core logic can already encode its own extension with quantifiers, and modalities for actions. From this result, we derive several consequences. Firstly, we establish the intensionality of the logic, we characterize the equivalence it induces on processes, and we derive characteristic formulas. Secondly, we show that, unlike in static spatial logics, the composition adjunct adds to the expressiveness of the logic, so that adjunct elimination is not possible for dynamic spatial logics, even quantifier-free. Finally, we prove that both model-checking and satisfiability problems are undecidable in our logic. We also conclude that our results extend to other calculi, namely the \u03c0-calculus and the ambient calculus.", "authors": ["Lu\u00eds Caires", "Etienne Lozes"], "n_citation": 50, "title": "Elimination of quantifiers and undecidability in spatial logics for concurrency", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e5e7b9e8-8a73-47da-bf72-e46e8089e0d7"}
{"abstract": "We present a decision-theoretic model which determines whether to pre-send web documents a user is likely to request. Our model balances the cost of transmitting information with the inconvenience or cost to a user caused by having to wait for information. It uses a Markov model built from web-access data to predict which documents are likely to be requested next. A comparative evaluation of our pre-sending scheme identifies circumstances in which our scheme is beneficial for users.", "authors": ["Ann E. Nicholson", "Ingrid Zukerman", "David W. Albrecht"], "n_citation": 0, "title": "A decision-theoretic approach for pre-sending information on the WWW", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "e6e188a5-7e1c-4390-870e-144ea96bde5d"}
{"abstract": "Many transition mechanisms and scenarios have studied for the using of IPv6 network efficiently. The VPN in IPv4 network is the main security application. Also it is necessary to use VPN during the transition period to IPv6 network. The VPN service provides the confidentiality and integrity for transmitted packets. But there are some difficulties to connect end-to-end VPN without a conversion between IPv4 and IPv6. Therefore, we analyze the suitability of the various applying model of VPN in conjunction with the transition mechanisms and evaluate the performance cost of this model. In the case of using the transition mechanism and VPN technology simultaneous, we can know the performance factors as follows: whether end points of the VPN tunnel and IP tunnel agree upon or not, the applying place of transition mechanism and VPN model according to network environment. Therefore, it should consider carefully evaluated applying model according to the required security level and connectivity.", "authors": ["Hyung-Jin Lim", "Dong-Young Lee", "Tae-Kyung Kim", "Tai-Myoung Chung"], "n_citation": 0, "title": "An Evaluation and Analysis for IP VPN Model in IPv6 Transition Environment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e7e18df0-c54b-468b-afeb-cc90dfc7a199"}
{"abstract": "A key problem affecting the routing performance in a Super-Peer Overlay is the fluctuation of network, which makes DHT shifted and generates considerable overhead of maintaining the global state of routing information. In this paper we propose a Power Sorting Multicast algorithm and an adaptive self-recovery mechanism to solve this problem. The experiments show they can efficiently decrease maintenance traffic and restore the system to stable state, holding availability of discovery algorithm.", "authors": ["Feng Yang", "Shouyi Zhan", "Fouxiang Shen"], "n_citation": 0, "title": "Maintaining and self-recovering global state in a super-peer overlay for service discovery", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "eae147ee-fa29-48e0-ad75-902f2159876b"}
{"abstract": "There has been growing interest in developing nonlinear dimensionality reduction algorithms for vision applications. Although progress has been made in recent years, conventional nonlinear dimensionality reduction algorithms have been designed to deal with stationary, or independent and identically distributed data. In this paper, we present a novel method that learns nonlinear mapping from time series data to their intrinsic coordinates on the underlying manifold. Our work extends the recent advances in learning nonlinear manifolds within a global coordinate system to account for temporal correlation inherent in sequential data. We formulate the problem with a dynamic Bayesian network and propose an approximate algorithm to tackle the learning and inference problems. Numerous experiments demonstrate the proposed method is able to learn nonlinear manifolds from time series data, and as a result of exploiting the temporal correlation, achieve superior results.", "authors": ["Ruei-sung Lin", "Che-Bin Liu", "Ming-Hsuan Yang", "Narendra Ahuja", "Stephen C. Levinson"], "n_citation": 50, "title": "Learning Nonlinear Manifolds from Time Series", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "eb737f66-ce10-40f1-b430-6d3a3bcc6f13"}
{"abstract": "The infrastructure of grid information service constituted with highly distributed information providers and aggregate directory is brought forward on the basis of the characteristic of grid information resources in this paper. The Lightweight Directory Access Protocol (LDAP), one of the base protocols, is also analyzed in this paper. It is put forward that LDAP is a distributed database. The dynamic updating and replication of LDAP directory tree happens frequently. To solve the problem, it has been proposed that the strategy of fast spread and Cascading spread can boost the efficiency of grid information service system. Moreover, we use file-parted replication approach to divide the LDAP database file into several blocks that are replicated parallel between LDAP sever points then. In such a way, the system efficiency of parallel processing can be boosted by margin. In addition, based on the idea forenamed, we put forward the technique infrastructure and upload-controlling algorithm, both of which are proven to be effective in improving the system efficiency.", "authors": ["Jingwei Huang", "Qingfeng Fan", "Qiongli Wu", "Yanxiang He"], "n_citation": 0, "title": "Improved grid information service using the idea of file-parted replication", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "edff82a6-1a18-4585-9744-d64711dbf9e6"}
{"abstract": "This paper presents an ethnomethodologically informed study of the ways that more-or-less dependable systems are part of the everyday lifeworld of society members. Through case study material we explicate how dependability is a practical achievement and how it is constituted as a common sense notion. We show how attending to the logical grammar of dependability can clarify some issues and potential conceptual confusions around the term that occur between lay and 'professional' uses. The paper ends with a call to consider dependability in its everyday ordinary language context as well as more 'professional' uses of this term.", "authors": ["Alexander Voss", "Roger Slack", "Rob Procter", "Robin Williams", "Mark Hartswood", "Mark Rouncefield"], "n_citation": 50, "title": "Dependability as ordinary action", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f2640032-bab4-4d8a-aa9c-bf9e2122cc0b"}
{"abstract": "Pattern Matching with Properties (Property Matching, for short), involves a string matching between the pattern and the text, and the requirement that the text part satisfies some property. It is straightforward to do sequential matching in a text with properties. However, indexing in a text with properties becomes difficult if we desire the time to be output dependent. We present an algorithm for indexing a text with properties in O(n log|\u03a3 + n log log n) time for preprocessing and O(|P| log |\u03a3| + tocc \u03c0 ) per query, where n is the length of the text, P is the sought pattern, E is the alphabet, and tocc \u03c0  is the number of occurrences of the pattern that satisfy some property \u03c0. As a practical use of Property Matching we show how to solve Weighted Matching problems using techniques from Property Matching. Weighted sequences have been introduced as a tool to handle a set of sequences that are not identical but have many local similarities. The weighted sequence is a statistical image of this set, where we are given the probability of every symbol's occurrence at every text location. Weighted matching problems are pattern matching problems where the given text is weighted. We present a reduction from Weighted Matching to Property Matching that allows off-the-shelf solutions to numerous weighted matching problems including indexing, swapped matching, parameterized matching, approximate matching, and many more. Assuming that one seeks the occurrence of pattern P with probability \u2208 in weighted text T of length n, we reduce the problem to a property matching problem of pattern P in text T' of length O(n(1 6) 2  log 1 6).", "authors": ["Amihood Amir", "Eran Chencinski", "Costas S. Iliopoulos", "Tsvi Kopelowitz", "Hui Zhang"], "n_citation": 50, "title": "Property matching and weighted matching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f40ab0a2-e599-4607-8a03-65b5bde5dd95"}
{"abstract": "Advance of high-throughput technologies, such as the microarray and mass spectrometry, has provided an effective approach for the development of systems biology, which aims at understanding the complex functions and properties of biological systems and processes. Revealing the functional correlated genes with co-expression pattern from microarray data allows us to infer the transcriptional regulatory networks and perform functional annotation of genes, and has become one vital step towards the implementation of integrative systems biology. Clustering is particularly useful and preliminary methodology for the discovery of co-expressed genes, for which many conventional clustering algorithms developed in the literature can be potentially useful. However, due to existing large amount of noise and a variety of uncertainties in the microarray data, it is vital important to develop techniques which are robust to noise and effective to incorporate user-specified objectives and preference. For this particular purpose, this paper presented a Genetic Algorithm (GA) based hybrid method for the co-expression gene discovery, which intends to extract the gene groups that have maximal dissimilarity between groups and maximal similarity within a group. The experimental results show that the proposed algorithm is able to extract more meaningful, sensible and significant co-expression gene groups than the traditional clustering methods such as the K-means algorithm. Besides presenting the proposed hybrid GA-based clustering algorithm for co-expression gene discovery, this paper introduces a new framework of integrative systems biology employed in our current research.", "authors": ["Yutao Ma", "Yonghong Peng"], "n_citation": 50, "title": "Co-expression Gene Discovery from Microarray for Integrative Systems Biology", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f5756692-e28d-433f-9243-68d90fb4bb0a"}
{"abstract": "Motivated by trends in the industry towards transforming IT in large integrated service networks, this paper describes algorithms for the adaptive placement of services (as abstractions of collections of applications) in networks of servers (as abstractions for locations where services can be hosted). Networks comprised of interacting services as the foundation is also a vision pronounced by the Grid [9]. Manageability and self-operation of Grids is highly desirable. We analyze the requirements for algorithms one specific problem: the service placement problem. We discuss algorithms that neither require central control nor complete information about the system state. Algorithms are performed on a distributed overlay structure which summarizes load conditions in the underlying service network. The presented algorithms fulfill tasks of making initial placement decisions as well as initiating rearrangements when imbalance is detected. Presented algorithms have different characteristics regarding the tradeoff between accuracy (or quality) of a placement decision and its timeliness within which a decision can be made determining responsiveness.", "authors": ["Sven Graupner", "Artur Andrzejak", "Vadim E. Kotov", "Holger Trinks"], "n_citation": 0, "title": "Adaptive service placement algorithms for autonomous service networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f75f6ee6-e6e3-4838-94d1-0e5455049194"}
{"abstract": "We show that. the problem of predicting t steps of the ID cellular automaton Rule 110 is P-complete. The result is found by showing that Rule 110 simulates deterministic Turing machines in polynomial time. As a corollary we find that the small universal Turing machines of Mathew Cook run in polynomial time, this is an exponential improvement on their previously known simulation time overhead.", "authors": ["Turlough Neary", "Damien Woods"], "n_citation": 91, "title": "P-completeness of Cellular Automaton Rule 110", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f924e14e-c376-4be7-b74e-ed5c5f8ccf6f"}
{"abstract": "Although the Bayesian approach provides optimal performance for many inference problems, the computation cost is sometimes impractical. We herein develop a practical algorithm by which to approximate Bayesian inference in large single-layer feed-forward networks (perceptrons) based on belief propagation (BP). Although direct application of BP to the inference problem remains computationally difficult, by introducing methods and concepts from statistical mechanics that are related to the central limit theorem and the law of large numbers, the proposed BP-based algorithm exhibits nearly optimal performance in a practical time scale for ideal large networks. In order to demonstrate the practical significance of the proposed algorithm, an application to a problem that arises in a mobile communications system is also presented.", "authors": ["Yoshiyuki Kabashima", "Shinsuke Uda"], "n_citation": 0, "title": "A BP-based algorithm for performing Bayesian inference in large perceptron-type networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fa072d9e-e882-4729-b929-1058f8757de0"}
{"abstract": "An algorithm for time division multiple access (TDMA) is found to be applicable in converting existing distributed algorithms into a model that is consistent with sensor networks. Such a TDMA service needs to be self-stabilizing so that in the event of corruption of assigned slots and clock drift, it recovers to states from where TDMA slots are consistent. Previous self-stabilizing solutions for TDMA are either randomized or assume that the topology is known upfront and cannot change. Thus, the question of feasibility of self-stabilizing deterministic TDMA algorithm where topology is unknown remains open. In this paper, we present a self-stabilizing, deterministic algorithm for TDMA in networks where a sensor is aware of only its neighbors. This is the first such algorithm that achieves these properties. Moreover, this is the first algorithm that demonstrates the feasibility of stabilization-preserving, deterministic transformation of a shared memory distributed program on an arbitrary topology into a program that is consistent with the sensor network model.", "authors": ["Mahesh Arumugam", "Sandeep S. Kulkarni"], "n_citation": 0, "title": "Self-stabilizing deterministic TDMA for sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fbd7049b-b1e9-4e69-975a-909209d12046"}
{"abstract": "We show that if an NP-complete set or a coNP-complete set is polynomial-time disjunctive truth-table reducible to a sparse set then FP\u2225 NP  = FP NP [log]. Similarly, we show that if SAT is O(logn)-approximable then FP\u2225 NP  = FP NP [log]. Since FP\u2225 NP  = FP NP [log] implies that SAT is O(log n)-approximable [BFT97], it follows from our result that these two hypotheses are equivalent. We also show that if an NP-complete set or a coNP-complete set is disjunctively reducible to a sparse set of polylogarithmic density then, in fact, P = NP.", "authors": ["Vikraman Arvind", "Jacobo Tor\u00e1n"], "n_citation": 50, "title": "Sparse sets, approximable sets, and parallel queries to NP", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "00dc66fc-df33-4f46-a938-9c3b3180c157"}
{"abstract": "We propose a semidefinite relaxation technique for multiclass image labeling problems. In this context, we consider labeling as a special case of supervised classification with a predefined number of classes and known but arbitrary dissimilarities between each image element and each class. Using Markov random fields to model pairwise relationships, this leads to a global energy minimization problem. In order to handle its combinatorial complexity, we apply Lagrangian relaxation to derive a semidefinite program, which has several advantageous properties over alternative methods like graph cuts. In particular, there are no restrictions concerning the form of the pairwise interactions, which e.g. allows us to incorporate a basic shape concept into the energy function. Based on the solution matrix of our convex relaxation, a suboptimal solution of the original labeling problem can be easily computed. Statistical ground-truth experiments and several examples of multiclass image labeling and restoration problems show that high quality solutions are obtained with this technique.", "authors": ["Jens Keuchel"], "n_citation": 50, "title": "Multiclass Image Labeling with Semidefinite Programming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "026b471e-dbf3-45e8-9a29-c2a821e89ed6"}
{"abstract": "Part 1 of the ISO Prolog standard (ISO/IEC 13211) published in 1995 covers the core of Prolog, including syntax, operational semantics, streams and some built-in predicates. Libraries, DCGs, and global mutables are current standardization topics. Most Prolog implementations provide an ISO mode in which they adhere to the standard. Our goal is to improve parts of the Prolog standard already published by finding and fixing ambiguities and missing details. To do so, we have compiled a suite of more than 1000 test cases covering part 1, and ran it on several free and commercial Prolog implementations. In this study we summarize the reasons of the test case failures, and discuss which of these indicate possible flaws in the standard. We also discuss test framework and test case development issues specific to Prolog, as well as some portability issues encountered.", "authors": ["P. G. Szabo", "P\u00e9ter Szeredi"], "n_citation": 0, "title": "Improving the ISO Prolog standard by analyzing compliance test results", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "02a7e73b-de9e-491e-8308-8616d009fd34"}
{"abstract": "Developing efficient and meaningful search mechanisms for the Web is an active area of research in Information Management. With information explosion on the Internet, existing search engines encounter difficulty in accurate document positioning and retrieval. This situation is exacerbated by the language barrier for accessing web content provided in different languages. Sophisticated content-based search engines are needed for helping users find useful information quickly from multilingual knowledge sources on the Web. This paper presents a parametric linguistics based approach for flexible and scalable multilingual web querying with low complexity in query translation. The proposed methodology and the system architecture are discussed.", "authors": ["Epaminondas Kapetanios", "Vijayan Sugumaran", "Diana Tanase"], "n_citation": 0, "title": "Multi-lingual web querying : A parametric linguistics based approach", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "079e5538-2219-460a-80cc-ebfabbf5afe4"}
{"abstract": "This paper formalisms the idea of substitutability introduced by Zellig Harris in the 1950s and makes it the basis for a learning algorithm from positive data only for a subclass of context-free grammars. We show that there is a polynomial characteristic set, and thus prove polynomial identification in the limit of this class. We discuss the relationship of this class of languages to other common classes discussed in grammatical inference. We also discuss modifications to the algorithm that produces a reduction system rather than a context-free grammar, that will be much more compact. We discuss the relationship to Angluin's notion of reversibility for regular languages.", "authors": ["Alexander Clark", "R\u00e9mi Eyraud"], "n_citation": 0, "title": "Identification in the limit of substitutable context-free languages", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "07b92fdd-06d0-4d77-b032-0e0814ab049e"}
{"abstract": "In this paper, a modified Particle Swarm Optimisation (PSO) algorithm is presented to improve the performance of multi-objective optimisation. The PSO algorithm search capabilities are enhanced via the inclusion of the adaptive inertia weight and acceleration factor. In addition, a weighted aggregation function has been introduced within the algorithm to guide the selection of the personal and global bests, together with a non-dominated sorting algorithm to select the particles from one iteration to another. The proposed algorithm has been successfully applied to a series of well-known benchmark functions as well as to the multi-objective optimal design of alloy steels, which aims at determining the optimal heat treatment regimes and the required weight percentages for the chemical composites in order to obtain the pre-defined mechanical properties of the material. The results have shown that the algorithm can locate the constrained optimal design with a very good accuracy.", "authors": ["Mahdi Mahfouf", "Min-You Chen", "D.A. Linkens"], "n_citation": 0, "title": "Adaptive weighted Particle Swarm Optimisation for multi-objective optimal design of alloy steels", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "085fe298-5231-4ba2-8c24-a7c10cfc9c69"}
{"abstract": "What a human's eye tells a human's brain? In this paper, we analyze the information capacity of visual attention. Our hypothesis is that the limit of perceptible spatial frequency is related to observing time. Given more time, one can obtain higher resolution - that is, higher spatial frequency information, of the presented visual stimuli. We designed an experiment to simulate natural viewing conditions, in which time dependent characteristics of the attention can be evoked; and we recorded the temporal responses of 6 subjects. Based on the experiment results, we propose a person-independent model that characterizes the behavior of eyes, relating visual spatial resolution with the duration of attentional concentration time. This model suggests that the information capacity of visual attention is time-dependent.", "authors": ["Xiaodi Hou", "Liqing Zhang"], "n_citation": 0, "title": "A Time-Dependent Model of Information Capacity of Visual Attention", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0a47f467-2aa1-426b-a2fd-45cbbeedb05b"}
{"abstract": "We present a new heuristic for on-the-fly enumerative invariant verification. The heuristic is based on a construct for temporal scaling, called next, that compresses a sequence of transitions leading to a given target set into a single metatransition. First, we give an on-the-fly algorithm to search a process expression built using the constructs of hiding, parallel composition, and temporal scaling. Second, we show that as long the target set \u0398 of transitions includes all transitions that access variables shared with the environment, the process next \u0398 for P and P are equivalent according to the weak-simulation equivalence. As a result, to search the product of given processes, we can cluster processes into groups with as little communication among them as possible, and compose the groups only after applying appropriate hiding and temporal scaling operators. Applying this process recursively gives an expression that has multiple nested applications of next, and has potentially much fewer states than the original product. We report on an implementation, and show significant reductions for a tree-structured parity computer and a ring-structured leader-election protocol.", "authors": ["Rajeev Alur", "Bow-Yaw Wang"], "n_citation": 0, "title": "Next heuristic for on-the-fly model checking", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "0a9327f3-1683-40b0-8e0e-fb52b9591cce"}
{"abstract": "We describe here a formal proof in the Coq system of the structure theorem for subresultants, which allows to prove formally the correctness of our implementation of the subresultants algorithm. Up to our knowledge it is the first mechanized proof of this result.", "authors": ["Assia Mahboubi"], "n_citation": 0, "title": "Proving formally the implementation of an efficient gcd algorithm for polynomials", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0ad9ecfd-bbb7-4a03-97cd-bcaf958582c4"}
{"abstract": "In this paper, we explore a novel type of gene cluster called local conserved gene cluster or LC-Cluster for short. A gene's expression level is local conserved if it is expressed with the similar abundance only on a subset of conditions instead of on all the conditions. A subset of genes which are simultaneously local conserved across the same subset of samples form an LC-Cluster, where the samples correspond to some phenotype and the genes suggest all candidates related to the phenotype. Two efficient algorithms, namely FALCONER and E-FALCONER, are proposed to mine the complete set of maximal LC-Clusters. The test results from both real and synthetic datasets confirm the effectiveness and efficiency of our approaches.", "authors": ["Yuhai Zhao", "Guoren Wang", "Ying Yin", "Guangyu Xu"], "n_citation": 0, "title": "Mining Maximal Local Conserved Gene Clusters from Microarray Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0c5959b2-ff71-4586-80b2-3f5a6ae15ef5"}
{"abstract": "Due to the limited resources in thin-clients on mobile devices and the large amount of computation for decoding streaming media, it is not easy to support the QoS of streaming media for thin-client on mobile environment. To solve the problems, the terminal servers would he charged for decoding the streaming media and thin-clients have a role to update only the changed areas in their screen. In this paper, an intelligent media player is proposed to provide the improved QoS for streaming media in thin-client computing. Since the proposed method reflects the intrinsic property of streaming media, it provides both the enhanced video quality and the audio streaming fully synchronized with image frames.", "authors": ["Joa-Hyoung Lee", "Dongmahn Seo", "Yoon Kim", "Changyeol Choi", "Hwang-Kyu Choi", "Inbum Jung"], "n_citation": 0, "title": "Thin-Client Computing for Supporting the QoS of Streaming Media in Mobile Devices", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "11fe635a-3c3f-4285-ab5c-7823a9fb5fb1"}
{"abstract": "Multicasting is increasingly used as an efficient communication mechanism for group-oriented applications in the Internet. In order to offer secrecy for multicast applications, the traffic encryption key has to be changed whenever a user joins or leaves the system. Such a change has to be communicated to all the current group members. The bandwidth used for such rekeying operation could be high when the group size is large. The proposed solutions to cope with this limitation, commonly called 1 affects n phenomenon, consist of organizing group members into subgroups that use independent traffic encryption keys. This kind of solutions introduce a new challenge which is the requirement of decrypting and reencrypting multicast messages whenever they pass from one subgroup to another. This is a serious drawback for applications that require real-time communication such as video-conferencing. In order to avoid the systematic decryption / reencryption of messages, we propose in this paper an adaptive solution which structures group members into clusters according to the application requirements in term of synchronization and the membership change behavior in the secure session. Simulation results show that our solution is efficient and more adaptive compared to other schemes.", "authors": ["Yacine Challall", "Hatem Bettahar", "Abdelmadjid Bouabdallah"], "n_citation": 0, "title": "A scalable and adaptive key management protocol for group communication", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "16c5c44d-7916-482e-b2f7-0951527b1137"}
{"abstract": "Automatic semantic clustering of image databases is a very challenging research problem. Clustering is the unsupervised classification of patterns (data items or feature vectors) into groups (clusters). Clustering algorithms usually employ a similarity measure in order to partition the database such that data points in the same partition are more similar than points in different partitions. In this paper an Ant Colony Optimization (ACO) and its learning mechanism is integrated with the K-means approach to solve image classification problems. Our simulation results show that the proposed method makes K-Means less dependent on the initial parameters such as randomly chosen initial cluster centers. Selected results from experiments of the proposed method using two different image databases are presented.", "authors": ["Tomas Piatrik", "Ebroul Izquierdo"], "n_citation": 50, "title": "Image classification using an ant colony optimization approach", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "17e235be-eb16-4696-add6-313e6a050b5a"}
{"abstract": "Hybrid feature ranking is a feature selection method which combines the quickness of the filter approach and the accuracy of the wrapper approach. The main idea consists in a two steps procedure: building a sequence of feature subsets using an informational criterion, independently of the learning method; selecting the best one with a cross-validation error rate evaluation, using explicitly the learning method. In this paper, we show that in the protein discrimination domain, few examples but numerous descriptors, compared to a traditional approach where each descriptor is evaluated separately in the first step, to take account of their redundancy in the construction of candidate subsets of features reduces the size of the optimal subset and improves, in certain cases, the accuracy.", "authors": ["Ricco Rakotomalala", "Faouzi Mhamdi", "Mourad Elloumi"], "n_citation": 50, "title": "Hybrid feature ranking for proteins classification", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "198998af-e55b-4e12-8f94-3d02add7acf3"}
{"abstract": "A uniform active model adaptation is proposed for some specific applications related with realistic face reconstruction and sketch generation, which is separated into tow phases: global adaptation and local adaptation. During the global adaptation, a coarse matching is used for adjusting the facial contour and five sense organs according to the correspondence of feature points both in 2D facial image and 3D model. An interpolation is processed on the global adjusted 3D model for a fine matching, which forms the local adaptation. A personalized facial sketch generation method is present based on the similar idea of 3D facial model adaptation. By comparison between the feature points of specific person's face and those of a predefined generic face, metamorphosis can be drawn for generating a personalized cartoon facial sketch. Two systems are built, and face reconstruction and sketch results show that proposed method is efficient and desirable.", "authors": ["Yuehu Liu", "Yunfeng Zhu", "Yuanqi Su", "Zejian Yuan"], "n_citation": 0, "title": "Image based active model adaptation method for face reconstruction and sketch generation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1bb9f761-4d94-49e4-8ead-9ddaa7eb1703"}
{"abstract": "This paper presents a web environment for the Z formal specification language using the Scalable Vector Graphics (SVG) technology. The Z Specification Web Editor (ZSWE) is the first prototype of a web based graphical editor for the Z specification language. It not only supports graphical editing and global accessibility for the Z formal specifications, but also provides model comprehension facilities such as schema expansion, specification navigation and model querying. This paper outlines the requirement, design and implementation of the tool and its future improvements.", "authors": ["Jing Sun", "Hai Wang", "Sasanka Athauda", "Tazkiya Sheik"], "n_citation": 0, "title": "SVG web environment for Z specification language", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1c93d56c-d5ad-4316-a641-73e06e73bc43"}
{"abstract": "In the formulation of radial basis function (RBF) network, there are three factors mainly considered, i.e., centers, widths, and weights, which significantly affect the performance of the network. Within thus three factors, the placement of centers is proved theoretically and practically to be critical. In order to obtain a compact network, this paper presents an improved clustering (IC) scheme to obtain the location of the centers. What is more, since the location of the corresponding widths does affect the performance of the networks, a learning algorithms referred to as anisotropic gradient descent (AGD) method for designing the widths is presented as well. In the context of this paper, the conventional gradient descent method for learning the weights of the networks is combined with that of the widths to form an array of couple recursive equations. The implementation of the proposed algorithm shows that it is as efficient and practical as GGAP-RBF.", "authors": ["Delu Zeng", "Shengli Xie", "Zhiheng Zhou"], "n_citation": 0, "title": "Improved Clustering and Anisotropic Gradient Descent Algorithm for Compact RBF Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1d02e7c9-f2db-455c-aab5-6867e594af88"}
{"abstract": "In this paper, we propose a construction of Question Answering(QA) system, which synthesizes the answers retrieval from the frequent asked questions database and documents database, based on special domain about sightseeing information. A speech interface for the special domain was implemented along with the text interface, using an acoustic model HMM, a pronunciation lexicon, and a language model FSN on the basis of the feature of Chinese sentence patterns. We consider the synthetic model based on statistic VSM and shallow language analysis for sightseeing information. Experimental results showed high accuracy can be achieved for the special domain and the speech interface is available for frequently asked questions about sightseeing information.", "authors": ["Haiqing Hu", "Fuji Ren", "Shingo Kuroiwa", "Shuwu Zhang"], "n_citation": 0, "title": "A question answering system on special domain and the implementation of speech interface", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1e5d7b8e-c46f-4e0b-93c7-af6a6f626fa5"}
{"abstract": "This paper introduces a new pattern matching model that has been gaining importance recently, that of Asynchronous Pattern Matching. Traditional pattern matching has assumed the possibility of errors in the data content. We present motivation from text editing, computational biology, and computer architecture, that points to a new paradigm.- where the errors occur in the address. It turns out that there are differences in techniques, complexities, and tools between the two different models, making it important to recognize their differences. We motivate and define the new model and present some problems that are worth pursuing.", "authors": ["Amihood Amir"], "n_citation": 0, "title": "Asynchronous pattern matching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "202f53ff-e828-4052-97f8-a72010e3f3f6"}
{"abstract": "In this paper we study the possibility of removing aliasing in a scene from a single observation by designing an alias-free upsampling scheme. We generate the unknown high frequency components of the given partially aliased (low resolution) image by minimizing the total variation of the interpolant subject to the constraint that part of unaliased spectral components in the low resolution observation are known precisely and under the assumption of sparsity in the data. This provides a mathematical basis for exact reproduction of high frequency components with probability approaching one, from their aliased observation. The primary application of the given approach would be in super-resolution imaging.", "authors": ["C. V. Jiji", "Prakash Neethu", "Subhasis Chaudhuri"], "n_citation": 0, "title": "Alias-Free Interpolation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2061c468-a705-4fd6-8bae-d2048f76d67e"}
{"abstract": "Denote by an l-component a connected b-uniform hypergraph with k edges and k(b-1)-l vertices. We prove that the expected number of creations of l-component during a random hypergraph process tends to 1 as l and b tend to \u221e with the total number of vertices n such that l = o(3\u221an b). Under the same conditions, we also show that the expected number of vertices that ever belong to an l-component is approximately 12 1/3 (b-1) 1/3 l 1/3 n 2/3 . As an immediate consequence, it follows that with high probability the largest l-component during the process is of size O((b-1) 1/3 l 1/3 n 2/3 ). Our results give insight about the size of giant components inside the phase transition of random hypergraphs.", "authors": ["Vlady Ravelomanana", "Alphonse Laza Rijamamy"], "n_citation": 0, "title": "Creation and Growth of Components in a Random Hypergraph Process", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "208d3e6f-39ef-4417-9e12-feaf3b4aa53c"}
{"abstract": "Constructing parsimonious phylogenetic trees from species data is a central problem in phylogenetics, and has diverse applications, even outside biology. Many variations of the problem, including the cladistic Camin-Sokal (CCS) version, are NP-complete. We present Answer Set Programming (ASP) models for the binary CCS problem, as well as a simpler perfect phylogeny version, along with experimental results of applying the models to biological data. Our contribution is three-fold. First, we solve phylogeny problems which have not previously been tackled by ASP. Second, we report on variants of our CCS model which significantly affect run time, including the interesting case of making the program slightly tighter. This version exhibits some of the best performance, in contrast with a tight version of the model which exhibited poor performance. Third, we are able to find proven-optimal solutions for larger instances of the CCS problem than the widely used branch-and-bound-based PHYLIP package.", "authors": ["Jonathan Kavanagh", "David G. Mitchell", "Eugenia Ternovska", "J\u00e1n Manuch", "Xiaohong Zhao", "Arvind Gupta"], "n_citation": 0, "title": "Constructing camin-sokal phylogenies via answer set programming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2301142d-4344-48bd-a9dd-3c589e27167f"}
{"abstract": "The two visual systems hypothesis in neuroscience suggests that pointing without visual feedback may be less affected by spatial visual illusions than cognitive interactions such as judged target location. Our study examined predictions of this theory for target localization on a large-screen display. We contrasted pointing interactions under varying levels of visual feedback with location judgments of targets that were surrounded by an offset frame. As predicted by the theory, the frame led to systematic errors in verbal report of target location but not in pointing without visual feedback for some participants. We also found that pointing with visual feedback produced a similar level of error as location judgments, while temporally lagged visual feedback appeared to reduce these errors somewhat. This suggests that pointing without visual feedback may be a useful interaction technique in situations described by the two visual systems literature, especially with large-screen displays and immersive environments.", "authors": ["Barry A. Po", "Brian D. Fisher", "Kellogg S. Booth"], "n_citation": 0, "title": "Pointing and visual feedback for spatial interaction in large-screen display environments", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "233d59fd-0409-49a1-9aa5-2bc96a2e4879"}
{"abstract": "We present a method for retexturing non-rigid objects from a single viewpoint. Without reconstructing 3D geometry, we create realistic video with shape cues at two scales. At a coarse scale, a track of the deforming surface in 2D allows us to erase the old texture and overwrite it with a new texture. At a fine scale, estimates of the local irradiance provide strong cues of fine scale structure in the actual lighting environment. Computing irradiance from explicit correspondence is difficult and unreliable, so we limit our reconstructions to screen printing - a common printing techniques with a finite number of colors. Our irradiance estimates are computed in a local manner: pixels are classified according to color, then irradiance is computed given the color. We demonstrate results in two situations: on a special shirt designed for easy retexturing and on natural clothing with screen prints. Because of the quality of the results, we believe that this technique has wide applications in special effects and advertising.", "authors": ["Ryan White", "David A. Forsyth"], "n_citation": 55, "title": "Retexturing Single Views Using Texture and Shading", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2340c4ad-e331-40a8-8437-211d756f341d"}
{"abstract": "Motivated by the computational difficulty of analyzing very large Markov chains, we define a notion of clusters in (not necessarily reversible) Markov chains, and explore the possibility of analyzing a cluster in vitro, without regard to the remainder of the chain. We estimate the stationary probabilities of the states in the cluster using only transition information for these states, and bound the error of the estimate in terms of parameters measuring the quality of the cluster. Finally, we relate our results to searching in a hyperlinked environment, and provide supporting experimental results.", "authors": ["Nir Ailon", "Steve Chien", "Cynthia Dwork"], "n_citation": 50, "title": "On clusters in markov chains", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2369d198-e3d6-4601-8e67-3dd5a29e33f4"}
{"abstract": "Recently, there is an increasing interest in integrating rule based methods with statistical techniques for developing robust, wide coverage, high performance parsing systems. In this paper 1 , we describe an architecture, called UCSG shallow parser architecture, which combines linguistic constraints expressed in the form of finite state grammars with statistical rating using HMMs built from a POS-tagged corpus and an A* search for global optimization for determining the best shallow parse for a given sentence. The primary aim of the design of the UCSG parsing architecture is developing a judicious combination of linguistic and statistical methods to develop wide coverage robust shallow parsing systems, without the need for large scale manually parsed training corpora. The UCSG architecture uses a grammar to specify all valid structures and a statistical component to rate and rank the possible alternatives, so as to produce the best parse first without compromising on the ability to produce all possible parses. The architecture supports bootstrapping with an aim to reduce the need for parsed training corpora. The complete system has been implemented in Per1 under Linux. In this paper we first describe the UCSG shallow parsing architecture and then focus on the evaluation of the UCSG finite state grammar for the chunking task for English. Recall of 91.16% and 93.73% have been obtained on the Susanne parsed corpus and CoNLL 2000 chunking task test data set respectively. Extensive experimentation is under way to evaluate the other modules.", "authors": ["Guntur Bharadwaja Kumar", "K. P. N. Murthy"], "n_citation": 0, "title": "UCSG shallow parser", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "266d1050-7619-4b80-a71b-84151336f382"}
{"abstract": "Kernel functions are typically viewed as providing an implicit mapping of points into a high-dimensional space, with the ability to gain much of the power of that space without incurring a high cost if data is separable in that space by a large margin \u03b3. However, the Johnson-Lindenstrauss lemma suggests that in the presence of a large margin, a kernel function can also be viewed as a mapping to a low-dimensional space, one of dimension only O(1/\u03b3 2 ). In this paper, we explore the question of whether one can efficiently compute such implicit low-dimensional mappings, using only black-box access to a kernel function. We answer this question in the affirmative if our method is also allowed black-box access to the underlying distribution (i.e., unlabeled examples). We also give a lower bound, showing this is not possible for an arbitrary black-box kernel function, if we do not have access to the distribution. We leave open the question of whether such mappings can be found efficiently without access to the distribution for standard kernel functions such as the polynomial kernel. Our positive result can be viewed as saying that designing a good kernel function is much like designing a good feature space. Given a kernel, by running it in a black-box manner on random unlabeled examples, we can generate an explicit set of O(1/\u03b3 2 ) features, such that if the data was linearly separable with margin \u03b3 under the kernel, then it is approximately separable in this new feature space.", "authors": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "n_citation": 0, "title": "On kernels, margins, and low-dimensional mappings", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "26f5249b-110c-48fc-a647-0ed5ce106aaa"}
{"abstract": "A novel technique for extracting texture edges is introduced. It is based on the combination of two ideas: the patch-based approach, and non-parametric tests of distributions. Our method can reliably detect texture edges using only local information. Therefore, it can be computed as a preprocessing step prior to segmentation, and can be very easily combined with parametric deformable models. These models furnish our system with smooth boundaries and globally salient structures.", "authors": ["Lior Wolf", "Xiaolei Huang", "Ian Martin", "Dimitris N. Metaxas"], "n_citation": 0, "title": "Patch-Based Texture Edges and Segmentation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "281d2314-7954-499c-89bc-82e7ebe6dd6c"}
{"abstract": "General-purpose, high-availability database systems have lately proliferated to various network element platforms. In telecommunication, databases are expected to meet demanding availability levels while preserving the required throughput. However, so far, the effects of various high-availability configurations on overall database performance have not been analyzed. In this paper, the operation of a fully replicated, hot-standby database system is presented, together with some performance tuning possibilities. To study the effect of several database-tuning parameters, a telecom-oriented database benchmark, TM1, is used. The experiments involve varying of the read/write balance and various logging and replication parameters. It is shown that, by relaxing the reliability requirements, significant performance gains can be achieved. Also, it is demonstrated that a possibility to redirect the log writing from the local disk to the standby node is one of the most important benefits of a high-availability database system.", "authors": ["Antoni Wolski", "Vilho Raatikka"], "n_citation": 50, "title": "Performance Measurement and Tuning of Hot-Standby Databases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "29024219-4808-493f-9447-d6458b982697"}
{"abstract": "In wireless sensor networks, many kinds of failures may arise on sensor nodes because the nodes can be deployed and used even in harsh environments. Therefore. fault-tolerance mechanisms are needed for the wireless sensor networks have to maintain stability and normal operation of the networks. In this paper, we propose an adaptive mobile checkpointing mechanism for wireless sensor networks that gives fault-tolerance for the networks. It is a yet another checkpointing mechanism based on the diskless checkpointing which does not use stable storage but uses the redundant memory space of neighboring nodes. Our experimental results show that the lifetime and stability of sensor networks was dramatically increased compared with the case when the proposed mechanism was used or not.", "authors": ["Sangho Yi", "Junyoung Heo", "Yookun Cho", "Jiman Hong"], "n_citation": 0, "title": "Adaptive Mobile Checkpointing Facility for Wireless Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2913344a-e803-4020-aa40-1a52a4503f42"}
{"abstract": "As the number of resources on the web exceeds by far the number of documents one can track, it becomes increasingly difficult to remain up to date on ones own areas of interest. The problem becomes more severe with the increasing fraction of multimedia data, from which it is difficult to extract some conceptual description of their contents. One way to overcome this problem are social bookmark tools, which are rapidly emerging on the web. In such systems, users are setting up lightweight conceptual structures called folksonomies, and overcome thus the knowledge acquisition bottleneck. As more and more people participate in the effort, the use of a common vocabulary becomes more and more stable. We present an approach for discovering topic-specific trends within folksonomies. It is based on a differential adaptation of the PageRank algorithm to the triadic hypergraph structure of a folksonomy. The approach allows for any kind of data, as it does not rely on the internal structure of the documents. In particular, this allows to consider different data types in the same analysis step. We run experiments on a large-scale real-world snapshot of a social bookmarking system.", "authors": ["Andreas Hotho", "Robert J\u00e4schke", "Christoph Schmitz", "Gerd Stumme"], "n_citation": 124, "title": "Trend detection in folksonomies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2a0f50fe-806c-43a5-b125-c185dae20f19"}
{"abstract": "Current trends in Software Engineering and developments in Logic Programming lead us to believe that there will be an opportunity for Logic Programming to make a breakthrough in Software Engineering. In this paper, we explain how this has arisen, and justify our belief with a real-life application. Above all, we invite fellow workers to take up the challenge that the opportunity offers.", "authors": ["Kung-Kiu Lau", "Michel Vanden Bossche"], "n_citation": 50, "title": "Logic Programming for Software engineering: A second chance", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2d1f3b03-56c6-413b-bad5-dc66219f60c5"}
{"abstract": "Multimedia streaming is usually disturbed by the surroundings. Especially in the ubiquitous computing environment, seamless multimedia service is difficult to provide due to harsh environment of wireless network and limited power of small mobile devices. In this paper we propose a scheme which can enhance the QoS of MPEG-4 streaming. By exhaustive experimental test, we find that the QoS of multimedia streaming significantly depends on the SNR (Signal-to-Noise Ratio). The proposed scheme employs five different packet sizes and selects a one according to the SNR value and the data on the packet delay and loss. Experiment on MPEG-4 video transmission reveals that the proposed streaming with variable packet size allows fast adaptation before the error propagates, minimizes the power consumption, and saves the buffer space compared to the streaming with fixed packet size.", "authors": ["Hyung Su Lee", "Hee Yong Youn", "Hyedong Jung"], "n_citation": 50, "title": "Context-Aware Cross Layered Multimedia Streaming Based on Variable Packet Size Transmission", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2e263d5f-1878-4dae-a420-acc9379d19f1"}
{"abstract": "MPEG-7 can be used to create complex and comprehensive metadata descriptions of multimedia content. Since MPEG-7 is defined in terms of an XML schema, the semantics of its elements have no formal grounding. In addition, certain features can be described in multiple ways. MPEG-7 profiles are subsets of the standard that apply to specific application areas, which can be used to reduce this syntactic variability, but they still lack formal semantics. In this paper, we propose an approach for expressing semantics explicitly by formalizing the semantic constraints of a profile using ontologies and rules, thus enabling interoperability and automatic use for MPEG-7 based applications. We demonstrate the feasibility of the approach by implementing a validation service for a subset of the semantic constraints of the Detailed Audiovisual Profile (DAVP).", "authors": ["Rapha\u00ebl Troncy", "Werner Bailer", "Michael Hausenblas", "Philip Hofmair", "Rudolf Schlatte"], "n_citation": 51, "title": "Enabling multimedia metadata interoperability by defining formal semantics of MPEG-7 profiles", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2f708115-c9bf-42fd-984f-c395ef066a30"}
{"abstract": "In the vehicle routing problem with time windows (VRPTW), there are two main objectives. The primary objective is to reduce the number of vehicles, the secondary one is to minimise the total distance travelled by all vehicles. This paper describes some experiments with multiple ant colony systems, in particular a Triple Ant Colony System TACS, in which one colony (VMIN) tries to minimise the number of vehicles, one (DMIN) tries to minimise the total distance and a third (CWTsMAX) tries to maximise customer waiting time. The inclusion of this third colony improves the results very significantly, compared to not using it and to a range of other options. Experiments are conducted on Solomon's 56 benchmark problems. The results are comparable to those obtained by other state-of-the-art approaches.", "authors": ["Samer Sa'adah", "Peter Ross", "Ben Paechter"], "n_citation": 0, "title": "Improving vehicle routing using a customer waiting time colony", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "30a0d2f0-3d46-47c4-8a0a-53760deed6dd"}
{"abstract": "Application service providers (ASPs) and web services are becoming increasingly popular despite adverse IT market conditions. New languages and protocols like XML, SOAP, and UDDI provide the technical underpinnings for a global infrastructure where anybody with a networked computer has access to a large number of digital services. Not every potential customer, however, may feel comfortable about entrusting sensitive personal or corporate data to a service provider in an unprotected manner. Even if there is a high level of trust between customer and provider, there may be legal requirements that require a higher level of privacy. Customers may also want to be prepared for an unforeseen change of control on the provider's side - something that is not an uncommon occurrence especially among start-up companies. This paper reviews several solutions how customers can use a provider's services without giving it access to any sensitive data. After discussing the relative merits of trust vs. technology, we focus on privacy homomorphisms, an encryption technique originally proposed by Rivest et al. that maintains the structure of the input data while obscuring the actual content. We conclude with several proposals how to integrate privacy homomorphisms into existing service architectures.", "authors": ["Claus Boyens", "Oliver G\u00fcnther"], "n_citation": 0, "title": "Trust is not enough: Privacy and security in ASP and Web service environments", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "32d8bb53-279c-4e60-91ac-c98c292e0ebe"}
{"abstract": "A fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules. One of the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms. In this work, we propose a so-called approximative representation for fuzzy systems, where the antecedent of the rules are determined by a multivariate membership function defined in terms of Voronoi regions. Such representation guarantees the \u2208-completeness property and provides a synergistic relation between the rules. An evolutionary algorithm based on this representation can evolve all the components of the fuzzy system, and due to the properties of the representation, the algorithm (1) can benefit from the use of geometric genetic operators, (2) does not need genetic repair algorithms, (3) guarantees the completeness property and (4) can implement previous knowledge in a simple way by using adaptive a priori rules. The proposed representation is evaluated on an obstacle avoidance problem with a simulated mobile robot.", "authors": ["Carlos Kavka", "Marc Schoenauer"], "n_citation": 0, "title": "Evolution of Voronoi-based fuzzy controllers", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "34508c70-2bdb-4000-83c2-f4f80bc84a2f"}
{"abstract": "XML is increasingly becoming the preferred method of encoding structured data for exchange over the Internet. XML-Schema, which is an emerging text-based schema definition language, promises to become the most popular method for describing these XML-documents. While text-based languages, such as XML-Schema, offer great advantages for data interchange on the Internet, graphical modelling languages are widely accepted as a more visually effective means of specifying and communicating data requirements for a human audience. With this in mind, this paper investigates the use of Object Role Modelling (ORM), a graphical, conceptual modelling technique, as a means for designing XML-Schemas. The primary benefit of using ORM is that it is much easier to get the model 'correct' by designing it in ORM first, rather than in XML. To facilitate this process we describe an algorithm that enables an XML-Schema file to be automatically generated from an ORM conceptual data model. Our approach aims to reduce data redundancy and increase the connectivity of the resulting XML instances.", "authors": ["Linda Bird", "Andrew Goodchild", "Terry A. Halpin"], "n_citation": 32, "title": "Object Role Modelling and XML-Schema", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "35a5fe7f-d4a6-412e-ba9e-ade0ce8899af"}
{"abstract": "In peer-to-peer(P2P) network, the overall efficiency heavily depends on the performance of the lookup procedure. So, we propose a simple caching protocol, which intuitively obtains an information about physical network structure. Our caching protocol utilizes the internet address(IP) i.e. first 16 bits of IP address (can be adaptive), The metadata used in our caching protocol is exchanged using piggy-back mechanism, and we extract useful IP prefix set by using round trip time threshold value. We have deployed our caching protocol into Chord, a well-known distributed hash table-based lookup protocol. And our result showed genuine relationship between physical and logical network structure.", "authors": ["Hyung Soo Jung", "Heon Young Yeom"], "n_citation": 0, "title": "Efficient lookup using proximity caching for P2P networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "35c778fd-6273-4ae9-b67b-78bd62d18821"}
{"abstract": "We develop an efficient identity based signature scheme based on pairings whose security relies on the hardness of the Diffie-Hellman problem in the random oracle model. We describe how this scheme is obtained as a special version of a more general generic scheme which yields further new provably secure identity based signature schemes if pairings are used. The generic scheme also includes traditional public key signature schemes. We further discuss issues of key escrow and the distribution of keys to multiple trust authorities. The appendix contains a brief description of the relevant properties of supersingular elliptic curves and the Weil and Tate pairings.", "authors": ["Florian Hess"], "n_citation": 973, "title": "Efficient identity based signature schemes based on pairings", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "367d97b7-21f2-404e-94d1-edbc49888596"}
{"abstract": "Current findings suggest that human-computer interaction following the basics of human-human interaction, in which emotions play a critical role. We performed a set of consecutive experiments in a laboratory environment for identification, recognition, visualization and interactive computing of affective states within a gaming application framework. To start our research and to get a data base we analyzed more than 90 test hours of user tests using rating-scales and physiological measurements. As results we provide (1) a mini-game with extra features for the induction and obtaining of affective states, (2) integrated data mining methods with recognition rates up to 70 percent, (3) different kind of visual representation of recognized emotions, and (4) an architecture for control of an affective game.", "authors": ["Holger Diener", "Karina Oertel"], "n_citation": 0, "title": "Experimental approach to affective interaction in games", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37fd4435-9a33-4f4e-bab6-52eed97c8ed6"}
{"abstract": "The inverse method is a generalization of resolution that can be applied to non-classical logics. We have recently shown how Andreoli's focusing strategy can be adapted for the inverse method in linear logic. In this paper we introduce the notion of focusing bias for atoms and show that it gives rise to forward and backward chaining, generalizing both hyperresolution (forward) and SLD resolution (backward) on the Horn fragment. A key feature of our characterization is the structural, rather than purely operational, explanation for forward and backward chaining. A search procedure like the inverse method is thus able to perform both operations as appropriate, even simultaneously. We also present experimental results and an evaluation of the practical benefits of biased atoms for a number of examples from different problem domains.", "authors": ["Kaustuv Chaudhuri", "Frank Pfenning", "Greg Price"], "n_citation": 0, "title": "A logical characterization of forward and backward chaining in the inverse method", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "39ff2ce8-c857-446e-a87f-eb6d4833c492"}
{"abstract": "To improve the security of iterated block ciphers, the resistance against linear cryptanalysis has been formulated in terms of provable security which suggests the use of highly nonlinear functions as round functions. Here, we show that some properties of such functions enable to find a new upper bound for the degree of the product of its Boolean components. Such an improvement holds when all values occurring in the Walsh spectrum of the round function are divisible by a high power of 2. This result leads to a higher order differential attack on any 5-round Feistel ciphers using an almost bent substitution function. We also show that the use of such a function is precisely the origin of the weakness of a reduced version of MISTY1 reported in [23, 1].", "authors": ["Anne Canteaut", "Marion Videau"], "n_citation": 79, "title": "Degree of composition of highly nonlinear functions and applications to higher order differential cryptanalysis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3b8b07e5-7bad-4cba-9780-e17e6087e2aa"}
{"abstract": "This paper introduces notions of resource policy for mobile code to be run on smart devices, to integrate with the proof-carrying code architecture of the Mobile Resource Guarantees (MRG) project. Two forms of policy are used: guaranteed policies which come with proofs and target policies which describe limits of the device. A guaranteed policy is expressed as a function of a methods input sizes, which determines a bound on consumption of some resource. A target policy is defined by a constant bound and input constraints for a method. A recipient of mobile code chooses whether to run methods by comparing between a guaranteed policy and the target policy. Since delivered code may use methods implemented on the target machine, guaranteed policies may also be provided by the platform; they appear symbolically as assumptions in delivered proofs. Guaranteed policies entail proof obligations that must be established from the proof certificate. Before proof, a policy checker ensures that the guaranteed policy refines the target policy; our policy format ensures that this step is tractable and does not require proof. Delivering policies thus mediates between arbitrary target requirements and the desirability to package code and certificate only once.", "authors": ["David Aspinall", "Kenneth MacKenzie"], "n_citation": 50, "title": "Mobile Resource Guarantees and Policies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3e952061-ae6e-44a9-b9d4-6b358922e3aa"}
{"abstract": "We present a new technique called balanced randomized tree splitting. It is useful in constructing unknown trees recursively. By applying it we obtain two new results on efficient construction of evolutionary trees: a new upper time-bound on the problem of constructing an evolutionary tree from experiments, and a relatively fast approximation algorithm for the maximum agreement subtree problem for binary trees for which the maximum number of leaves in an optimal solution is large. We also present new lower bounds for the problem of constructing an evolutionary tree from experiments and for the problem of constructing a tree from an ultrametric distance matrix.", "authors": ["Ming-Yang Kao", "Andrzej Lingas", "Anna \u00d6stlin"], "n_citation": 0, "title": "Balanced randomized tree splitting with applications to evolutionary tree constructions", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "43b4c3b1-6b59-471c-90b9-fcc2149160af"}
{"abstract": "In this paper we present a general design methodology suitable for a class of complex multi-agent systems which are capable of self-assembly. Our methodology is based on a top-down, bottom-up approach, which has the potential to achieve a range of global design goals whilst retaining emergent behaviour somewhere in the system, and thereby allowing access to a richer solution space. Our experimental environment is a software system to model 2-dimensional self-assembly of groups of autonomous agents, where agents are defined as square smart blocks. The general design goal for such systems is to direct the self-assembly process to produce a specified structure. The potential of this design methodology has been realised by demonstrating its application to a toy problem - the self-assembly of rectangles of different sizes and shapes in a two-dimensional mesoblock environment. The design procedure shows different choices available for decomposing a system goal into subsidiary goals, as well as the steps needed to ensure a match to what is achievable from the bottom-up process. Encouraging results have been obtained, which allows mesoblock rectangles of specified size to be assembled in a directed fashion. Two different approaches to the same problem were presented, showing the flexibility of the method.", "authors": ["Geoff Poulton", "Ying Guo", "Geoff James", "P. Valencia", "Vadim Gerasimov", "Jiaming Li"], "n_citation": 0, "title": "Directed self-assembly of 2-dimensional mesoblocks using top-down/bottom-up design", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4557e28d-fa4b-429a-89d0-c9b6c4171bf9"}
{"abstract": "An important milestone in the evolution of the Web is the Semantic Web: a Web in which the semantics of the available content and functionality is made explicit. Web design methods, originally aimed at offering a well-structured, systematic approach to Web design, now face new opportunities and challenges: Semantic Web technology can be used to make the semantics of the conceptual design models explicit; however a major challenge is to (semi-) automatically generate the semantic annotations, effectively enabling the Semantic Web. In this paper, we describe how WSDM, a well-known Web design method, was adapted to use Semantic Web technology for its conceptual modeling and how this can be exploited to generate semantically annotated websites. We consider two types of semantic annotations: content-related annotations and structural annotations. The first type allows to describe the semantics of the content of the website, the latter are annotations that explicitly describe the semantics of the different structural elements used in the website.", "authors": ["Sven Casteleyn", "Peter Plessers", "Olga De Troyer"], "n_citation": 50, "title": "On Generating Content and Structural Annotated Websites Using Conceptual Modeling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "458917f9-e823-4d42-b936-4abe40d840b3"}
{"abstract": "In this paper algorithms for model checking CSL (continuous stochastic logic) against infinite-state continuous-time Markov chains of so-called quasi birth-death type are developed. In doing so we extend the applicability of CSL model checking beyond the recently proposed case for finite-state continuous-time Markov chains, to an important class of infinite-state Markov chains. We present syntax and semantics for CSL and develop efficient model checking algorithms for the steady-state operator and the time-bounded next and until operator. For the former, we rely on the so-called matrix-geometric solution of the steady-state probabilities of the infinite-state Markov chain. For the time-bounded until operator we develop a new algorithm for the transient analysis of infinite-state Markov chains, thereby exploiting the quasi birth-death structure. A case study shows the feasibility of our approach.", "authors": ["Anne Remke", "Boudewijn R. Haverkort", "Lucia Cloth"], "n_citation": 50, "title": "Model checking infinite-state Markov chains", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "45ea584c-e850-4fb1-91a2-783992afd47b"}
{"authors": ["Defu Lian", "Y. C. Zhu", "Xing Xie", "Enhong Chen"], "n_citation": 0, "title": "Analyzing location predictability on location-based social networks", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "47bfce4c-9a14-48ef-9519-10444612b06b"}
{"abstract": "In this paper we introduce semi-nice tree-decompositions and show that they combine the best of both branchwidth and treewidth. We first give simple algorithms to transform a given tree-decomposition or branch-decomposition into a semi-nice tree-decomposition. We then give two templates for dynamic programming along a semi-nice tree-decomposition, one for optimization problems over vertex subsets and another for optimization problems over edge subsets. We show that the resulting runtime will match or beat the runtimes achieved by doing dynamic programming directly on either a branch- or tree-decomposition. For example, given a graph G on n vertices with path-, tree- and branch-decompositions of width pw, tw and bw respectively, the Minimum Dominating Set problem on G is solved in time O(n2 min{1.58  pw,2 tw,  2.38bw} ) by a single dynamic programming algorithm along a semi-nice tree-decomposition.", "authors": ["Frederic Dorn", "Jan Arne Telle"], "n_citation": 11, "title": "Two birds with one stone : The best of branchwidth and treewidth with one algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "482405bc-9b88-48fb-88f0-8790e17befb9"}
{"abstract": "In an environment where fitness evaluations are disturbed by noise, the selection operator is prone to errors, occasionally and intendedly selecting the worse individual. A common method to reduce the noise is to sample an individual's fitness several times, and use the average as an estimate of the true fitness. Unfortunately, such a noise reduction is computationally rather expensive. Sequential sampling does not fix the number of samples in advance for all individuals, but instead selects samples one at a time, until a certain level of confidence is achieved. This allows to reduce the number of samples, because individuals with very different true fitness values can be compared on the basis of only few samples (as the signal-to-noise ratio is rather high in this case) while very similar individuals are evaluated often enough to guarantee the desired level of confidence. In this paper, for the case of tournament selection, we show that the use of a state-of-the-art sequential sampling procedure may save a significant portion of the fitness evaluations, without increasing the selection error. Furthermore, we design a new sequential sampling procedure and show that it saves an even larger portion of the fitness evaluations. Finally, we compare the three methods also empirically on a simple onemax function.", "authors": ["J\u00fcrgen Branke", "Christian Schmidt"], "n_citation": 0, "title": "Sequential sampling in noisy environments", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "49bf5a34-dff1-4e7c-a647-37b5038b7d98"}
{"abstract": "We present a new methodology to automate decision making over time and uncertainty. We adopt a knowledge-based model construction approach to support automated and interactive formulation of dynamic decision models, i.e., models that explicitly consider the effects of time. Our work integrates and extends different features of the existing frameworks. We incorporate a hybrid knowledge representation scheme that integrates categorical knowledge, probabilistic knowledge, and deterministic knowledge. We provide a set of knowledge-based modification operations for automatic and interactive generation, abstraction, and refinement of the model components. We have built a knowledge base in a real-world domain and shown that it can support automated construction of a reasonable dynamic decision model. The results indicate the practical promise of the proposed design.", "authors": ["C Wang", "Tze Yun Leong"], "n_citation": 0, "title": "Knowledge-based formulation of dynamic decision models", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "4a3e9eca-2dcd-4cb4-8d0e-649960016771"}
{"abstract": "Recent development on Web/Grid services have achieved some success but also exposes several open issues such as state management. In this paper, we analyze the requirements for state management. We compare different ways of achieving state management in Web Service, Grid Service and the recent Web Service Resource Framework, and articulate the costs and benefits of each approach in terms of fault tolerance, communication cost, etc. We propose a new approach for grid service state management and its prototype implementation.", "authors": ["Yong Xie", "Yong-Meng Teo"], "n_citation": 0, "title": "State management issues and Grid services", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4b444831-637e-4002-975f-3431d3c44045"}
{"abstract": "DNA microarray technology has demonstrated to be an effective methodology for the diagnosis of cancers by means of microarray data classification. Although much research has been conducted during the recent years to apply machine learning techniques for microarray data classification, there are two important issues that prevent the use of conventional machine learning techniques, namely the limited availability of training samples and the existence of various uncertainties (e.g. biological variability and experiment variability). This paper presents a new ensemble machine learning approach to address these issues in order to achieve a robust microarray data classification. Ensemble learning combines a set of base classifiers as a committee to make appropriate decisions when classifying new data instances. In order to enhance the performance of the ensemble learning process, the approach presented includes a procedure to select optimal ensemble members that maximize the behavioural diversity. The proposed approach has been verified by three microarray datasets for cancer diagnosis. Experimental results have demonstrated that the classifier constructed by the proposed method outperforms not only the classifiers generated by the conventional machine learning techniques, but also the classifiers generated by two widely-used conventional Bagging and Boosting ensemble learning methods.", "authors": ["Yonghong Peng"], "n_citation": 0, "title": "Robust ensemble learning for cancer diagnosis based on microarray data classification", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4cf7427f-a9da-4468-9e7e-9499812bdf06"}
{"abstract": "In the area of Web services and service-oriented architectures, business protocols are rapidly gaining importance and mindshare as a necessary part of Web service descriptions. Their immediate benefit is that they provide developers with information on how to write clients that can correctly interact with a given service or with a set of services. In addition, once protocols become an accepted practice and service descriptions become endowed with protocol information, the middleware can be significantly extended to better support service development, binding, and execution in a number of ways, considerably simplifying the whole service life-cycle. This paper discusses the different ways in which the middleware can leverage protocol descriptions, and focuses in particular on the notions of protocol compatibility, equivalence, and replace-ability. They characterise whether two services can interact based on their protocol definition, whether a service can replace another in general or when interacting with specific clients, and which are the set of possible interactions among two services.", "authors": ["Boualem Benatallah", "Fabio Casati", "Farouk Toumani"], "n_citation": 0, "title": "Analysis and management of Web service protocols", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4d73058f-e267-4466-901a-546683215aa2"}
{"abstract": "To enable copyright protection and authentication, robust digital watermark can be embedded into multimedia contents imperceptibly. However, geometric distortions pose a significant threat to robust image watermarking because it can desynchronize the watermark information while preserving the visual quality. To overcome this, we developed an invariant domain with three transforms; Fast Fourier Transform (FFT), Log-Polar Mapping (LPM), and Dual Tree-Complex Wavelet Transform (DT-CWT). Shift invariance is obtained using FFT. Rotation and scaling invariance are achieved by taking the DT-CWT of a LPM output. Unlike most invariant schemes, our method eliminates explicit re-synchronization. The method resists geometric distortions at both global and local scales. It is also robust against JPEG compression and common image processing. In addition, it exploits perceptual masking property of the DT-CWT subbands, and its watermark detection step does not require the cover image. Experiment on a large set of natural images shows the robustness of the new scheme.", "authors": ["Chaw-Seng Woo", "Jiang Du", "Binh Pham"], "n_citation": 0, "title": "Geometric invariant domain for image watermarking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4d90987e-44aa-4252-965e-eafac398ba28"}
{"abstract": "CSP-style synchronizations have been used extensively in the construction of mathematical models for the verification of embedded systems. Although they allow for the modeling of complex cooperation among many processes in a natural environment, not many tools have been developed to support the modeling capability in this regard. In this paper, we first give examples to argue that special algorithms are needed for the efficient verification of systems with complex synchronizations. We then define our models of distributed real-time systems with synchronized cooperation among many processes. We present algorithms for the construction of BDD-like data-structures for the characterization of complex synchronizations among many processes. We present weakest precondition algorithms that take advantage of the just-mentioned BDD-like data-structures for the efficient verification of complex real-time systems. Finally, we report experiments and argue that the techniques could be useful in practice.", "authors": ["Farn Wang"], "n_citation": 2, "title": "Symbolic verification of distributed real-time systems with complex synchronizations", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4daaa6b4-7100-4fa2-9f70-ec4818b26bac"}
{"abstract": "In the field of data mining (DM), feature selection is one of the basic strategies handling with high-dimensionality problems. This paper makes a review of current methods of feature selection and proposes a unified strategy of feature selection, which divides overall procedures of feature selection into two stages, first to determine the FIF (Feature Important Factor) of features according to DM tasks, second to select features according to FIF. For classifying problems, we propose a new method for determining FIF based on decision trees and provide practical suggestion for feature selection. Through analysis on experiments conducted on UCI datasets, such a unified strategy of feature selection is proven to be effective and efficient.", "authors": ["Peng Liu", "Naijun Wu", "Jiaxian Zhu", "Junjie Yin", "Wei Zhang"], "n_citation": 0, "title": "A Unified Strategy of Feature Selection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4dad2608-d4ed-4f47-afa8-0e9388391b2f"}
{"abstract": "We describe our current work on volume visualization techniques developed with the aim to enhance interactive exploration of medical datasets. Volumetric lenses can be applied to a volume dataset to interactively focus regions of interest within these datasets. During rendering the parts of a volume dataset intersecting the lens, which is defined by a convex 3D shape, are rendered using a different visual appearance. The lenses proposed allow to apply non-photorealistic rendering techniques interactively to aid comprehension for medical diagnosis.", "authors": ["Timo Ropinski", "Rank Steinicke", "Klaus H. Hinrichs"], "n_citation": 0, "title": "Tentative results in focus-based medical volume visualization", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4f775cc6-24ea-4b14-8e93-342023ae5b55"}
{"abstract": "In this paper, we propose a reliability analysis model using the RBD technique to evaluate the reliability of grid environments. In the end the uses of the model are also presented.", "authors": ["Xuanhua Shi", "Hai Jin", "Weizhong Qiang", "Deqing Zou"], "n_citation": 0, "title": "Reliability analysis for grid computing", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5209f6df-25f6-423d-9840-52ceaf27ff82"}
{"abstract": "Based on inductive definitions, we develop an automated tool for defining partial recursive functions in Higher-Order Logic and providing appropriate reasoning tools for them. Our method expresses termination in a uniform manner and includes a very general form of pattern matching, where patterns can be arbitrary expressions. Termination proofs can be deferred, restricted to subsets of arguments and are interchangeable with other proofs about the function. We show that this approach can also facilitate termination arguments for total functions, in particular for nested recursions. We implemented our tool as a definitional specification mechanism for Isabelle/HOL.", "authors": ["Alexander Krauss"], "n_citation": 0, "title": "Partial recursive functions in higher-order logic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5394fff1-948e-498b-8c1a-88870397dcb0"}
{"abstract": "We consider hierarchical systems where nodes represent entities and edges represent binary relationships among them. An example is a hierarchical composition of Web services where the nodes denote services and edges represent the parent-child relationship of a service invoking another service. A fundamental issue to address in such systems is, for two nodes X and Y in the hierarchy whether X can see Y, that is, whether X has visibility over Y. In a general setting, X seeing Y may depend on (i) X wishing to see Y, (ii) Y wishing to be seen by X, and (iii) other nodes not objecting to X seeing Y. The visibility could be with respect to certain attributes like operational details, execution logs, security related issues, etc. In this paper, we develop a generic conceptual model to express visibility. We study two complementary notions: sphere of visibility of a node X that includes all the nodes in the hierarchy that X sees; and sphere of noticeability of X that includes all the nodes that see X. We also identify the dual properties, coherence and correlation, that relate the visibility and noticeability notions. We propose elegant methods of constructing the spheres with these properties.", "authors": ["Debmalya Biswas", "K. Vidyasankar"], "n_citation": 0, "title": "Modeling Visibility in Hierarchical Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "58774021-1d28-44a9-9e81-e5a85ba3823a"}
{"abstract": "Context Unification is the problem to decide for a given set of second-order equations E where all second-order variables are unary, whether there exists a unifier, such that for every second-order variable X, the abstraction Ax.r instantiated for X has exactly one occurrence of the bound variable x in r. Stratified Context Unification is a specialization where the nesting of second-order variables in E is restricted. It is already known that Stratified Context Unification is decidable, NP-hard, and in PSPACE, whereas the decidability and the complexity of Context Unification is unknown. We prove that Stratified Context Unification is in NP by proving that a size-minimal solution can be represented in a singleton tree grammar of polynomial size, and then applying a generalization of Plandowski's polynomial algorithm that compares compacted terms in polynomial time. This also demonstrates the high potential of singleton tree grammars for optimizing programs maintaining large terms. A corollary of our result is that solvability of rewrite constraints is NP-complete.", "authors": ["Jordi Levy", "Manfred Schmidt-Schauss", "Mateu Villaret"], "n_citation": 0, "title": "Stratified context unification is NP-complete", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "593e77e8-f94c-4d04-8f58-2bbf4f1a0fd0"}
{"abstract": "Triggered Message Sequence Charts (TMSCs) are a scenario-based visual formalism for early stage requirements specifications of distributed systems. In this paper, we present a formal operational semantics for TMSCs that allow the simulation of TMSC system descriptions, so that errors and inconsistencies in specification may be detected early on. The semantics is defined in terms of Structured Operational Semantics (SOS) rules that guide the step-wise execution of TMSC specifications. We also consider the equivalence of this semantics and the TMSC denotational semantics that has been presented in previous work.", "authors": ["Bikram Sengupta", "Rance Cleaveland"], "n_citation": 4, "title": "Executable requirements specifications using triggered message sequence charts", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5ab6648a-eb01-4e25-b32c-edd00697434c"}
{"authors": ["Alexander Grigoriev", "L. Marchal", "Natalya Usotskaya", "J. van Leeuwen"], "n_citation": 0, "title": "Algorithms for the Minimum Edge Cover of H-Subgraphs of a Graph", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "5bdd8335-ab8b-4b29-a07e-a68511205ee7"}
{"abstract": "Parametric design systems model a design as a constrained collection of schemata. Designers work in such systems at two levels: definition of schemata and constraints; and search within a schema collection for meaningful instances. Propagation-based systems yield efficient algorithms that are complete within their domain, require explicit specification of a directed acyclic constraint graph and allow relatively simple debugging strategies based on antecedents and consequents. The requirement to order constraints appears to be useful in expressing specific designer intentions and in disambiguating interaction. A key feature of such systems in practice appears to be a need for multiple views onto the constraint model and simultaneous interaction across views. We describe one multiple-view structure, its development and refinement through a large group of architecture practitioners and its realization in the system Generative Components.", "authors": ["Robert Aish", "Robert F. Woodbury"], "n_citation": 0, "title": "Multi-level interaction in parametric design", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5d9d297d-aecc-48a9-9777-2a896fd69597"}
{"abstract": "Bag-of-features representations have recently become popular for content based image classification owing to their simplicity and good performance. They evolved from texton methods in texture analysis. The basic idea is to treat images as loose collections of independent patches, sampling a representative set of patches from the image, evaluating a visual descriptor vector for each patch independently, and using the resulting distribution of samples in descriptor space as a characterization of the image. The four main implementation choices are thus how to sample patches, how to describe them, how to characterize the resulting distributions and how to classify images based on the result. We concentrate on the first issue, showing experimentally that for a representative selection of commonly used test databases and for moderate to large numbers of samples, random sampling gives equal or better classifiers than the sophisticated multiscale interest operators that are in common use. Although interest operators work well for small numbers of samples, the single most important factor governing performance is the number of patches sampled from the test image and ultimately interest operators can not, provide enough patches to compete. We also study the influence of other factors including codebook size and creation method, histogram normalization method and minimum scale for feature extraction.", "authors": ["Eric Nowak", "Fr\u00e9d\u00e9ric Jurie", "Bill Triggs"], "n_citation": 0, "title": "Sampling Strategies for Bag-of-Features Image Classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5ecdd4c6-97ba-4c8f-ada4-bcb6486fe383"}
{"abstract": "Data Stream Management (DSM) addresses the continuous processing of sensor data. DSM requires the combination of stream operators, which may run on different distributed devices, into stream processes. Due to the recent advantages in sensor technologies and wireless communication, DSM is increasingly gaining importance in various application domains. Especially in healthcare, the continuous monitoring of patients at home (telemonitoring) can significantly benefit from DSM. A vital requirement in telemonitoring is however that DSM provides a high degree of reliability. In this paper, we present a novel approach to efficient and coordinated stream operator checkpointing supporting reliable DSM while maintaining the high result quality needed for healthcare applications. Furthermore, we present evaluation results of our checkpointing approach implemented within our process and data stream management infrastructure OSIRIS-SE. OSIRIS-SE supports flexible failure handling and efficient and coordinated checkpointing by means of consistent operator migration. This ensures complete and consistent continuous data stream processing even in the case of failures.", "authors": ["Gert Brettlecker", "Heiko Schuldt", "Hans-J\u00f6rg Schek"], "n_citation": 0, "title": "Efficient and coordinated checkpointing for reliable distributed data stream management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "61fe2a6a-f24d-465b-8346-08e391ec1975"}
{"abstract": "We present a technique for Merkle tree traversal which requires only logarithmic space and time. For a tree with N leaves, our algorithm computes sequential tree leaves and authentication path data in time 2log 2 (N) and space less than 3log 2 (N), where the units of computation are hash function evaluations or leaf value computations, and the units of space are the number of node values stored. This result is an asymptotic improvement over all other previous results (for example, measuring cost = space * time). We also prove that the complexity of our algorithm is optimal: There can exist no Merkle tree traversal algorithm which consumes both less than O(log 2 (N)) space and less than O(log 2 (N)) time. Our algorithm is especially of practical interest when space efficiency is required.", "authors": ["Michael Szydlo"], "n_citation": 0, "title": "Merkle tree traversal in log space and time", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6285dae2-594c-4332-bde7-7d531f72ecac"}
{"abstract": "The performance of ICA algorithms significantly depends on the choice of the contrast function and the optimisation algorithm used in obtaining the demixing matrix. In this paper we focus on the standard linear nonparametric ICA problem from an optimisation point of view. It is well known that after a pre-whitening process, the problem can be solved via an optimisation approach on a suitable manifold. We propose an approximate Newton's method on the unit sphere to solve the one-unit linear nonparametric ICA problem. The local convergence properties are discussed. The performance of the proposed algorithms is investigated by numerical experiments.", "authors": ["Hao Shen", "Knut H\u00fcper", "Alexander J. Smola"], "n_citation": 50, "title": "Newton-Like Methods for Nonparametric Independent Component Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "633ae780-0446-439f-96cd-e5ea8946bf83"}
{"abstract": "Functional Magnetic Resonance Imaging (fMRI) requires ultra-fast imaging in order to capture the on-going spatio-temporal dynamics of the cognitive task. We make use of correlations in both k-space and time, and thereby reconstruct the time series by acquiring only a fraction of the data, using an improved form of the well-known dynamic imaging technique k-t BLAST (Broad-use Linear Acquisition Speed-up Technique). k-t BLAST (\u03ba-tB) works by unwrapping the aliased Fourier conjugate space of k-t ( y-f space). The unwrapping process makes use of an estimate of the true y-f space, obtained by acquiring a blurred unaliased version. In this paper, we propose two changes to the existing algorithm. Firstly, we improve the map estimate using generalized series reconstruction. The second change is to incorporate phase constraints from the training map. The proposed technique is compared with existing k-tB on visual stimulation fMRI data obtained on 5 volunteers. Results show that the proposed changes lead to gain in temporal resolution by as much as a factor of 6. Performance evaluation is carried out by comparing activation maps obtained using reconstructed images, against that obtained from the true images. We observe upto 10dB improvement in PSNR of activation maps. Besides, RMSE reduction on fMRI images, of about 10% averaged over the entire time series, with a peak improvement of 35% compared to the existing k-tB, averaged over 5 data sets, is also observed.", "authors": ["Neelam Sinha", "Manojkumar Saranathan", "A. G. Ramakrishnan", "Juan Zhou", "Jagath C. Rajapakse"], "n_citation": 0, "title": "Ultra-Fast fMRI Imaging with High-Fidelity Activation Map", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "652c1a5a-7bea-4b27-b924-b398718a3027"}
{"abstract": "In recent years, logical frameworks which support formalizing language specifications together with their meta-theory have been pervasively used in small and large-scale applications, from certifying code [2] to advocating a general infrastructure for formalizing the meta-theory and semantics of programming languages [5]. In particular, the logical framework LF [9], based on the dependently typed lambdacalculus, and light-weight variants of it like LF i  [17] have played a major role in these applications. While the acceptance of logical framework technology has grown and they have matured, one of the most criticized points is concerned with the run-time performance. In this tutorial we give a brief introduction to logical frameworks, describe its state-of-the art and present recent advances in addressing some of the existing performance issues.", "authors": ["Brigitte Pientka"], "n_citation": 0, "title": "Overcoming performance barriers : Efficient verification techniques for logical frameworks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65af631d-5a9c-4d1c-9680-13c4b37f166e"}
{"abstract": "The benefits of architecture description languages (ADLs) cannot be fully captured without a automated and validated realization of software architecture designs. In addition to the automated realization of software architecture designs, we validate the realization process by exploring the runtime verification technique and aspect-oriented programming. More specifically, system properties are not only verified against design models, but also verified during execution of the generated implementation of software architecture designs. All these can be done in an automated way. In this paper, we show that our methodology of automated realization of software architecture designs and validation of the implementation is viable through a case study.", "authors": ["Zhijiang Dong", "Yujian Fu", "Yue Fu", "Xudong He"], "n_citation": 0, "title": "Automated runtime validation of software architecture design", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "67081b96-3c8b-434c-862b-ff6a6d3412c6"}
{"abstract": "As an extension of Mobile IPv6 to the case of network mobility (NEMO), the NEMO basic support protocol has been proposed. However, it suffers from some drawbacks such as pinball routing and large packet sizes in nested mobile networks (MoNET). In this paper we introduce new pinball routing problems associated with local mobile nodes in nested MoNETs and discuss on the effectiveness of several RO schemes in tackling the problems. Among the schemes we demonstrate that the RCS scheme can be an appropriate solution due to its low signaling overhead and short RO setup time compared to other approaches. Most importantly, we show that the RCS scheme is the unique solution for mobility support when a mobile network taking part in a nested MoNET changes its attachment point within the MoNET.", "authors": ["Young Beom Kim", "Young-Jae Park", "Sangbok Kim", "Eui-Nam Huh"], "n_citation": 0, "title": "Route Optimization Problems with Local Mobile Nodes in Nested Mobile Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "68065b94-b8b8-4765-a116-3dc7efd525f5"}
{"abstract": "In this paper, we present an extended self-organizing map. It keeps full connectivity between adjacent layers but adds new virtual connections between neurons of competitive layer so that the structure of competitive layer can be regarded as a graph and can be expressed by an adjacent matrix. Thus, the conventional SOMs can be regarded as special cases of the extended model. Then we can evolve the graph into arbitrary topology such as small world graph and random graph. After evolution we can obtain arbitrary nonlinear neighborhood kernel of neurons and the obtained topology of competitive layer is expected to simulate the distribution of input samples. The experimental results show that the new extended model has better performance in speed and self-organization than conventional ones.", "authors": ["Shuzhong Yang", "Siwei Luo", "Jianyu Li"], "n_citation": 0, "title": "An Extended Model on Self-Organizing Map", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "68bc1848-4ea6-41d6-ac2e-0c334e1418bf"}
{"abstract": "Most documents available over the web conform to the HTML specification. Such documents are hierarchically structured in nature. The existing graph-based or tree-based data models for the web only provide a very low level representation of such hierarchical structure. In this paper, we introduce a conceptual model for the web that is able to represent the complex hierarchical structure within the web documents at a high level that is close to human conceptualization/visualization of the documents. We also describe how to convert HTML documents based on this conceptual model. Using the conceptual model and conversion method, we can capture the essence (i.e., semistructure) of HTML documents in a natural and simple way.", "authors": ["Mengchi Liu", "Tok Wang Ling"], "n_citation": 0, "title": "A conceptual model for the Web", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "6b2d9f08-a6c8-4dfd-bc42-c9a23538700a"}
{"abstract": "Linear Second-Order Unification and Context Unification are closely related problems. However, their equivalence was never formally proved. Context unification is a restriction of linear second-order unification. Here we prove that linear second-order unification can be reduced to context unification with tree-regular constraints. Decidability of context unification is still an open question. We comment on the possibility that linear second-order unification is decidable, if context unification is, and how to get rid of the tree-regular constraints. This is done by reducing rank-bound tree-regular constraints to word-regular constraints.", "authors": ["Jean-Jacques L\u00e9vy", "Mateu Villaret"], "n_citation": 50, "title": "Linear Second-Order Unification and Context Unification with tree-regular constraints", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "6c3448e6-c7c7-4523-9f77-c7d869c5af51"}
{"abstract": "We show how the understandability and speed of genetic programming classification algorithms can be improved, without affecting the classification accuracy. By analyzing the decision trees evolved we can remove the unessential parts, called introns, from the discovered decision trees. Since the resulting trees contain only useful information they are smaller and easier to understand. Moreover, by using these pruned decision trees in a fitness cache we can significantly reduce the number of unnecessary fitness calculations.", "authors": ["Jeroen Eggermont", "Joost N. Kok", "Walter A. Kosters"], "n_citation": 0, "title": "Detecting and pruning introns for faster decision tree evolution", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6c7e26b8-f25f-43dc-a590-dbe96d720c09"}
{"abstract": "In this paper we describe a new method for automatically estimating where a person is looking in images where the head is typically in the range 20 to 40 pixels high. We use a feature vector based on skin detection to estimate the orientation of the head, which is discretised into 8 different orientations, relative to the camera. A fast sampling method returns a distribution over previously-seen head-poses. The overall body pose relative to the camera frame is approximated using the velocity of the body, obtained via automatically-initiated colour-based tracking in the image sequence. We show that, by combining direction and head-pose information gaze is determined more robustly than using each feature alone. We demonstrate this technique on surveillance and sports footage.", "authors": ["Neil Robertson", "Ian D. Reid 0001"], "n_citation": 78, "title": "Estimating Gaze Direction from Low-Resolution Faces in Video", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "71a1d48f-0ab6-435a-84d7-cda2ae57652a"}
{"abstract": "This paper presents systems for first-order intuitionistic logic and several of its extensions in which all the propositional rules are local, in the sense that, in applying the rules of the system, one needs only a fixed amount of information about the logical expressions involved. The main source of non-locality is the contraction rules. We show that the contraction rules can be restricted to the atomic ones, provided we employ deep-inference, i.e., to allow rules to apply anywhere inside logical expressions. We further show that the use of deep inference allows for modular extensions of intuitionistic logic to Dummett's intermediate logic LC, Godel logic and classical logic. We present the systems in the calculus of structures, a proof theoretic formalism which supports deep-inference. Cut elimination for these systems are proved indirectly by simulating the cut-free sequent systems, or the hypersequent systems in the cases of Dummett's LC and Godel logic, in the cut free systems in the calculus of structures.", "authors": ["Alwen Tiu"], "n_citation": 0, "title": "A local system for intuitionistic logic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7212dc6d-90a8-4cf5-89e7-1901bb677989"}
{"abstract": "We introduce a formal, time aware framework for modelling and analysis multiclocked VLSI systems. We define a delay calculus framework for our timed formalism, and, furthermore, constraints with which to confine the correctness of the system under development, not only logically but also with respect to timing characteristics. We give an elaborate definition of the timed formalism, Timed Action Systems, and its delay models. With the timing aware formal development framework it is possible to obtain information of multiclocked VLSI systems already at high abstraction levels as our application, a GALS (globally asynchronous, locally synchronous) system, shows.", "authors": ["Tomi Westerlund", "Juha Plosila"], "n_citation": 0, "title": "Time Aware Modelling and Analysis of Multiclocked VLSI Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "733d0e2b-3dcd-402f-9ece-976a8cd0c459"}
{"abstract": "In this paper, we propose a high performance multi-objective evolutionary algorithm (HPMOEA) based on the principles of the minimal free energy in thermodynamics. The main innovations of HPMOEA are: (1) providing of a new fitness assignment strategy by combining Pareto dominance relation and Gibbs entropy, (2) the provision of a new criterion for selection of new individuals to maintain the diversity of the population. We use convergence and diversity to measure the performance of the proposed HPMOEA, and compare it with the other four well-known multi-objective evolutionary algorithms (MOEAs): NSGA II, SPEA, PAES, TDGA for a number of test problems. Simulation results show that the HPMOEA is able to find much better spread of solutions and has better convergence near the true Pareto-optimal front on most problems.", "authors": ["Xiufen Zou", "Minzhong Liu", "Lishan Kang", "Jun He"], "n_citation": 18, "title": "A high performance multi-objective evolutionary algorithm based on the principles of thermodynamics", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7375be86-a232-4208-ad97-248bcc552337"}
{"abstract": "This paper shows that if the curvature of the boundary of the set of superpredictions for a game vanishes in a nontrivial way, then there is no predictive complexity for the game. This is the first result concerning the absence of complexity for games with convex sets of superpredictions. The proof is further employed to show that for some games there are no certain variants of weak predictive complexity. In the case of the absolute-loss game we reach a tight demarcation between the existing and non-existing variants of weak predictive complexity.", "authors": ["Yuri Kalnishkan", "Michael V. Vyugin"], "n_citation": 0, "title": "On the absence of predictive complexity for some games", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7414a0b7-0891-4baa-9b66-d1525238e784"}
{"abstract": "Algebraic attacks on LFSR-based stream ciphers recover the secret key by solving an overdefined system of multivariate algebraic equations. They exploit multivariate relations involving key bits and output bits and become very efficient if such relations of low degrees may be found. Low degree relations have been shown to exist for several well known constructions of stream ciphers immune to all previously known attacks. Such relations may be derived by multiplying the output function of a stream cipher by a well chosen low degree function such that the product function is again of low degree. In view of algebraic attacks, low degree multiples of Boolean functions are a basic concern in the design of stream ciphers as well as of block ciphers. This paper investigates the existence of low degree multiples of Boolean functions in several directions: The known scenarios under which low degree multiples exist are reduced and simplified to two scenarios, that are treated differently in algebraic attacks. A new algorithm is proposed that allows to successfully decide whether a Boolean function has low degree multiples. This represents a significant step towards provable security against algebraic attacks. Furthermore, it is shown that a recently introduced class of degree optimized Maiorana-McFarland functions immanently has low degree multiples. Finally, the probability that a random Boolean function has a low degree multiple is estimated.", "authors": ["Willi Meier", "Enes Pasalic", "Claude Carlet"], "n_citation": 0, "title": "Algebraic attacks and decomposition of Boolean functions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "749a4f96-d93c-4210-9465-6c5109beb21e"}
{"abstract": "Often the selfish and strong are believed to be favored by natural selection, even though cooperative interactions thrive at all levels of organization in living systems. Recent empirical data shows that networks representing the social interactions between people exhibit typically high average connectivity and associated single-to-broad-scale heterogeneity, a feature which precludes the emergence of cooperation in any static network. Here, we employ a model in which individuals are able to self-organize both their strategy and their social ties throughout evolution, based exclusively on their self-interest. The entangled evolution of individual strategy and network structure provides a key mechanism toward the sustainability of cooperation in social networks. The results show that simple topological dynamics reflecting the individual capacity for self-organization of social ties can produce realistic networks of high average connectivity with associated single-to-broad-scale heterogeneity, in which cooperation thrives. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "authors": ["Jorge M. Pacheco", "Tom Lenaerts", "Francisco C. Santos"], "n_citation": 0, "title": "Evolution of cooperation in a population of selfish adaptive agents", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "7523e595-c4e1-41af-b373-811dee1d7162"}
{"authors": ["Terry A. Halpin", "Gerd Wagner"], "n_citation": 0, "title": "Modeling reactive behavior in ORM", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "76d98fdf-8fe7-4027-9d49-26da06cf621f"}
{"abstract": "A novel framework, called MAEDD (Mobile Agent Enabled Deadlock Detection), for distributed deadlock detection using mobile agents is proposed. In MAEDD, mobile agents are dispatched to collect and analyze deadlock information distributed over the network sites, detect deadlock cycles in the system, and then resolve the deadlocks. A prototype implementing the proposed framework is developed using IBM's Aglets. The performance of our mobile agent enabled approach is evaluated against traditional message-passing based solutions. Preliminary performance evaluation results indicate that MAEDD can detect deadlock faster than massage-passing solutions, with increased network traffic.", "authors": ["Jiannong Cao", "Jingyang Zhou", "Weiwei Zhu", "Daoxu Chen", "Jian Lu"], "n_citation": 0, "title": "A mobile Agent Enabled approach for distributed deadlock detection", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "78cc8f50-8dea-4d74-87ff-4c5aa3a5774f"}
{"abstract": "The mediation architecture is widely used for bridging heterogeneous data sources. We investigate how such architecture can be extended to embrace information processing services and suggest a framework that supports declarative specification of mediation logic. In this paper we show how our framework can be applied to enrich interface descriptions of distributed objects and to integrate them with other client/server environments.", "authors": ["Sergey Melnik"], "n_citation": 50, "title": "Declarative mediation in distributed systems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "7955f77e-a465-499d-8a5b-2cea7829010d"}
{"abstract": "IDS use different sources of observation data and a variety of techniques to differentiate between benign and malicious behaviors. In the current work, Hidden Markov Models (HMM) are used in a manner analogous to their use in text categorization. The proposed approach performs host-based intrusion detection by using HMM along with STIDE methodology (enumeration of sub-sequences) in a hybrid fashion. The proposed method differs from STIDE in that only one profile is created for the normal behavior of all applications using short sequences of system calls issued by the normal runs of the programs. Subsequent to this, HMM with simple states along with STIDE is used to categorize an unknown program's sequence of system calls to be either normal or an intrusion. The results on 1998 DARPA data show that the hybrid method results in low false positive rate with high detection rate.", "authors": ["C. V. Raman", "Atul Negi"], "n_citation": 0, "title": "A hybrid method to intrusion detection systems using HMM", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7b15d96e-96f9-465d-bb10-8976d591ed0e"}
{"abstract": "Mining association rules is an important problem in data mining. Algorithms for mining boolean data have been well studied and documented, but they cannot deal with quantitative and categorical data directly. For quantitative attributes, the general idea is partitioning the domain of a quantitative attribute into intervals, and applying boolean algorithms to the intervals. But, there is a conflict between the minimum support problem and the minimum confidence problem, while existing partitioning methods cannot avoid the conflict. Moreover, we expect the intervals to be meaningful. Clustering in data mining is a discovery process which groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized. The discovered clusters are used to explain the characteristics of the data distribution. The present paper will propose a novel method to find quantitative association rules by clustering the transactions of a database into clusters and projecting the clusters into the domains of the quantitative attributes to form meaningful intervals which may be overlapped. Experimental results show that our approach can efficiently find quantitative association rules, and can find important association rules which may be missed by the previous algorithms.", "authors": ["Qiang Tong", "Baoping Yan", "Yuanchun Zhou"], "n_citation": 0, "title": "Mining quantitative association rules on overlapped intervals", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7bb6fc3c-320e-4bd3-8848-f415ad9ff9c6"}
{"abstract": "We study the problem of assigning applicants to jobs. Each applicant has a weight and provides a preference list, which may contain ties, ranking a subset of the jobs. An applicant x may prefer one matching over the other (or be indifferent between them, in case of a tie) based on the jobs x gets in the two matchings and x's personal preference. A matching M is popular if there is no other matching M' such that the weight of the applicants who prefer M' over M exceeds the weight of those who prefer M over M'. We present two algorithms to find a popular matching; or in case none exists, to establish so. For the case of strict preferences we develop an 0(n + m) time algorithm. When ties are allowed a more involved algorithm solves the problem in 0(mm(k\u221an. n)m) time, where k is the number of distinct weights the applicants are given.", "authors": ["Juli\u00e1n Mestre"], "n_citation": 50, "title": "Weighted Popular Matchings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7c7f65c2-12c3-4861-98b7-7e633631ab6c"}
{"abstract": "An open challenge is to integrate XML and conceptual modeling in order to satisfy large-scale enterprise needs. Because enterprises typically have many data sources using different assumptions, formats, and schemas, all expressed in - or soon to be expressed in - XML, it is easy to become lost in an avalanche of XML detail. This creates an opportunity for the conceptual modeling community to provide improved ions to help manage this detail. We present a vision for Conceptual XML (C-XML) that builds on the established work of the conceptual modeling community over the last several decades to bring improved modeling capabilities to XML-based development. Building on a framework such as C-XML will enable better management of enterprise-scale data and more rapid development of enterprise applications.", "authors": ["David W. Embley", "Stephen W. Liddle", "Reema Al-Kamha"], "n_citation": 0, "title": "Enterprise modeling with conceptual XML", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7d3c3e84-3467-49bb-815a-7b4f875e5e5c"}
{"abstract": "This paper addresses a new classification technique: partially supervised classification (PSC), which is used to identify a specific land-cover class of interest from a remotely sensed image by using unique training samples belong to a specifically selected class. This paper also presents and discusses a novel Support Vector Machine (SVM) algorithm for PSC. Its training set includes labeled samples belong to the class of interest and unlabeled samples of all classes randomly selected from a remotely sensed image. Moreover, all unlabeled samples are assumed to be training samples of other classes and each of them is assigned a weighting factor indicating the likelihood of this assumption; hence, the algorithm is so-called 'Weighted Unlabeled Sample SVM' (WUS-SVM). Experimental results with both simulated and real data sets indicate that the proposed PSC method is more robust than 1-SVM and has comparable accuracy to a standard SVM.", "authors": ["Zhigang Liu", "Wenzhong Shi", "Deren Li", "Qianqing Qin"], "n_citation": 50, "title": "Partially supervised classification : Based on weighted unlabeled samples support vector machine", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7edb2c61-58ba-4c5a-995e-db5a02412cfa"}
{"abstract": "We explore the possibility of using multiple processors to improve the encoding and decoding tasks of Lempel Ziv schemes. A new layout of the processors is suggested and it is shown how LZSS and LZW can be adapted to take advantage of such parallel architectures. Experimental results show an improvement in compression and time over standard methods.", "authors": ["Shmuel T. Klein", "Yair Wiseman"], "n_citation": 0, "title": "Parallel Lempel Ziv coding", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "7f887ff2-4426-4f9d-b6ce-1ebc7408b187"}
{"abstract": "Hotfile is a user level file management system. It wraps GridFTP and GASS or any other file transfer protocol compatible with hotfile structure into a unified vegafile protocol. Based on virtual grid file layer and a set of basic grid file operations, users can access grid file without knowing the physical transport protocol of the file. Test result shows the overhead of vegafile protocol is little in file transfer and operation. Further vegafile transfer experiment shows when file size is smaller than 1M byte, Vega file copy with GASS has higher bandwidth, where as file is larger than 1M byte, Vega file copy with GridFTP has higher bandwidth.", "authors": ["Liqiang Cao", "Jie Qiu", "Li Zha", "Haiyan Yu", "Wei Li", "Yuzhong Sun"], "n_citation": 0, "title": "Design and implementation of grid file management system hotfile", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "80800d3a-2472-437e-90e9-39f245a92166"}
{"abstract": "The equational prover of the THEOREMA system is described. It is implemented on Mathematica and is designed for unit equalities in the first order or in the applicative higher order form. A (restricted) usage of sequence variables and Mathematica built-in functions is allowed.", "authors": ["Temur Kutsia"], "n_citation": 50, "title": "Equational prover of THEOREMA", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "80a79b5f-c61d-43a9-89dc-cd429556d1ab"}
{"abstract": "Database fragmentation is a process for reducing irrelevant data accesses by grouping data frequently accessed together in dedicated segments. In this paper, we address multimedia database fragmentation by extending existing fragmentation algorithms to take into account key characteristics of multimedia objects. We particularly discuss multimedia primary horizontal fragmentation and provide a partitioning strategy based on low-level multimedia features. Our approach particularly emphasizes the importance of multimedia predicates implications in optimizing multimedia fragments. To validate our approach, we have implemented a prototype computing multimedia predicates implications. Experimental results are satisfactory.", "authors": ["Samir Saad", "Joe Tekli", "Richard Chbeir", "Kokou Yetongnon"], "n_citation": 0, "title": "Towards multimedia fragmentation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "80c7e6e0-6be9-4776-a656-f07a964e2bc3"}
{"authors": ["Alexandre Campo", "Marco Dorigo"], "n_citation": 0, "title": "Efficient multi-foraging in swarm robotics", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "81ac7474-103c-4d93-a13f-94e472ea96e3"}
{"abstract": "Patients suffering from Parkinson's disease display a number of symptoms such a resting tremor, bradykinesia, etc. Bradykinesia is the hallmark and most disabling symptom of Parkinson's disease (PD). Herein, a basal ganglia-cortico-spinal circuit for the control of voluntary arm movements in PD bradykinesia is extended by incorporating DAergic innervation of cells in the cortical and spinal components of the circuit. The resultant model simulates successfully several of the main reported effects of DA depletion on neuronal, electromyographic and movement parameters of PD bradykinesia.", "authors": ["Vassilis Cutsuridis"], "n_citation": 0, "title": "Neural Model of Dopaminergic Control of Arm Movements in Parkinson's Disease Bradykinesia", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8287342c-bca0-46b3-8aec-e6a48594401c"}
{"abstract": "The enumerate-and-expand paradigm for solving NP-hard problems has been introduced and applied to some VERTEX COVER variants in a recently published preliminary paper. In this paper we improve on the runtime for CONNECTED VERTEX COVER, obtaining a bound of O*(2.76116 k ), 1  and use the technique in order to gain the fastest known method for counting the number of vertex covers in a graph, which takes O*(1.3803 k ) time.", "authors": ["Daniel M\u00f6lle", "Stefan Richter", "Peter Rossmanith"], "n_citation": 50, "title": "Enumerate and Expand : New Runtime Bounds for Vertex Cover Variants", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "83631124-a49c-438b-ab72-1137f5f88464"}
{"abstract": "Well-structured transition systems (WSTS) are a broad and well-studied class of infinite-state systems, for which the problem of verifying the reachability of an upward-closed set of error states is decidable (subject to some technicalities). Recently, Bingham proposed a new algorithm for this problem, but applicable only to the special cases of broadcast protocols and petri nets. The algorithm exploits finite-state symbolic model checking and was shown to outperform the classical WSTS verification algorithm on a contrived example family of petri nets. In this work, we generalize the earlier results to handle a larger class of WSTS, which we dub nicely sliceable, that includes broadcast protocols, petri nets, context-free grammars, and lossy channel systems. We also add an optimization to the algorithm that accelerates convergence. In addition, we introduce a new reduction that soundly converts the verification of parameterized systems with unbounded conjunctive guards into a verification problem on nicely sliceable WSTS. The reduction is complete if a certain decidable side condition holds. This allows us to access industrially relevant challenge problems from parameterized memory system verification. Our empirical results show that, although our new method performs worse than the classical approach on small petri net examples, it performs substantially better on the larger examples based on real, parameterized protocols (e.g., German's cache coherence protocol, with data paths).", "authors": ["Jesse D. Bingham", "Alan J. Hu"], "n_citation": 50, "title": "Empirically efficient verification for a class of infinite-state systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "83b7b53f-47dc-4808-b335-17fcb40ba117"}
{"abstract": "This paper investigates GA approaches for solving the reliable communication network design problem. For solving this problem a network with minimum cost must be found that satisfies a given network reliability constraint. To consider the additional reliability constraint different approaches are possible. We show that existing approaches using penalty functions can result in invalid solutions and are therefore not appropriate for solving this problem. To overcome these problems we present a repair heuristic, which is based on the number of spanning trees in a network. This heuristic always generates a valid solution, which when compared to a greedy cheapest repair heuristic shows that the new approach finds better solutions with less computational effort.", "authors": ["Dirk Reichelt", "Franz Rothlauf", "Peter Gmilkowsky"], "n_citation": 50, "title": "Designing reliable communication networks with a genetic algorithm using a repair heuristic", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "852075b1-e25d-4e2d-97b8-082686e60734"}
{"abstract": "In this paper, we propose a reversible watermarking algorithm where an original image can be recovered from watermarked image data. Most watermarking algorithms cause degradation of image quality in original digital content in the process of embedding watermark. In the proposed algorithm, the original image can be obtained when the degradation is removed from the watermarked image after extracting watermark information. In the proposed algorithm, we utilize a peak point of image histogram and the location map and modify pixel values slightly to embed data. Because the peak point of image histogram and location map are employed in this algorithm, there is no need of extra information transmitted to receiving side. Also, because a slight modification on pixel values is conducted, highly imperceptibly images can be achieved. As locations of watermark embedding are identified using location map, amount of watermark data can dramatically increases through recursive embedding. Experimental results show that it can embed 5K to 130K bits of additional data.", "authors": ["Jin-Ha Hwang", "Jong-Weon Kim", "Jong-Uk Choi"], "n_citation": 0, "title": "A reversible watermarking based on histogram shifting", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "897a8bb7-dad1-45a5-88f2-4574212358f3"}
{"abstract": "One of the current trends in computer science leads to the design of computing organizations based on the activity of a multitude of tiny cheap decentralized computing entities. Whether these chips are integrated into paintings or disseminated in open environments like dust, the fundamental problem lies in their cooperative operation so that global functions are obtained collectively. In this paper, we address the issue of the creation of visual ambiences based on the coordinated activity of computing entities. These entities are distributed randomly on a 2D canvas and can only change their own color and perceive their immediate neighbors.", "authors": ["Guillaume Bour", "Guillaume Hutzler", "Bernard Gortais"], "n_citation": 50, "title": "Ambient cognitive environments and the distributed synthesis of visual ambiences", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "89f906ca-863e-43a9-8f24-b306096484ae"}
{"abstract": "Cohesion between components of collocations is already acknowledged measurable by means of the Web, and cohesion measurements are used for some applications and extraction of new collocations. Taking a specific cohesion criterion SCI, we performed massive evaluations of collocate cohesion in Oxford Collocations Dictionary. For three groups of modificative collocations (adjective-noun, adverb-adjective, and adverb verb) SCI distributions proved to be one-peaked and compact, with rather close mean values and standard deviations. Thus we suggest a reliable numeric criterion for extraction of collocations from the Web.", "authors": ["Igor A. Bolshakov", "Sof\u00eda N. Galicia-Haro"], "n_citation": 0, "title": "Web-based measurements of intra-collocational cohesion in oxford collocations dictionary", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8ae5e446-5f87-4cfe-94a0-cdec2e17b294"}
{"abstract": "Current techniques for processing XML queries can be divided into two groups. One is structural index techniques that reduce the search space by traversing a structural summary instead of a data graph. And the other is structural join techniques that can efficiently compute ancestor-descendant pairs from two lists of nodes without traversing the data graph. In this paper, we propose a structural index technique and a novel structural join technique and we prove that these two proposed techniques are integrated to accelerate processing of a path query in the experiments.", "authors": ["Jongik Kim", "Soo-Cheol Lee", "Oh-Cheon Kwon"], "n_citation": 0, "title": "Integration of a Structural Index with a Structural Join for Accelerating Path Queries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8b42c110-3f4a-40bb-99a8-bf287b60d81b"}
{"abstract": "Statistical summaries in relational databases mainly focus on the distribution of data values and have been found useful for various applications, such as query evaluation and data storage. As xml has been widely used, e.g. for online data exchange, the need for (corresponding) statistical summaries in xml has been evident. While relational techniques may be applicable to the data values in xml documents, novel techniques are requried for summarizing the structures of xml documents. In this paper, we propose metrics for major structural properties, in particular, nestings of entities and one-to-many relationships, of XML documents. Our technique is different from the existing ones in that we generate a quantitative summary of an xml structure. By using our approach, we illustrate that some popular real-world and synthetic xml benchmark datasets are indeed highly skewed and hardly hierarchical and contain few recursions. We wish this preliminary finding shreds insight on improving the design of xml benchmarking and experimentations.", "authors": ["Zi Lin", "Bingsheng He", "Byron Choi"], "n_citation": 0, "title": "A Quantitative Summary of XML Structures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8b7669d6-f482-4d2b-bfc0-d5d295fcf404"}
{"abstract": "We prove a version of the derandomized Direct Product Lemma for deterministic space-bounded algorithms. Suppose a Boolean function g: {0,1} n  \u2192 {0,1} cannot be computed on more than 1 - \u03b4 fraction of inputs by any deterministic time T and space S algorithm, where 6 \u2264 1/t for some t. Then, for t-step walks w = (v 1 ,..., v t ) in some explicit d-regular expander graph on 2 vertices, the function g'(w)  def  g(v 1 )... g(v t ) cannot be computed on more than 1 - \u03a9(t\u03b4) fraction of inputs by any deterministic time \u2243 T/d t  - poly(n) and space \u2243 S - O(t). As an application, by iterating this construction, we get a deterministic linear-space worst-case to constant averagecase hardness amplification reduction, as well as a family of logspace encodable/decodable error-correcting codes that can correct up to a constant fraction of errors. Logspace encodable/decodable codes (with linear-time encoding and decoding) were previously constructed by Spielman [14]. Our codes have weaker parameters (encoding length is polynomial, rather than linear), but have a conceptually simpler construction. The proof of our Direct Product Lemma is inspired by Dinur's remarkable recent proof of the PCP theorem by gap amplification using expanders [4].", "authors": ["Venkatesan Guruswami", "Valentine Kabanets"], "n_citation": 0, "title": "Hardness amplification via space-efficient direct products", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8b98c02a-6d54-44a3-bc01-c82f9cf67787"}
{"authors": ["Jiageng Chen", "Keita Emura", "Atsuko Miyaji"], "n_citation": 0, "title": "Non-interactive Opening for Ciphertexts Encrypted by Shared Keys", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "8b9beab6-de60-4a27-b685-7e2454158128"}
{"abstract": "It has been pointed out by McMillan that modern satisfiability (SAT) solvers have the ability to perform on-the-fly model abstraction when examining it for the existence of paths satisfying certain conditions. The issue has therefore been raised of whether explicit abstraction refinement schemes still have a role to play in SAT-based model checking. Recent work by Gupta and Strichman has addressed this issue for bounded model checking (BMC), while in this paper we consider unbounded model checking based on interpolation. We show that for passing properties abstraction refinement leads to proofs that often require examination of shorter paths. On the other hand, there is significant overhead involved in computing efficient abstractions. We describe the techniques we have developed to minimize such overhead to the point that even for failing properties the abstraction refinement scheme remains competitive.", "authors": ["Bing Li", "Fabio Somenzi"], "n_citation": 50, "title": "Efficient abstraction refinement in interpolation-based unbounded model checking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8be74da1-b6f8-4e7d-9e80-6eb7b9a401f9"}
{"abstract": "We present a new bit-parallel technique for approximate string matching. We build on two previous techniques. The first one [Myers, J. of the ACM, 1999], searches for a pattern of length m in a text of length n permitting k differences in O(mn/w) time, where w is the width of the computer word. The second one [Navarro and Raffinot, ACM JEA, 2000], extends a sublinear-time exact algorithm to approximate searching. The latter technique makes use of an O(kmn/w) time algorithm [Wu and Manber, Comm. ACM, 1992] for its internal workings. This algorithm is slow but flexible enough to support all the required operations. In this paper we show that the faster algorithm of Myers can be adapted to support all those operations. This involves extending it to compute edit distance, to search for any pattern suffix, and to detect in advance the impossibility of a later match. The result is an algorithm that performs better than the original version of Navarro and Raffinot and that is the fastest for several combinations of m, k and alphabet sizes that are useful, for example, in natural language searching and computational biology.", "authors": ["Heikki Hyyr\u00f6", "Gonzalo Navarro"], "n_citation": 0, "title": "Faster bit-parallel approximate string matching", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8c26d61f-66ad-4873-a504-636da66dfe00"}
{"abstract": "In this paper the sequential prediction problem with expert advice is considered when the loss is unbounded under partial monitoring scenarios. We deal with a wide class of the partial monitoring problems: the combination of the label efficient and multi-armed bandit problem, that is, where the algorithm is only informed about the performance of the chosen expert with probability e < 1. For bounded losses an algorithm is given whose expected regret scales with the square root of the loss of the best expert. For unbounded losses we prove that Hannan consistency can be achieved, depending on the growth rate of the average squared losses of the experts.", "authors": ["Chamy Allenberg", "Peter Auer", "L\u00e1szl\u00f3 Gy\u00f6rfi", "Gy\u00f6rgy Ottucs\u00e1k"], "n_citation": 50, "title": "Hannan consistency in on-line learning in case of unbounded losses under partial monitoring", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8e387f9c-4606-4811-be6f-9bd29785e09d"}
{"abstract": "The integration of wireless telephony and data services in 3G and 4G wireless systems that use packet-switched air interfaces poses new challenges in the management of network resources. Although highly compressed voice traffic is given priority over data traffic, scheduling algorithms which exploit multiuser diversity have been shown to significantly improve the data throughput. In this paper, we quantify the effect of prioritized voice traffic on the performance of data users in the system using a mix of analysis and simulation. We analytically characterize the scheduled rate, delay and packet service times for data in the presence of prioritized voice traffic by using a general scheduling metric that incorporates a measure of the user's channel quality in addition to a delay constraint. The results provide important tools for cellular network operators to evaluate system performance and provision resources for traffic with varying Quality of Service(QoS) requirements.", "authors": ["Roshni Srinivasan", "John S. Baras"], "n_citation": 0, "title": "Analyzing the performance of data users in packet switched wireless systems with prioritized voice traffic", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8e6528c9-5043-4611-a92a-87f27cd39e6c"}
{"abstract": "Several software systems have been developed recently for the automated generation of combustion reactions kinetic mechanisms using different representations of species and reactions and different generation algorithms. In parallel, several software systems based on rewriting have been developed for the easy modeling and prototyping of systems using rules controlled by strategies. This paper presents our current experience in using the rewrite system ELAN for the automated generation of the combustion reactions mechanisms previously implemented in the EXGAS kinetic mechanism generator system. We emphasize the benefits of using rewriting and rule-based programming controlled by strategies for the generation of kinetic mechanisms.", "authors": ["Olivier Bournez", "Guy-Marie C\u00f4me", "Val\u00e9rie Conraud", "H\u00e9l\u00e8ne Kirchner", "Liliana Ibanescu"], "n_citation": 0, "title": "A rule-based approach for automated generation of kinetic chemical mechanisms", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "8e98a350-1530-4f3a-a371-f401268f4cf3"}
{"abstract": "Masquerade attacks may be one of the most serious attacks in computer security context. To avoid being detected, masqueraders sometimes insert some common commands such as Is into their command sequences intentionally for concealing their actual purpose. This causes the masquerade attacks difficult to be detected. We refer to these command sequences mixed with confusable commands as gap-insensitive. To eliminate the effects on the insertion, we present a string kernel called gap-insensitive kernel without regard to the gaps in the command sequences, and use it to detect masquerade attacks. We test it and other kernels on the dataset from keyboard commands on a UNIX platform. We find that many users' attacks against other users can be easily detected by our gap-insensitive kernel, which means that the command sequences of these attackers are gap-insensitive. The results reveal that gap-insensitive kernel can determine gap-insensitivity in command sequences, and efface the gaps in the sequences.", "authors": ["Chuanhuan Yin", "Shengfeng Tian", "Shaomin Mu"], "n_citation": 0, "title": "Using gap-insensitive string kernel to detect masquerading", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8ee29d15-2834-4f51-88b5-3cc1f9a67d82"}
{"abstract": "Image registration has been a very active research area in the computer vision community. In the last few years, there is an increasing interest on the application of Evolutionary Computation in this field and several evolutionary approaches have been proposed obtaining promising results. In this contribution we introduce the use of an advanced evolutionary algorithm, Scatter Search, to solve the 3D image registration problem. The new proposal will be validated using two different shapes (both synthetic and MRI), considering three different transformations for each of them, and testing its performance with a Basic Memetic Algorithm and the classical, problem-specific ICP algorithm.", "authors": ["Oscar Cord\u00f3n", "Sergio Damas", "Jos\u00e9 Santamar\u00eda"], "n_citation": 0, "title": "A Scatter search algorithm for the 3D image registration problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8f56a1b5-8d7c-438e-b217-f29256e79cb0"}
{"abstract": "In Computer Vision applications, one usually has to work with uncertain data. It is therefore important to be able to deal with uncertain geometry and uncertain transformations in a uniform way. The Geometric Algebra of conformal space offers a unifying framework to treat not only geometric entities like points, lines, planes, circles and spheres, but also transformations like reflection, inversion, rotation and translation. In this text we show how the uncertainty of all elements of the Geometric Algebra of conformal space can be appropriately described by covariance matrices. In particular, it will be shown that it is advantageous to represent uncertain transformations in Geometric Algebra as compared to matrices. Other important results are a novel pose estimation approach, a uniform framework for geometric entity fitting and triangulation, the testing of uncertain tangentiality relations and the treatment of catadioptric cameras with parabolic mirrors within this framework. This extends previous work by Forstner and Heuel from points, lines and planes to non-linear geometric entities and transformations, while keeping the linearity of the estimation method. We give a theoretical description of our approach and show exemplary applications.", "authors": ["Christian Perwass", "Christian Gebken", "Gerald Sommer"], "n_citation": 50, "title": "Geometry and Kinematics with Uncertain Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "90d5c4f5-3a5b-4982-82de-6081d3eaa3bf"}
{"abstract": "Reducing power consumption and increasing battery life of nodes in an ad hoc network requires an integrated power control and routing strategy. To maximize the lifetime of mobile networks, the power consumption rate of each node must be evenly distributed. This objective alone cannot be satisfied by the use of routing algorithms proposed in previous work. In this paper a new route selection mechanism for MANET routing protocol, called as Self Organizing Routing (SOR). Self Organized Routing (SOR) algorithm is devised to enable high-energy nodes to participate in routing of data packets using a virtual backbone. Hence the lifetime and stability of the network is increased as nodes having high energy are involved in routing of packets. Based on the simulation results obtained using GloMoSim (simulator), it is observed that SOR algorithm increase the lifetime of mobile ad hoc networks and validate the environment suitable for the various techniques.", "authors": ["K. Murugan", "S. Shanmugavel"], "n_citation": 0, "title": "Performance study and implementation of self organized routing algorithm for mobile ad hoc network using GloMoSim", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "912928e8-c7bc-49cc-908c-31e3cb471ada"}
{"abstract": "In this paper, we investigate transition systems of a class of Petri nets suitable for the modelling and behavioural analysis of globally asynchronous locally synchronous systems. The considered model of Elementary Net Systems with Localities (ENL-systems) is basically that of Elementary Net Systems (EN-systerns) equipped with an explicit notion of locality. Each locality identifies a distinct set of events which may only be executed synchronously, i.e., in a maximally concurrent manner. For this reason, the overall behaviour of an ENL-systern cannot be represented by an interleaved transition system, with arcs being labelled by single events, but rather by a suitable notion of a step transition system, with arcs being labelled by sets of events executed concurrently. We completely characterise transition systems which can be generated by Elementary Net Systems with Localities under their intended concurrency semantics. In developing a suitable characterisation, we follow the standard approach in which key relationships between a Petri net and its transition system are established via the regions of the latter defined as specific sets of states of the transition system. We argue that this definition is insufficient for the class of transition systems of ENL-systems, and then augment the standard notion of a region with some additional information, leading to the notion of a region with explicit input and output events (or io-region). We define, and show consistency of, two behaviour preserving translations between ENL-systems and their transition systems. As a result, we provide a solution to the synthesis problem of Elementary Net Systems with Localities. which consists in constructing an ENL-system for a given transition system in such a way that the transition system of the former is isomorphic to the latter.", "authors": ["Maciej Koutny", "Marta Pietkiewicz-Koutny"], "n_citation": 50, "title": "Transition systems of elementary net systems with localities", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "962e158d-0d34-4e53-8749-bcd8af8cfe0b"}
{"abstract": "We present a type theory with some proof-irrelevance built into the conversion rule. We argue that this feature is particularly useful when type theory is used as the logical formalism underlying a theorem prover. We also show a close relation with the subset types of the theory of PVS. Finally we show that in these theories, because of the additional extentionality, the axiom of choice implies the decidability of equality, that is, almost classical logic.", "authors": ["Benjamin Werner"], "n_citation": 50, "title": "On the strength of proof-irrelevant type theories", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "96406430-a308-44da-ace7-f8e902560001"}
{"abstract": "This paper describes a method and a system ONTOQUERY for content-based querying of texts based on the availability of an ontology for the concepts in the text domain. A key principle in the system is the extraction of conceptual content of noun phrases into descriptors forming an integral part of the ontology. The retrieval of text passages rests on matching descriptors from the text against descriptors from the noun phrases in the query. The match need not be exact but is mediated by the ontology, invoking in particular taxonomic reasoning with sub- and super concepts. The paper also reports on a prototype implementation of the system.", "authors": ["Troels Andreasen", "Per Anker Jensen", "J\u00f8rgen Fischer Nilsson", "Patrizia Paggio", "Bolette Sandford Pedersen", "Hanne Erdman Thomsen"], "n_citation": 0, "title": "Ontological extraction of content for text querying", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "97c21db1-bf6d-4ab2-ae55-a82c4879f9e8"}
{"abstract": "We introduce the study of Kolmogorov complexity with error. For a metric d, we define C a (x) to be the length of a shortest program p which prints a string y such that d(x,y) < a. We also study a conditional version of this measure C a,b (x|y) where the task is, given a string y' such that d(y, y') < b, print a string x' such that d(x, x') < a. This definition admits both a uniform measure, where the same program should work given any y' such that d(y,y') < b, and a nonuniform measure, where we take the length of a program for the worst case y'. We study the relation of these measures in the case where d is Hamming distance, and show an example where the uniform measure is exponentially larger than the nonuniform one. We also show an example where symmetry of information does not hold for complexity with error under either notion of conditional complexity.", "authors": ["Lance Fortnow", "Troy Lee", "Nikolai K. Vereshchagin"], "n_citation": 0, "title": "Kolmogorov complexity with error", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "97e256aa-25fc-4fe9-930b-d47888546587"}
{"abstract": "Bottom-up approaches, which rely mainly on continuity principles, are often insufficient to form accurate segments in natural images. In order to improve performance, recent methods have begun to incorporate top-down cues, or object information, into segmentation. In this paper, we propose an approach to utilizing category-based information in segmentation, through a formulation as an image labelling problem. Our approach exploits bottom-up image cues to create an over-segmented representation of an image. The segments are then merged by assigning labels that correspond to the object category. The model is trained on a database of images, and is designed to be modular: it learns a number of image contexts, which simplify training and extend the range of object classes and image database size that the system can handle. The learning method estimates model parameters by maximizing a lower bound of the data likelihood. We examine performance on three real-world image databases, and compare our system to a standard classifier and other conditional random field approaches, as well as a bottom-up segmentation method.", "authors": ["Xuming He", "Richard S. Zemel", "Debajyoti Ray"], "n_citation": 0, "title": "Learning and Incorporating Top-Down Cues in Image Segmentation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9a1e0a62-212a-4d31-a0ce-d985e73ff10f"}
{"abstract": "In spite of significant advances in image segmentation techniques, evaluation of these methods thus far has been largely subjective. Typically, the effectiveness of a new algorithm is demonstrated only by the presentation of a few segmented images that are evaluated by some method, or it is otherwise left to subjective evaluation by the reader. We propose a new approach for evaluation of segmentation that takes into account not only the accuracy of the boundary localization of the created segments but also the under-segmentation and over-segmentation effects, regardless to the number of regions in each partition. In addition, it takes into account the way humans perceive visual information. This new metric can be applied both to automatically provide a ranking among different segmentation algorithms and to find an optimal set of input parameters of a given algorithm.", "authors": ["Fernando C. Monteiro", "Aur\u00e9lio C. Campilho"], "n_citation": 0, "title": "Performance Evaluation of Image Segmentation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9b3a4220-bc7c-48a2-887a-9ef88d4600af"}
{"abstract": "Resilient Packet Ring is a novel metro access standard. Here, we analyze it from the reliability viewpoint. The reliability function, availability as well as Mean Time to Failure metrics are analyzed. Simulation experiments are performed to confirm that the formulas obtained on the theoretical basis are correct.", "authors": ["Piotr Cholda", "Jerzy Domzal", "Andrzej Jajszczyk", "Krzysztof Wajda"], "n_citation": 50, "title": "Reliability Analysis of Resilient Packet Rings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9d105b9e-f333-40c5-a05e-4182c70ab8bc"}
{"abstract": "Our goal is to automatically segment and recognize basic human actions, such as stand, walk and wave hands, from a sequence of joint positions or pose angles. Such recognition is difficult due to high dimensionality of the data and large spatial and temporal variations in the same action. We decompose the high dimensional 3-D joint space into a set of feature spaces where each feature corresponds to the motion of a single joint or combination of related multiple joints. For each feature, the dynamics of each action class is learned with one HMM. Given a sequence, the observation probability is computed in each HMM and a weak classifier for that feature is formed based on those probabilities. The weak classifiers with strong discriminative power are then combined by the Multi-Class AdaBoost (AdaBoost.M2) algorithm. A dynamic programming algorithm is applied to segment and recognize actions simultaneously. Results of recognizing 22 actions on a large number of motion capture sequences as well as several annotated and automatically tracked sequences show the effectiveness of the proposed algorithms.", "authors": ["Fengjun Lv", "Ramakant Nevatia"], "n_citation": 0, "title": "Recognition and Segmentation of 3-D Human Action Using HMM and Multi-class AdaBoost", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9d50085d-0456-46e9-97e9-dc6018a48710"}
{"abstract": "In this paper we investigate the problem of maintaining connectivity under jamming in multihop ad hoc wireless networks. Connectivity is measured using a connectivity index, which indicates the probability that there exists a path between two nodes. We first show that connectivity can be drastically reduced with a relatively small number of jammers. We show that using sectored antennas can maintain connectivity in the presence of a significantly higher number of jammers at the expense of higher average number of hops. Finally, we show that mobility allows further resiliency to jamming.", "authors": ["Guevara Noubir"], "n_citation": 0, "title": "On connectivity in ad hoc networks under jamming using directional antennas and mobility", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a007549f-5fee-48b0-a686-bda0d03aaa82"}
{"abstract": "This article describes the rationale behind the PHASAR system (Phrase-based Accurate Search And Retrieval), a professional Information Retrieval and Text Mining system under development for the collection of information about metabolites from the biological literature. The system is generic in nature and applicable (given suitable linguistic resources and thesauri) to many other forms of professional search. Instead of keywords, the PHASAR search engine uses Dependency Triples as terms. Both the documents and the queries are parsed, transduced to Dependency Triples and lemmatized. Queries consist of a set of Dependency Triples, whose elements may be generalized or specialized in order to achieve the desired precision and recall. In order to help in interactive exploration, the search process is supported by document frequency information from the index, both for terms from the query and for terms from the thesaurus.", "authors": ["Cornelis H. A. Koster", "Olaf Seibert", "Marc Seutter"], "n_citation": 0, "title": "The PHASAR search engine", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a25c0ec9-d4a3-483c-bd9f-dc32fcf04467"}
{"authors": ["Min Zhang", "Kazuhiro Ogata", "Masaki Nakamura"], "n_citation": 50, "title": "Specification Translation of State Machines from Equational Theories into Rewrite Theories", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "a2e9530f-2689-4f88-93bb-2411834d3553"}
{"abstract": "The Web has become the world's largest information source. Unfortunately, the main success factor of the Web, the inherent principle of distribution and autonomy of the participants, is also its main problem. When trying to make this information machine processable, common structures and semantics have to be identified. The goal of information extraction (IE) is exactly this, to transform text into a structural format. In this paper, we present a novel approach for information extraction developed as part of the XI 3  project. Central to our approach is the assumption that we can obtain a better understanding of a text fragment if we consider its integration into higher-level concepts by exploiting text fragments from different parts of a source. In addition to previous approaches, we offer higher expressiveness of the extraction schema and an advanced method to deal with ambiguous texts. Our approach provides a way to use one extraction schema for multiple sources.", "authors": ["Richard Vlach", "Wassili Kazakos"], "n_citation": 0, "title": "Using common schemas for information extraction from heterogeneous Web catalogs", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "a2eeac61-6f50-4d6b-b9fb-b01b3b36294f"}
{"abstract": "Bucket elimination (BE) is an exact technique based on variable elimination, commonly used for solving constraint satisfaction problems. We consider the hybridization of BE with evolutionary algorithms endowed with tabu search. The resulting memetic algorithm (MA) uses BE as a mechanism for recombining solutions, providing the best possible child from the parental set. This MA is applied to the maximum density still life problem. Experimental tests indicate that the MA provides optimal or near-optimal results at an acceptable computational cost.", "authors": ["Jos\u00e9 E. Gallardo", "Carlos Cotta", "Antonio J. Fern\u00e1ndez"], "n_citation": 50, "title": "A memetic algorithm with bucket elimination for the still life problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a4c52bbf-d4f5-4b25-98bc-e99ecc0a4484"}
{"abstract": "In CDMA-based packet data systems such as HDR and HS-DPA which are designed to support high rate services, BSs transmit data packets with a maximum power based on time multiplexing mode such that only one user can be serviced at a time. In this paper, we propose a power-allocation-combined scheduling algorithm for HDR-like systems in which we adopt a code division multiplexing (CDM) transmission method in the downlink common channel in order to utilize channel orthogonality such that we can serve more than one user at a time slot specially when there exist remaining resources after serving the firstly selected user by the scheduler. Simulation results show that the proposed scheme outperforms the conventional scheme as the traffic load increases.", "authors": ["Insoo Koo", "Jens Zander", "Kiseon Kim"], "n_citation": 0, "title": "A power-allocation-combined scheduling algorithm for CDMA-based high-rate packet data systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a54b8247-ccff-416a-bc1e-259777c041e3"}
{"abstract": "The paper introduces the notion of off-line justification for Answer Set Programming (ASP). Justifications provide a graph-based explanation of the truth value of an atom w.r.t. a given answer set. The notion of justification accounts for the specifics of answer set semantics. The paper extends also this notion to provide justification of atoms during the computation of an answer set (on-line justification), and presents an integration of on-line justifications within the computation model of SMODELS. Justifications offer a basic data structure to support methodologies and tools for debugging answer set programs. A preliminary implementation has been developed in ASP - PROLOG.", "authors": ["Enrico Pontelli", "Tran Cao Son"], "n_citation": 0, "title": "Justifications for logic programs under answer set semantics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a5537b2d-c528-4994-bbf2-62a11cd1d41c"}
{"abstract": "We introduce take-it-or-leave-it auctions (TLAs) as an allocation mechanism that allows buyers to retain much of their private valuation information as do the most common auction mechanisms (English and Dutch auctions), yet, unlike them, generates close-to-optimal expected utility for the seller. We show that if each buyer receives at most one offer, each buyer's dominant strategy is to act truthfully. In more general TLAs, the buyers' optimal strategies are more intricate, and we derive the perfect Bayesian equilibrium for the game. We develop algorithms for finding the equilibrium and also for optimizing the offers in both types of TLAs so as to maximize the seller's expected utility. We prove that optimal TLAs have many desirable features. In several example settings we show that the seller's expected utility already is close to optimal for a small number of offers. As the number of buyers increases, the seller's expected utility increases, and becomes increasingly (but not monotonically) more competitive with Myerson's expected utility maximizing auction. Myerson's uses full valuation revelation and is arguably impractical because its rules are unintuitive, unlike ours.", "authors": ["Thomas Sandholm", "Andrew Gilpin"], "n_citation": 0, "title": "Sequences of take-it-or-leave-it offers: Near-optimal auctions without full valuation revelation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a5aa2f08-baf1-4978-aaca-074155f826ec"}
{"abstract": "In this paper we present a novel multistage classification strategy for handwriting Chinese character recognition. In training phase, we search for the most representative prototypes and divide the whole class set into several groups using prototype-based clustering. These groups are extended by nearest-neighbor rule and their centroids are used for coarse classification. In each group, we extract the most discriminative feature by local linear discriminant analysis and design the local classifier. The above-mentioned prototypes and centroids are optimized by a hierarchical learning vector quantization. In recognition phase, we first find the nearest group of the unknown sample, and then get the desired class label through the local classifier. Experiments have been implemented on CA-SIA database and the results show that the proposed method reaches a reasonable tradeoff between efficiency and accuracy.", "authors": ["Lei Xu", "Baihua Xiao", "Chunheng Wang", "Ruwei Dai"], "n_citation": 0, "title": "A Novel Multistage Classification Strategy for Handwriting Chinese Character Recognition Using Local Linear Discriminant Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a5f661dc-bbb9-422d-b6eb-9ab1afb0608d"}
{"abstract": "Mobile nodes communicate with each others having the limited energy in mobile ad hoc networks. The large amount of control packets and the frequent flooding of control packets cause energy exhaustion of nodes. The main goal of routing protocols in mobile ad hoc networks is throughput enhancement. Hence, this paper proposes a new routing protocol for mobile ad hoc networks called the Routing Protocol for Throughput Enhancement and Energy Saving (TEES) in mobile ad hoc networks. The simulation results show that TEES increases the amount of packet delivery and reduces the control packet overhead.", "authors": ["Hyo-Jin Kim", "Seung-Jae Han", "JooSeok Song"], "n_citation": 50, "title": "A Routing Protocol for Throughput Enhancement and Energy Saving in Mobile Ad Hoc Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a73dbe3c-6243-49d8-a207-5c585d6488b1"}
{"abstract": "In content-based music retrieval systems, since both the correctness and the performance of retrievals are important, a few content-based music retrieval systems have the melody index which contains the representative melodies of music to be likely used as users' queries. In this paper, we describe the development of a content-based music retrieval system in which the multidimensional index of time-sequenced representative melodies extracted appropriately based on musical composition forms is used to support quick and appropriate retrievals to users' melody queries. From the experimental results, we can see that the developed system can retrieve more relevant results than previous systems with smaller storage overhead than whole melody index.", "authors": ["Kyong-I Ku", "Jae-Yong Won", "Jae-Hyun Park", "Yoo-Sung Kim"], "n_citation": 1, "title": "A content-based music retrieval system using multidimensional index of time-sequenced representative melodies from music database", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a7f6f62a-36e2-4c72-8caa-8a01decdcbca"}
{"abstract": "RT-LOTOS is a timed process algebra which enables compact and abstract specification of real-time systems. This paper proposes and illustrates a structural translation of RT-LOTOS terms into behaviorally equivalent (timed bisimilar) finite Time Petri nets. It is therefore possible to apply Time Petri nets verification techniques to the profit of RT-LOTOS. Our approach has been implemented in RTL2TPN, a prototype tool which takes as input an RT-LOTOS specification and outputs a TPN. The latter is verified using TINA, a TPN analyzer developed by LAAS-CNRS. The toolkit made of RTL2TPN and TINA has been positively benchmarked against previously developed RT-LOTOS verification tool.", "authors": ["Tarek Sadani", "Marc Boyer", "Pierre de Saqui-Sannes", "Jean-Pierre Courtiat"], "n_citation": 0, "title": "Mapping RT-LOTOS Specifications into Time Petri Nets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a84ce6af-41e9-4c72-bead-821cfb54481e"}
{"authors": ["P. M. Meijer", "Stefaan Poedts", "J. P. Goedbloed", "Andreas Jakoby"], "n_citation": 0, "title": "A parallel semi-implicit method for 3D nonlinear magnetohydrodynamics", "venue": "Lecture Notes in Computer Science", "year": 1995, "id": "a8d62bc4-46f0-4953-9565-e77f50d91fd2"}
{"abstract": "Processing continuous queries over unbounded streams require unbounded memory. A common solution to this issue is to restrict the range of continuous queries into a sliding window that contains the most recent data of data streams. Sliding window join aggregates are often-used queries in data stream applications. The processing method to date is to construct steaming binary operator tree and pipeline execute. This method consumes a great deal of memory in storing the sliding window join results, therefore it isn't suitable for stream query processing. To handle this issue, we present a set of novel sliding window join aggregate operators and corresponding realized algorithms, which achieve memory-saving and efficient performance. Because the performances of proposed algorithms vary with the states of data streams, a scheduling strategy is also investigated to maximize the processing efficiency. The algorithms in this paper not only can process the complex sliding window join aggregate, but also can process the multi-way sliding window join aggregate.", "authors": ["Weiping Wang", "Jianzhong Li", "Dongdong Zhang", "Longjiang Guo"], "n_citation": 0, "title": "Processing sliding window join aggregate in continuous queries over data streams", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a90fac71-74c8-4edb-9e28-d681d973586b"}
{"abstract": "This paper presents a new multibit watermarking method. The method uses multiple orthonormalized watermark patterns of the same size as the host signal. In particular, the elements of each watermark pattern follow independent normal distribution. Each bit of the transmitted message is hidden using the dither quantizers to modify the projection of the host signal onto its corresponding watermark pattern. As a result, every hidden bit is spread over all elements of the host data and the extracting procedure is blind. Meanwhile, we consider how to choose a suitable quantization step size under the given distortion constraint. It is also proved mathematically that the upper bound of the bit error probability of our method is equal to one of the spread-transform dithered modulation (STDM) under the same situations. Experimental results show our scheme performs better than STDM in terms of bit error probability.", "authors": ["Xinshan Zhu", "Zhi Tang", "Liesen Yang"], "n_citation": 0, "title": "A novel multibit watermarking scheme combining spread spectrum and quantization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa9906d7-2e9a-4ba3-a8de-17869c21e65c"}
{"abstract": "Given a graph G(V, E), the identifying codes problem is to find the smallest set of vertices D C V such that no two vertices in V are adjacent to the same set of vertices in D. The identifying codes problem has been applied to fault diagnosis and sensor based location detection in harsh environments. In this paper, we introduce and study a generalization of this problem, namely, the d-identifying codes problem. We propose a polynomial time approximation algorithm based on ideas from information theory and establish its approximation ratio that is very close to the best possible. Using analysis on random graphs, several fundamental properties of the optimal solution to this problem are also derived.", "authors": ["Ying Xiao", "Christoforos N. Hadjicostis", "Krishnaiyan Thulasiraman"], "n_citation": 0, "title": "The d-Identifying Codes Problem for Vertex Identification in Graphs : Probabilistic Analysis and an Approximation Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ab49f606-6d42-4fd2-ad51-4b7b29f72529"}
{"abstract": "This paper explores various factors involved in the resolution of zero anaphora in Chinese discourse. Our study differs from previous ones in distinguishing three types of utterances and using clauses as the unit of resolution. The hierarchical structures of utterances enable us to process inter- and intra-utterance anaphora uniformly. Experimental results show that (1) clauses function significantly better than sentences as the unit of resolution, providing an improvement of precision from 36.0% to 63.4%; (2) the inclusion of cataphors and the use of NP forms as a criterion in Cf ranking do not lead to significant improvement of precision; and (3) when assigning antecedents to more than one zero pronoun in the same utterance, the criterion based on grammatical functions gives rise to better performance than that with linear orders.", "authors": ["Yuzhen Cui", "Qinan Hu", "Haihua Pan", "Jianhua Hu"], "n_citation": 50, "title": "Zero anaphora resolution in Chinese discourse", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "abe3c020-d870-4134-a83d-67d2c824c33c"}
{"abstract": "The development of automatic techniques for oil slick identification on the sea surface, captured through remote sensing images, cause a positive impact to a complete monitoring of the oceans and seas. C-band SAR (ERS-1, ERS-2, Radarsat and Envisat projects) is well adapted to detect ocean pollution because the backscatter is reduced by oil slick. This work propose a system for segmentation and feature extraction of oil slicks candidates based on techniques of digital image processing (filters, gradients, mathematical morphology) and artificial neural network (ANN). Different algorithms of speckle filtering are tested and a comparison for the considered system is presented. The process is thought to possess a level of automatization that minimizes the intervention of a human operator, being possible the processing of larger amount data. The focus of the work is to present a study detailed for feature extraction block proposed (architecture used and computational tools).", "authors": ["Danilo Lima de Souza", "Adri\u00e3o Duarte D\u00f3ria Neto", "Wilson da Mata"], "n_citation": 0, "title": "Intelligent System for Feature Extraction of Oil Slick in SAR Images : Speckle Filter Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ac14ef58-ffc4-487a-9a0e-86c6bd5947e7"}
{"abstract": "We prove that a quantum circuit together with measurement apparatuses and EPR sources can be self-tested, i.e. fully verified without any reference to some trusted set of quantum devices. To achieve our goal we define the notions of simulation and equivalence. Using these two concepts, we construct sets of simulation conditions which imply that the physical device of interest is equivalent to the one it is supposed to implement. Another benefit of our formalism is that our statements can be proved to be robust. Finally, we design a test for quantum circuits whose complexity is polynomial in the number of gates and qubits, and the required precision.", "authors": ["Fr\u00e9d\u00e9ric Magniez", "Dominic Mayers", "Michele Mosca", "Harold Ollivier"], "n_citation": 50, "title": "Self-testing of Quantum Circuits", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ac6ea387-d054-46b2-9ee5-b6c73062d599"}
{"abstract": "In the field of Intelligent Transport Systems, there have been many attempts to predict the speed of the vehicle by adopting the pattern of the speed using artificial intelligence and statistical methods. Traffic information has been collected mainly by fixed devices which are costly and hard to maintain. Recently, traffic data is progressively being collected by the probe cars equipped with GPS receivers. Most of probe cars are comprised of public and commercial transportation such as taxis and buses since the private drivers are reluctant to give their location information due to the privacy issues. This creates problem of insufficient number of cars available and the traditional analysis methods used for analyzing the data collected by the fixed devices are not applicable. The aim of this research is to propose and test a new method of calculating the optimal link speed for the collected information from probe cars. We propose the adoption of a fuzzy c-mean method for this purpose. In this paper the GPS speed data are automatically classified into three groups of speed patterns such as low, middle, and high speed and the link speed is predicted from the pattern clusters. In performance tests, the proposed method provides significantly better results than normal average speed data.", "authors": ["Seung-Heon Lee", "Byung-Wook Lee", "Young-Kyu Yang"], "n_citation": 50, "title": "Estimation of Link Speed Using Pattern Classification of GPS Probe Car Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ade5c977-181c-4190-af7a-9fd45aff8e48"}
{"abstract": "In this paper, multi-hop wireless infrastructures are identified as a way to increase user data rates and/or capacity of wireless systems by means of a high base station density without high base station interconnection costs. For such a system, a new routing algorithm, named Balanced Interference Routing Algorithm (BIRA), is proposed. One of the main features of this new routing algorithm is to take the interference between wirelessly transmitting nodes into account. In BIRA a link cost is calculated considering the interference level of a node and a fixed cost for each link. Based on this link cost, the Dijkstra algorithm is used to compute routes. From the performance analysis, we see that BIRA outperforms other algorithms in terms of obtained data rates for a given available spectrum. BIRA helps to reduce the interference in the network and to achieve higher throughput.", "authors": ["Geert J. Heijenk", "Fei Liu"], "n_citation": 0, "title": "Interference-based routing in multi-hop wireless infrastructures", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ae1b0d79-0d36-4fe4-ac75-38806663828f"}
{"abstract": "We present a natural language question/answering system to interface the University of Evora databases that uses clarification dialogs in order to clarify user questions. It was developed in an integrated logic programming framework, based on constraint logic programming using the GnuProlog(-cx) language [2,11] and the ISCO framework [1]. The use of this LP framework allows the integration of Prolog-like inference mechanisms with classes and inheritance, constraint solving algorithms and provides the connection with relational databases, such as PostgreSQL. This system focus on the questions' pragmatic analysis, to handle ambiguity, and on an efficient dialogue mechanism, which is able to place relevant questions to clarify the user intentions in a straightforward manner. Proper Nouns resolution and the pp-attachment problem are also handled. This paper briefly presents this innovative system focusing on its ability to correctly determine the user intention through its dialogue capability.", "authors": ["Luis Quintano", "Irene Pimenta Rodrigues"], "n_citation": 50, "title": "Using a logic programming framework to control database query dialogues in natural language", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b1502612-f1e5-433d-be62-1c362380ad3f"}
{"abstract": "Business modelling research is increasingly interested in exploring how domain ontologies can be used as reference models for business models. The Resource Event Agent (REA) ontology is a primary candidate for ontology-driven modelling of business processes because the REA point of view on business reality is close to the conceptual modelling perspective on business models. In this paper Ontology Engineering principles are employed to reengineer REA in order to make it more suitable for ontology-driven business modelling. The new conceptual representation of REA that we propose uses a single representation formalism, includes a more complete domain axiomatization (containing definitions of concepts, concept relations and ontological axioms), and is proposed as a generic model that can be instantiated to create valid business models. The effects of these proposed improvements on REA-driven business modelling are demonstrated using a business modelling example.", "authors": ["Frederik Gailly", "Geert Poels"], "n_citation": 50, "title": "Ontology-driven business modelling: improving the conceptual representation of the REA ontology", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "b368b0d6-8d3e-4edc-9177-c6ed04f527a9"}
{"abstract": "Recent developments in reification of ER schemata include automatic generation of web-based database administration systems [1, 2]. These systems enforce the schema cardinality constraints, but, beyond unsatisfiable schemata, this feature may create unreachable instances. We prove sound and complete characterisations of schemata whose instances satisfy suitable reachability properties; these theorems translate into linear algorithms that can be used to prevent the administrator from reifying schemata with unreachable instances.", "authors": ["Sebastiano Vigna"], "n_citation": 0, "title": "Reachability problems in entity-relationship schema instances", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b4bea4bc-6034-4a2a-8563-c4b749f1f9b0"}
{"abstract": "In this paper, we proposed a system that extracts keywords using thesaurus which contains data saved by category. The system enhances precision of extracted keywords based on considering correlation of the category. 30 Living Modified Organisms related experimental documents were used in order for performance measurement of the system. The proposed system showed better precision than frequency-based system by 47 % and thesaurus-based system by 18%.", "authors": ["Young-Ho Woo", "Do-Hyun Nam", "Tai-Sung Hur", "Young-Bae Park", "Woong Huh", "Yoseop Woo", "Hongki Min"], "n_citation": 0, "title": "Automated Keyword Extraction Using Category Correlation of Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bac8b089-613e-4117-8d8b-c006655bfb0b"}
{"abstract": "We show that a set is m-autoreducible if and only if it is m-mitotic. This solves a long standing open question in a surprising way. As a consequence of this unconditional result and recent work by Glasser et al. [12], complete sets for all of the following complexity classes are m-mitotic: NP, coNP, \u25cb+P, PSPACE, and NEXP, as well as all levels of PH, MODPH, and the Boolean hierarchy over NP. In the cases of NP, PSPACE, NEXP, and PH, this at once answers several well-studied open questions. These results tell us that complete sets share a redundancy that was not known before. In particular, every NP-complete set A splits into two NP-complete sets A 1  and A 2 . We disprove the equivalence between autoreducibility and mitoticity for all polynomial-time-bounded reducibilities between 3-tt-reducibility and Turing-reducibility: There exists a sparse set in EXP that is polynomi al-time 3-tt-autoreducible, but not weakly polynomial-time T-mitot ic. In particular, polynomial-time T-autoreducibility does not imply polynomia 1-time weak T-mitoticity, which solves an open question by Buhrman and Torenvliet. We generalize autoreducibility to define poly-autoreducibility and give evidence that NP-complete sets are poly-autoreducible.", "authors": ["Christian Glasser", "Aduri Pavan", "Alan L. Selman", "Liyu Zhang"], "n_citation": 0, "title": "Redundancy in complete sets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bb2fbf99-7353-4663-a511-cc79ffcb2ee9"}
{"authors": ["Bart Preneel", "Pc vanOorschot"], "n_citation": 0, "title": "On the Security of Two MAC Algorithms", "venue": "Lecture Notes in Computer Science", "year": 1996, "id": "c3a0dd34-1a37-4281-a313-8d3b8926e8b0"}
{"abstract": "Most of the work in Machine Learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generates the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to monitor the online error-rate of a learning algorithm looking for significant deviations. The method can be used as a wrapper over any learning algorithm. In most problems, a change affects only some regions of the instance space, not the instance space as a whole. In decision models that fit different functions to regions of the instance space, like Decision Trees and Rule Learners, the method can be used to monitor the error in regions of the instance space, with advantages of fast model adaptation. In this work we present experiments using the method as a wrapper over a decision tree and a linear model, and in each internal-node of a decision tree. The experimental results obtained in controlled experiments using artificial data and a real-world problem show a good performance detecting drift and in adapting the decision model to the new concept.", "authors": ["Jo\u00e3o Gama", "Gladys Castillo"], "n_citation": 0, "title": "Learning with Local Drift Detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c3b00e3f-cc66-4870-b513-1e7c35e0cbd0"}
{"abstract": "Given a set of leaf-labeled trees with identical leaf sets, the well-known MAXIMUM AGREEMENT SUBTREE problem (MAST) consists of finding a subtree homeomorphically included in all input trees and with the largest number of leaves. Its variant called MAXIMUM COMPATIBLE TREE (MCT) is less stringent, as it allows the input trees to be refined. Both problems are of particular interest in computational biology, where trees encountered have often small degrees. In this paper, we study the parameterized complexity of MAST and MCT with respect to the maximum degree, denoted D, of the input trees. While MAST is polynomial for bounded D [1,6,3], we show that MAST is W[1]-hard with respect to parameter D. Moreover, relying on recent advances in parameterized complexity we obtain a tight lower bound: while MAST can be solved in O(N O(D) ) time where N denotes the input length, we show that an O(N\u00b0 (D) ) bound is not achievable, unless SNP C SE. We also show that MCT is W[1]-hard with respect to D, and that MCT cannot be solved in O(N\u00b0 (2D/2) ) time, unless SNP C SE.", "authors": ["Sylvain Guillemot", "Fran\u00e7ois Nicolas"], "n_citation": 0, "title": "Solving the maximum agreement subtree and the maximum compatible tree problems on many bounded degree trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c84e5fb5-1a32-4a85-ab6a-aa53669bbb70"}
{"abstract": "Despite its benefits and wide-spread acceptance, SQL [5] is not a perfect query language. Although graphical tools for query construction mask some of the complexity, complex database schema challenge even experienced database users during query formulation because a user is responsible for mapping the semantics of their query to the structure of the database. In this work, we propose a semantic query language for graphically querying relational database systems that allows a user to query the database by semantics instead of structure. Database semantics are described using a global dictionary and semantic specifications that are combined to form an integrated, context view. Users query the semantic view by concept name, and the query processor translates semantic queries to SQL. This translation involves automatically determining attribute and relation mappings and join conditions.", "authors": ["Ramon Lawrence", "Ken Barker"], "n_citation": 0, "title": "Querying relational databases without explicit joins", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c88838ea-6d1d-404c-ae21-8e7454dd1aa4"}
{"abstract": "We propose an agent-oriented methodology for modelling business processes between enterprises that consists of the steps of analysis and design. The analysis starts by modelling intentional dependencies between the actors of the problem domain at hand, and continues by modelling intentional relationships that are internal to the actors, such as task decomposition links. In the design, intentional dependencies between the actors are transformed to commitment-based models of interactions between the agents. Models of tasks performed by the actors, obtained as a result of means-ends analysis, are transformed to activities and reaction rules, defining the behaviours of agents. The methodology is evaluated by using the case study of an electronic advertising process in newspapers.", "authors": ["Kuldar Taveter", "Gerd Wagner"], "n_citation": 50, "title": "A multi-perspective methodology for modelling inter-enterprise business processes", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c9142628-b520-4195-9054-abc7eb4a16b3"}
{"abstract": "In this paper, we propose a new similarity measure between vague sets and apply vague logic in a relational database environment with the objective of capturing the vagueness of the data. By introducing a new vague Similar Equality (S EQ ) for comparing data values, we first generalize the classical Functional Dependencies (FDs) into Vague Functional Dependencies (VFDs). We then present a set of sound and complete inference rules. Finally, we study the validation process of VFDs by examining the satisfaction degree of VFDs, and the merge-union and merge-intersection on vague relations.", "authors": ["An Lu", "Wilfred Ng"], "n_citation": 0, "title": "Managing merged data by vague functional dependencies", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ca8414cc-2593-44ba-86d0-b7fdee163978"}
{"abstract": "A novel ID method based on Support Vector Machine (SVM) is proposed to solve the classification problem for the large amount of raw intrusion event dataset of the grid computing environment. A new radial basic function (RBF), based on heterogeneous value difference metric (HVDM) of heterogeneous datasets, is developed. Two different types of SVM, Supervised C\u2015SVM and unsupervised One\u2015Class SVM algorithms with kernel function, are applied to detect the anomaly network connection records. The experimental results of our method on the corpus of data collected by Lincoln Labs at MIT for an intrusion detection system evaluation sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) shows that the proposed method is feasible and effective.", "authors": ["Qinghua Zheng", "Hui Li", "Yun Xiao"], "n_citation": 0, "title": "A classified method based on Support Vector Machine for grid computing intrusion detection", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cd99db21-e3b1-48f9-bbe3-bfc8448b4765"}
{"abstract": "For a constructor-based rewrite system R, a regular set of ground terms E, and assuming some additional restrictions, we build a finite tree automaton that recognizes the descendants of E, i.e. the terms issued from E by rewriting, according to innermost, innermost-leftmost, and outermost strategies.", "authors": ["Pierre R\u00e9ty", "Julie Vuotto"], "n_citation": 0, "title": "Regular sets of descendants by some rewrite strategies", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "cf410551-587c-4c77-a1de-ce881032a02e"}
{"abstract": "Partially blind signature was first introduced by Abe and Fujisaki. Subsequently, Abe and Okamoto proposed a provably secure construction for partially blind signature schemes with a formalized definition in their work. In this paper, based on discrete logarithm problem and the Schnorr's blind signature scheme, we propose a new efficient partially blind signature scheme. Follow the construction proposed by Abe and Okamoto, we prove its security in random oracle model. The computation and communication costs are both reduced in our scheme. It will make privacy-oriented applications which based on partially blind signatures more efficient and suitable for hardware-limited environment, such as smart phones and PDAs.", "authors": ["Zheng Gong", "Xiangxue Li", "Kefei Chen"], "n_citation": 1, "title": "Efficient Partially Blind Signature Scheme with Provable Security", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d1654606-bebf-4729-8e79-26b0c1038879"}
{"abstract": "We introduce a formalism to reason about program properties at an infinite number of runtime control points, called instances. Infinite sets of instances are represented by rational languages. This framework gives a formal foundation to the well known concept of iteration vectors, extending it to recursive programs with any structured control flow (nested loops and recursive calls). We also extend the concept of induction variables to recursive programs. For a class of monoid-based data structures, including arrays and trees, induction variables capture the exact memory location accessed at every step of the execution. This compile-time characterization is computed in polynomial time as a rational function. Applications include dependence and region analysis for array and tree algorithms, array expansion, and automatic parallelization of recursive programs.", "authors": ["Pierre Amiranoff", "Albert Cohen", "Paul Feautrier"], "n_citation": 50, "title": "Beyond iteration vectors : Instancewise relational abstract domains", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d63043ff-e7a8-4998-b4d7-bf80134dd69e"}
{"abstract": "A number of analogies to cryptographic concepts have been made about watermarking. In this paper, we argue that these analogies are misleading or incorrect, and highlight several analogies to support our argument. We believe that the fundamental role of watermarking is the reliable embedding and detection of information and should therefore be considered a form of communications. We note that the fields of communications and cryptography are quite distinct and while communications systems often combine technologies from the two fields, a layered architecture is applied that requires no knowledge of the layers above. We discuss how this layered approach can be applied to watermarking applications.", "authors": ["Ingemar J. Cox", "Gwena\u00ebl J. Do\u00ebrr", "Teddy Furon"], "n_citation": 0, "title": "Watermarking is not cryptography", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d66adb89-a280-4549-a5ac-66ca5171cc78"}
{"abstract": "The LZ-index is a compressed full-text self-index able to represent a text T 1...u , over an alphabet of size a = O(polylog(u)) and with k-th order empirical entropy H k (T), using 4uH k (T) + o(u log \u03c3) bits for any k = o(log \u03c3  u). It can report all the occ occurrences of a pattern P 1...m  in T in O(m3 log \u03c3 + (m + occ) log u) worst case time. Its main drawback is the factor 4 in its space complexity, which makes it larger than other state-of-the-art alternatives. In this paper we present two different approaches to reduce the space requirement of LZ-index. In both cases we achieve (2 + e)uH k (T) + o(u log \u03c3) bits of space, for any constant e > 0, and we simultaneously improve the search time to O(m 2  log m + (m + occ) log u). Both indexes support displaying any sub-text of length l in optimal O(l/log\u03c3 u) time. In addition, we show how the space can be squeezed to (1 + \u2208)uH k (T) + o(u log \u03c3) to obtain a structure with O(m 2 ) average search time for m \u2265 2log \u03c3 , u.", "authors": ["Diego Arroyuelo", "Gonzalo Navarro", "Kunihiko Sadakane"], "n_citation": 62, "title": "Reducing the space requirement of LZ-index", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "da41c439-ed37-43c4-b098-3a7413daff02"}
{"abstract": "In this paper we address the problem of aligning 3-D data with articulated shapes. This problem resides at the core of many motion tracking methods with applications in human motion capture, action recognition, medical-image analysis, etc. We describe an articulated and bending surface representation well suited for this task as well as a method which aligns (or registers) such a surface to 3-D data. Articulated objects, e.g., humans and animals, are covered with clothes and skin which may be seen as textured surfaces. These surfaces are both articulated and deformable and one realistic way to model them is to assume that they bend in the neighborhood of the shape's joints. We will introduce a surface-bending model as a function of the articulated-motion parameters. This combined articulated-motion and surface-bending model better predicts the observed phenomena in the data and therefore is well suited for surface registration. Given a set of sparse 3-D data (gathered with a stereo camera pair) and a textured, articulated, and bending surface, we describe a register-and-fit method that proceeds as follows. First, the data-to-surface registration problem is formalized as a classifier and is carried out using an EM algorithm. Second, the data-to-surface fitting problem is carried out by minimizing the distance from tlie registered data points to the surface over the joint variables. In order to illustrate the method we applied it to the problem of hand tracking. A hand model with 27 degrees of freedom is successfully registered and fitted to a sequence of 3-D data points gathered with a stereo camera pair.", "authors": ["Guillaume Dewaele", "Fr\u00e9d\u00e9ric Devernay", "Radu Horaud", "Florence Forbes"], "n_citation": 0, "title": "The Alignment Between 3-D Data and Articulated Shapes with Bending Surfaces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "db2b986e-90a2-4de2-8799-0b0aa3aa131b"}
{"abstract": "Given a dictionary W consisting of n binary strings of length m each, a d-query asks if there exists a string in W within Hamming distance d of a given binary query string q. The problem was posed by Minsky and Papert in 1969 as a challenge to data structure design. There is a tradeoff between time and space in solving the problem of answering a d-query. Recently developed time-efficient methods for text indexing with errors can be used to answer a d-query in 0(m) time. However, these methods use O (n log d  n) (or more) additional space which is not practical for large databases. We present a method for the problem assuming the standard RAM model of computation. We process the dictionary to construct an edge-labelled tree with distinct labels to siblings, and with bounded branching factor and height. Storing the resulting tree does not require asymptotically more space than the size of an ordinary trie that stores the given dictionary. We present an algorithm for the d-query problem that takes O (m(3 log 4/3  n-1) d (log 2  n) d+1 ) time, and uses only O(m) additional space. We also generalize the results for the case of the problem when a larger alphabet, or edit distance are used. We achieve 0(m(2|\u03a3|-1) d [log 2|\u03a3|/(2|\u03a3|-1)  n-1) d (log 2  n) d+1 ) time complexity for the problem when Hamming distance is used. The time complexity increases by a factor of O(d(2|\u03a3|-1) d (log 2  n) d ) when we use edit distance. The algorithms are efficient when the approximate dictionary look-up involves long words defined over small alphabets. The algorithm can be modified such that it allows for words of different lengths as well as different lengths of query strings.", "authors": ["Abdullah N. Arslan"], "n_citation": 2, "title": "Efficient approximate dictionary look-up for long words over small alphabets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "db97e1ec-48aa-47e9-9402-19c5a36c1d55"}
{"abstract": "In this paper we propose an approach to increase TCP's fairness in multihop wireless networks, using ECN as a congestion signalling mechanism. The novel idea we introduce is that the marking probability at a wireless node is a function of the aggregate utilization in the node's neighborhood, which is determined by the sum of the receiving rates of all nodes within its collision domain. A node's received rate can be communicated to neighboring nodes by piggy-backing it on control packets, such as CTS and RTS messages, or data packets. Simulation results demonstrate that our approach can improve TCP's fairness in a multihop wireless network compared to drop tail queueing, while achieving the same aggregate throughput. Moreover, the proposed approach yields smaller average packet delay and delay jitter compared to drop tail queueing.", "authors": ["Vasilios A. Siris", "Despina Triantafyllidou"], "n_citation": 0, "title": "ECN marking for congestion control in multihop wireless networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e0a16b49-1ee2-407b-9b34-24b0e6e7878c"}
{"authors": ["Ren\u00e9 Govaerts", "Bart Preneel", "Joos Vandewalle"], "n_citation": 0, "title": "On the Power of Memory in the Design of Collision Resistant Hash Functions", "venue": "Lecture Notes in Computer Science", "year": 1992, "id": "e0b9554a-3cd8-44cb-8571-a87588c65c78"}
{"abstract": "High angular resolution diffusion imaging (HARDI) permits the computation of water molecule displacement probabilities over a sphere of possible displacement directions. This probability is often referred to as the orientation distribution function (ODF). In this paper we present a novel model for the diffusion ODF namely, a mixture of von Mises-Fisher (vMF) distributions. Our model is compact in that it requires very few variables to model complicated ODF geometries which occur specifically in the presence of heterogeneous nerve fiber orientation. We also present a Riemannian geometric framework for computing intrinsic distances, in closed-form, and performing interpolation between ODFs represented by vMF mixtures. As an example, we apply the intrinsic distance within a hidden Markov measure field segmentation scheme. We present results of this segmentation for HARDI images of rat spinal cords - which show distinct regions within both the white and gray matter. It should be noted that such a fine level of parcellation of the gray and white matter cannot be obtained either from contrast MRI scans or Diffusion Tensor MRI scans. We validate the segmentation algorithm by applying it to synthetic data sets where the ground truth is known.", "authors": ["Tim McGraw", "Baba C. Vemuri", "Robert P. Yezierski", "Thomas H. Mareci"], "n_citation": 0, "title": "Segmentation of High Angular Resolution Diffusion MRI Modeled as a Field of von Mises-Fisher Mixtures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e1b253ff-4541-4555-9ea0-d03035601835"}
{"abstract": "Since the potential of the impact of complex problem like CFD on design of material that is difficult to test actually has been established, CFD analysis that consist of two parts - simulation part and visualization part is considered as important work. But many researcher think it is difficult to analyze CFD problem because of following two reasons. First reason is that CFD analysis usually needs numerous processing time and much computing power. and Second is that the tasks of simulation and visualization are usually done independently - numerical simulation then visualization with stand-alone visualizing system. we propose solution of two problem. First problem is solved by using parallel/distributed programming on problem solving environment on grid environment - WISE system. and Second problem is solved by interactive visualization pipeline architecture that enables user to conducting simulation and visualization at the same time automatically applying pipeline concept. Therefore we presented interactive visualization pipeline architecture and implemented using WISE system.", "authors": ["Jinsung Park", "So-Hyun Ryu", "Yong-Won Kwon", "Chang-Sung Jeong"], "n_citation": 0, "title": "Interactive visualization pipeline architecture using work-flow management system on grid for CFD analysis", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e22fa430-a2ed-40a4-95ef-68df91d3657d"}
{"abstract": "BGP optimal egress selection is one of a recent trend in routing research. Current hot-potato routing has the problem of routing instability. This paper provides a new algorithm named BGP-RO-ES to minimize hot-potato disruptions while satisfying multiple constraints of different metrics, such as traffic engineering, the cost of traffic across the network and the cost of maintaining lightweight tunnels. The algorithm can be adaptive to different link failures and can be implemented in current BGP routers with a little change. Simulation results show that the control stability of BGP-RO-ES can be very close to that of TIE algorithm and that of RTF_TIE algorithm spending only half of the cost of maintaining lightweight tunnels.", "authors": ["Yaping Liu", "Zhenghu Gong", "Baosheng Wang", "Jinshu Shu"], "n_citation": 0, "title": "A routing optimization algorithm for BGP egress selection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e271e9df-3ef5-4576-ae3b-21ed9cbf912c"}
{"abstract": "This paper studies the (in)equational theory of simulation preorder and equivalence over the process algebra BCCSP. We prove that in the presence of a finite alphabet with at least two actions, the (in)equational theory of BCCSP modulo simulation preorder or equivalence does not have a finite basis. In contrast, in the presence of an alphabet that is infinite or a singleton, the equational theory for simulation equivalence does have a finite basis.", "authors": ["Taolue Chen", "Wan Fokkink"], "n_citation": 0, "title": "On finite alphabets and infinite bases III: Simulation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e3740a6e-6f72-4a10-91ac-33b41f908277"}
{"abstract": "A wide variety of powerful cryptographic tools have been built using RSA, Diffie-Hellman, and other similar assumptions as their basis. Computational security has been achieved relative to complexity assumptions about the computational difficulty of a variety of number theoretic problems. However, these problems are closely related, and it is likely that if any one of them turns out to be efficiently solvable with new mathematical advances or new kinds of computational devices, then similar techniques could be applicable to all of them. To provide greater diversity of security assumptions so that a break of one of them is less likely to yield a break of many or all of them, it is important to expand the body of computational problems on which security systems are based. Specifically, we suggest the use of hardness assumptions based on the complexity of logic problems, and in particular, we consider the well known Boolean 3SAT problem. In this paper, we consider the use of the 3SAT problem to provide a cryptographic primitive, secure set membership. Secure set membership is a general problem for participants holding set elements to generate a representation of their set that can then be used to prove knowledge of set elements to others. Set membership protocols can be used, for example, for authentication problems such as digital credentials and some signature problems such as timestamping.", "authors": ["Michael de Mare", "Rebecca N. Wright"], "n_citation": 0, "title": "Secure set membership using 3SAT", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e3d03c1b-5047-4311-b6e5-e15fa68f32eb"}
{"abstract": "CORAS is a research and technological development project under the Information Society Technologies (IST) Programme (Commission of the European Communities, Directorate-General Information Society). One of the main objectives of CORAS is to develop a practical framework, exploiting methods for risk analysis, semiformal methods for object-oriented modelling, and computerised tools, for a precise, unambiguous, and efficient risk assessment of security critical systems. This paper presents the CORAS framework and the related conclusions from the CORAS project so far.", "authors": ["Rune Fredriksen", "Monica Kristiansen", "Bj\u00f8rn Axel Gran", "Ketil St\u00f8len", "Tom Arthur Opperud", "Theo Dimitrakos"], "n_citation": 62, "title": "The CORAS framework for a model-based risk management process", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e4d8eca7-5d38-4681-94b4-cbbd618517eb"}
{"abstract": "Web caching and replication tune capacity with performance and they have become essential components of the Web. In practice, caching and replication techniques have been applied in proxy servers and Content Distribution Networks (CDNs) respectively. In this paper, we investigate the benefits of integrating caching policies on a CDN' s infrastructure. Using a simulation testbed, our results indicate that there is much room for performance improvement in terms of perceived latency, hit ratio and byte hit ratio. Moreover, we show that the combination of caching with replication fortifies CDNs against flash crowd events.", "authors": ["Konstantinos Stamos", "George Pallis", "Athena Vakali"], "n_citation": 0, "title": "Integrating caching techniques on a content distribution network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e5bfffa6-9508-40b6-bb2b-f08bf4f4c69e"}
{"abstract": "With the appearance of a number of e-mail worms in recent years, we urgently need a solution to detect unknown e-mail worms rather than using the traditional solution: signature-based scanning which does not deal with the new e-mail worms well. Our collected data shows that the quantitative trend of e-mail worms is really exploding. In this paper, we propose an e-mail worm Detection System that is based on analysis on human and worm behavior for detecting unknown e-mail worms. Message data such as e-mail or short messages are the result of human behavior. The proposed system detects unknown worms by assessment of behavior in communication because human behavior and worm behavior have different projection on data.", "authors": ["Tao Jiang", "Won-Il Kim", "Kyung-suk Lhee", "Manpyo Hong"], "n_citation": 5, "title": "E-mail worm detection using the analysis of behavior", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e679091e-d3ea-4bb9-9f7d-c01976b16432"}
{"abstract": "Configuring and applying complex requirements processes in organisations remains a challenging problem. This paper reports the application of the Map-driven Modular Method Re-engineering approach (MMMR) to a research-based requirements process called RESCUE. RESCUE had evolved in the light of research findings and client requests. The MMMR approach was applied to model the RESCUE process, identify omissions and weaknesses, and to reason about improvements to RESCUE that are currently being implemented. Results have implications for both the scalability and effectiveness of the MMMR approach and for innovative requirements processes such as RESCUE.", "authors": ["Jolita Ralyt\u00e9", "Neil A. M. Maiden", "Colette Rolland", "Rebecca Deneckere"], "n_citation": 0, "title": "Applying modular method engineering to validate and extend the RESCUE requirements process", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e73d0c0e-5287-4a47-83b0-48568d91e897"}
{"abstract": "Loops are a major bottleneck in formal software verification, because they generally require user interaction: typically, induction hypotheses or invariants must be found or modified by hand. This involves expert knowledge of the underlying calculus and proof engine. We show that one can replace interactive proof techniques, such as induction, with automated first-order reasoning in order to deal with parallelizable loops, where a loop can be parallelized whenever it avoids dependence of the loop iterations from each other. We develop a dependence analysis that ensures parallelizability. It guarantees soundness of a proof rule that transforms a loop into a universally quantified update of the state change information represented by the loop body. This makes it possible to use automatic first order reasoning techniques to deal with loops. The method has been implemented in the KeY verification tool. We evaluated it with representative case studies from the JAVA CARD domain.", "authors": ["Tobias Gedell", "Reiner H\u00e4hnle"], "n_citation": 0, "title": "Automating verification of loops by parallelization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e77180d0-69c8-4d55-bac7-76c496164eb0"}
{"abstract": "We use the notion of general dimension to show that any p-evaluatable concept class with polynomial query complexity can be learned in polynomial time with the help of an oracle in the polynomial hierarchy, where the complexity of the required oracle depends on the query-types used by the learning algorithm. In particular, we show that for subset and superset queries an oracle in \u03a3 P  3  suffices. Since the concept class of DNF formulas has polynomial query complexity with respect to subset and superset queries with DNF formulas as hypotheses, it follows that DNF formulas are properly learnable in polynomial time with subset and superset queries and the help of an oracle in \u03a3 P  3 . We also show that the required oracle in our main theorem cannot be replaced by an oracle in a lower level of the polynomial-time hierarchy, unless the hierarchy collapses.", "authors": ["Johannes K\u00f6bler", "Wolfgang Lindner"], "n_citation": 0, "title": "The complexity of learning concept classes with polynomial general dimension", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e7ac35d0-5649-4d51-8fb6-d3b36535fd25"}
{"abstract": "An embedded system is a combination of hardware and software subsystems. Interaction between these two subsystems may lead to unexpected behavior when faults are present in either. An effective technique is required to detect the presence of such interaction faults in an embedded system. We propose a test data selection technique for interaction testing in the embedded system using hardware fault injection and mutation test criteria. The proposed technique simulates hardware faults as software faults and uses these to mutate the software component. The mutants so created are then used as a means to select test data that differentiates the original program from the mutants. An experimental evaluation of the proposed technique is also presented.", "authors": ["Ahyoung Sung", "Byoungju Choi"], "n_citation": 0, "title": "Interaction testing in an embedded system using hardware fault injection and program mutation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e90ea910-8195-40ea-a7c3-ac18a27e839a"}
{"abstract": "It is not easy to develop a game that applies physical laws. In this paper we proposed a physics editor to make it easy to produce a physics game. The physics editor uses XML for processing attribute values of each object. This paper presents the architecture of the physics editor, and describes its detailed components. The physics editor provides an efficient method that is easily applied to physical attributes of all objects in the game. In this paper, we showed the process of a car's creation, using the physics editor. The physics editor that is presented in this paper automatically creates physical objects, but it is limited to the rigid body for car racing.", "authors": ["Byungyoon Lee", "Jonghwa Choi", "Dongkyoo Shin", "Dongil Shin"], "n_citation": 0, "title": "Design and implementation of a game physics editor using XML", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ebd0f4c6-84a0-4944-a2f1-7d85bf6be669"}
{"abstract": "Transductive Confidence Machine (TCM) is a way of converting standard machine-learning algorithms into algorithms that output predictive regions rather than point predictions. It has been shown recently that TCM is well-calibrated when used in the on-line mode: at any confidence level 1 - \u03b4, the long-run relative frequency of errors is guaranteed not to exceed 6 provided the examples are generated independently from the same probability distribution P. Therefore, the number of uncertain predictive regions (i.e., those containing more than one label) becomes the sole measure of performance. The main result of this paper is that for any probability distribution P (assumed to generate the examples), it is possible to construct a TCM (guaranteed to be well-calibrated even if the assumption is wrong) that performs asymptotically as well as the best region predictor under P.", "authors": ["Vladimir Vovk"], "n_citation": 0, "title": "Asymptotic optimality of transductive Confidence Machine", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ef47d876-4b13-4399-89fd-4b52e3a91b61"}
{"abstract": "We propose a new way of extending Logic Programming (LP) for reasoning with uncertainty. Probabilistic finite domains (Pfd) capitalise on ideas introduced by Constraint LP, on how to extend the reasoning capabilities of the LP engine. Unlike other approaches to the field, Pfd syntax can be intuitively related to the axioms defining Probability and to the underlying concepts of Probability Theory, (PT) such as sample space, events, and probability function. Probabilistic variables are core computational units and have two parts. Firstly, a finite domain, which at each stage holds the collection of possible values that can be assigned to the variable, and secondly a probabilistic function that can be used to assign probabilities to the elements of the domain. The two constituents are kept in isolation from each other. There are two benefits in such an approach. Firstly, that propagation techniques from finite domains research are retained, since a domain's representation is not altered. Thus, a probabilistic variable continues to behave as a finite domain variable. Secondly, that the probabilistic function captures the probabilistic behaviour of the variable in a manner which is, to a large extent, independent of the particular domain values. The notion of events as used in PT can be captured by LP predicates containing probabilistic variables and the derives operator (?) as defined in LP. Pfd stores hold conditional constraints which are a computationally useful restriction of conditional probability from PT. Conditional constraints are defined by D 1 : \u03c0 1  \u25cb+...\u25cb+D n : \u03c0 n I Q 1  ^... ^Q m  where, D i  and Q j  are predicates and each \u03c0 i  is a probability measure (0 < \u03c0 i  < 1, 1 < i < n, 1 < j < m). The conjuction of Qj's qualifies probabilistic knowledge about D i . In particular, the constraint is evidence that the probability of D i  in the qualified cases (i.e. when? Q 1 ,..., Q m ) is equal to \u03c0 i . On the other hand a conditional provides no evidence for the cases where? Q 1 ,...., Q m . Pfd has been used to model a well known example, the Monty Hall problem, which is often used to caution about the counter-intuitive results when reasoning with probabilities. Analysis of the computations over this model, has shown that Pfd emulates extensional methods that are used in statistics. The main benefits of our approach are (i) minimal changes of the core LP paradigm, and (ii) clear and intuitive way for arriving at probabilistic statements. Intuitiveness of probabilistic computations is facilitated by, (a) separation of the finite domain and the probability assigning function of a variable, and (b) using predicates to represent composite events.", "authors": ["Nicos Angelopoulos"], "n_citation": 0, "title": "Probabilistic finite domains: A brief overview", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f0a0ef96-0bb7-498f-b88b-6972a5e15843"}
{"abstract": "We mathematically explore a model for the shortness and security for passwords that are stored in hashed form. The model is implicitly in the NIST publication [8] and is based on conditions of the Shannon, Guessing and Min Entropy. We establish various new relations between these three notions of entropy, providing strong improvements on existing bounds such as the McEliece-Yu bound from [7] and the Min entropy lowerbound on Shannon entropy [3]. As an application we present an algorithm generating near optimally short passwords given certain security restrictions. Such passwords are specifically applicable in the context of one time passwords (e.g. initial passwords, activation codes).", "authors": ["Eric R. Verheul"], "n_citation": 0, "title": "Selecting secure passwords", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f144cd6a-bfe4-4956-8599-391864058b81"}
{"abstract": "We give faster approximation algorithms for the generalization of two NP-hard spanning tree problems. First, we investigate the problem of minimizing the degree of minimum spanning forests. Fischer [3] has shown how to compute a minimum spanning tree of degree at most b \u0394* + [log b  n] in time O(n 4+1/ln b ) for any b > 1, where \u0394* is the value of an optimal solution. We model our generalization as a multi-objective optimization problem and give a deterministic algorithm that computes for each number of connected components a solution with the same approximation quality as the algorithm of Fischer and runs in time O(n 3+1/ln b ). After that, we take a multi-objective view on the problem of computing minimum spanning trees with nonuniform degree bounds, which has been examined by Konemann and Ravi [7]. Given degree bounds B v  for each vertex v \u2208 V, we construct an algorithm that computes for each number of connected components a spanning forest in which each vertex v has degree O(B u  + log n) and whose weight is at most a constant times the weight of a minimum spanning forest obeying the degree bounds. The total runtime of our algorithm is O(n 3+2/ln b ) for an arbitrary constant b > 1. Setting b = e k , k > 2/3 an arbitrary constant, the runtime is by a factor n 3-2/k  log n less than the given bound by Konemann and Ravi.", "authors": ["Frank Neumann", "Marco Laumanns"], "n_citation": 0, "title": "Speeding up approximation algorithms for NP-hard spanning forest problems by multi-objective optimization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f285af71-b8f5-403c-a70d-b1cd3e4ce5b4"}
{"abstract": "This paper presents a methodology and tools used for performability analysis of storage systems in Sun Microsystems. A Markov modeling tool is used to evaluate the probabilities of normal and fault states in the storage system, based on field reliability data collected from customer sites. Fault injection tests are conducted to measure the performance of the storage system in various degraded states with a performance benchmark developed within Sun Microsystems. A graphic metric is introduced for performability assessment and comparison. An example is used throughout the paper to illustrate the methodology and process.", "authors": ["Hairong Sun", "Tina Tyan", "Steven Johnson", "Richard Elling", "Nisha Talagala", "Robert Wood"], "n_citation": 50, "title": "Performability Analysis of Storage Systems in Practice : Methodology and Tools", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f50552b3-73e8-4a48-9b24-8b3016445f39"}
{"abstract": "Cartesian trees have found numerous applications due to a peculiar rigid structure whose properties can be exploited in various ways. This rigidness, however, is also an obstacle when updating the structure since it can lead to a very unbalanced shape and so up to now most applications either assumed a random distribution of the keys or considered only the static case. In this paper we present a framework for efficiently maintaining a Cartesian tree under insertions and weak deletions in O(logn) amortized time per operation, using O(n) space. We show that the amortized cost of updating a Cartesian tree is 0(1 + H(T)/n) where H(T) = O(n log n) is an entropy-related measure for the partial order encoded by T. We also show how to exploit this property by implementing an algorithm which performs these updates in O(logn) time per operation. No poly-logarithmic update bounds were previously known.", "authors": ["Iwona Bia\u0142ynicka-Birula", "Roberto Grossi"], "n_citation": 0, "title": "Amortized rigidness in dynamic cartesian trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f64e3b80-2b19-47c6-b9e7-c83e4262ba82"}
{"abstract": "Database modeling is still a job of an artisan. Due to this approach database schemata evolve by growth without any evolution plan. Finally, they cannot be examined, surveyed, consistently extended or analyzed. Querying and maintenance become very difficult. Distribution of database fragments becomes a performance bottleneck. Currently, databases evolve to huge databases. Their development must be performed with the highest care. This paper aims in developing an approach to systematic schema composition based on components. The approach is based on the internal skeletal meta-structures inside the schema. We develop a theory of database components which can be composed to schemata following a architecture skeleton of the entire database.", "authors": ["Peggy Schmidt", "Bernhard Thalheim"], "n_citation": 0, "title": "Component-based modeling of Huge databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f966f4b0-e462-4300-9f59-9f0104c6a188"}
{"abstract": "Software specifications are of great use for more rigorous software development. They are useful for formal verification and automated testing, and they improve program understanding. In practice, specifications often do not exist and developers write software in an ad-hoc fashion. We describe a new way to automatically infer specifications from code. Our approach infers a likely specification for any method such that the method's behavior, i.e., its effect on the state and possible result values, is summarized and expressed in terms of some other methods. We use symbolic execution to analyze and relate the behaviors of the considered methods. In our experiences, the resulting likely specifications are compact and human-understandable. They can be examined by the user, used as input to program verification systems, or as input for test generation tools for validation. We implemented the technique for.NET programs in a tool called Axiom Meister. It inferred concise specifications for base classes of the.NET platform and found flaws in the design of a new library.", "authors": ["Nikolai Tillmann", "Feng Chen", "Wolfram Schulte"], "n_citation": 0, "title": "Discovering Likely Method Specifications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f9e950da-2da3-4e70-b6e4-256a13489d65"}
{"abstract": "Updates over virtual XML views that wrap the relational data have not been well supported by current XML data management systems. This paper studies the problem of the existence of a correct relational update translation for a given view update. First, we propose a clean extended-source theory to decide whether a translation mapping is correct. Then to answer the question of the existence of a correct mapping, we classify a view update as either un-translatable, conditionally or unconditionally translatable under a given update translation policy. We design a graph-based algorithm to classify a given update into one of the three update categories based on schema knowledge extracted from the XML view and the relational base. This now represents a practical approach that could be applied by any existing view update system in industry and in academic for analyzing the translatability of a given update statement before translation of it is attempted.", "authors": ["Ling Wang", "Elke A. Rundensteiner"], "n_citation": 0, "title": "On the updatability of XML views published over relational data", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fe3fdb78-79ab-455a-a94c-7d2554fa1b44"}
{"authors": ["Gautham Sekar", "Nicky Mouha", "Vesselin Velichkov", "Bart Preneel"], "n_citation": 0, "title": "Meet-in-the-Middle Attacks on Reduced-Round XTEA", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "ff2a6a46-bdff-4cf9-963a-07559ef7df09"}
{"abstract": "The k-Nearest-Neighbors (kNN) method for classification is simple but effective in many cases. The success of kNN in classification depends on the selection of a good value for k. In this paper, we proposed a contextual probability-based classification algorithm (CPC) which looks at multiple sets of nearest neighbors rather than just one set of k nearest neighbors for classification to reduce the bias of k. The proposed formalism is based on probability, and the idea is to aggregate the support of multiple neighborhoods for various classes to better reveal the true class of each new instance. To choose a series of more relevant neighborhoods for aggregation, three neighborhood selection methods: distance-based, symmetric-based, and entropy-based neighborhood selection methods are proposed and evaluated respectively. The experimental results show that CPC obtains better classification accuracy than kNN and is indeed less biased by k after saturation is reached. Moreover, the entropy-based CPC obtains the best performance among the three proposed neighborhood selection methods.", "authors": ["Gongde Guo", "Hui Wang", "David A. Bell", "Zhining Liao"], "n_citation": 0, "title": "Contextual probability-based classification", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ff78851c-23b3-4d7b-bae0-51e4d1818ca8"}
{"abstract": "Aesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities of photographs is a highly subjective task. Hence, there is no unanimously agreed standard for measuring aesthetic value. In spite of the lack of firm rules, certain features in photographic images are believed, by many, to please humans more than certain others. In this paper, we treat the challenge of automatically inferring aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated online photo sharing Website as data source. We extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images. Automated classifiers are built using support vector machines and classification trees. Linear regression on polynomial terms of the features is also applied to infer numerical aesthetics ratings. The work attempts to explore the relationship between emotions which pictures arouse in people, and their low-level content. Potential applications include content-based image retrieval and digital photography.", "authors": ["Ritendra Datta", "Dhiraj Joshi", "Js Li", "James Ze Wang"], "n_citation": 0, "title": "Studying Aesthetics in Photographic Images Using a Computational Approach", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fff9b450-849c-4878-bf0e-4c5feead2502"}
{"abstract": "We consider the model-checking problem for C programs with (1) data ranging over very large domains, (2) (recursive) procedure calls, and (3) concurrent parallel components that communicate via synchronizing actions. We model such programs using communicating pushdown systems, and reduce the reachability problem for this model to deciding the emptiness of the intersection of two context-free languages L 1  and L 2 . We tackle this undecidable problem using a CounterExample Guided Abstraction Refinement (CEGAR) scheme. We implemented our technique in the model checker MAGIC and found a previously unknown bug in a version of a Windows NT Bluetooth driver.", "authors": ["Sagar Chaki", "Edmund M. Clarke", "Nicholas Kidd", "Thomas W. Reps", "Tayssir Touili"], "n_citation": 92, "title": "Verifying concurrent message-passing C programs with recursive calls", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "002fc6fc-ead3-49dd-9f3d-d72e63979f21"}
{"abstract": "This paper describes the selective attention and perception system designed for the synthetic creatures of the ANIMUS[1] project. This system allows agents to evaluate objects, other characters, properties, and events happening in their virtual environment. Perceived data is prioritized to direct the attention of the character toward salient stimuli, providing means for action-reaction and conditioned learning, automatic triggering of reflexive responses to certain stimuli, and further optimization of the cognitive processes by creating a more natural and realistic interface. The system can be customized to fit the artistic and conceptual constraints of each character, facilitating the creation of synthetic creatures with complex behavior and personality.", "authors": ["Daniel Torres", "Pierre Boulanger"], "n_citation": 50, "title": "A perception and selective attention system for synthetic creatures", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "013e9652-92da-4c8a-a196-515ac34aa775"}
{"abstract": "The notion of Oblivious Commitment Based Envelope (OCBE) was recently proposed; it enables attribute-based access control without revealing any information about the attributes. Previous OCBE protocols are designed by taking zero-knowledge proof protocols that prove a committed value satisfies some property and changing the protocols so that instead of one party proving to the other party, the two parties compute two keys that agree if and only if the committed value indeed satisfy the property. In this paper, we introduce a more general approach for designing OCBE protocols that uses zero-knowledge proof protocols in a black-box fashion. We present a construction such that given a zero-knowledge proof protocol that proves a committed value satisfies a predicate, we have an OCBE protocol for that predicate with constant additional cost. Compared with previous OCBE protocols, our construction is more general, more efficient, and has wide applicability.", "authors": ["Jiangtao Li", "Ninghui Li"], "n_citation": 50, "title": "A construction for general and efficient oblivious commitment based envelope protocols", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "049be008-48e8-4a2f-80b8-259c5c836813"}
{"abstract": "Due to the tremendous increase rate and the high change frequency of Web documents, maintaining an up-to-date index for searching purposes (search engines) is becoming a challenge. The traditional crawling methods are no longer able to catch up with the constantly updating and growing Web. Realizing the problem, in this paper we suggest an alternative distributed crawling method with the use of mobile agents. Our goal is a scalable crawling scheme that minimizes network utilization, keeps up with document changes, employs time realization, and is easily upgradeable.", "authors": ["Odysseas Papapetrou", "Stavros Papastavrou", "George Samaras"], "n_citation": 0, "title": "UCYMICRA: Distributed indexing of the Web using migrating crawlers", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "0653e139-e9e7-4d46-ab39-0e361936a670"}
{"abstract": "Evolutionary game theory is an adaptation of classical game theory used to analyse and explain observed natural phenomena where organisms choose between alternative behavioural strategies. Recent analyses and simulations of evolutionary games have shown that implementation choices such as population size and selection method can have unexpected effects on the course of evolution. In this paper, we propose a new evolutionary interpretation of such games that uses a more biologically faithful selection scheme, in which selection and population size emerge from the interactions of the players and their environment. Using the well-known Hawks and Doves game as an example, we show that the resulting models are also tractable, easily simulated, and flexible.", "authors": ["Philip Hingston", "Luigi Barone"], "n_citation": 0, "title": "Hawks, Doves and lifetime reproductive success", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "089f8d7f-42b6-4c31-9bfe-cac9cf45c187"}
{"abstract": "In the simultaneous acquisition of EEG and fMRI, analysis of EEG signals is a difficult task due to ballistocardiogram (BCG) and electro-oculogram (EOG) artifacts. It gets worse if evoked potentials are measured inside MRI for their minute responses in comparison to the spontaneous brain responses. In this paper, we propose a new method for removing both artifacts simultaneously from the evoked EEG signals acquired inside MRI using constrained Independent component analysis (cICA). With properly designed reference functions for the BCG and EOG artifacts as constraints, cICA identifies the independent components (ICs) corresponding to the artifacts. Then artifact-removed EEG signals are reconstructed after removing the identified ICs to obtain evoked potentials. To evaluate our proposed technique, we have removed the artifacts with cICA and the standard template subtraction technique and generated visual evoked potentials (VEPs) respectively which are compared to the VEPs obtained from EEG signals measured outside MRI. Our results indicate that our cICA technique performs better than the standard BCG artifact removal methods with some efficient features.", "authors": ["Tahir Rasheed", "Myung Ho In", "Young-Koo Lee", "Sungyoung Lee", "Soo Yeol Lee", "Tae-Seong Kim"], "n_citation": 0, "title": "Constrained ICA Based Ballistocardiogram and Electro-Oculogram Artifacts Removal from Visual Evoked Potential EEG Signals Measured Inside MRI", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "08d3f615-0b21-422c-8e44-d8838840aeb2"}
{"abstract": "The size and complexity of modem information systems together with requirements for short development time increase the demands for reuse of already existing solutions. The idea of reuse itself is not novel and the ability of reuse is even a part of the learning process. However, not much support for reuse can be found for the analysis phase of information systems design. Collecting reusable solutions, called patterns, in a library and supporting the search for an appropriate pattern within such a library is one approach addressed in this area. A tool for this purpose has been partly implemented and the results from a case study testing this tool are reported here.", "authors": ["Petia Wohed"], "n_citation": 24, "title": "Tool support for reuse of analysis patterns : A case study", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "0a9e73c0-1bb9-470c-ba24-ad7a4a95cee8"}
{"abstract": "The extreme sensitivity of quantum information to ambient noise has prompted the study of quantum error-correcting codes. In this paper two families of quantum error-correcting codes are constructed based on Hermitian curves. Unlike the case of classical codes, it is a difficult, sometimes impossible, task to puncture a quantum stabilizer code. The puncture code of Hermitian codes is computed that allows one to determine the admissible puncturings. A large number of punctured Hermitian codes are derived by exploiting known facts about the weight distribution of these puncture codes.", "authors": ["Pradeep Kiran Sarvepalli", "Andreas Klappenecker"], "n_citation": 0, "title": "Nonbinary Quantum Codes from Hermitian Curves", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0b5662cd-2162-4358-ad47-661fb1ebce25"}
{"abstract": "Data integration systems represent today a key technological infrastructure for managing the enormous amount of information even more and more distributed over many data sources, often stored in different heterogeneous formats. Several different approaches providing transparent access to the data by means of suitable query answering strategies have been proposed in the literature. These approaches often assume that all the sources have the same level of reliability and that there is no need for preferring values extracted from a given source. This is mainly due to the difficulties of properly translating and reformulating source preferences in terms of properties expressed over the global view supplied by the data integration system. Nonetheless preferences are very important auxiliary information that can be profitably exploited for refining the way in which integration is carried out. In this paper we tackle the above difficulties and we propose a formal framework for both specifying and reasoning with preferences among the sources. The semantics of the system is restated in terms of preferred answers to user queries, and the computational complexity of identifying these answers is investigated as well.", "authors": ["Gianluigi Greco", "Domenico Lembo"], "n_citation": 0, "title": "Data integration with preferences among sources", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0b5d9cca-1414-4ae3-a90f-44a720c27887"}
{"abstract": "Two variants of pebble tree-walking automata on binary trees are considered that were introduced in the literature. It is shown that for each number of pebbles, the two models have the same expressive power both in the deterministic case and in the nondeterministic case. Furthermore, nondeterministic (resp. deterministic) tree-walking automata with n + I pebbles can recognize more languages than those with n pebbles. Moreover, there is a regular tree language that is not recognized by any tree-walking automaton with pebbles. As a consequence, FO+posTC is strictly included in MSO over trees.", "authors": ["Mikoiaj Bojanczyk", "Mathias Samuelides", "Thomas Schwentick", "Luc Segoufin"], "n_citation": 0, "title": "Expressive Power of Pebble Automata", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0b61f61f-f62b-43b0-9f5d-8a4e72fa49ad"}
{"abstract": "We present for the first-order theory of atomic Boolean algebras of sets with linear cardinality constraints a quantifier elimination algorithm. In the case of atomic Boolean algebras of sets, this is a new generalization of Boole's well-known variable elimination method for conjunctions of Boolean equality constraints. We also explain the connection of this new logical result with the evaluation of relational calculus queries on constraint databases that contain Boolean linear cardinality constraints.", "authors": ["Peter Revesz"], "n_citation": 0, "title": "Quantifier-elimination for the first-order theory of Boolean algebras with linear cardinality constraints", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0c402a1b-d7a9-4240-9998-4198bd067086"}
{"abstract": "We describe an empirical study of Marve, a virtual receptionist located at the entrance of our research laboratory. Marve engages with lab members and visitors in natural face-to-face communication, takes and delivers messages, tells knock-knock jokes, conducts natural small talk on movies, and discusses the weather. In this research, we investigate the relative popularity of Marve's social conversational capabilities and his role-specific messaging tasks, as well as his perceived social characteristics. Results indicate that users are interested in interacting with Marve, use social conversational conventions with Marve, and perceive and describe him as a social entity.", "authors": ["Sabarish V. Babu", "Stephen J. Schmugge", "Tiffany Barnes", "Larry F. Hodges"], "n_citation": 0, "title": "What would you like to talk about? An evaluation of social conversations with a virtual receptionist", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0cd97a36-cefa-444c-864e-eb2230dd8f8c"}
{"abstract": "We present a framework for approximate inference in probabilistic data models which is based on free energies. The free energy is constructed from two approximating distributions which encode different aspects of the intractable model. Consistency between distributions is required on a chosen set of moments. We find good performance using sets of moments which either specify factorized nodes or a spanning tree on the nodes. The abstract should summarize the contents of the paper using at least 70 and at most 150 words. It will be set in 9-point font size and be inset 1.0 cm from the right and left margins. There will be two blank lines before and after the Abstract....", "authors": ["Manfred Opper", "Ole Winther"], "n_citation": 0, "title": "Approximate inference in probabilistic models", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0d59ccf4-92f2-4584-96e0-df2e2e2cdb28"}
{"abstract": "Different formal learning models address different aspects of learning. Below we compare learning via queries-interpreting learning as a one-shot process in which the learner is required to identify the target concept with just one hypothesis-to Gold-style learning-interpreting learning as a limiting process in which the learner may change its mind arbitrarily often before converging to a correct hypothesis. Although these two approaches seem rather unrelated, a previous study has provided characterisations of different models of Gold-style learning (learning in the limit, conservative inference, and behaviourally correct learning) in terms of query learning. Thus under certain circumstances it is possible to replace limit learners by equally powerful one-shot learners. Both this previous and the current analysis are valid in the general context of learning indexable classes of recursive languages. The main purpose of this paper is to solve a challenging open problem from the previous study. The solution of this problem leads to an important observation, namely that there is a natural query learning type hierarchically in-between Gold-style learning in the limit and behaviourally correct learning. Astonishingly, this query learning type can then again be characterised in terms of Gold-style inference. In connection with this new in-between inference type we have gained new insights into the basic model of conservative learning and the way conservative learners work. In addition to these results, we compare several further natural inference types in both models to one another.", "authors": ["Steffen Langel", "Sandra Zilles"], "n_citation": 0, "title": "Comparison of Query learning and gold-style learning in dependence of the hypothesis space", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "13b3cda1-7a57-4f86-aacf-aaf20005c7ff"}
{"abstract": "A novel deformable model for image segmentation and shape recovery is presented. The model is inspired by fluid dynamics and is based on a flooding simulation similar to the watershed paradigm. Unlike most watershed methods, our model has a continuous formulation, being described by two partial differential equations. In this model, different fluids, added by placing density (dye) sources manually or automatically, are attracted towards the contours of the objects of interest by an image force. In contrast to the watershed method, when different fluids meet they may mix. When the topographical relief of the image is flooded, the interfaces separating homogeneous fluid regions can be traced to yield the object contours. We demonstrate the flexibility and potential of our model in two experimental settings: shape recovery using manual initializations and automated segmentation.", "authors": ["Andrei C. Jalba", "Jos B. T. M. Roerdink"], "n_citation": 0, "title": "A Physically-Motivated Deformable Model Based on Fluid Dynamics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1527e734-ed95-490d-b05d-1a93d9c23fff"}
{"abstract": "We analyze the security of the CTR + CBC-MAC (CCM) encryption mode. This mode, proposed by Doug Whiting, Russ Housley, and Niels Ferguson, combines the CTR (counter) encryption mode with CBC-MAC message authentication and is based on a block cipher such as AES. We present concrete lower bounds for the security of CCM in terms of the security of the underlying block cipher. The conclusion is that CCM provides a level of privacy and authenticity that is in line with other proposed modes such as OCB.", "authors": ["Jakob Jonsson"], "n_citation": 0, "title": "On the security of CTR + CBC-MAC", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1675dd97-b712-4260-ae26-cfcbed0bb3cf"}
{"abstract": "Cadastral data composed of land boundary and ownership has a very dynamic nature while conventional GIS data model assumes the world is static. In order to fill the gap, authors clarified the dynamic nature of cadastral data and presented the analysis of functional requirements, and then developed a spatio-temporal model and relevant functionalities, which can deal with real world cadastral data. The proposed model and functions were implemented using prolog, XPCE, and C++. The value of this study is to uncover the need for the integration of all historical cadastral data and the feasibility of the system development for the future.", "authors": ["Joon Heo", "Jeong Hyun Kim", "Seoungpil Kang"], "n_citation": 0, "title": "Temporal Land Information System (TLIS) for Dynamically Changing Cadastral Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "183d748f-97dd-4569-9cc1-3e3c2a24be2b"}
{"abstract": "Scale and affine-invariant local features have shown excellent performance in image matching, object and texture recognition. This paper optimizes keypoint detection to achieve stable local descriptors, and therefore, an improved image representation. The technique performs scale selection based on a region descriptor, here SIFT, and chooses regions for which this descriptor is maximally stable. Maximal stability is obtained, when the difference between descriptors extracted for consecutive scales reaches a minimum. This scale selection technique is applied to multi-scale Harris and Laplacian points. Affine invariance is achieved by an integrated affine adaptation process based on the second moment matrix. An experimental evaluation compares our detectors to Harris-Laplace and the Laplacian in the context of image matching as well as of category and texture classification. The comparison shows the improved performance of our detector.", "authors": ["Gyuri Dork\u00f3", "Cordelia Schmid"], "n_citation": 50, "title": "Maximally Stable Local Description for Scale Selection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "19750d08-4cba-4e94-95a8-5fdcf5bdb4bf"}
{"abstract": "We consider the use of top-points for object retrieval. These points are based on scale-space and catastrophe theory, and are invariant under gray value scaling and offset as well as scale-Euclidean transformations. The differential properties and noise characteristics of these points are mathematically well understood. It is possible to retrieve the exact location of a top-point from any coarse estimation through a closed-form vector equation which only depends on local derivatives in the estimated point. All these properties make top-points highly suitable as anchor points for invariant matching schemes. By means of a set of repeatability experiments and receiver-operator-curves we demonstrate the performance of top-points and differential invariant features as image descriptors.", "authors": ["Bram Platel", "E. Balmachnova", "Luc Florack", "B.M. ter Haar Romeny"], "n_citation": 0, "title": "Top-Points as Interest Points for Image Matching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1a2e6470-582b-4e6f-b9ca-ee5a13c3bbef"}
{"abstract": "This paper proposes a new method of Self-Organizing Map (SOM) in which an input space is represented as a graph by modifications of a distance measure and a updating rule. The distance between input node and reference element is defined by the shortest distance between them in the graph. The reference elements are updated along the shortest path to the input node. The effectiveness of the proposed method is verified by applying it to a Traveling Salesman Problem.", "authors": ["Takeshi Yamakawa", "Keiichi Horio", "Masaharu Hoshino"], "n_citation": 0, "title": "Self-Organizing Map with Input Data Represented as Graph", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1c469b6b-e51c-42aa-90e7-8f47157652a0"}
{"abstract": "Reasoning about multithreaded object-oriented programs is difficult, due to the non-local nature of object aliasing, data races, and deadlocks. We propose a programming model that prevents data races and deadlocks, and supports local reasoning in the presence of object aliasing and concurrency. Our programming model builds on the multi-threading and synchronization primitives as they are present in current mainstream languages. Java or C# programs developed according to our model can be annotated by means of stylized comments to make the use of the model explicit. We show that such annotated programs can be formally verified to comply with the programming model. In other words, if the annotated program verifies, the underlying Java or C# program is guaranteed to be free from data races and deadlocks, and it is sound to reason locally about program behavior. We have implemented a verifier for programs developed according to our model in a custom build of the Spec# programming system, and have validated our approach on a case study.", "authors": ["Bart Jacobs", "Jan Smans", "Frank Piessens", "Wolfram Schulte"], "n_citation": 0, "title": "A Statically Verifiable Programming Model for Concurrent Object-Oriented Programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1cd46923-9d1e-4a07-bc68-21be1f13416d"}
{"abstract": "In this paper, we start with presenting and defining scalability of algorithm-architecture combinations based on the fixed ratio of computation time to communication cost, analyze the performance and scalability of a number of parallel matrix multiplication algorithms, and compare them with the related work. The performance analysis and the analytical scalability expressions for these algorithms show that our scalability metric is better than the isoefficiency metric.", "authors": ["Xingfu Wu"], "n_citation": 50, "title": "An approach to scalability of parallel matrix multiplication algorithms", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "1cda63a1-e3ef-4a94-a2a1-007dec62f07b"}
{"abstract": "In this paper, we propose a new approach to virtual actors for 3D animation, in which the actors' behaviour can generate situations of narrative interest, as a consequence of the limited rationality of these actors. The actors behaviour is determined by plans generated in real-time in response to a single goal acting as a narrative drive. We use Heuristic Search Planning (HSP) as a non-optimal planning technique, which generate believable plans, yet susceptible of failure. We discuss the changes required in the representation of preconditions, the role of the heuristic function, and the narrative aspects of optimal plans and backtracking. Throughout this paper, we illustrate the discussion with actual examples from a first prototype system.", "authors": ["Marc Cavazza", "Fred Charles", "Steven J. Mead"], "n_citation": 0, "title": "Intelligent virtual actors that plan ... to fail", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1d20bd97-dcc6-40c9-ab71-bb0879a35e4f"}
{"abstract": "Although formal methods provide excellent techniques for the precise description of systems, understanding these descriptions is often restricted to experts. This paper investigates a practical solution to assist the understanding of a formal specification, written in B, by providing a complementary view of the specification as UML class diagram. Our technique Improves the state of the art by taking into account operations in the construction of the diagram, through the use of concept formation techniques. A documentation tool automates the approach. It has been applied to several specifications built independently of the tool.", "authors": ["Akram Idani", "Yves Ledru", "Didier Bert"], "n_citation": 0, "title": "Derivation of UML class diagrams as static views of formal B developments", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "20de2f78-efc3-4e85-845d-dd00de532cf8"}
{"authors": ["Pedro Evangelista", "I. Rocha", "E. C. Ferreira", "Miguel Rocha"], "n_citation": 0, "title": "A software tool for the simulation and optimization of dynamic metabolic models", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "21877e52-6a65-4ddc-97ac-aa40f5e20deb"}
{"abstract": "This article presents a novel method for acquiring high-quality solid models of complex 3D shapes from multiple calibrated photographs. After the purely geometric constraints associated with the silhouettes found in each image have been used to construct a coarse surface approximation in the form of a visual hull, photoconsistency constraints are enforced in three consecutive steps: (1) the rims where the surface grazes the visual hull are first identified through dynamic programming; (2) with the rims now fixed, the visual hull is carved using graph cuts to globally optimize the photoconsistency of the surface and recover its main features; (3) an iterative (local) refinement step is finally used to recover fine surface details. The proposed approach has been implemented, and experiments with six real data sets are presented, along with qualitative comparisons with several state-of-the-art image-based-modeling algorithms.", "authors": ["Yasutaka Furukawa", "Jean Ponce"], "n_citation": 0, "title": "Carved Visual Hulls for Image-Based Modeling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "22c40fb3-6f89-4af7-b3c5-7e2de3ac599a"}
{"abstract": "Assume that each vertex of a graph G is assigned a nonnegative integer weight and that l and u are nonnegative integers. One wish to partition G into connected components by deleting edges from G so that the total weight of each component is at least l and at most u. Such an almost uniform partition is called an (l, u)-partition. We deal with three problems to find an (l, u)-partition of a given graph. The minimum partition problem is to find an (l,u)-partition with the minimum number of components. The maximum partition problem is defined similarly. The p-partition problem is to find an (l, u)-partition with a fixed number p of components. All these problems are NP-complete or NP-hard even for series-parallel graphs. In this paper we show that both the minimum partition problem and the maximum partition problem can be solved in time O(u 4 n) and the p-partition problem can be solved in time O(p 2 u 4 n) for any series-parallel graph of n vertices. The algorithms can be easily extended for partial k-trees, that is, graphs with bounded tree-width.", "authors": ["Takehiro Ito", "Kazuya Goto", "Xiao Zhou", "Takao Nishizeki"], "n_citation": 0, "title": "Partitioning a Multi-weighted Graph to Connected Subgraphs of Almost Uniform Size", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "24224d81-5359-403f-a0f9-e711a52ac5bb"}
{"abstract": "This paper presents the integration between a co-operative co-evolutionary genetic algorithm (CCGA) and four evolutionary multi-objective optimisation algorithms (EMOAs): a multi-objective genetic algorithm (MOGA), a niched Pareto genetic algorithm (NPGA), a non-dominated sorting genetic algorithm (NSGA) and a controlled elitist non-dominated sorting genetic algorithm (CNSGA). The resulting algorithms can be referred to as co-operative co-evolutionary multi-objective optimisation algorithms or CCMOAs. The CCMOAs are benchmarked against the EMOAs in seven test problems. The first six problems cover different characteristics of multi-objective optimisation problems, namely convex Pareto front, non-convex Pareto front, discrete Pareto front, multi-modality, deceptive Pareto front and non-uniformity of solution distribution. In contrast, the last problem is a two-objective real-world problem, which is generally referred to as the continuum topology design. The results indicate that the CCMOAs are superior to the EMOAs in terms of the solution set coverage, the average distance from the non-dominated solutions to the true Pareto front, the distribution of the non-dominated solutions and the extent of the front described by the non-dominated solutions.", "authors": ["Kuntinee Maneeratana", "Kittipong Boonlong", "Nachol Chaiyaratana"], "n_citation": 0, "title": "Multi-objective optimisation by co-operative co-evolution", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "24d62531-f4f2-4dfe-9c45-9cc57693eb49"}
{"abstract": "A hidden Markov model is introduced for descriptive modelling the mosaic-like structures of haplotypes, due to iterated recombinations within a population. Methods using the minimum description length principle are given for fitting such models to training data. Possible applications of the models are delineated, and some preliminary analysis results on real sets of haplotypes are reported, demonstrating the potential of our methods.", "authors": ["Mikko Koivisto", "Teemu Kivioja", "Heikki Mannila", "Pasi Rastas", "Esko Ukkonen"], "n_citation": 0, "title": "Hidden Markov modelling techniques for haplotype analysis", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "24dde4e7-1fd4-49f4-b01e-6483c69b0c7b"}
{"abstract": "A proxy signature enables an original signer to delegate her signing capability to a proxy signer and then the proxy signer can sign a message on behalf of the original signer. In this paper we propose an ID-based proxy signature scheme from bilinear pairings. We provide exact security proof of the proposed ID-based proxy signature scheme in the random oracle model under the Computational Diffie-Hellman assumption without using Forking Lemma.", "authors": ["Kyung-Ah Shim"], "n_citation": 21, "title": "An identity-based proxy signature scheme from pairings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "28450fc5-7cf8-4e6a-8bb8-7b05406ab9ae"}
{"abstract": "The task of developing protocols for humans to securely authenticate themselves to a remote server has been an interesting topic in cryptography as a replacement for the traditional, less secure, password based systems. The protocols proposed in literature are based on some underlying difficult mathematical problem, which are tuned so as to make them easily computable by humans. As a result these protocols are easily broken when desired to be efficiently executable. We present a Human Identification Protocol based on the ability of humans to efficiently process an image given a secret predicate. It is a challenge-response protocol in which a subset of images presented satisfies a secret predicate shared by the challenger and the user. We conjecture that it is hard to guess this secret predicate for adversaries, both humans and programs. It can be efficiently executed by humans with the knowledge of the secret which in turn is easily memorable and replaceable. We prove the security of the protocol separately for human adversaries and programs based on two separate assumptions and justify these assumptions with the help of an example implementation.", "authors": ["Hassan Jameel", "Riaz Ahmed Shaikh", "Heejo Lee", "Sungyoung Lee"], "n_citation": 0, "title": "Human identification through image evaluation using secret predicates", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "28caab65-d993-4801-b888-f8ed2d01eeb8"}
{"abstract": "We address the problem of integrating objects from a source taxonomy into a master taxonomy. This problem is not only currently pervasive on the web, but also important to the emerging semantic web. A straightforward approach to automating this process would be to train a classifier for each category in the master taxonomy, and then classify objects from the source taxonomy into these categories. Our key insight is that the availability of the source taxonomy data could be helpful to build better classifiers in this scenario, therefore it would be beneficial to do transductive learning rather than inductive learning, i.e., learning to optimize classification performance on a particular set of test examples. In this paper, we attempt to use a powerful transductive learning algorithm, Spectral Graph Transducer (SGT), to attack this problem. Noticing that the categorizations of the master and source taxonomies often have some semantic overlap, we propose to further enhance SGT classifiers by incorporating the affinity information present in the taxonomy data. Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration.", "authors": ["Dell Zhang", "Xiaoling Wang", "Yisheng Dong"], "n_citation": 0, "title": "Web taxonomy integration using Spectral Graph Transducer", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2a22ce46-d75d-4880-b1a4-131a2386fb8e"}
{"abstract": "A Web site generally contains a wide range of topics which provide information for users who have different access interests and goals. This information is not randomly scattered, but well organized under a hierarchy encoded in the hyperlink structure of a Web site. It is intended to mold the user's mental models of how the information is organized. On the other hand, user traversals over hyperlinks between Web pages can reveal semantic relationships between these pages. Unfortunately, the link structure of a Web site which represent the Web designer's expectation on visitors may be quite different from the organization expected by visitors to this site. Discovering the conceptual page hierarchy from a user's angle can help web masters to have an sight into real relationships among the Web pages and refine the link structure of the Web site to facilitate effective user navigation. In this paper, we propose a method to generate a conceptual page hierarchy of a Web site on the basis of user traversal history. We use maximal forward references to model user's traversal behavior over the underlying link hierarchy of a Web site. We then build a weighted directed graph to represent the inter-relationships between Web pages. Finally we apply a Maximum Spanning Tree (MST) algorithm to generate a conceptual page hierarchy of the Web site. We demonstrate the effectiveness of our approach by conducting a preliminary experiment based on a real world Web data.", "authors": ["Xia Chen", "Minqiang Li", "Wei Zhao", "Ding-Yi Chen"], "n_citation": 50, "title": "Discovering conceptual page hierarchy of a web site from user traversal history", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2d0251f9-20bc-4219-b743-8e432cfac30e"}
{"abstract": "Cancer chemotherapy is a complex treatment mode that requires balancing the benefits of treating tumours using anti-cancer drugs with the adverse toxic side-effects caused by these drugs. Some methods of computational optimisation, Genetic Algorithms in particular, have proven to be useful in helping to strike the right balance. The purpose of this paper is to study how an alternative optimisation method - Particle Swarm Optimisation - can be used to facilitate finding optimal chemotherapeutic treatments, and to compare its performance with that of Genetic Algorithms.", "authors": ["Andrei Petrovski", "Bhavani Sudha", "John A. W. McCall"], "n_citation": 0, "title": "Optimising cancer chemotherapy using Particle Swarm Optimisation and Genetic Algorithms", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2e60a21f-7970-41ac-a8b7-6ad9da5513c9"}
{"abstract": "Computer Security is one of today's hot topic and the need for conceptual models of security features have brought up a number of proposals ranging from UML extensions to novel conceptual models. What is still missing, however, are models that focus on high-level security requirements, without forcing the modeler to immediately get down to security mechanisms. The modeling process itself should make it clear why encryption, authentication or access control are necessary, and what are the tradeoffs, if they are selected. In this paper we show that the i*/Tropos framework lacks the ability to capture these essential features and needs to be augmented. To motivate our proposal, we build upon a substantial case study - the modeling of the Secure Electronic Transactions e-commerce suites by VISA and MasterCard - to identify missing modeling features. In a nutshell, the key missing concept is the separation of the notion of offering a service (of a handling data, performing a task or fulfilling a goal) and ownership of the very same service. This separation is what makes security essential. The ability of the methodology to model a clear dependency relation between those offering a service (the merchant processing a credit card number), those requesting the service (the bank debiting the payment), and those owning the very same data (the cardholder), make security solutions emerge as a natural consequence of the modeling process.", "authors": ["Paolo Giorgini", "Fabio Massacci", "John Mylopoulos"], "n_citation": 0, "title": "Requirement engineering meets security: A case study on modelling Secure electronic transactions by VISA and mastercard", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2f98389d-ba7d-48a7-a592-7d9a3ac1be72"}
{"abstract": "FIPA has provided a standardised model for implementing interoperable agent systems that may prove the basis for new solutions to the problem of managing the growing complexity of the telecommunications infrastructure. However, the ability of agents to operate in the telecommunications domain would be greatly enhanced if the FIPA architecture was refined to deal with the standardised Signalling System No. 7 (SS.7) communications protocol suite almost universally used for the control of telecommunications networks. The current FIPA model is TCP/IP centred, thus in order to use the SS.7 infrastructure a mapping from FIPA message transport requirements to SS.7 protocol transport capabilities must be performed. A protocol for the distribution of FIPA ACL messages in SS.7 networks must also be defined. Analysis of the possibilities shows that there are different design choices depending upon the exact deployment model and agent capabilities required. In addition it is necessary to allow the use of SS.7 addresses within the FIPA management protocols. An evaluation of best common practice for CORBA-based FIPA implementations in the SS.7 domain must also be defined. This treatment highlights the failings of the current FIPA model when applied to the real-time, bandwidth-constrained, specialised protocol environment of telecommunications signalling.", "authors": ["Robert W. Brennan", "Brendan Jennings", "Thomas Curran"], "n_citation": 0, "title": "Signalling System No. 7 as an agent platform message transport protocol", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "32cdbc05-3012-4257-9074-b1217727ed56"}
{"abstract": "This contribution proposes a compositional approach to visual object categorization of scenes. Compositions are learned from the Caltech 101 database 1  and form intermediate abstractions of images that are semantically situated between low-level representations and the high-level categorization. Salient regions, which are described by localized feature histograms, are detected as image parts. Subsequently compositions are formed as bags of parts with a locality constraint. After performing a spatial binding of compositions by means of a shape model, coupled probabilistic kernel classifiers are applied thereupon to establish the final image categorization. In contrast to the discriminative training of the categorizer, intermediate compositions are learned in a generative manner yielding relevant part agglomerations, i.e. groupings which are frequently appearing in the dataset while simultaneously supporting the discrimination between sets of categories. Consequently, compositionality simplifies the learning of a complex categorization model for complete scenes by splitting it up into simpler, sharable compositions. The architecture is evaluated on the highly challenging Caltech 101 database which exhibits large intra-category variations. Our compositional approach shows competitive retrieval rates in the range of 53.6\u00b10.88% or, with a multi-scale feature set, rates of 57.8 \u00b1 0.79%.", "authors": ["Bj\u00f6rn Ommer", "Joachim M. Buhmann"], "n_citation": 56, "title": "Learning Compositional Categorization Models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "32dee8bc-22e3-4db9-af2f-0b0c492a2da7"}
{"abstract": "Electronically available data on the Web is exploding at an ever increasing pace. Much of this data is unstructured, which makes searching hard and traditional database querying impossible. Many Web documents, however, contain an abundance of recognizable constants that together describe the essence of a document's content. For these kinds of data-rich documents (e.g., advertisements, movie reviews, weather reports, travel information, sports summaries, financial statements, obituaries, and many others) we can apply a conceptual-modeling approach to extract and structure data. The approach is based on an ontology-a conceptual model instance-that describes the data of interest, including relationships, lexical appearance, and context keywords. By parsing the ontology, we can automatically produce a database scheme and recognizers for constants and keywords, and then invoke routines to recognize and extract data from unstructured documents and structure it according to the generated database scheme. Experiments show that it is possible to achieve good recall and precision ratios for documents that are rich in recognizable constants and narrow in ontological breadth.", "authors": ["David W. Embley", "Douglas M. Campbell", "Yuejun Jiang", "Stephen W. Liddle", "Yiu Kai Ng", "Dallan Quass", "Randy D. Smith"], "n_citation": 0, "title": "A conceptual-modeling approach to extracting data from the Web", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "36953494-6136-4e41-a2bf-d7cfd2b8003b"}
{"authors": ["Joerg Endrullis", "Clemens Grabmayer", "R.D.A. Hendriks", "Vu1012417", "Faculteit der Exacte Wetenschappen"], "n_citation": 21, "title": "Data-Oblivious Stream Productivity", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "3781ef92-60a0-48fa-9349-999ef4c62b1d"}
{"abstract": "We will completely describe the solutions of the equation (x+1) d = x d +1 in the field GF(q 2 ), where q= p k  and d is of Niho type, i.e., d\u2261 1 (mod q-1). Our results have applications in the theory of cross-correlation functions of m-sequences and in the theory of cyclic codes.", "authors": ["Kalle Ranto", "Petri Rosendahl"], "n_citation": 0, "title": "The Solutions of the Third Power Sum Equation for Niho Type Decimations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "38cc7656-9527-46f5-b90c-85f4deb71b54"}
{"abstract": "The explosion of information on the web and the increased trend towards seeking information from the web has given rise to a need for a mechanism that would allow querying from a group of sites simultaneously. Visualization of all the retrieved data tiled appropriately on a single screen goes a long way in assisting the user in assimilating the retrieved information. A new mechanism for blended querying is suggested here, which comprises a two-fold mechanism of prioritized multi-database querying which retrieves a series of combinations in the prioritized order from different web sites and summary querying which allows further querying on the previously retrieved result. The associated visualization tool simultaneously serums the various results retrieved into multiple windows, which are tiled on the screen so that all are visible at once. The implementation details are also described in this paper.", "authors": ["Mona Marathe", "Hemalatha Diwakar"], "n_citation": 50, "title": "Architecture of a blended-query and result-visualization mechanism for Web-accessible databases and associated implementation issues", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "39b8f29c-e1c0-44d2-a5b3-3f74a6f00cd0"}
{"abstract": "In this paper, we present a new view of multiclass classification and introduce the constraint classification problem, a generalization that captures many flavors of multiclass classification. We provide the first optimal, distribution independent bounds for many multiclass learning algorithms, including winner-take-all (WTA). Based on our view, we present a learning algorithm that learns via a single linear classifier in high dimension. In addition to the distribution independent bounds, we provide a simple margin-based analysis improving generalization bounds for linear multiclass support vector machines.", "authors": ["Sariel Har-Peled", "Dan Roth", "Dav Zimak"], "n_citation": 0, "title": "Constraint classification: A new approach to multiclass classification", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3a0db9e2-18aa-4843-86d3-522c28b6fab0"}
{"abstract": "Using Markov chains, we systematically compute all the truncated differentials of Skipjack, assuming the nonlinear G boxes are random permutations. We prove that an attacker with one random truncated differential from each of 2 128  independently-keyed encryption oracles has advantage of less than 2 -16  in distinguishing whether the oracles are random permutations or the Skipjack algorithm.", "authors": ["Ben W. Reichardt", "David Wagner"], "n_citation": 50, "title": "Markov truncated differential cryptanalysis of Skipjack", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "3af23e17-5c3b-481e-ac10-dfdc881a3994"}
{"abstract": "Dining cryptographers networks (or DC-nets) are a privacy-preserving primitive devised by Chaum for anonymous message publication. A very attractive feature of the basic DC-net is its non-interactivity. Subsequent to key establishment, players may publish their messages in a single broadcast round, with no player-to-player communication. This feature is not possible in other privacy-preserving tools like mixnets. A drawback to DC-nets, however, is that malicious players can easily jam them, i.e., corrupt or block the transmission of messages from honest parties, and may do so without being traced. Several researchers have proposed valuable methods of detecting cheating players in DC-nets. This is usually at the cost, however, of multiple broadcast rounds, even in the optimistic case, and often of high computational and/or communications overhead, particularly for fault recovery. We present new DC-net constructions that simultaneously achieve non-interactivity and high-probability detection and identification of cheating players. Our proposals are quite efficient, imposing a basic cost that is linear in the number of participating players. Moreover, even in the case of cheating in our proposed system, just one additional broadcast round suffices for full fault recovery. Among other tools, our constructions employ bilinear maps, a recently popular cryptographic technique for reducing communication complexity.", "authors": ["Philippe Golle", "Ari Juels"], "n_citation": 0, "title": "Dining cryptographers revisited", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3f45d494-7731-4a6c-b35e-1ab2018c6057"}
{"authors": ["Celine Vens", "Sofie Van Gassen", "Tom Dhaene", "Yvan Saeys"], "n_citation": 0, "title": "Complex aggregates over clusters of elements", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "4364f8c9-805f-4656-bc96-53e2ca69f852"}
{"abstract": "This paper proposes an effective scoring scheme for feature selection in Text Mining, using characteristics of Small-World Phenomenon on the semantic networks of documents. Our focus is on the reservation of both syntactic and statistical information of words, rather than solely simple frequency summarization in prevailing scoring schemes, such as TFIDF. Experimental results on TREC dataset show that our scoring scheme outperforms the prevailing schemes.", "authors": ["Chong Huang", "Yonghong Tian", "Tiejun Huang", "Wen Gao"], "n_citation": 0, "title": "Semantic Scoring Based on Small-World Phenomenon for Feature Selection in Text Mining", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4447d0c8-8b71-4664-bc14-86a2086567dd"}
{"abstract": "The travelling salesman problem with time windows is a difficult optimization problem that appears, for example, in logistics. Among the possible objective functions we chose the optimization of the makespan. For solving this problem we propose a so-called Beam-ACO algorithm, which is a hybrid method that combines ant colony optimization with beam search. In general, Beam-ACO algorithms heavily rely on accurate and computationally inexpensive bounding information for differentiating between partial solutions. In this work we use stochastic sampling as an alternative to bounding information. Our results clearly demonstrate that the proposed algorithm is currently a state-of-the-art method for the tackled problem.", "authors": ["M Ibanez", "Christian Blum", "Dhananjay R. Thiruvady", "Andreas T. Ernst", "Bernd Meyer"], "n_citation": 7, "title": "Beam-ACO based on stochastic sampling for makespan optimization concerning the TSP with time windows.", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "4487fdf6-e004-4522-b594-7ce0860ce5d9"}
{"authors": ["Adnan Darwiche"], "n_citation": 50, "title": "Searching while keeping a trace : The evolution from satisfiability to knowledge compilation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "46c41013-d51e-48cb-ae55-511b2ed9597b"}
{"abstract": "Network intrusion detection systems often rely on matching patterns that are learned from known attacks. While this method is reliable and rarely produces false alarms, it has the disadvantage that it cannot detect novel attacks. An alternative approach is to learn a model of normal traffic and report deviations, but these anomaly models are typically restricted to modeling IP addresses and ports. We describe an anomaly detection system which models all the fields of network, transport layer and payload of a packet at the byte level, by giving more weight to the most anomalous attributes. We investigated all the attributes and assigned weights to the attributes based on their anomalous behavior. We detect 144 of 185 attacks in the DARPA off-line intrusion detection evaluation data set [1] at 10 false alarms per day (total 100 false alarms), after training on one week of attack-free traffic. We investigate the performance of the system when attack free training data is not available.", "authors": ["Suresh Reddy", "Sukumar Nandi"], "n_citation": 0, "title": "Enhanced network traffic anomaly detector", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "47d03154-e8c0-45de-827a-ad50c434dff4"}
{"abstract": "For fitting an ellipse to a point sequence, ML (maximum likelihood) has been regarded as having the highest accuracy. In this paper, we demonstrate the existence of a hyperaccurate method which outperforms ML. This is made possible by error analysis of ML followed by subtraction of high-order bias terms. Since ML nearly achieves the theoretical accuracy bound (the KCR lower bound), the resulting improvement is very small. Nevertheless, our analysis has theoretical significance, illuminating the relationship between ML and the KCR lower bound.", "authors": ["Kenichi Kanatani"], "n_citation": 50, "title": "Ellipse Fitting with Hyperaccuracy", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "47fa8920-6555-479d-b008-49ad29b1daba"}
{"abstract": "In this paper we present a new decision procedure for the satisfiability of Linear Arithmetic Logic (LAL), i.e. boolean combinations of propositional variables and linear constraints over numerical variables. Our approach is based on the well known integration of a propositional SAT procedure with theory deciders, enhanced in the following ways. First, our procedure relies on an incremental solver for linear arithmetic, that is able to exploit the fact that it is repeatedly called to analyze sequences of increasingly large sets of constraints. Reasoning in the theory of LA interacts with the boolean top level by means of a stack-based interface, that enables the top level to add constraints, set points of backtracking, and backjump, without restarting the procedure from scratch at every call. Sets of inconsistent constraints are found and used to drive backjumping and learning at the boolean level, and theory atoms that are consequences of the current partial assignment are inferred. Second, the solver is layered: a satisfying assignment is constructed by reasoning at different levels of abstractions (logic of equality, real values, and integer solutions). Cheaper, more abstract solvers are called first, and unsatisfiability at higher levels is used to prune the search. In addition, theory reasoning is partitioned in different clusters, and tightly integrated with boolean reasoning. We demonstrate the effectiveness of our approach by means of a thorough experimental evaluation: our approach is competitive with and often superior to several state-of-the-art decision procedures.", "authors": ["Marco Bozzano", "Roberto Bruttomesso", "Alessandro Cimatti", "Tommi A. Junttila", "Peter van Rossum", "Stephan Schulz", "Roberto Sebastiani"], "n_citation": 0, "title": "An incremental and layered procedure for the satisfiability of linear arithmetic logic", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "480c78fb-e65a-4586-9676-792125491609"}
{"abstract": "TBE (Trigger-By-Example) is proposed to assist users in writing trigger rules. TBE is a graphical trigger rule specification language and system to help users understand and specify active database triggers. Since TBE borrowed its basic idea from QBE, it retained many benefits of QBE while extending the features to support triggers. Hence, TBE is a useful tool for novice users to create simple trigger rules easily. Further, since TBE is designed to insulate the details of underlying trigger systems from users, it can be used as a universal trigger interface for rule formation.", "authors": ["Dongwon Lee", "Wenlei Mao", "Wesley W. Chu"], "n_citation": 50, "title": "TBE: Trigger-by-example", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "4860a927-7f6c-48c5-ab78-dfa0b4f30a68"}
{"abstract": "Advanced Data Mining and Applications: 5th International Conference, ADMA 2009, Beijing, China, August 17-19, 2009. Proceedings", "authors": ["Tomonari Masada", "Tsuyoshi Hamada", "Yuichiro Shibata", "Kiyoshi Oguri"], "n_citation": 0, "title": "Bayesian multi-topic microarray analysis with hyperparameter reestimation", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "489e8697-6598-47fd-b76c-6919acad657b"}
{"abstract": "This paper presents SALT. SALT is a general purpose specification and assertion language developed for creating concise temporal specifications to be used in industrial verification environments. It incorporates ideas of existing approaches, such as specification patterns, but also provides nested scopes, exceptions, support for regular expressions and real-time. The latter is needed in particular for verification tasks to do with reactive systems imposing strict execution times and deadlines. However, unlike other formalisms used for temporal specification of properties, SALT does not target a specific domain. The paper details on the design rationale, syntax and semantics of SALT in terms of a translation to temporal (real-time) logic, as well as on the realisation in form of a compiler. Our results will show that the higher level of abstraction introduced with SALT does not deprave the efficiency of the subsequent verification tools-rather, on the contrary.", "authors": ["Andreas Bauer", "Martin Leucker", "Jonathan Streit"], "n_citation": 0, "title": "SALT-Structured Assertion Language for Temporal Logic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4b9c47c8-dda3-4323-9709-78f751bba554"}
{"abstract": "A model and a Java Framework to aid the construction of 3D Web-based Collaborative Virtual Environments (3D-CVE) are described in this paper. The proposal emphasizes on the interaction and collaboration among the entities (users' avatars, agents, etc.) that populate the virtual world; and the services that they offer to each other in order to interact, or to carry out some kind of collaborative task. We propose a model for the conceptualization of CVE under the concept of social groups, a graph-based high level notation to specify the interactions among the entities that populate the CVE, and a Java based software framework that gives support to the model and the interaction graph in order to facilitate the implementation of the 3D-CVE. From the developing process point of view, the goal of the collaboration model and the framework proposed here is not only to develop 3D-CVE faster, but also the resulting 3D-CVEs have similar structures, they are easier to maintain and eventually to integrate. Design patterns were very useful to fulfill these characteristics, as we describe in the next sections. CVEs developed with the framework enable, through the Web browser, a shared understanding regarding many scientific. engineering and entertainment subjects concerned with three-dimensional data.", "authors": ["Leandro Balladares", "Rolando Menchaca", "Rub\u00e9n Peredo"], "n_citation": 0, "title": "3D collaborative virtual environments over the web", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4c8ab651-57b7-468c-bdbf-9c6753007504"}
{"abstract": "During the last decade research groups as well as a number of commercial software developers have started to deploy embodied conversational characters in the user interface especially in those application areas where a close emulation of multimodal human-human communication is needed. Most of these characters have one thing in common: In order to enter the user's physical world, they need to be physical themselves. The paper focuses on challenges that arise when embedding synthetic conversational agents in the user's physical world. We will start from work on synthetic agents that populate virtual worlds and anthropomorphic robots that inhabit physical worlds and discuss how the two areas need to be combined in order to populate physical worlds with synthetic characters. Finally, we will report on so-called traversable interfaces that allow agents to cross the border from the physical space to the virtual space and vice versa.", "authors": ["Elisabeth Andr\u00e9", "Klaus Dorfm\u00fcller-Ulhaas", "Matthias Rehm"], "n_citation": 0, "title": "Engaging in a conversation with synthetic characters along the virtuality continuum", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4e26ccfc-e0eb-4d4a-b695-02fa10883c2c"}
{"abstract": "A regular pattern is a string of constant symbols and distinct variables. A semantics of a set P of regular patterns is a union L(P) of erasing pattern languages generated by patterns in P. The paper deals with the class RP k  of sets of at most k regular patterns, and an efficient learning from positive examples of the language class defined by RP k . In efficient learning languages, the complexity for the MINL problem to find one of minimal languages containing a given sample is one of very important keys. Arimura et al.[5] introduced a notion of compactness w.r.t. containment for more general framework, called generalization systems, than RP k  of language description which guarantees the equivalency between the semantic containment L(P) C L(Q) and the syntactic containment P C Q, where C is a syntactic subsumption over the generalization systems. Under the compactness, the MINL problem reduces to finding one of minimal sets in RP k  for a given sample under the subsumption C. They gave an efficient algorithm to find such minimal sets under the assumption of compactness and some conditions. We first show that for each k > 1, the class RP k  has compactness if and only if the number of constant symbols is greater than k+1. Moreover, we prove that for each P \u2208 RP k , a finite subset S 2 (P) is a characteristic set of L(P) within the class, where S 2  (P) consists of strings obtained from P by substituting strings with length two for each variable. Then our class RP k  is shown to be polynomial time inferable from positive examples using the efficient algorithm of the MINL problem due to Arimura et al.[5], provided the number of constant symbols is greater than k + 1.", "authors": ["Jin Uemura", "Masako Sato"], "n_citation": 0, "title": "Compactness and learning of classes of unions of erasing regular pattern languages", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "503c714a-546f-4879-9fc2-43415900f99f"}
{"abstract": "During evolution, chromosomal rearrangements, such as reciprocal translocation, transposition and inversion, disrupt gene content and gene order on chromosomes. We discuss algorithmic and statistical approaches to the analysis of comparative genomic data. In a phylogenetic context, a combined approach is suggested, leading to the median problem for breakpoints. We solve this problem first for the case where all genomes have the same gene content, and then for the general case.", "authors": ["David Sankoff", "Mathieu Blanchette"], "n_citation": 0, "title": "The median problem for breakpoints in comparative genomics", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "50665096-8003-4141-8f49-fe45f184b955"}
{"abstract": "Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8% miss rate on our Test Set 1.", "authors": ["Navneet Dalal", "Bill Triggs", "Cordelia Schmid"], "n_citation": 0, "title": "Human Detection Using Oriented Histograms of Flow and Appearance", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "50dd226e-b05d-4e1f-a055-f317dba148ee"}
{"abstract": "Variability is a central issue in deep submicron technologies, in which it becomes increasingly difficult to produce two chips with the same behavior. While the impact of variability is well understood from the microelectronic point of view, very few works investigated its significance for cryptographic implementations. This is an important concern as 65-nanometer and smaller technologies are soon going to equip an increasing number of security-enabled devices. Based on measurements performed on 20 prototype chips of an AES S-box, this paper provides the first comprehensive treatment of variability issues for side-channel attacks. We show that technology scaling implies important changes in terms of physical security. First, common leakage models (e.g. based on the Hamming weight of the manipulated data) are no longer valid as the size of transistors shrinks, even for standard CMOS circuits. This impacts both the evaluation of hardware countermeasures and formal works assuming that independent computations lead to independent leakage. Second, we discuss the consequences of variability for profiled side-channel attacks. We study the extend to which a leakage model that is carefully profiled for one device can lead to successful attacks against another device. We also define the perceived information to quantify this context, which generalizes the notion of mutual information with possibly degraded leakage models. Our results exhibit that existing side-channel attacks are not perfectly suited to this new context. They constitute an important step in better understanding the challenges raised by future technologies for the theory and practice of leakage resilient cryptography.", "authors": ["Mathieu Renauld", "Fran\u00e7ois-Xavier Standaert", "Nicolas Veyrat-Charvillon", "Dina Kamel", "Denis Flandre"], "n_citation": 109, "title": "A Formal Study of Power Variability Issues and Side-Channel Attacks for Nanoscale Devices", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "51485b6f-7f8d-4cd4-b2df-383fdb0691b8"}
{"abstract": "The paper describes the current regulatory situation in England with respect to medical devices and healthcare providers. Trusts already produce evidence to the Healthcare Commission that they operate in accordance with standards set out by the Department of Health and the NHS. The paper illustrates how the adoption of an explicit goal-based argument could facilitate the identification and assessment of secondary implications of proposed changes. The NHS is undergoing major changes in accordance with its 10-year modernisation plan. These changes cannot be confined to the Trust level, but will have NHS-wide implications. The paper explores the possibility of an organisational safety case, which could be a useful tool in the management of such fundamental changes.", "authors": ["Mark A. Sujan", "Michael D. Harrison", "Alison Steven", "Pauline Pearson", "Susan J. Vernon"], "n_citation": 0, "title": "Demonstration of Safety in Healthcare Organisations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "53356e0d-ddb3-4b65-8e23-c962f01f84dd"}
{"abstract": "In large deployments of H.323 based Voice-over-IP (VoIP) systems, achieving the desired availability is a major challenge. A major factor in determining the availability is the resilience of the recovery mechanisms in the H.323 protocol suite against server and network failures. In this paper, we focus on the registration aspect of the H.225 protocol in H.323 suite. Specifically, we tackle the registration-flood problem which occurs after a server or network failure, when more IP Phones attempt to register with the VoIP server than the server can handle. The most significant ramification of overload is longer registration times resulting in lower overall availability of the VoIP system. Existing solutions to mitigate registration-floods are either server centric or network centric. In this paper, we propose a complementary end-point based technique using random back-off. Discrete event simulation based evaluation shows that the proposed technique can yield significant reduction in the recovery time thereby increasing service availability. We also compare the performance of existing solutions with the proposed technique, particularly the relative effect of network delay and loss on the performance of the techniques.", "authors": ["Sachin Garg", "Chandra M. R. Kintala", "David Stott"], "n_citation": 0, "title": "Reducing the Recovery Time of IP-Phones in an H.323 Based VoIP System", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "55e214b5-7756-4309-95c2-3f64f0b33796"}
{"abstract": "Given a multivariate polynomial P(X 1 ,..., X n ) over a finite field F q , let N(P) denote the number of roots over F n  q . The modular root counting problem is given a modulus r, to determine N r (P) = N(P) mod r. We study the complexity of computing N r (P), when the polynomial is given as a sum of monomials. We give an efficient algorithm to compute N r (P) when the modulus r is a power of the characteristic of the field. We show that for all other moduli, the problem of computing N r (P) is NP-hard. We present some hardness results which imply that that our algorithm is essentially optimal for prime fields. We show an equivalence between maximum-likelihood decoding for Reed-Solomon codes and a root-finding problem for symmetric polynomials.", "authors": ["Parikshit Gopalan", "Venkatesan Guruswam", "Richard J. Lipton"], "n_citation": 50, "title": "Algorithms for modular counting of roots of multivariate polynomials", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "58730522-0c7a-484b-a3d7-3339553779dd"}
{"abstract": "The pages on the World Wide Web and their hyperlinks induce a huge directed graph - the Web Graph. Many models have been brought up to explain the static and dynamic properties of the graph. Most of them pay much attention to the pages without considering their essential relations. In fact, Web pages are well organized in Web sites as a tree hierarchy. In this paper, we propose a hierarchical model of Web graph which exploits both link structure and hierarchical relations of Web pages. The analysis of the model reveals many properties about the evolution of pages, sites and the relation among them.", "authors": ["Jie Han", "Yong Yu", "Chenxi Lin", "Dingyi Han", "Gui-Rong Xue"], "n_citation": 0, "title": "A Hierarchical Model of Web Graph", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "58d1c3a4-10c7-47e9-8bf8-ce8379f3192c"}
{"abstract": "In this paper we propose a multi-set modulation technique to increase the hiding capacity within a binary document image. As part of this technique we propose an Automatic Threshold Calculation and Threshold Buffering, Shifted Space Distribution and Letter Space Compensation technique. The Automatic Threshold Calculation is used to distinguish word spaces from letter spaces. The Threshold Buffering is used to reduce the chance of misinterpretation of spaces during the detection phase, following printing and scanning. The Shifted Space Distribution and Letter Space Compensation techniques robustly embed a watermark into the binary document image. The Automatic Threshold Calculation has been shown to be successful in identifying word spaces for different types of fonts and font sizes. The combination of the Shifted Space Distribution, Letter Space Compensation and Threshold Buffering techniques have been shown to create a watermark that is robust to printing and scanning.", "authors": ["Chris Culnane", "Helen Treharne", "Anthony T. S. Ho"], "n_citation": 0, "title": "A new multi-set modulation technique for increasing hiding capacity of binary watermark for print and scan processes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "59113e7f-14b2-4dfb-a292-5e085bb0f3ce"}
{"abstract": "Corollary to the development of new kinds of application - like decision support ones - manipulating large quantities of data structured in new kinds of data supports -like data warehouses, queries formulated to access this data have been growing in their complexity. However, with the exception of approaches facilitating syntax problems, there is a noticeable absence of models taking charge of the user through from specification of their needs to formula' tion of their queries. In the case of object-oriented database systems, whose data models and concepts are more complex than those of relational databases, database users need assistance with several kind of formulation problems. In this article, we take a novel look at object-oriented queries - reifying the ones which users formulate as components then reusing them, by means of strategies for selecting, assembling and adapting them to help in the formulation of new complex queries.", "authors": ["Chabane Oussalah", "Abdelhak Seriai"], "n_citation": 0, "title": "A reuse-based object-oriented framework towards easy formulation of complex queries", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "5a8b8125-82f9-4179-a6fd-2e22a16bc3b1"}
{"abstract": "This paper describes an e-commerce application build on the Electronic Trading Opportunities System. This system enables Trade Points' and trade related bodies to exchange information by e-mail. This environment offers an enormous trade potential and opportunities to small and medium enterprises, but its efficiency is limited since the amount of circulating messages surpasses the human limit to analyze them. The application described here aids this process of analysis, allowing the extraction of the most relevant characteristics from the messages. The application is structured in three phases. The first is responsible for analyzing and for providing structural information about texts. The second identifies relevant information on texts through clustering and categorization processes. The third applies Information Extraction techniques, which are aided by the use of a domain specific knowledge base, to transform the unstructured information into a structured one. By the end, the user gets more quality in the analysis and can more easily find interesting ideas, trends and details, creating new trade opportunities to small and medium enterprises.", "authors": ["Rui Gureghian Scarinci", "Leandro Krug Wives", "Stanley Loh", "Christian Zabenedetti", "Josh Palazzo Moreira De Oliveira"], "n_citation": 0, "title": "Managing unstructured E-commerce information", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5b2ca080-460a-40bd-a0cd-df29f91fd360"}
{"abstract": "Authentication, Authorization, and Accounting (AAA) for a mobile node should be accomplished by home domain when the mobile need continuous service on a visited network. The related recent studies have shown their drawback in the performance of AAA procedure. This study suggests a novel approach extending to the Fast Handoff scheme, which will shorten authentication delay by using Assertion mechanism. It allows mobile nodes to access visited network resources efficiently. Our model with Assertion process is an efficient approach employing authentication procedure through mutual and secure authentication between the Visit AAA servers. Especially, when the distance or the network delay between V_AAA and Home Agent (HA) become longer, it outperforms rather than the previous approaches. The proposed scheme verifies its significant efficiency in terms of cost analysis through several simulated experiments.", "authors": ["Seung-Yeon Lee", "Eui-Nam Huh", "Yang-Woo Kim", "Kyesan Lee"], "n_citation": 0, "title": "An Efficient Authentication Mechanism for Fast Mobility Service in MIPv6", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5bcb74ee-48ca-41dd-9cb8-445c9d3882e5"}
{"abstract": "The objectives of this paper is to survey classical and recent expressive completeness results and to provide some external yardsticks by which the expressive power of temporal logics can be measured.", "authors": ["Alexander Rabinovich"], "n_citation": 50, "title": "Expressive power of temporal logics", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5ed636de-95cb-4222-886f-6bd828447bc3"}
{"abstract": "In this paper, we present a digital repair method for archeological relics with some vacant areas. We first use the simple method to find the neighboring area which influences the shape of the vacant area. Then we construct the bi-cubic Bezier spline surface to interpolate given points in the influencing area. Re-sampling points on the bi-cubic Bezier spline surface, we can use triangle meshes to repair vacant areas. The advantage of this method is that the repaired area can smoothly be patched with other parts of archeological relics and it can keep the real shape of vacant areas.", "authors": ["Zhong Li", "Lizhuang Ma", "Mingxi Zhao", "Zhihong Mao"], "n_citation": 0, "title": "Digital repair research on archeological relics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5fa266c1-d686-4fce-a322-fc9a4fc2efe9"}
{"abstract": "We introduce a new class O p  2  as a subclass of the symmetric alternation class S p  2 . An O p  2  proof system has the flavor of an S p  2  proof system, but it is more restrictive in nature. In an S p  2  proof system, we have two competing provers and a verifier such that for any input, the honest prover has an irrefutable certificate. In an O p  2 proof system, we require that the irrefutable certificates depend only on the length of the input, not on the input itself. In other words, the irrefutable proofs are oblivious of the input. For this reason, we call the new class oblivious symmetric alternation. While this might seem slightly contrived, it turns out that this class helps us improve some existing results. For instance, we show that if NP C P/poly then PH = O p  2 , whereas the best known collapse under the same hypothesis was PH = S p  2 . We also define classes YO p  2  and NO p  2 , bearing relations to O p  2  as NP and coNP are to P, and show that these along with O p  2  form a hierarchy, similar to the polynomial hierarchy. We investigate other inclusions involving these classes and strengthen some known results. For example, we show that MA C NO p  2  which sharpens the known result MA C S p  2  [16]. Another example is our result that AM C O p  2 . NP C \u03a0 p  2 , which is an improved upper bound on AM. Finally, we also prove better collapses for the 2-queries problem as discussed by [12,1,7]. We prove that P NP[1]  = P NP[2]  \u21d2 PH = NO p  2  \u2229 YO p  2 .", "authors": ["Venkatesan T. Chakaravarthy", "Sambuddha Roy"], "n_citation": 0, "title": "Oblivious symmetric alternation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6780770e-5b81-4c20-9f0e-c9d9f6c31489"}
{"abstract": "The application schema based on Web Services is developing quickly, and the component-based software architecture is also researched more and more deeply. The traditional data center architecture based on database service method has been unfit for this technical progress. According to requirements of specific domain data resources development and utilization, a service-oriented architecture of specific domain data center was proposed in this paper. Its essential characteristics are that database and data processing closely combined into data services to be distributed in networks, these services are registered to the service registry center, and data resources are transparently processed by requesting the corresponding data services. The China National Water Resources Data Center is an instance of the service-oriented architecture of specific do-main data center.", "authors": ["Ping Ai", "Zhi-Jing Wang", "Yingchi Mao"], "n_citation": 0, "title": "Service-oriented architecture of specific domain Data Center", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6867cd22-e87b-4a6b-9c11-02faafe2811e"}
{"abstract": "The calculus of structures is a proof theoretical formalism which generalizes the sequent calculus with the feature of deep inference: In contrast to the sequent calculus, inference rules can be applied at any depth inside a formula, bringing shorter proofs than any other formalisms supporting analytical proofs. However, deep applicability of the inference rules causes greater nondeterminism than in the sequent calculus regarding proof search. In this paper, we introduce a new technique which reduces nondeterminism without breaking proof theoretical properties and provides a more immediate access to shorter proofs. We present this technique on system BV, the smallest technically non-trivial system in the calculus of structures, extending multiplicative linear logic with the rules mix, nullary mix, and a self-dual non-commutative logical operator. Because our technique exploits a scheme common to all the systems in the calculus of structures, we argue that it generalizes to these systems for classical logic, linear logic, and modal logics.", "authors": ["Ozan Kahramano\u011fullar\u0131"], "n_citation": 0, "title": "Reducing nondeterminism in the calculus of structures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "695d7ae7-acc3-4840-a9bf-91af398c9036"}
{"abstract": "The Unified Modeling Language (UML) has become the de facto standard for object-oriented analysis and design, providing different diagrams for modeling different aspects of a system. In this paper, we present the development of multidimensional (MD) models for data warehouses (DW) using UML package diagrams. In this way, when modeling complex and large DW systems, we are not restricted to use flat UML class diagrams. We present design guidelines and illustrate them with various examples. We show that the correct use of the package diagrams using our design guidelines will produce a very simple yet powerful design of MD models. Furthermore, we provide a UML extension by means of stereotypes of the particular package items we use. Finally, we show how to use these stereotypes in Rational Rose 2000 for MD modeling.", "authors": ["Sergio Luj\u00e1n-Mora", "Juan Trujillo", "Il-Yeol Song"], "n_citation": 0, "title": "Multidimensional Modeling with UML package diagrams", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "697b449c-c62a-480a-8d4c-a3bb02d867d0"}
{"abstract": "Information Bottleneck method can be used as a dimensionality reduction approach by grouping \u201csimilar\u201d features together [1]. In application, a natural question is how many \u201cfeatures groups\u201d will be appropriate. The dependency on prior knowledge restricts the applications of many Information Bottleneck algorithms. In this paper we alleviate this dependency by formulating the parameter determination as a model selection problem, and solve it using the minimum message length principle. An efficient encoding scheme is designed to describe the information bottleneck solutions and the original data, then the minimum message length principle is incorporated to automatically determine the optimal cardinality value. Empirical results in the documentation clustering scenario indicates that the proposed method works well for the determination of the optimal parameter value for information bottleneck method.", "authors": ["Gang Li", "Dong Liu", "Yiqing Tu", "Yangdong Ye"], "n_citation": 0, "title": "Finding the optimal cardinality value for information bottleneck method", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "69f89f80-891e-49ba-ad0a-9a54c1e1f83b"}
{"abstract": "MIPv6 provides the L3 connectivity when the IPv6 mobile node moves between subnets. Nevertheless, the mobile node cannot receive IP packet because of the handover latency. The handover latency is not efficient to provide the real-time multimedia application service. Notable protocols of the extensions of MIPv6 are FMIPv6 and HMIPv6. In HMIPv6, if the mobile node moves from one access router to another in the different MAP domain, such a movement is called macro mobility handover, then the mobile node creates a new RCoA and LCoA and performs registration with the new MAP and HA. Until the address registration with MAP and HA complete, the mobile node cannot receive IP packet. Therefore, we need to execute the macro mobility handover efficiently and reduce the handover latency and packet loss. We propose a method to perform the macro mobility handover efficiently in HMIPv6. To provide seamless service and minimize packet loss when the mobile node performs the macro mobility handover, we will adjust the fast handover technology of FMIPv6 to the MAP's characteristics in the proposed scheme. To reduce the handover latency and packet loss, we use a tunnel between the edge access routers and perform the L3 handover earlier before the L2 handover. We compare the procedure of the macro mobility handover of the proposed scheme with the original HMIPv6 by means of using the cost analysis comparison. We observe that the proposed scheme can be reduced the 82% of the total cost of the macro mobility handover of the original HMIPv6.", "authors": ["Kyunghye Lee", "Young-Hwan Lim", "Seongjin Ahn", "Youngsong Mun"], "n_citation": 0, "title": "A Macro Mobility Handover Performance Improvement Scheme for HMIPv6", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6ebebd70-60c3-4b2f-9eb2-63a95ba192e9"}
{"abstract": "The air traffic management system is under generalized upgrading process. New requirements of safety arise based on several emerging technologies. This work presents a model to automatically verify safety of actions taken by human traffic controllers in frequent situations. The model uses the formalism of hybrid automata, and consists basically on segmenting the space of routes and associating each segment to a location of the automaton. Some kind of trajectory optimisation is also possible with this model.", "authors": ["\u00cdtalo Romani de Oliveira", "Paulo S\u00e9rgio Cugnasca"], "n_citation": 0, "title": "Checking safe trajectories of aircraft using hybrid automata", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "710f68c6-da6b-4438-83ea-71f675047942"}
{"abstract": "Packet loss and delay in Internet degrade the quality of requested services like VoIP (Voice over IP) or Video Streaming. In novel network scenarios where wired and wireless connections are melted together, a real measure of these parameters is fundamental in a planning process of new services over novel network infrastructures. Nowadays networks are heterogeneous in terms of access network technologies (wired LAN Ethernet 10/100/1000, Wireless LAN - 802.11a, 802.11b, 802.11g -, GPRS, UMTS, GSM, Bluetooth,...), end-users' devices (workstation, PC desktop, Laptop/Notebook, PDA, Advanced Mobile Phone,...) and finally operating systems (Unix, Linux, Win 98/NT/2000/XP, Win CE, Linux Familiar, OS Embedded,...). In this work we provide a heterogeneous network performance characterization with respect to delay and throughput in UDP and TCP environments. In order to determine our results we use an innovative traffic generator named D-ITG (Distributed Internet Traffic Generator). Results presented in this paper can be used as performance references for development of wireless communication applications over multiservice and heterogeneous networks.", "authors": ["Giulio Iannello", "Antonio Pescape", "Giorgio Ventre", "Luca Vollero"], "n_citation": 0, "title": "Experimental analysis of heterogeneous Wireless networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7113b1ad-6a3d-42c3-abff-d87d9890d545"}
{"abstract": "This paper presents a hybrid metaheuristic to address the vehicle routing problem with time windows (VRPTW). The VRPTW can be described as the problem of designing least cost routes from a depot to geographically dispersed customers. The routes must be designed such that each customer is visited only once by exactly one vehicle without violating capacity and time window constraints. The proposed solution method is a multi-start local search approach which combines reactively the systematic diversification mechanisms of Greedy Randomized Adaptive Search Procedures with a novel Variable Neighborhood Tabu Search hybrid metaheuristic for intensification search. Experimental results on well known benchmark instances show that the suggested method is both efficient and robust in terms of the quality of the solutions produced.", "authors": ["Panagiotis P. Repoussis", "Dimitris C. Paraskevopoulos", "Christos D. Tarantilis", "George Ioannou"], "n_citation": 0, "title": "A reactive greedy randomized variable neighborhood tabu search for the vehicle routing problem with time windows", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7115bd58-cc93-4d3e-84d0-986411d8d0d8"}
{"abstract": "In this paper, we consider the problem of enumerating all maximal cliques in a complex network G = (V, E) with n vertices and m edges. We propose an algorithm for enumerating all maximal cliques based on researches of the complex network properties. A novel branch and bound strategy by considering the clustering coefficient of a vertex is proposed. Our algorithm runs with time O [d ^ 2*N*S) delay and in O (n + m) space. It requires O (n*D^2) time as a preprocessing, where D, N, S, d denote the maximum degree of G, the number of maximal cliques, the size of the maximum clique, and the number of triangles of a vertex with degree D respectively. Finally, we apply our algorithm to the telecommunication customer-chum-prediction and the experimental results show that the application promotes the capabilities of the churn prediction system effectively.", "authors": ["Li Wan", "Bin Wu", "Nan Du", "Qi Ye", "Ping Chen"], "n_citation": 0, "title": "A New Algorithm for Enumerating All Maximal Cliques in Complex Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7170c870-9496-429b-8a44-82e92a702885"}
{"abstract": "HMAC is the internet standard for message authentication [BCK96,KBC97]. What distinguishes HMAC from other MAC algorithms is that it provides proofs of security assuming that the underlying cryptographic hash (e.g. SHA-1) has some reasonable properties. HMAC is efficient for long messages, however, for short messages the nested constructions results in a significant inefficiency. For example to MAC a message shorter than a block, HMAC requires at least two calls to the compression function rather than one. This inefficiency may be particularly high for some applications, like message authentication of signaling messages, where the individual messages may all fit within one or two blocks. Also for TCP/IP traffic it is well known that a large number of packets (e.g. acknowledgement) have sizes around 40 bytes which fit within a block of most cryptographic hashes. We propose an enhancement that allows both short and long messages to be message authenticated more efficiently than HMAC while also providing proofs of security. In particular, for a message smaller than a block our MAC only requires one call to the compression function.", "authors": ["Sarvar Patel"], "n_citation": 0, "title": "An efficient MAC for short messages", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7389c714-31b1-40e8-9a8e-bf372c099b55"}
{"abstract": "In this paper we focus on the unsplittable flow problem (UFP): given a directed network with arc capacities and a set of connections (requests) de-fined by origin node, destination node and bandwidth requirement, find a subset of the connections of maximum total demand for which each connection uses only one path and the sum of demands crossing the arc does not exceed its capacity. The UFP can be applied in survivable connection-oriented network (e.g. MPLS) for assignment of backup paths of failed connections. Since the UFP is NP-complete, we propose two new effective heuristic algorithms for the UFP. We evaluate the performance of proposed schemes by making a comparison with their counterparts using various network topologies and demand patterns. Obtained results indicate that proposed heuristics provide substantial improvement comparing to existing methods.", "authors": ["Krzysztof Walkowiak"], "n_citation": 0, "title": "New Algorithms for the Unsplittable Flow Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "73f35eaa-4eb2-468f-a941-823d2bb27c02"}
{"abstract": "We define the universal type class of an individual sequence x n 1, in analogy to the classical notion used in the method of types of information theory. Two sequences of the same length are said to be of the same universal (LZ) type if and only if they yield the same set of phrases in the incremental parsing of Ziv and Lempel (1978). We show that the empirical probability distributions of any finite order k of two sequences of the same universal type converge, in the variational sense, as the sequence length increases. Consequently, the logarithms of the probabilities assigned by any k-th order probability assignment to two sequences of the same universal type converge, for any k. We estimate the size of a universal type class, and show that its behavior parallels that of the conventional counterpart, with the LZ78 code length playing the role of the empirical entropy. We present efficient procedures for enumerating the sequences in a universal type class, and for drawing a sequence from the class with uniform probability. As an application, we consider the problem of universal simulation of individual sequences. A sequence drawn with uniform probability from the universal type class of x n  1  is a good simulation of x n  1  in a well defined mathematical sense.", "authors": ["Gadiel Seroussi"], "n_citation": 0, "title": "Universal types and simulation of individual sequences", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "741203c5-ddca-4ecd-b6aa-ba74adfc7a32"}
{"abstract": "25% of the total revenue earning is achieved from Textile exports for some countries like Bangladesh. It is thus important to produce defect free high quality garment products. Inspection processes done on fabric industries are mostly manual hence time consuming. To reduce error on identifying fabric defects requires automotive and accurate inspection process. Considering this lacking, this research implements a Textile Defect detector. A multi-layer neural network is determined that best classifies the specific problems. To feed neural network the digital fabric images taken by a digital camera and converts the RGB images are first converted into binary images by restoration process and local threshold techniques, then three different features are determined for the actual input to the neural network, which are the area of the defects, number of the objects in a image and finally the shape factor. The develop system is able to identify two very commonly defects such as Holes and Scratches and other types of minor defects. The developed system is very suitable for Least Developed Countries, identifies the fabric defects within economical cost and produces less error prone inspection system in real time.", "authors": ["Md. Atiqul Islam", "Shamim Akhter", "Tamnun E. Mursalin", "M. Ashraful Amin"], "n_citation": 50, "title": "A Suitable Neural Network to Detect Textile Defects", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "74c855b2-fb27-4a27-984c-c597c5fa247b"}
{"abstract": "Token-based MAC(Medium Access Control) Protocols are proposed for WDM Burst-Switched Ring Network which consists of nodes using TT-TR(Tunable Transmitter-Tunable Receiver). The node architectures with TT-TR may make an efficient use of network resources, even though traffic pattern such as IP traffic with high self-similarity are dynamically changed, and can also support good expandability. However, MAC protocols suitable for TT-TR node architecture must be designed with consideration for various factors in order to use the limited resources of network efficiently. A variety of Token-based MAC protocols are suggested to increase the performance while reducing the processing overhead at each node. The performance of the MAC protocols are evaluated and compared in terms of average packet delay, channel utilization and burst loss rate through OPNET simulation. Finally, we provide insight into the design of MAC protocols by investigating the effect of various parameters.", "authors": ["Li-Mei Peng", "Young-Chul Kim", "Kyoung-Min Yoo", "Kyeong-Eun Han", "Young-Chon Kim"], "n_citation": 50, "title": "Design and performance evaluation of token-based MAC protocols in WDM burst switched ring networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "75e6b100-eca5-4834-9fbc-d8b0d173b1f1"}
{"abstract": "We provide a denotational semantics for first-order logic that captures the two-level view of the computation process typical for constraint programming. At one level we have the usual program execution. At the other level an automatic maintenance of the constraint store takes place. We prove that the resulting semantics is sound with respect to the truth definition. By instantiating it by specific forms of constraint management policies we obtain several sound evaluation policies of first-order formulas. This semantics can also be used a basis for sound implementation of constraint maintenance in presence of block declarations and conditionals.", "authors": ["Krzysztof R. Apt", "C. F. M. Vermeulen"], "n_citation": 0, "title": "First-order logic as a constraint programming-language", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7632e1b5-9b39-4796-bbd2-142ec67a31ca"}
{"abstract": "A new formal framework of learning - learning by consistency queries - is introduced and studied. The theoretical approach outlined here is implemented as the core technology of a prototypical development system named LExIKON which supports interactive information extraction in practically relevant cases exactly in the way described in the present paper. The overall scenario of learning by consistency queries for information extraction is formalized and different constraints on the query learners are discussed and formulated. The principle learning power of the resulting types of query learners is analyzed by comparing it to the power of well-known types of standard learning devices including unconstrained inductive inference machines as well as consistent, total, finite, and iterative learners.", "authors": ["Gunter Grieser", "Klaus P. Jantke", "Steffen Lange"], "n_citation": 0, "title": "Consistency queries in information extraction", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "76e64381-18f2-4b05-984c-db45bda5721f"}
{"authors": ["Marta Burza\u0144ska", "Krzysztof Stencel", "Piotr Wi\u015bniewski"], "n_citation": 0, "title": "Pushing Predicates into Recursive SQL Common Table Expressions", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "778479da-5639-4e05-a257-72b1ecbd3dd5"}
{"abstract": "We derive rewrite-based ordered resolution calculi for semilattices, distributive lattices and boolean lattices. Using ordered resolution as a metaprocedure, theory axioms are first transformed into independent bases. Focused inference rules are then extracted from inference patterns in refutations. The derivation is guided by mathematical and procedural background knowledge, in particular by ordered chaining calculi for quasiorderings (forgetting the lattice structure), by ordered resolution (forgetting the clause structure) and by Knuth-Bendix completion for non-symmetric transitive relations (forgetting both structures). Conversely, all three calculi are derived and proven complete in a transparent and generic way as special cases of the lattice calculi.", "authors": ["Georg Struth"], "n_citation": 50, "title": "Deriving focused lattice calculi", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "77fb884d-f07d-4857-80fd-47e919879c48"}
{"abstract": "The concept lattice is a conceptual model firstly introduced by Wille in formal concept analysis, a theory of concept formation derived from lattice and order theory. Various concept lattice based applications have been reported in several domains such as conceptual clustering, conceptual knowledge representation and acquisition, and information retrieval. In this paper, we propose an object database approach for managing concept lattices in these applications. The goal of our work is two-fold. First, we extend the concept lattice model by basic operations supporting concept analysis. These operations allow to search and discover data directly from the concept lattice. Then, we present an approach for modeling and querying concept lattices within an object database framework.", "authors": ["Kitsana Waiyamai", "Rafik Taouil", "Lotfi Lakhal"], "n_citation": 0, "title": "Towards an object database approach for managing concept lattices", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "79b9b6d0-a3e8-4c7f-9811-28a6b738dcfb"}
{"abstract": "We propose a fast algorithm, EMD-Li, for computing the Earth Mover's Distance (EMD) between a pair of histograms. Compared to the original formulation, EMD-L 1  has a largely simplified structure. The number of unknown variables in EMD-L 1 is O(N) that is significantly less than O(N 2 ) of the original EMD for a histogram with N bins. In addition, the number of constraints is reduced by half and the objective function is also simplified. We prove that the EMD-L 1  is formally equivalent to the original EMD with L 1  ground distance without approximation. Exploiting the L 1  metric structure, an efficient tree-based algorithm is designed to solve the EMD-L 1  computation. An empirical study demonstrates that. the new algorithm has the time complexity of O(N 2 ), which is much faster than previously reported algorithms with super-cubic complexities. The proposed algorithm thus allows the EMD to be applied for comparing histogram-based features, which is practically impossible with previous algorithms. We conducted experiments for shape recognition and interest point matching. EMD-Li is applied to compare shape contexts on the widely tested MPEG7 shape dataset and SIFT image descriptors on a set of images with large deformation, illumination change and heavy noise. The results show that our EMD-L 1 -based solutions outperform previously reported state-of-the-art features and distance measures in solving the two tasks.", "authors": ["Haibin Ling", "Kazunori Okada"], "n_citation": 7, "title": "EMD-L1 : An Efficient and Robust Algorithm for Comparing Histogram-Based Descriptors", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7c8de51c-3c28-45d9-a55a-2e86aeec0ba7"}
{"authors": ["Stefaan Poedts", "P. M. Meijer", "J. P. Goedbloed", "H.A. van der Vorst", "Andreas Jakoby"], "n_citation": 50, "title": "Parallel magnetohydrodynamics on the CM-5", "venue": "Lecture Notes in Computer Science", "year": 1994, "id": "7dd6dcbf-e1c8-4c30-8f52-1b45f66bf46d"}
{"authors": ["Tibor Bosse", "Ghazanfar F. Siddiqui", "Jan Treur", "Vu1010095", "Faculteit der Exacte Wetenschappen"], "n_citation": 50, "title": "Modelling Greed of Agents in Economical Context", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "826f4c49-ec91-42a0-84f7-962ed111de60"}
{"abstract": "In computational biology, an important problem is to identify a word of length k present in each of a given set of sequences. Here, we investigate the problem of calculating the probability that such a word exists in a set of r random strings. Existing methods to approximate this probability are either inaccurate when r > 2 or are restricted to Bernoulli models. We introduce two new methods for computing this probability under Bernoulli and Markov models. We present generalizations of the methods to compute the probability of finding a word of length k shared among q of r sequences, and to allow mismatches. We show through simulations that our approximations are significantly more accurate than methods previously published.", "authors": ["Eric Blais", "Mathieu Blanchette"], "n_citation": 0, "title": "Common substrings in random strings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "83de12cd-a766-43c7-b8bc-4b12a4952530"}
{"abstract": "The Sharable Content Object Reference Model (SCORM) specification has been playing an important role in sharing and distributing contents among e-learning systems as well as in tracking the learner of the contents. Nonetheless, its limitations hinder its application to actual fieldwork particularly the inadequacy of the flexible contents structure, e-learning system performance, utilization of the necessary auxiliary resources together with the contents, support for collaborative learning, and utilization of contents linked to the external assessment system. Therefore, this study proposed the addition of Jump Control and expansion of Auxiliary Resources to the SCORM 2004 specification in order to improve the flexibility of the learning object-based e-learning contents structure and enhance e-learning system performance. A prototype was also developed to show the benefits of the improved efficiency of the manifest file describing the contents structure by adding the jump control concept. The processing of the auxiliary resource description and utilization in the standardization specification was also described.", "authors": ["Yong-Sang Cho", "Dae-Jun Hwang", "Tae-Myung Chung", "Sung-Ki Choi", "Woo-In Bae"], "n_citation": 0, "title": "Enhanced SCORM sequencing rule for e-learning system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "899a055c-f3e3-4785-b35f-c9d94d50402c"}
{"abstract": "Embodied Conversational Agents (ECAs) with realistic faces are becoming an intrinsic part of many graphics systems employed in HCI applications. A fundamental issue is how people visually perceive the affect of a speaking agent. In this paper we present the first study evaluating the relation between objective and subjective visual perception of emotion as displayed on a speaking human face, using both full video and sparse point-rendered representations of the face. We found that objective machine learning analysis of facial marker motion data is correlated with evaluations made by experimental subjects, and in particular, the lower face region provides insightful emotion clues for visual emotion perception. We also found that affect is captured in the abstract point-rendered representation.", "authors": ["Zhigang Deng", "Jeremy N. Bailenson", "John P. Lewis", "Ulrich Neumann"], "n_citation": 0, "title": "Perceiving visual emotions with speech", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "899a478c-4bf5-472f-a785-97325b3c8c71"}
{"abstract": "We report on our experience in using the Isabelle/HOL theorem prover to mechanize proofs of observation equivalence for systems with infinitely many states, and for parameterized systems. We follow the direct approach: An infinite relation containing the pair of systems to be shown equivalent is defined, and then proved to be a weak bisimulation. The weak bisimilarity proof is split into many cases, corresponding to the derivatives of the pairs in the relation. Isabelle/HOL automatically proves simple cases, and guarantees that no case is forgotten. The strengths and weaknesses of the approach are discussed.", "authors": ["Christine R\u00f6ckl", "Javier Esparza"], "n_citation": 50, "title": "Proof-checking protocols using bisimulations", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "89e68dd7-4718-4770-8293-3448ba3ba078"}
{"abstract": "This article presents an extension of the Through-the-Lens Camera Control approach proposed by Gleicher and Witkin. It first provides a higher means of control on the camera by using virtual composition primitives and second offers a means for through-the-lens interaction with both the location of the objects in the scene and the light ing. By setting properties on the composition primitives, users convey constraints to enforce the positioning of the camera, the objects and tlie lights directly through the lens. The paper presents how to express all three problems of indirect camera, object and light interaction in a consistent way and provides some first results. The solving techniques rely on the expression of the image Jacobian coupled with a constrained optimizer based on Quadratic Programming. The Jacobian expresses the relation between the user input and the possible degrees of freedom on the entity to manipulate; in order to avoid solving failures that are delicate to manage in user interfaces, we propose a mass-spring interaction model. As a result, the user should be able concentrate on higher level properties such as composition, balance and unity through a natural and effective interaction process.", "authors": ["Marc Christie", "Hiroshi Hosobe"], "n_citation": 0, "title": "Through-the-lens cinematography", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8a50a5e8-5555-4f0e-9395-1dddacfe94a2"}
{"abstract": "We describe algorithms that directly infer regular expressions from positive data and characterize the regular language classes that can be learned this way.", "authors": ["Henning Fernau"], "n_citation": 24, "title": "Algorithms for learning regular expressions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8a955b1c-48bd-47fc-929e-cce3c8110e1a"}
{"abstract": "A novel intelligent approach into 3D freeform surface reconstruction from planar sketches is proposed. A multilayer perceptron (MLP) neural network is employed to induce 3D freeform surfaces from planar freehand curves. Planar curves were used to represent the boundaries of a freeform surface patch. The curves were varied iteratively and sampled to produce training data to train and test the neural network. The obtained results demonstrate that the network successfully learned the inverse-projection map and correctly inferred the respective surfaces from fresh curves.", "authors": ["Usman Khan", "Abdelaziz Terchi", "Sungwoo Lim", "David K. Wright", "Shengfeng Qin"], "n_citation": 50, "title": "3D Freeform Surfaces from Planar Sketches Using Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8b2e1ade-da36-4bd0-b653-e60acf9b46a2"}
{"abstract": "A two-population Genetic Algorithm for constrained optimization is exercised and analyzed. One population consists of feasible candidate solutions evolving toward optimality. Their infeasible but promising offspring are transferred to a second, infeasible population. Four striking features are illustrated by executing challenge problems from the literature. First, both populations evolve essentially optimal solutions. Second, both populations actively exchange offspring. Third, beneficial genetic materials may originate in either population, and typically diffuse into both populations. Fourth, optimization vs. constraint tradeoffs are revealed by the infeasible population.", "authors": ["Steven O. Kimbrough", "Ming Lu", "David Harlan Wood"], "n_citation": 0, "title": "Exploring the evolutionary details of a feasible-infeasible two-population GA", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8c13cd56-2ae9-43ee-bc89-575dbc2e5823"}
{"abstract": "This paper presents a scalable solution to the problem of tracking objects across spatially separated, uncalibrated, non-overlapping cameras. Unlike other approaches this technique uses an incremental learning method, to model both the colour variations and posterior probability distributions of spatio-temporal links between cameras. These operate in parallel and are then used with an appearance model of the object to track across spatially separated cameras. The approach requires no pre-calibration or batch preprocessing, is completely unsupervised, and becomes more accurate over time as evidence is accumulated.", "authors": ["Andrew Gilbert", "Richard Bowden"], "n_citation": 176, "title": "Tracking Objects Across Cameras by Incrementally Learning Inter-camera Colour Calibration and Patterns of Activity", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8ca9e0dd-e847-40fa-8f3b-8336516cdbfe"}
{"abstract": "Binary sequences with small aperiodic correlations play an important role in many applications ranging from radar to modulation and testing of systems. In 1977, M. Golay introduced the merit factor as a measure of the goodness of the sequence and conjectured an upper bound for this. His conjecture is still open. In this paper we survey the known results on the Merit Factor problem and comment on the recent experimental results by R.A.Kristiansen and M. Parker and by P. Borwein,K.-K.S.Choi and J. Jedwab.", "authors": ["Tom H\u00f8holdt"], "n_citation": 22, "title": "The Merit Factor Problem for Binary Sequences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8d1da10a-4eb2-4330-939c-ec164074e1bc"}
{"abstract": "We consider the online auction problem in which an auctioneer is selling an identical item each time when a new bidder arrives. It is known that results from online prediction can be applied and achieve a constant competitive ratio with respect to the best fixed price profit. These algorithms work on a predetermined set of price levels. We take into account the property that the rewards for the price levels are not independent and cast the problem as a more refined model of online prediction. We then use Vovk's Aggregating Strategy to derive a new algorithm. We give a general form of competitive ratio in terms of the price levels. The optimality of the Aggregating Strategy gives an evidence that our algorithm performs at least as well as the previously proposed ones.", "authors": ["Shigeaki Harada", "Eiji Takimoto", "Akira Maruoka"], "n_citation": 0, "title": "Aggregating Strategy for Online Auctions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8edd5f96-1380-478f-94d1-316bae831e06"}
{"abstract": "It has been widely acknowledged in the areas of human memory and cognition that behaviour and emotion are essentially grounded by autobiographic knowledge. In this paper we propose an overall framework of human autobiographic memory for modelling believable virtual characters in narrative story-telling systems and role-playing computer games. We first lay out the background research of autobiographic memory in Psychology, Cognitive Science and Artificial Intelligence. Our autobiographic agent framework is then detailed with features supporting other cognitive processes which have been extensively modelled in the design of believable virtual characters (e.g. goal structure, emotion, attention, memory schema and reactive behaviour-based control at a lower level). Finally we list directions for future research at the end of the paper.", "authors": ["Wan Ching Ho", "Scott Watson"], "n_citation": 0, "title": "Autobiographic knowledge for believable virtual characters", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9171e9c9-9619-4df0-977a-482131ee1a69"}
{"abstract": "This paper utilizes Ant-Miner - the first Ant Colony algorithm for discovering classification rules - in the field of web content mining, and shows that it is more effective than C5.0 in two sets of BBC and Yahoo web pages used in our experiments. It also investigates the benefits and dangers of several linguistics-based text preprocessing techniques to reduce the large numbers of attributes associated with web content mining.", "authors": ["Nicholas Holden", "Alex Alves Freitas"], "n_citation": 0, "title": "Web page classification with an Ant Colony algorithm", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "933e76ed-8ef0-442a-b3b4-2b57fb8b1d7d"}
{"abstract": "Network intrusion detection is an important technique in computer security. However, the performance of existing intrusion detection systems (IDSs) is unsatisfactory since new attacks are constantly developed and the speed of network traffic volumes increases fast. To improve the performance of IDSs both in accuracy and speed, this paper proposes a novel adaptive intrusion detection method based on principal component analysis (PCA) and support vector machines (SVMs). By making use of PCA, the dimension of network data patterns is reduced significantly. The multi-class SVMs are employed to construct classification models based on training data processed by PCA. Due to the generalization ability of SVMs, the proposed method has good classification performance without tedious parameter tuning. Dimension reduction using PCA may improve accuracy further. The method is also superior to SVMs without PCA in fast training and detection speed. Experimental results on KDD-Cup99 intrusion detection data illustrate the effectiveness of the proposed method.", "authors": ["Xin Xu", "Xuening Wang"], "n_citation": 74, "title": "An adaptive network intrusion detection method based on PCA and support vector machines", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9442c5f7-d0a4-4997-8740-a85c247e0c48"}
{"abstract": "In the field of pattern recognition or outlier detection, it is necessary to estimate the region where data of a particular class are generated. In other words, it is required to accurately estimate the support of the distribution that generates the data. Considering the 1-dimensional distribution whose support is a finite interval, the data region is estimated effectively by the maximum value and the minimum value in the samples. Limiting distributions of these values have been studied in the extreme-value theory in statistics. In this research, we propose a method to estimate the data region using the maximum value and the minimum value in the samples. We calculate the average loss of the estimator, and derive the optimally improved estimators for given loss functions.", "authors": ["Kazuho Watanabe", "Sumio Watanabe"], "n_citation": 0, "title": "Estimation of the data region using extreme-value distributions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9c8716c3-2dee-483d-a100-8ab251bbe681"}
{"abstract": "Annotated corpora are valuable resources for NLP which are often costly to create. We introduce a method for transferring annotation from a morphologically annotated corpus of a source language to a target language. Our approach assumes only that an unannotated text corpus exists for the target language and a simple textbook which describes the basic morphological properties of that language is available. Our paper describes experiments with Polish, Czech, and Russian. However, the method is not tied in any way to these languages. In all the experiments we use the TnT tagger ([3]), a second-order Markov model. Our approach assumes that the information acquired about one language can be used for processing a related language. We have found out that even breath-takingly naive things (such as approximating the Russian transitions by Czech and/or Polish and approximating the Russian emissions by (manually/automatically derived) Czech cognates) can lead to a significant improvement of the tagger's performance.", "authors": ["Anna Feldman", "Jirka Hana", "Chris Brew"], "n_citation": 0, "title": "Experiments in cross-language morphological annotation transfer", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9d203123-e1dc-44e1-b242-139c77592d2a"}
{"authors": ["Nao Hirokawa", "Georg Moser"], "n_citation": 83, "title": "Automated Complexity Analysis Based on the Dependency Pair Method", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "9dd618c0-cce4-42ff-921e-4d2ed7745129"}
{"abstract": "We present a general translation of term rewrite systems (TRS) to logic programs such that basic rewriting derivations become logic deductions. Certain TRS result in so-called cs-programs, which were originally studied in the context of constraint systems and tree tuple languages. By applying decidability and computability results of cs-programs we obtain new classes of TRS that have nice properties like decidability of unification, regular sets of descendants or finite representations of R-unifiers. Our findings generalize former results in the field of term rewriting.", "authors": ["S\u00e9bastien Limet", "Gernot Salzer"], "n_citation": 0, "title": "Proving properties of term rewrite systems via logic programs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a08ea71e-bcea-4c60-94eb-5e8d878e261d"}
{"abstract": "Gene expression data that are gathered from tissue samples are expected to significantly help the development of efficient tumor diagnosis and classification platforms. Since DNA microarray experiments provide us with huge amount of gene expression data and only a few of genes are related to tumor, gene selection algorithms should be emphatically explored to extract those informative genes related tumor from gene expression data. So we propose a novel feature selection approach to further improve the SVM-based classification performance of gene expression data, which projects high dimensional data onto lower dimensional feature space. We examine a set of gene expression data that include sets of tumor and normal clinical samples by means of SVMs classifier. Experiments show that SVM has a superior performance in classification of gene expression data as long as the selected features can represent the principal components of all gene expression samples.", "authors": ["Shulin Wang", "Ji Wang", "Huowang Chen", "Boyun Zhang"], "n_citation": 0, "title": "SVM-Based Tumor Classification with Gene Expression Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a1ad5e4c-bc88-4917-930d-2aabc38c0fd9"}
{"abstract": "Telecommunicating systems should have a high degree of availability, i.e., high probability of correct and timely provision of requested services. To achieve this, correctness of software for such systems should be ensured. Application of formal methods helps us to gain confidence in building correct software. However, to be used in practice, the formal methods should be well integrated into existing development process. In this paper we propose a formal model-driven approach to development of communicating systems. Essentially our approach formalizes Lyra - a top-down service-oriented method for development of communicating systems. Lyra is based on transformation and decomposition of models expressed in UML2. We formalize Lyra in the B Method by proposing a set of formal specification and refinement patterns reflecting the essential models and transformations of Lyra. The proposed approach is illustrated by a case study.", "authors": ["Linas Laibinis", "Elena Troubitsyna", "Sari Lepp\u00e4nen", "Johan Lilius", "Qaisar A. Malik"], "n_citation": 0, "title": "Formal model-driven development of communicating systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a20e442b-a8db-401b-8385-b1464abb4ae8"}
{"abstract": "In this paper, we propose a computer interaction method for a game. The game is played by using finger shadow cast by a projector to move a ball rendered by a computer. We use a camera to track the movement of the finger shadow. A simple algorithm to move this ball was developed, which depends on the position of the shadow in the camera picture. The algorithm consist of three parts: the recognition of the shadow, the calculation of the ball by using transformation matrices, and the recognition of the collision between the ball and the shadow and consequential the direction of the ball-movement.", "authors": ["Torsten Engelbrecht", "Linqiang Chen", "Yigang Wang"], "n_citation": 0, "title": "Computer interaction by camera tracking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a2becc37-715a-4bd9-945a-5e875fba007e"}
{"abstract": "Dance is one of the entertainments where the physical movement is the key factor. On the other hand, the main reason why robots are kind of boom is that they have physical body. Combining there two element, a concept of a dancing robot is proposed. First, various kind of factors concerning entertainment and dance is studied. Also we describe the functions required for robots to achieve dancing. Then we describe one of the dancing robot we have developed focusing its hardware functions and show several examples of dance performance we have developed.", "authors": ["Kuniya Shinozaki", "Yousuke Oda", "Satoshi Tsuda", "Ryohei Nakatsu", "Akitsugu Iwatani"], "n_citation": 0, "title": "Study of dance entertainment using robots", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a2f8cbc6-7899-4102-8653-56483428ef27"}
{"abstract": "We discuss the principles of distributed transactions, then we define an operational model which meets the basic requirements and we give a prototyping implementation for it in join-calculus. Our model: (1) extends BizTalk with multiway transactions; (2) exploits an original algorithm, for distributed commit; (3) can deal with dynamically changing communication topology; (4) is almost language-independent. In fact, the model is based on a two-level classification of resources, which should be easily conveyed to distributed calculi and languages, providing them with a uniform transactional mechanism.", "authors": ["Roberto Bruni", "Cosimo Laneve", "Ugo Montanari"], "n_citation": 0, "title": "Orchestrating transactions in join calculus", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a3636ed8-3d6d-44ac-860f-0db049abecd7"}
{"abstract": "In this work, we initiate the study of realizing a ranking functionality (m 1 , ..., m n ) \u2192(r 1 ,...,r n ) in the non-adaptive malicious model, where r, = 1+ #{m j : m j < mi}. Generically, it has been solved by a general multi-party computation technique (via a circuit formulation). However, such a solution is inefficient in either round complexity or communication complexity. In this work, we propose an efficient construction without a circuit. Our protocol is constant round and efficient in communication complexity as well. Furthermore, we show it is directly secure in the non-adaptive malicious model (i.e., without a compiler, as is used in many general constructions).", "authors": ["Shaoquan Jiang", "Guang Gong"], "n_citation": 0, "title": "A round and communication efficient secure ranking protocol", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a5e22c39-9912-43cb-a6cf-33c5cfd1d4ca"}
{"abstract": "A Boolean formula is unsatisfiable if and only if its representing binary decision diagram (BDD) is reduced to the single leaf false. When BDD variables represent first-order atoms including equalities between terms, uninterpreted predicates or linear arithmetic constraints, a path to the true leaf in the BDD might not give a model. So BDDs representing unsatisfiable quantifier-free first-order logic formulas may not reduce to the single leaf false. Decision procedures for combinations of theories can be used to eliminate all those unsatisfiable paths. In a naive approach every path would be considered; this would be very inefficient. We provide efficient algorithms to find general constraints (connections) from unsatisfiable paths to true in the BDD. Adding those connections to the BDD will eliminate many paths to true at one go. This procedure also ensures that no unnecessary constraint is added. In the context of invariant validation, this gives good results when using BDDs with a rich quantifier-free language.", "authors": ["Pascal Fontaine", "E. Pascal Gribomont"], "n_citation": 50, "title": "Using BDDs with combinations of theories", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a64b86b9-c3aa-4d10-935e-12013e912823"}
{"abstract": "Intrusion detection systems (IDSs) are an important component of defensive measures against system abuse. Firstly, some disadvantages of existing IDSs were analyzed. For solving these problems, an Independent Agents-based Distributed Intrusion Detection Framework - IADIDF was proposed. This paper describes the function of entities, defines the communication and alert mechanism. Each agent operates cooperatively yet independently of the others, providing for efficiency alerts and distribution of resources. The proposed model is an open system, which has good scalability. All the entities of IADIDF were developed in C program under Linux platform. Experiment results indicate that the operating of agents will not impact system performance heavily.", "authors": ["Ye Du", "Huiqiang Wang", "Yonggang Pang"], "n_citation": 0, "title": "IADIDF: A Framework for Intrusion Detection", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ac17f6c8-f8bd-43dd-9671-c4f7838167cf"}
{"abstract": "In this paper we address the joint problem of minimising both the transport and inventory costs of a retail chain that is supplied from a central warehouse. We propose a hybrid evolutionary algorithm where the delivery patterns are evolved for each shop, while the delivery routes are obtained employing the multistart sweep algorithm. The experiments performed show that this method can obtain acceptable results consistently and within a reasonable timescale. The results are also of a lower cost than those obtained by other strategies employed in previous research. Furthermore, they confirm the interest of addressing the optimisation problem jointly, rather than minimising separately inventory and transport.", "authors": ["Anna I. Esparcia-Alcaazar", "Lidia Lluch-Revert", "Manuel Card\u00f3s", "Ken Sharman", "Carlos Andr\u00e9s-Romano"], "n_citation": 0, "title": "Design of a retail chain stocking up policy with a hybrid evolutionary algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ada9f570-db48-4ca7-882d-b64f862a72de"}
{"abstract": "This paper states that the RoleOf Relationship can provide a general approach to resolve instantiation problems of design patterns. The problems come from the fact that pattern logic scatters across multiple business classes (classes specific to each application). This causes problems such as decreasing reusability of pattern logic, and losing of the instantiation information of pattern (traceability and overlapping problem) etc. To resolve these problems in design level, an approach for design pattern instantiation based on RoleOf relationship is proposed. In our approach, roles of pattern are treated as the independent modeling elements and RoleOf relationship is used to associate a role with a business class. The meta model of RoleOf relationship for pattern instantiation and its semantics are proposed as well. Examples are used to illustrate this approach. Implementation and behavior description of RoleOf relationship are also presented in the paper.", "authors": ["Chengwan He", "Fei He", "Keqing He", "Jin Liu", "Wenjie Tu"], "n_citation": 0, "title": "Roleof relationship and its meta model for design pattern instantiation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b0e4508e-4055-453a-bab9-6555535ab392"}
{"abstract": "The key problem of inductive-learning in Bayes network is the estimator of prior distribution. This paper adopts general naive Bayes to handle continuous variables, and proposes a kind of kernel function constructed by orthogonal polynomials, which is used to estimate the density function of prior distribution in Bayes network. The paper then makes further researches into optimality of the kernel estimation of density and derivatives. When the sample is fixed, the estimators can keep continuity and smoothness, and when the sample size tends to infinity, the estimators can keep good convergence rates.", "authors": ["Hengqing Tong", "Yanfang Deng", "Ziling Li"], "n_citation": 0, "title": "Optimality of Kernel Density Estimation of Prior Distribution in Bayes Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b683cd88-5f99-4096-88a8-7994d86c4738"}
{"abstract": "Magnetoencephalography (MEG) is an important noninvasive, non-hazardous technology for functional brain mapping, measuring the magnetic fields due to the intracellular neuronal current flow in the brain. However, the inherent level of noise in the data collection process is large enough to obscure the signal(s) of interest most often. In this paper, a practical denoising technique based on the wavelet transform and the multiresolution signal decomposition technique is presented. The proposed technique is substantiated by the application results using three different mother wavelets on the recorded MEG signal.", "authors": ["Abhisek Ukil"], "n_citation": 0, "title": "Practical Denoising of MEG Data Using Wavelet Transform", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b8eff2aa-185f-4252-8428-887f3f6ad061"}
{"abstract": "The Biphase Mark Protocol (BMP) and 8N1 Protocol are physical layer protocols for data transmission. We present a generic model in which timing and error values are parameterized by linear constraints, and then we use this model to verify these protocols. The verifications are carried out using SRI's SAL model checker that combines a satisfiability modulo theories decision procedure with a bounded model checker for highly-automated induction proofs of safety properties over infinite-state systems. Previously, parameterized formal verification of real-time systems required mechanical theorem-proving or specialized real-time model checkers; we describe a compelling case-study demonstrating a simpler and more general approach. The verification reveals a significant error in the parameter ranges for 8N1 given in a published application note [1].", "authors": ["Geoffrey M. Brown", "Lee Pike"], "n_citation": 0, "title": "Easy parameterized verification of biphase mark and 8N1 protocols", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bae74d1b-9a1b-4521-9c96-4e8c5e688638"}
{"abstract": "Our objective is to develop a computational model to predict visual attention behavior for an embodied conversational agent. During interpersonal interaction, gaze provides signal feedback and directs conversation flow. Simultaneously, in a dynamic environment, gaze also directs attention to peripheral movements. An embodied conversational agent should therefore employ social gaze not only for interpersonal interaction but also to possess human attention attributes so that its eyes and facial expression portray and convey appropriate distraction and engagement behaviors.", "authors": ["Erdan Gu", "Norman I. Badler"], "n_citation": 76, "title": "Visual attention and eye gaze during multiparty conversations with distractions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bb9f5ad7-56a8-45f7-93c7-1c4f24cbf258"}
{"abstract": "The materialized view technology has been widely adopted in the data warehouse systems to improve query performance. Two key underlying techniques are materialized view incremental refresh and query rewrite. However, due to the potential complexity of the materialized view defining query and dependency on the materialized view change log, not all materialized views are incrementally refreshable or generally query rewritable. Many restrictions need to be applied and addressed which make the materialized view not easy to use. This paper presents Oracle's tuning mechanism/tool to facilitate the materialized view creation by automatically fixing and/or decomposing the defining query and addressing the change log requirements. The resulting materialized view implemented by a set of recommendation statements achieves the goals of incremental maintenance and broad query rewrite.", "authors": ["Tsae-Feng Yu", "Thomas Tong", "Min Xiao", "Jack Raitto"], "n_citation": 0, "title": "Materialized view tuning mechanism and usability enhancement", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bd6d3605-6dcb-419f-99f6-b844bae1f918"}
{"abstract": "In real-time safety-critical systems, it is important to predict the consequences of specific faults in databus logic and driver software on the safe operation of a databus. For this purpose we have developed a test-bench based on the TrueTime simulator extended by adding a fault injection capability, with new network models and fault modeling strategy. Faults are simulated by disturbing specified parameters of the databus model. In this paper, we present the modeling approach, the fault injection scenarios, and illustrate it with examples of the impact of the simulated faults on data throughput, message latency and bus scheduling for CAN and TTCAN networks.", "authors": ["Dawid Trawczynski", "Janusz Sosnowski", "Janusz Zalewski"], "n_citation": 0, "title": "A Tool for Databus Safety Analysis Using Fault Injection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "beec8809-0457-4430-8a98-1c22a5812748"}
{"abstract": "Explicit model checking with magnetic disk is prohibitively slow if file input/output (IO) is not carefully managed. We give an empirical analysis of the two published algorithms for model checking with magnetic disk and show that both algorithms minimize file IO time but are dominated by delayed duplicate detection time (which is required to avoid regenerating parts of the transition graph). We present and analyze a more time-efficient algorithm for model checking with magnetic disk that requires more file IO time, but less delayed duplicate detection time and less total execution time. The new algorithm is a variant of parallel partitioned hash table algorithms and uses a time-efficient chained hash table implementation.", "authors": ["Tonglaga Bao", "Michael D. Jones"], "n_citation": 0, "title": "Time-efficient model checking with magnetic disk", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bf58fb8e-26d8-4c8b-ab24-dd81166dd67e"}
{"abstract": "Traditional researches on data stream mining only put emphasis on building classifiers with high accuracy, which always results in classifiers with dramatic drop of accuracy when concept drifts. In this paper, we present our RTRC system that has good classification accuracy when concept drifts and enough samples are scanned in data stream. By using Markov chain and least-square method, the system is able to predict not only on which the next concept is but also on when the concept is to drift. Experimental results confirm the advantages of our system over Weighted Bagging and CVFDT, two representative systems in streaming data mining.", "authors": ["Yong Wang", "Zhanhuai Li", "Yang Zhang", "Longbo Zhang", "Yun Jiang"], "n_citation": 0, "title": "Improving the Performance of Data Stream Classifiers by Mining Recurring Contexts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bfd0b1bb-ef5b-4c4e-bf67-45f800b4baa1"}
{"abstract": "The present study aims at insights into the nature of incremental learning in the context of Gold's model of identification in the limit. With a focus on natural requirements such as consistency and conservativeness, incremental learning is analysed both for learning from positive examples and for learning from positive and negative examples. The results obtained illustrate in which way different consistency and conservativeness demands can affect the capabilities of incremental learners. These results may serve as a first step towards characterising the structure of typical classes learnable incrementally and thus towards elaborating uniform incremental learning methods.", "authors": ["Sanjay Jain", "Steffen Lange", "Sandra Zilles"], "n_citation": 0, "title": "Towards a better understanding of incremental learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c0a5c1c1-ad3c-43ea-98ea-7064b8d33c2a"}
{"abstract": "A new model for evolving the structure of a Particle Swarm Optimization (PSO) algorithm is proposed in this paper. The model is a hybrid technique that combines a Genetic Algorithm (GA) and a PSO algorithm. Each GA chromosome is an array encoding a meaning for updating the particles of the PSO algorithm. The evolved PSO algorithm is compared to a human-designed PSO algorithm by using ten artificially constructed functions and one real-world problem. Numerical experiments show that the evolved PSO algorithm performs similarly and sometimes even better than standard approaches for the considered problems.", "authors": ["Laura Diosan", "Mihai Oltean"], "n_citation": 50, "title": "Evolving the structure of the particle swarm optimization algorithms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c1f72cca-e1bf-490d-927f-469d004aeb46"}
{"abstract": "This paper addresses the reconstruction of canal surfaces from single images. A canal surface is obtained as the envelope of a family of spheres of constant radius, whose center is swept along a space curve, called axis. Previous studies either used approximate relationships (quasi-invariants), or they addressed the recognition based on a geometric model. In this paper we show that, under broad conditions, canal surfaces can be reconstructed from single images under exact perspective. In particular, canal surfaces with planar axis can even be reconstructed from a single fully-uncalibrated image. An automatic reconstruction method has been implemented. Simulations and experimental results on real images are also presented.", "authors": ["Vincenzo Caglioti", "Alessandro Giusti"], "n_citation": 50, "title": "Reconstruction of Canal Surfaces from Single Images Under Exact Perspective", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c3133bf6-bd47-420b-81c7-a6fd2580cb2d"}
{"abstract": "The edge assembly crossover (EAX) is considered the best available crossover for traveling salesman problems (TSPs). In this paper, a modified EAX algorithm is proposed. The key idea is to maintain population diversity by eliminating any exchanges of edges by the crossover that does not contribute to an improved evaluation value. The proposed method is applied to several benchmark problems up to 4461 cities. Experimental results shows that the proposed method works better than other genetic algorithms using other improvements of the EAX. The proposed method can reach optimal solutions in most benchmark problems up to 2392 cities with probabilities higher than 90%. For the fn14461 problem, this method can reach the optimal solution with a 60% probability for a population size of 300 - an extremely small population compared to that needed in previous studies.", "authors": ["Yuichi Nagata"], "n_citation": 0, "title": "The EAX algorithm considering diversity loss", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c390c05c-d798-474c-8046-d39c464b40e6"}
{"abstract": "Despite the widespread use of the Transmission Control Protocol (TCP) as the main transport protocol in the Internet, the procedures for connection establishment and release are still not fully understood. This paper extends the analysis of a Coloured Petri net model of TCP's Connection Management procedures by applying the state explosion alleviation technique known as the sweep-line method. The protocol is assumed to be operating over a reordering lossless channel. Termination and absence of deadlock properties are investigated for many scenarios, including client-server and simultaneous connection establishment, orderly release and abortion. The sweep-line method provides a reduction in memory usage of around a factor of 10 and allows investigation of many scenarios that were previously out of the reach of conventional methods.", "authors": ["Guy Edward Gallasch", "Bing Han", "Jonathan Billington"], "n_citation": 50, "title": "Sweep-line analysis of TCP connection management", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c3d814dd-7efa-4468-8ac8-d5921a6e7907"}
{"abstract": "Recently, the wide deployment of practical face recognition systems gives rise to the emergence of the inter-modality face recognition problem. In this problem, the face images in the database and the query images captured on spot are acquired under quite different conditions or even using different equipments. Conventional approaches either treat the samples in a uniform model or introduce an intermediate conversion stage, both of which would lead to severe performance degradation due to the great discrepancies between different modalities. In this paper, we propose a novel algorithm called Common Discriminant Feature Extraction specially tailored to the inter-modality problem. In the algorithm, two transforms are simultaneously learned to transform the samples in both modalities respectively to the common feature space. We formulate the learning objective by incorporating both the empirical discriminative power and tlie local smoothness of the feature transformation. By explicitly controlling the model complexity through the smoothness constraint, we can effectively reduce the risk of overfitting and enhance the generalization capability. Furthermore, to cope with the nongaussian distribution and diverse variations in the sample space, we develop two non-linear extensions of the algorithm: one is based on kernelization, while the other is a multi-mode framework. These extensions substantially improve the recognition performance in complex situation. Extensive experiments are conducted to test our algorithms in two application scenarios: optical image-infrared image recognition and photo-sketch recognition. Our algorithms show excellent performance in the experiments.", "authors": ["Dahua Lin", "Xiaoou Tang"], "n_citation": 0, "title": "Inter-modality Face Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c6c5ba18-3cdb-4eba-ac24-953cf3918553"}
{"abstract": "Context information other than faces, such as clothes, picture-taken-time and some logical constraints, can provide rich cues for recognizing people. This aim of this work is to automatically cluster pictures according to person's identity by exploiting as much context information as possible in addition to faces. Toward that end, a clothes recognition algorithm is first developed, which is effective for different types of clothes (smooth or highly textured). Clothes recognition results are integrated with face recognition to provide similarity measurements for clustering. Picture-taken-time is used when combining faces and clothes, and the cases of faces or clothes missing are handled in a principle way. A spectral clustering algorithm which can enforce hard constraints (positive and negative) is presented to incorporate logic-based cues (e.g. two persons in one picture must be different individuals) and user feedback. Experiments on real consumer photos show the effectiveness of the algorithm.", "authors": ["Yang Song", "Thomas Leung"], "n_citation": 0, "title": "Context-Aided Human Recognition : Clustering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c76f6d7e-5ba3-40ed-8dd8-d02eb0e84975"}
{"abstract": "One of the most important issues in educational systems is to define effective teaching policies according to the students learning characteristics. This paper proposes to use the Reinforcement Learning (RL) model in order for the system to learn automatically sequence of contents to be shown to the student, based only in interactions with other students, like human tutors do. An initial clustering of the students according to their learning characteristics is proposed in order the system adapts better to each student. Experiments show convergence to optimal teaching tactics for different clusters of simulated students, concluding that the convergence is faster when the system tactics have been previously initialised.", "authors": ["Ana Iglesias", "Paloma Mart\u00ednez", "Ricardo Aler", "Fernando Fern\u00e1ndez"], "n_citation": 0, "title": "Learning content sequencing in an educational environment according to student needs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c7c9824a-c840-41b3-aed0-23652f9ab5f1"}
{"abstract": "One of the advantages of temporal-logic model-checking tools is their ability to accompany a negative answer to the correctness query by a counterexample to the satisfaction of the specification in the system. On the other hand, when the answer to the correctness query is positive, most model-checking tools provide no additional information. In the last few years there has been growing awareness to the importance of suspecting the system or the specification of containing an error also in the case model checking succeeds. The main justification of such suspects are possible errors in the modeling of the system or of the specification. The goal of sanity checks is to detect such errors by further automatic reasoning. Two leading sanity checks are vacuity and coverage. In vacuity, the goal is to detect cases where the system satisfies the specification in some unintended trivial way. In coverage, the goal is to increase the exhaustiveness of the specification by detecting components of the system that do not play a role in verification process. For both checks, the challenge is to define vacuity and coverage formally, develop algorithms for detecting vacuous satisfaction and low coverage, and suggest methods for returning to the user helpful information. We survey existing work on vacuity and coverage and argue that, in many aspects, the two checks are essentially the same: both are based on repeating the verification process on some mutant input. In vacuity, mutations are in the specifications, whereas in coverage, mutations are in the system. This observation enables us to adopt work done in the context of vacuity to coverage, and vise versa.", "authors": ["Orna Kupferman"], "n_citation": 0, "title": "Sanity checks in formal verification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c8513dde-4f04-4f86-b1ad-9b4c3d62f981"}
{"abstract": "The estimation of the epipolar geometry is especially difficult where the putative correspondences include a low percentage of inlier correspondences and/or a large subset of the inliers is consistent with a degenerate configuration of the epipolar geometry that is totally incorrect. This work presents the Balanced Exploration and Exploitation Model Search (BEEM) algorithm that works very well especially for these difficult scenes. The BEEM algorithm handles the above two difficult cases in a unified manner. The algorithm includes the following main features: (1) Balanced use of three search techniques: global random exploration, local exploration near the current best solution and local exploitation to improve the quality of the model. (2) Exploits available prior information to accelerate the search process. (3) Uses the best found model to guide the search process, escape from degenerate models and to define an efficient stopping criterion. (4) Presents a simple and efficient method to estimate the epipolar geometry from two SIFT correspondences. (5) Uses the locality-sensitive hashing (LSH) approximate nearest neighbor algorithm for fast putative correspondences generation. The resulting algorithm when tested on real images with or without degenerate configurations gives quality estimations and achieves significant speedups compared to the state of the art algorithms!.", "authors": ["Liran Goshen", "Ilan Shimshoni"], "n_citation": 50, "title": "Balanced Exploration and Exploitation Model Search for Efficient Epipolar Geometry Estimation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ca24fe02-59fc-4b87-9e18-eda0d87adf76"}
{"abstract": "The steelmaking process consists of two phases: primary steelmaking and finishing lines. The scheduling of the continuous galvanizing lines (CGL) is regarded as the most difficult process among the finishing lines due to its multi-objective and highly-constrained nature. In this paper, we present a multi-population parallel genetic algorithm (MPGA) with a new genetic representation called K th  nearest neighbor representation, and with a new communication operator for performing better communication between subpopulations in the scheduling of CGL. The developed MPGA consists of two phases. Phase one generates schedules from a primary work in process (WIP) inventory filtered according to the production campaign, campaign tonnage, priorities of planning department, and the due date information of each steel coil. If the final schedule includes the violations of some constraints, phase two repairs these violations by using a secondary WIP inventory of steel coils. The developed scheduling system is currently being used in a steel making company with encouraging preliminary results.", "authors": ["Muzaffer Kapanoglu", "Ilker Ozan Koc"], "n_citation": 0, "title": "A multi-population parallel genetic algorithm for highly constrained continuous galvanizing line scheduling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ca83999c-dcba-4cc1-af38-ab3b12f02ed3"}
{"abstract": "In this system, we first build the geometrical model of outdoor scene. Then by establishing an energy equation based on principle of energy equilibrium, the surface temperatures of the infrared scene under various conditions are acquired. We propose a new method to obtain infrared texture from its corresponding visible image. The infrared effect due to different, weather conditions such as rain, cloud, fog is accounted for. The infrared shadow with high-reality and the high dynamic range effect of infrared dynamic scene are also simulated, and real-time walkthrough of large-scale infrared dynamic scene is carried out.", "authors": ["Zhangye Wang", "Changbo Wang", "Yan Zhou", "Qunsheng Peng"], "n_citation": 0, "title": "Real-time walkthrough system of large-scale infrared dynamic scene", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cad7ffdb-39f9-44fb-ac3d-3ff80329d137"}
{"abstract": "This paper presents a gradient routing algorithm a modified approach of DIR (compass routing) method to suit for mobile ad hoc network. It is a direction based localized algorithm where each node makes forwarding decisions solely based on the position of itself, its neighbors and destination. Source node selects a neighbor node to forward a message which is closest (having minimum gradient i.e. angle) towards the direction of destination. This algorithm makes use of the position information of nodes to improve the performance of routing protocols in mobile ad hoc network. The performance of gradient algorithms is compared with other directional routing algorithms LAR and DREAM in mobile environment using proactive approach. The experimental results show that gradient algorithm have higher success rate and lower flooding rate compared to LAR and DREAM.", "authors": ["Anand Praksh Ruhil", "D. K. Lobiyal", "Ivan Stojmenovic"], "n_citation": 5, "title": "Position based gradient routing in mobile ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cd347484-eb93-488d-948f-aeb686794231"}
{"authors": ["Bart Preneel"], "n_citation": 0, "title": "The First 30 Years of Cryptographic Hash Functions and the NIST SHA-3 Competition", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "cd84a91d-e926-42e8-b1ff-27132e6e3ac0"}
{"abstract": "In this paper approaches to conceptual modelling of spatio-temporal domains are identified and classified into five general categories: location-based, object or feature-based, event-based, functional or behavioural and causal approaches. Much work has been directed towards handling the problem from the first four view points, but less from a causal perspective. It is argued that more fundamental studies are needed of the nature of spatio-temporal objects and of their interactions and possible causal relationships, to support the development of spatio-temporal conceptual models. An analysis is carried out on the nature and type of spatio-temporal causation and a general classification is presented.", "authors": ["Baher A. El-Geresy", "Alia I. Abdelmoty", "Christopher B. Jones"], "n_citation": 0, "title": "Spatio-temporal geographic information systems: A causal perspective", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d33c4f30-fc28-4712-937a-dad31f803fba"}
{"abstract": "A multi-view multi-hypothesis approach to segmenting and tracking multiple (possibly occluded) persons on a ground plane is proposed. During tracking, several iterations of segmentation are performed using information from human appearance models and ground plane homography. To more precisely locate the ground location of a person, all center vertical axes of the person across views are mapped to the top-view plane and their intersection point on the ground is estimated. To tackle the explosive state space due to multiple targets and views, iterative segmentation-searching is incorporated into a particle filtering framework. By searching for people's ground point locations from segmentations, a set of a few good particles can be identified, resulting in low computational cost. In addition, even if all the particles are away from the true ground point, some of them move towards the true one through the iterated process as long as they are located nearby. We demonstrate the performance of the approach on several video sequences.", "authors": ["Kyungnam Kim", "Larry S. Davis"], "n_citation": 0, "title": "Multi-camera Tracking and Segmentation of Occluded People on Ground Plane Using Search-Guided Particle Filtering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d381cfe7-f944-4e30-a5c1-036000bf2807"}
{"abstract": "We investigate sorting or typing for Leifer and Milner's reactive systems. We focus on transferring congruence properties for bisim-ulations from unsorted to sorted systems. Technically, we give a general definition of sorting; we adapt Jensen's work on the transfer of congruence properties to this general definition; we construct a predicate sorting, which for any decomposible predicate P filters out agents not satisfying P; we prove that the predicate sorting preserves congruence properties and that it suitably retains dynamics; and finally, we show how the predicate sortings can be used to achieve context-aware reaction.", "authors": ["Lars Birkedal", "S\u00f8ren Debois", "Thomas T. Hildebrandt"], "n_citation": 0, "title": "Sortings for reactive systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d38c9076-cee9-4a6f-8d30-323781361e32"}
{"abstract": "One of buzzwords for modern manufacturing industry are flexible manufacturing systems (FMS), in which several machines are interlinked by an automated information and material flow system. Description and control upon these systems are of prominent significance. This paper is concerned with mining and construction of the established FMS from work event logs. A novel Petri nets based algorithm is developed to implement such an idea. When an FMS is mined and constructed. its corresponding Petri net is used to evaluate, analyze, and control the system. Theoretical and experimental results are illustrated to show the effectiveness and efficiency of this approach.", "authors": ["Hesuan Hu", "Zhiwu Li", "Anrong Wang"], "n_citation": 0, "title": "Mining of Flexible Manufacturing System Using Work Event Logs and Petri Nets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d39d32f5-f797-494f-b07c-d1c6fe3392c5"}
{"abstract": "In this paper we investigate, for intuitionistic implicational logic, the relationship between normalization in natural deduction and cut-elimination in a standard sequent calculus. First we identify a subset of proofs in the sequent calculus that correspond to proofs in natural deduction. Then we define a reduction relation on those proofs that exactly corresponds to normalization in natural deduction. The reduction relation is simulated soundly and completely by a cut-elimination procedure which consists of local proof transformations. It follows that the sequent calculus with our cut-elimination procedure is a proper extension that is conservative over natural deduction with normalization.", "authors": ["Kentaro Kikuchi"], "n_citation": 0, "title": "On a local-step cut-elimination procedure for the intuitionistic sequent calculus", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d4279ead-e172-4156-9d4f-27536ae54891"}
{"authors": ["Johan Baeten", "Bas Luttik", "P. van Tilburg", "Vu1094002"], "n_citation": 0, "title": "Computations and Interaction.", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "d5a0426a-7213-470a-8086-636ec5abf24d"}
{"abstract": "In this paper, we propose a video packet scheduling scheme, which guarantees QoS requirements for video streaming over digital audio broadcast (DAB) IP tunneling networks as a specific access Grid example. Video packet scheduler assumes an architectural framework for multimedia networks based on sub-streams or flows. Each sub-stream has a different QoS requirement, and we can control degree of satisfaction for the required QoS considering network traffic characteristics (e.g. delay, jitter, and packet loss). In this paper, firstly, we designed a classification scheme to partition video data into multiple sub-streams which have their own QoS requirements. Secondly, we designed a management (reservation and scheduling) scheme for sub-streams to support better perceptual video quality such as the bound of end-to-end jitter. We used MPEG-4 advanced visual coding (AVC) for our source coding. We have shown that our video packet scheduling scheme satisfies QoS requirements using real video experiment over DAB networks.", "authors": ["Seong-Whan Kim", "Shan Suthaharan"], "n_citation": 0, "title": "Perceptually tuned packet scheduler for video streaming for DAB based access GRID", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d91aa56d-74eb-4bd7-ac06-21d764621530"}
{"abstract": "In this paper a tabu search algorithm is proposed for the optimization of constrained gas distribution networks. The problem consists in finding the least cost combination of diameters, from a discrete set of commercially available ones, for the pipes of a given gas network, satisfying the constraints related to minimum pressure requirements and upstream pipe conditions. Since this is a nonlinear mixed integer problem, metaheuristic approaches seem to be more suitable and to provide better results than classical optimization methods. In this work, a tabu search heuristics is applied to the problem and the results of the proposed algorithm are compared with the results of a genetic algorithm and two other versions of tabu search algorithms. The results are very promising, regarding both quality of solutions and computational time.", "authors": ["Herbert De Melo Duarte", "Elizabeth Ferreira Gouvea Goldbarg", "Marco C\u00e9sar Goldbarg"], "n_citation": 0, "title": "A tabu search algorithm for optimization of gas distribution networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dbb495d4-3a30-4cd2-b78b-5e20df3ea2d3"}
{"abstract": "Current software and hardware systems, being parallel and reconfigurable, raise new safety and reliability problems, and the resolution of these problems requires new methods. Numerous proposals attempt at reducing the threat of bugs and preventing several kinds of attacks. In this paper, we develop an extension of the calculus of Mobile Ambients, named Controlled Ambients, that is suited for expressing such issues, specifically Denial of Service attacks. We present a type system for Controlled Ambients, which makes resource control possible in our setting.", "authors": ["David Teller", "Pascal Zimmer", "Daniel Hirschkoff"], "n_citation": 0, "title": "Using ambients to control resources", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "dcbedcae-82fb-4a11-9488-ee83165d81d6"}
{"abstract": "We use entropy rates and Schur concavity to prove that, for every integer k > 2, every nonzero rational number q, and every real number \u03b1, the base-k expansions of \u03b1, q+ \u03b1, and qa all have the same finite-state dimension and the same finite-state strong dimension. This extends, and gives a new proof of, Wall's 1949 theorem stating that the sum or product of a nonzero rational number and a Borel normal number is always Borel normal.", "authors": ["David Doty", "Jack H. Lutz", "Satyadev Nandakumar"], "n_citation": 0, "title": "Finite-State Dimension and Real Arithmetic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "de3e0d17-3302-4f17-9e4a-25ca4cf9656c"}
{"abstract": "Workflows are based on different modelling concepts and are described in different representation models. In this paper we present a meta model for block structured workflow models in the form of classical nested control structure representation as well as the frequently used graph representations. We support reuse of elementary and complex activities in several workflow definitions, and the separation of workflow specification from (expanded) workflow models. Furthermore, we provide a set of equivalence transformations which allow to map workflows between different representations and to change the positions of control elements without changing the semantics of the workflow.", "authors": ["Johann Eder", "Wolfgang Gruber"], "n_citation": 0, "title": "A meta model for structured workflows supporting workflow transformations", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e077282c-a65f-4ec5-b086-c42dec275ead"}
{"abstract": "It is well-known that RSA signatures such as FDH, PSS or PSS-R are as secure as RSA is hard to invert in the random oracle (RO) model. Such proofs, however, have never been discovered in the standard model. This paper provides an explanation of this gap by pointing out a strong impossibility of equivalence between inverting RSA and any form of unforgeability for a wide class of RSA signatures. In particular, our impossibility results explicitly assume that the public key is made of a single RSA instance, that hash functions involved in the signature padding are unkeyed and that key generation fulfils a natural property which we call instance-non-malleability. Beyond showing that any RSA-based signature scheme of that type black-box separates the RO model from the standard model in a strong sense, our work leaves the real-life security of well-known signatures in a state of uncertainty.", "authors": ["Pascal Paillier"], "n_citation": 50, "title": "Impossibility proofs for RSA signatures in the standard model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e2038f25-a426-4837-ba76-a4058998f1ca"}
{"abstract": "We consider algorithmic questions concerning the existence, tractability and quality of atomic congestion games, among users that are considered to participate in (static) selfish coalitions. We carefully define a coalitional congestion model among atomic players. Our findings in this model are quite interesting, in the sense that we demonstrate many similarities with the non-cooperative case. For example, there exist potentials proving the existence of Pure Nash Equilibria (PNE) in the (even unrelated) parallel links setting; the Finite Improvement Property collapses as soon as we depart from linear delays, but there is an exact potential (and thus PNE) for the case of linear delays, in the network setting; the Price of Anarchy on identical parallel links demonstrates a quite surprising threshold behavior: it persists on being asymptotically equal to that in the case of the non-cooperative KP-model, unless we enforce a sublogarithmic number of coalitions. We also show crucial differences, mainly concerning the hardness of algorithmic problems that are solved efficiently in the non cooperative case. Although we demonstrate convergence to robust PNE, we also prove the hardness of computing them. On the other hand, we can easily construct a generalized fully mixed Nash Equilibrium. Finally, we propose a new improvement policy that converges to PNE that are robust against (even dynamically forming) coalitions of small size, in pseudo-polynomial time.", "authors": ["Dimitris Fotakis", "Spyros C. Kontogiannis", "Paul Spirakis"], "n_citation": 0, "title": "Atomic Congestion Games Among Coalitions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e5bba845-6bf0-4d6a-b779-6a5a3ed6abc4"}
{"abstract": "The paper discusses the requirements of high-availability that vendors are facing today in the telecommunication area, and some of the solutions that shall be considered to provide such highly reliable systems. Through simulation, we compare multi-cluster redundancy models, namely, configurations of hot standby and load-sharing clusters using Ericsson's TSP clusters as a basis. The TSP cluster simulator is extended to provide the necessary features. Then through examples of simulated processor crashes, cluster failures and reconfigurations we demonstrate some of the issues that need to be taken into account when such systems are designed and dimensioned for fault tolerance.", "authors": ["Maria Toeroe"], "n_citation": 0, "title": "A Simulation-Based Case Study of Multi-cluster Redundancy Solutions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e5f60bc5-a61d-4d78-8e93-c9e9115aea12"}
{"abstract": "While the cost per megabyte of magnetic disk storage is economical, organizations are alarmed by the increasing cost of managing storage. Storage Area Network (SAN) architectures strive to minimize this cost by consolidating storage devices. A SAN is a special-purpose network that interconnects different data storage devices with servers. While there are many definitions for a SAN, there is a general consensus that it provides access at the granularity of a block and is typically used for database applications. In this study, we focus on SAN switches that include an embedded storage management software in support of virtualization. We describe an On-line Re-organization Environment, ORE, that controls the placement of data to improve the average response time of the system. ORE is designed for a heterogeneous collection of storage devices. Its key novel feature is its use of time to quantify the benefit and cost of a migration. It migrates a fragment only when its net benefit exceeds a prespecified threshold. We describe a taxonomy of techniques for fragment migration and employ a trace driven simulation study to quantify their tradeoff. Our performance results demonstrate a significant improvement in response time (order of magnitude) for those algorithms that employ ORE's cost/benefit feature. Moreover, a technique that employs bandwidth of all devices intelligently is superior to one that simply migrates data to the fastest devices.", "authors": ["Shahram Ghandeharizadeh", "Shan Gao", "Chris Gahagan", "Russ Krauss"], "n_citation": 50, "title": "An on-line reorganization framework for SAN file systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e81a4995-a258-4caa-aef9-e05295edce4c"}
{"abstract": "We study orderings?s on reductions in the style of Levy reflecting the growth of information w.r.t. (super)stable sets S of 'values' (such as head-normal forms or Bohm-trees). We show that sets of co-initial reductions ordered by?s form finitary \u03c9-algebraic complete lattices, and hence form computation and Scott domains. As a consequence, we obtain a relativized version of the computational semantics proposed by Boudol for term rewriting systems. Furthermore, we give a pure domain-theoretic characterization of the orderings?s in the spirit of Kahn and Plotkin's concrete domains. These constructions are carried out in the framework of Stable Deterministic Residual Structures, which are abstract reduction systems with an axiomatized residual relations on redexes, that model all orthogonal (or conflict-free) reduction systems as well as many other interesting computation structures.", "authors": ["Zurab Khasidashvili", "John R. W. Glauert"], "n_citation": 1, "title": "Stable computational semantics of conflict-free rewrite systems (Partial orders with duplication)", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e82dd180-2735-4a04-8809-f4a71027c177"}
{"abstract": "One of the most important trends in Information System research is concerning Web application development. Generally, Web Information Systems are developed ad-hoc, that is, without modeling them before their implementation. At best, developers use techniques and methodologies directly imported from the traditional Software Engineering field. However, classic methodologies do not fit to some key aspects of Web applications as, for example, navigation design. This is the reason why different modeling techniques and methodologies for Web Information Systems development have appeared. MIDAS is a methodological framework for Web Information System development that can be customized according to the type of application we want to develop (hypermedia applications, database applications, etc.). In this paper we present MIDAS/BD, the specific part of MIDAS for Web Database development. MIDAS/BD is based on XML and (object -) relational technology.", "authors": ["Esperanza Marcos", "Paloma C\u00e1ceres", "Bel\u00e9n Vela", "Jos\u00e9 Mar\u00eda Cavero"], "n_citation": 0, "title": "MIDAS/BD: A methodological framework for Web database design", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "eb3ed93f-2287-4c0e-8dda-e3eaeb51820f"}
{"abstract": "Affine gap penalties are generally considered appropriate for aligning DNA and protein sequences. (Affine means that a gap of length k is penalized \u03b1 + k\u03b2, i.e., it costs \u03b1 to open up a gap plus \u03b2 for each symbol in the gap.) For certain applications, such as aligning a cDNA sequence with a genomic DNA sequence, it might be adequate to use the restricted affine gap penalties which penalize long gaps with a constant penalty. As it turns out, several techniques developed for solving the approximate string matching problem can be utilized to yield efficient algorithms for computing the optimal alignment with restricted affine gap penalties. In particular, efficient algorithms can be derived based on the suffix automaton with failure transitions and on the diagonalwise monotonicity of the cost tables. To speedup the computation, the q-gram paradigm can be used to locate the interval in the longer sequence that should be aligned with the shorter sequence. We have implemented the above methods in C on Sun workstations running SunOS Unix. Preliminary experiments show that these approaches are very promising for aligning a cDNA sequence with a genomic DNA sequence.", "authors": ["Kuan-Hua Chao"], "n_citation": 0, "title": "Fast algorithms for aligning sequences with restricted affine Gap penalties", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "ec77fc4d-3d24-4569-a38e-57e4f80f7ae6"}
{"abstract": "As technologies develop in faster and more complicated ways, it is getting more important to expect the direction of technological progresses. So many methods are being proposed all around world and one of them is to use patent information. Moreover, with efforts of governments in many countries, many patent analysis methods have been exploited and suggested usually on the basis of patent documents. However, current patent analysis methods have some limitations. In this paper, we suggest a new visualization method for a patent map, which represents patent analysis results with considering both structured and unstructured items of each patent document. And by the adoption of the k-means clustering algorithm and semantic networks, we suggest concrete steps to make a patent map which gives a clear and instinctive insight on the targeted technology. In application, we built up a patent map for the ubiquitous computing technology and discussed an overall view of its progresses.", "authors": ["Jong Hwan Suh", "Sang Chan Park"], "n_citation": 0, "title": "A New Visualization Method for Patent Map : Application to Ubiquitous Computing Technology", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ecd0e3a7-316d-484f-ae49-a2caec6bdcc4"}
{"abstract": "In this paper artificial regulatory networks (ARN) are evolved to match the dynamics of test functions. The ARNs are based on a genome representation generated by a duplication / divergence process. By creating a mapping between the protein concentrations created by gene excitation and inhibition to an output function, the network can be evolved to match output functions such as sinusoids, exponentials and sigmoids. This shows that the dynamics of an ARN may be evolved and thus may be suitable as a method for generating arbitrary time-series for function optimization.", "authors": ["P. Dwight Kuo", "Andr\u00e9 Leier", "Wolfgang Banzhaf"], "n_citation": 0, "title": "Evolving dynamics in an artificial regulatory network model", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ed0478a8-a571-4a00-bb96-d7e96b45e317"}
{"abstract": "This paper proposes a blind watermarking scheme for 3D motion signal based on frequency domain, using addition property of DFT and noise eliminating. The algorithm embeds a watermark into the 3D motion by perturbing the middle frequency coefficients. During watermark extraction, the detected signal is smoothed to get an approximation of original signal by noise eliminating to obtain the watermark noise, which is amplified to extract watermark. Experimental results shows that the algorithm for 3D motion is resilient to many attacks such as Gaussian White Noise, resampling, smoothing, cropping, enhancement, attenuation and rearranging.", "authors": ["Li Li", "Zhigeng Pan", "Shusen Sun"], "n_citation": 0, "title": "A blind watermarking algorithm for animation used in digital heritage", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ee1f1691-d1ac-416d-bddd-7a0f2dc3db4e"}
{"abstract": "In this paper we introduce and investigate a logic for the schema calculus of Z. The schema calculus is arguably the reason for Z's popularity but so far no true calculus (a sound system of rules for reasoning about schema expressions) has been given. Presentations to date have either failed to provide a calculus (e.g. the draft standard [3]) or have fallen back on informal descriptions at a syntactic level (most text books e.g. [7]). Alongside the calculus, we introduce a derived equational logic; this enables us to formalise properly the informal notions of schema expression equality to be found in the literature.", "authors": ["Martin C. Henson", "S. Reeves"], "n_citation": 0, "title": "A logic for the schema calculus", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "efc4d9eb-a328-4db0-aabc-031b96054b05"}
{"abstract": "Most current Grid security techniques concentrate on traditional security aspects such as authentication, authorization, etc. While they have shown their usefulness, the significance of the information hidden in the historical data corpus denoting the user-Grid interactions has been largely neglected. In fact, such information provide great insight into Grid security and if properly harnessed, will help better protect the Grid against potential attacks. To utilize these hidden information in a service-oriented Grid environment, we propose a hybrid machine learning and statistical model. The machine learning component predicts the security of a service by considering the probability distribution of the past services, while the statistical component evaluates a service's security statistically based on its own past behaviors and users' opinions. We construct an overall architecture based on this hybrid model and demonstrate through examples its effectiveness and potential to offer stronger security to the Grid.", "authors": ["Guang Xiang", "Ge Yu", "Xiangli Qu", "Xiaomei Dong", "Lina Wang"], "n_citation": 0, "title": "A hybrid machine learning/statistical model of Grid security", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f06b7c9d-8fac-4ac7-9e2b-aa9b1af9b887"}
{"abstract": "Equational tree automata provide a powerful tree language framework that facilitates to recognize congruence closures of tree languages. In the paper we show the emptiness problem for AC-tree automata and the intersection-emptiness problem for regular AC-tree automata, each of which was open in our previous work [20], are decidable, by a straightforward reduction to the reachability problem for ground AC-term rewriting. The newly obtained results generalize decidability of so-called reachable property problem of Mayr and Rusinowitch [17]. We then discuss complexity issue of AC-tree automata. Moreover, in order to solve some other questions about regular A- and AC-tree automata, we recall the basic connection between word languages and tree languages.", "authors": ["Hitoshi Ohsaki", "Toshinori Takai"], "n_citation": 0, "title": "Decidability and closure properties of equational tree languages", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f283b42d-9e69-43d8-85ad-85d108cd7130"}
{"abstract": "Simultaneous recording of multiple spike trains from population of neurons provides the possibility for understanding how neurons work together in response to various stimulations. But currently method is still lacking for researchers to perform multiple spike train data analysis and those existing techniques either allow people to analyze pairwise neuronal activities only or are seriously subject to the selection of parameters. In this paper, a new measurement of information discrepancy, which is based on the comparisons of subsequence distributions, is applied to deal with a group of spike trains (n > 2) and analyze the synchronization pattern among the neurons, where the analytical result mostly depends on the experimental data and is affected little by subjective interference.", "authors": ["Guang-Li Wang", "Xue Liu", "Pu-Ming Zhang", "Pei-Ji Liang"], "n_citation": 50, "title": "A New Method for Multiple Spike Train Analysis Based on Information Discrepancy", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f35ed78c-cb80-44a1-8b6f-e4aa9b757e62"}
{"abstract": "In this article, we introduce a general framework for monitoring patterns and detecting interesting changes without continuously mining the data. Using our approach, the effort spent on data mining can be drastically reduced while the knowledge extracted from the data is kept up to date. Our methodology is based on a temporal representation for patterns, in which both the content and the statistics of a pattern are modeled. We divide the KDD process into two phases. In the first phase, data from the first period is mined and interesting rules and patterns are identified. In the second phase, using the data from subsequent periods, statistics of these rules are extracted in order to decide whether or not they still hold. We applied this technique in a case study on mining mail log data. Our results show that a minimal set of patterns reflecting the invariant properties of the dataset can be identified, and that interesting changes to the population can be recognized indirectly by monitoring a subset of the patterns found in the first phase.", "authors": ["Steffan Baron", "Myra Spiliopoulou", "Oliver G\u00fcnther"], "n_citation": 0, "title": "Efficient monitoring of patterns in data mining environments", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f3dbbcdf-d19c-44c5-b8a4-d30701db5893"}
{"abstract": "In this paper we present a new technique for worst-case analysis of compression algorithms which are based on the Burrows-Wheeler Transform. We deal mainly with the algorithm purposed by Burrows and Wheeler in their first paper on the subject [6], called BW0. This algorithm consists of the following three steps: 1) Compute the Burrows-Wheeler transform of the text, 2) Convert the transform into a sequence of integers using the move-to-front algorithm, 3) Encode the integers using Arithmetic code or any order-0 encoding (possibly with run-length encoding). We prove a strong upper bound on the worst-case compression ratio of this algorithm. This bound is significantly better than bounds known to date and is obtained via simple analytical techniques. Specifically, we show that for any input string s, and \u03bc > 1, the length of the compressed string is bounded by \u03bc, |s|H k (s) + log(\u03b6(\u03bc Is + g k  where H k  is the k-th order empirical entropy, g k  is a constant depending only on k and on the size of the alphabet, and \u03b6(\u03bc) = 1 1\u03bc + 1 2\u03bc +... is the standard zeta function. As part of the analysis we prove a result on the compressibility of integer sequences, which is of independent interest. Finally, we apply our techniques to prove a worst-case bound on the compression ratio of a compression algorithm based on the Burrows-Wheeler transform followed by distance coding, for which worst-case guarantees have never been given. We prove that the length of the compressed string is bounded by 1.7286 |s|H k (s) + g k . This bound is better than the bound we give for BW0.", "authors": ["Haim Kaplan", "Shir Landau", "Elad Verbin"], "n_citation": 0, "title": "A simpler analysis of burrows-wheeler based compression", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f417c646-abbd-4b3f-840f-6ddc3ce83e58"}
{"abstract": "We present a method that checks Query Containment for queries with negated IDB predicates. Existing methods either deal only with restricted cases of negation or do not check actually containment but uniform containment, which is a sufficient but not necessary condition for containment. Additionally, our queries may also contain equality, inequality and order comparisons. The generality of our approach allows our method to deal straightforwardly with query containment under constraints. Our method is sound and complete both for success and for failure and we characterize the databases where these properties hold. We also state the class of queries that can be decided by our method.", "authors": ["Carles Farr\u00e9", "Ernest Teniente", "Toni Urp\u00ed"], "n_citation": 0, "title": "Query Containment with negated IDB predicates", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f4b261a1-69c9-49f9-ab3d-dc2b605d580d"}
{"abstract": "While the complexity of min-max and min-max regret versions of most classical combinatorial optimization problems has been thoroughly investigated, there are very few studies about their approximation. For a bounded number of scenarios, we establish a general approximation scheme which can be used for min-max and min-max regret versions of some polynomial problems. Applying this scheme to shortest path and minimum spanning tree, we obtain fully polynomial-time approximation schemes with much better running times than the ones previously presented in the literature.", "authors": ["Hassene Aissi", "Cristina Bazgan", "Daniel Vanderpooten"], "n_citation": 0, "title": "Approximating Min-Max (Regret) Versions of Some Polynomial Problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f66bd65b-5645-44c2-ae2c-e66af93aebbe"}
{"abstract": "Understanding texture regularity in real images is a challenging computer vision task. We propose a higher-order feature matching algorithm to discover the lattices of near-regular textures in real images. The underlying lattice of a near-regular texture identifies all of the texels as well as the global topology among the texels. A key contribution of this paper is to formulate lattice-finding as a correspondence problem. The algorithm finds a plausible lattice by iteratively proposing texels and assigning neighbors between the texels. Our matching algorithm seeks assignments that maximize both pair-wise visual similarity and higher-order geometric consistency. We approximate the optimal assignment using a recently developed spectral method. We successfully discover the lattices of a diverse set of unsegmented, real-world textures with significant geometric warping and large appearance variation among texels.", "authors": ["James Hays", "Marius Leordeanu", "Alexei A. Efros", "Yanxi Liu"], "n_citation": 0, "title": "Discovering Texture Regularity as a Higher-Order Correspondence Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f70f869d-73ac-4451-b907-7c387860266c"}
{"abstract": "Developing techniques to increase the availability of web services in the event of failure has become increasingly important given their key role in providing access to online information, financial, and retail resources. This paper describes an approach to improving availability by using failover between similar but not identical services, and the use of cooperative fault tolerance between the providers of these services. With this approach, a similar service can be used as a backup, with the protocol and service differences between the two services masked by the use of transformation web services that are generated semi-automatically. The basic idea of cooperative fault-tolerance using similar services is presented based on an example involving two stock broker services. Tlie software architecture and the process for generating the transformation web services using a code generation tool are also described, along with experimental results from the stock broker example. These results suggest that the transformation overhead is modest compared with the typical cost of communication.", "authors": ["Toshiyuki Moritsu", "Matti A. Hiltunen", "Richard D. Schlichting", "Junichi Toyouchi", "Yasuharu Namba"], "n_citation": 50, "title": "Using Web Service Transformations to Implement Cooperative Fault Tolerance", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f85ac920-b16d-4d8b-a8fd-bc3d3ca44e8a"}
{"abstract": "In this paper we propose a new, lightweight, no bandwidth consuming authentication and integrity scheme for VoIP service based on SIP as a signalling protocol. It is shared password mechanism and this solution exploits digital watermarking. Nowadays, there are many applications of this technique, such as solving copyright protection problems, but we propose to use it to secure the transmitted audio and signalling protocol that IP Telephony is based on simultaneously. This solution can be the potential answer to the problem VoIP faces today: finding a scalable and universal mechanism for securing VoIP traffic (voice and the signalling protocol messages) at the same time. It can greatly improve, if we combine it with existing security mechanisms, overall IP Telephony system's security.", "authors": ["Wojciech Mazurczyk", "Zbigniew Kotulski"], "n_citation": 50, "title": "New VoIP Traffic Security Scheme with Digital Watermarking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fae7b488-876d-4cb9-a651-bfe38f406f56"}
{"abstract": "Estimation of local orientations in multivariate signals (including optical flow estimation as special case of orientation in space-time-volumes) is an important problem in image processing and computer vision. Modelling a signal using only a single orientation is often too restrictive, since occlusions and transparency happen frequently, thus necessitating the modelling and analysis of multiple orientations. In this paper, we therefore develop a unifying mathematical model for multiple orientations: beyond describing an arbitrary number of orientations in multivariate vector-valued image data such as color image sequences, it allows the unified treatment of transparently and occludingly superimposed oriented structures. Based on this model, we derive novel estimation schemes for an arbitrary number of superimposed orientations in bivariate images as well as for double orientations in signals of arbitrary signal dimensionality. The estimated orientations themselves, but also features like the number of local orientations or the angles between multiple orientations (which are invariant under rotation) can be used for various inspection, tracking and segmentation problems. We evaluate the performance of our framework on both synthetic and real data.", "authors": ["Matthias M\u00fchlich", "Til Aach"], "n_citation": 50, "title": "A Theory of Multiple Orientation Estimation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fbd51901-75e6-4c1d-a307-ba4171414af3"}
{"abstract": "Traditional cache replacement algorithms are not easily applicable to a dynamic and heterogeneous environment. Moreover, the frequently used hit-ratio and byte-hit ratio are not appropriate measures in grid applications, because non-uniformity of the resource object sizes and non-uniformity cost of cache misses in resource information traffic. In this paper, we propose a Least Frequently Cost cache replacement algorithm based on at most K backward references, LFC-K. We define average retrieval cost ratio (ARCR), as the cost saved by using a cache divided by the total retrieval cost if no cache was used. We compare performance of LFC-K with other caching algorithms using ARCR, hit-ratio and byte-hit ratio as performance metrics. Our experimental results indicate that LFC-2 outperforms LRU, LFU and LFU-2.", "authors": ["Dong Li", "Linpeng Huang", "Minglu Li"], "n_citation": 0, "title": "LFC-K cache replacement algorithm for grid index information service (GIIS)", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fc6eb774-5f8e-44ee-8e50-cece99dc6120"}
{"abstract": "It is difficult to communicate graphical ideas or images to computers using current WIMP-style GUI. Freeform User Interfaces is an interface design framework that leverages the power of freeform strokes to achieve fluent interaction between users and computers in performing graphical tasks. Users express their graphical ideas as freeform strokes using pen-based systems, and the computer takes appropriate actions based on the perceptual features of the strokes. The results are displayed in an informal manner to facilitate exploratory thinking. This paper explores the concept of Freeform UI and shows its possibilities with four example systems: beautification and prediction for 2D geometric drawing, a stroke-based 3D navigation, an electronic office whiteboard, and a sketch-based 3D freeform modeling. While Freeform UI is not suitable for precise, production-oriented applications because of its ambiguity and imprecision, it does provide a natural, highly interactive computing environment for pre-productive, exploratory activities in various graphical applications.", "authors": ["Takeo Igarashi"], "n_citation": 50, "title": "Freeform User Interfaces for graphical computing", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "fd1318e5-fa58-4deb-b7e0-84fb470845af"}
{"abstract": "Reduction theorems allow one to deduce properties of a concurrent system specification from properties of a simpler, coarser-grained version called the reduced specification. We present reduction theorems based upon a more precise relation between the original and reduced specifications than earlier ones, permitting the use of reduction to reason about a larger class of properties. In particular, we present reduction theorems that handle general liveness properties.", "authors": ["Ernie Cohen", "Leslie Lamport"], "n_citation": 92, "title": "Reduction in TLA", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "fee33df3-e8b8-423a-9895-5ca54427719a"}
{"abstract": "Communities play an important role for knowledge creation in the knowledge society. Conversational communications play a primary means for supporting a collective activity of people for knowledge creation, management, and application. In this paper, I propose a framework of the conversational knowledge process for supporting communities, and present the knowledge channel approach featuring knowledge cards for representing conversational units, the knowledge lifecycle support, and the strategic control of information stream. I show some implemented systems to show how these ideas are implemented.", "authors": ["Toyoaki Nishida"], "n_citation": 0, "title": "Supporting the conversational knowledge process in the networked community", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "03b5f8a6-d4f3-46f4-97a5-c0a3e4b568e4"}
{"abstract": "Mathematical modelling of the spread of infectious diseases is a well established field with high practical importance. Underlying most analytical approaches is the assumption of perfect mixing, that is the idea that the spatial structure of the population can be neglected. This assumption is crucial to the solvability of the models, but can be dropped when using computational models instead of analytical approaches. Using methods from Artificial Life, we investigate under which conditions the perfect mixing assumption becomes a good approximation to describe the spread of vector borne disease in a population with spatial structure.", "authors": ["Dominique Chu", "Jonathan E. Rowe"], "n_citation": 0, "title": "Spread of vector borne diseases in a population with spatial structure", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "07a6fc59-2f23-41ee-8ee7-4e3cba7b3127"}
{"abstract": "This paper presents a semi-automatic approach for extracting knowledge from natural language texts in Spanish. The knowledge is acquired and learned through the combination of NLP techniques for analyzing text fragments, the ontological technology for representing knowledge and MCRDR, a case-based reasoning methodology. This approach has been applied in the oncology domain and the results of this application are discussed in this work.", "authors": ["Rafael Valencia-Garc\u00eda", "Dagoberto Castellanos-Nieves", "Jesualdo Tom\u00e1s Fern\u00e1ndez-Breis", "Pedro Jos\u00e9 Vivancos-Vicente"], "n_citation": 0, "title": "A methodology for extracting ontological knowledge from Spanish documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0941602a-be32-44f6-87b1-402201707eaa"}
{"abstract": "Set-valued attributes are convenient to model complex objects occurring in the real world. Currently available database systems support the storage of set-valued attributes in relational tables but contain no primitives to query them efficiently. Queries involving set-valued attributes either perform full scans of the source data or make multiple passes over single-value indexes to reduce the number of retrieved tuples. Existing techniques for indexing set-valued attributes (e.g., inverted files, signature indexes or RD-trees) are not efficient enough to support fast access of set-valued data in very large databases. In this paper we present the hierarchical bitmap index-a novel technique for indexing set-valued attributes. Our index permits to index sets of arbitrary length and its performance is not affected by the size of the indexed domain. The hierarchical bitmap index efficiently supports different classes of queries, including subset, superset and similarity queries. Our experiments show that the hierarchical bitmap index outperforms other set indexing techniques significantly.", "authors": ["Miko\u0142aj Morzy", "Tadeusz Morzy", "Alexandros Nanopoulos", "Yannis Manolopoulos"], "n_citation": 0, "title": "Hierarchical bitmap index: An efficient and scalable indexing technique for set-valued attributes", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "0a31751a-49af-4dca-b6d7-c73c8b627573"}
{"abstract": "In recent years, mobile ad-hoc networks (MANETs) have been deployed in various scenarios, but their scalability is severely restricted by the human operators' ability to configure and manage the network in the face of rapid change of the network structure and demand patterns. In this paper, we present a self-organizing approach to MANET management that follows general principles of engineering swarming applications.", "authors": ["Sven Brueckner", "H. Van Dyke Parunak"], "n_citation": 0, "title": "Self-organizing MANET management", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0b573eb3-15ad-44ed-90aa-9ca615410241"}
{"abstract": "We propose a nonparametric local linear logistic approach based on local likelihood in multi-class discrimination. The combination of the local linear logistic discriminant analysis and partial least square components yields better prediction results than the conventional statistical classifiers in case where the class boundaries have curvature. We applied our method to both synthetic and real data sets.", "authors": ["Jangsun Baek", "Young Sook Son"], "n_citation": 0, "title": "Local Linear Logistic Discriminant Analysis with Partial Least Square Components", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0fb8929b-c8c4-4ed7-b86c-49b3840918ef"}
{"abstract": "A new type of resource discovery mechanism is proposed for simulation oriented data grid. By introducing distributed resource brokers at edge routers of each domain, a majority of resource discovery queries can be processed solely by resource brokers at edge routers, instead of accessing remote global resource broker. The response time and network overhead of resource discovery is reduced and system scalability is enhanced. Experiment results show that the proposed resource discovery mechanism is feasible.", "authors": ["Hai Huang", "Shaofeng Wang", "Yan Zhang", "Wei Wu"], "n_citation": 0, "title": "Resource discovery mechanism for large-scale distributed simulation oriented data grid", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "136154ab-9a1e-4963-8bed-af59b911e80d"}
{"abstract": "Digital information economies require information goods producers to learn how to position themselves within a potentially vast product space. Further, the topography of this space is often nonstationary, due to the interactive dynamics of multiple producers changing their positions as they try to learn the distribution of consumer preferences and other features of the problem's economic structure. This presents a producer or its agent with a difficult learning problem: how to locate profitable niches in a very large space. In this paper, we present a model of an information goods duopoly and show that, under complete information, producers would prefer not to compete, instead acting as local monopolists and targeting separate niches in the consumer population. However, when producers have no information about the problem they are solving, it can be quite difficult for them to converge on this solution. We show how a modest amount of economic knowledge about the problem can make it much easier, either by reducing the search space, starting in a useful area of the space, or introducing a gradient. These experiments support the hypothesis that a producer using some knowledge of a problem's (economic) structure can outperform a producer that is performing a naive, knowledge-free form of learning.", "authors": ["Christopher H. Brooks", "Robert S. Gazzale", "Jeffrey K. MacKie Mason", "Edmund H. Durfee"], "n_citation": 0, "title": "Improving learning performance by applying economic knowledge", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "16a86583-4cf1-4fc9-9179-e2a99ef81066"}
{"abstract": "Although there are many register allocation algorithms that work well, it can be difficult to correctly implement these algorithms. As a result, it is common for bugs to remain in the register allocator, even after the compiler is released. The register allocator may run, but bugs can cause it to produce incorrect output code. The output program may even execute properly on some test data, but errors can remain. In this paper, we propose novel data flow analyses to statically check that the output code from the register allocator is correct in terms of its data dependences. The approach is accurate, fast, and can identify and report error locations and types. No false alarms are produced. The paper describes our approach, called SARAC, and a tool, called ra-analyzer, that statically checks a register allocation and reports the errors it finds. The tool has an average compile-time overhead of only 8% and a modest average memory overhead of 85KB.", "authors": ["Yuqiang Huang", "Bruce R. Childers", "Mary Lou Soffa"], "n_citation": 50, "title": "Catching and identifying bugs in register allocation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "16eb5793-3ebd-45ea-a9ee-376200977635"}
{"abstract": "In spite of the recent quick growth of the Evolutionary Multi-objective Optimization (EMO) research field, there has been few trials to adapt the general variation operators to the particular context of the quest for the Pareto-optimal set. The only exceptions are some mating restrictions that take in account the distance between the potential mates - but contradictory conclusions have been reported. This paper introduces a particular mating restriction for Evolutionary Multi-objective Algorithms, based on the Pareto dominance relation: the partner of a non-dominated individual will be preferably chosen among the individuals of the population that it dominates. Coupled with the BLX crossover operator, two different ways of generating offspring are proposed. This recombination scheme is validated within the well-known NSGA-II framework on three bi-objective benchmark problems and one real-world bi-objective constrained optimization problem. An acceleration of the progress of the population toward the Pareto set is observed on all problems.", "authors": ["Olga Rudenko", "Marc Schoenauer"], "n_citation": 0, "title": "Dominance based crossover operator for evolutionary multi-objective algorithms", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "182e6c32-5615-4492-aefa-072ca87673c7"}
{"abstract": "An automatic traffic sign detection system would be important in a driver assistance system. In this paper, an approach for detecting numbers on speed limit signs is proposed. Such a system would have to provide a high recognition performance in real-time. Thus, in this paper we propose to apply evolvable hardware for the classification of the numbers extracted from images. The system is based on incremental evolution of digital logic gates. Experiments show that this is a very efficient approach.", "authors": ["Jim Torresen", "Jorgen W. Bakke", "Lukas Sekanina"], "n_citation": 0, "title": "Recognizing speed limit sign numbers by evolvable hardware", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1c2e7dbc-df90-4ee4-9af3-95d04aa3795d"}
{"abstract": "Verification of parameterized systems for an arbitrary number of instances is generally undecidable. Existing approaches resort to non-trivial restrictions on the system or lack automation. In practice, applications can often provide a suitable bound on the parameter size. We propose a new technique toward the bounded formulation of parameterized reasoning: how to efficiently verify properties of a family of systems over a large finite parameter range. We show how to accomplish this with a single verification run on a model that aggregates the individual instances. Such a run takes significantly less time than if the systems were considered one by one. Our method is applicable to a completely inhomogeneous family of systems, where properties may not even be preserved across instances. In this case the method exposes the parameter values for which the verification fails. If symmetry is present in the systems, it is inherited by the aggregate representation, allowing for verification over a reduced model. Our technique is fully automatic and requires no approximation.", "authors": ["E. Allen Emerson", "Richard J. Trefler", "Thomas Wahl"], "n_citation": 0, "title": "Reducing Model Checking of the Few to the One", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1c3f5bcd-badd-4b8b-8350-04eea6f361e5"}
{"abstract": "A genetic algorithm for finding cocyclic Hadamard matrices is described. Though we focus on the case of dihedral groups, the algorithm may be easily extended to cover any group. Some executions and examples are also included, with aid of MATHEMATICA 4.0.", "authors": ["V\u00edctor \u00c1lvarez", "Jos\u00e9 \u00c1ndr\u00e9s Armario", "Mar\u00eda Dolores Frau", "Pedro Real"], "n_citation": 0, "title": "A Genetic Algorithm for Cocyclic Hadamard Matrices", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1d53fc43-76a6-4317-92ef-d6397c902844"}
{"abstract": "We uncover a new class of attacks that can potentially affect any cryptographic protocol. The attack is performed by an adversary that at some point has access to the physical memory of a participant, including all its previous states. In order to protect protocols from such attacks, we introduce a cryptographic primitive that we call erasable memory. Using this primitive, it is possible to implement the essential cryptographic action of forgetting a secret. We show how to use a small erasable memory in order to transform a large non-erasable memory into a large and erasable memory. In practice, this shows how to turn any type of storage device into a storage device that can selectively forget. Moreover, the transformation can be performed using the minimal assumption of the existence of any one-way function, and can be implemented using any block cipher, in which case it is quite efficient. We conclude by suggesting some concrete implementations of small amounts of erasable memory.", "authors": ["G. di Crescenzo", "Niels Ferguson", "Russell Impagliazzo", "Markus Jakobsson"], "n_citation": 0, "title": "How to forget a secret", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "2107a809-217b-43a2-a1ee-1afd9dbeeb90"}
{"abstract": "Back and von Wright have developed algebraic laws for reasoning about loops in the refinement calculus. We extend their work to reasoning about probabilistic loops in the probabilistic refinement calculus. We apply our algebraic reasoning to derive transformation rules for probabilistic action systems. In particular we focus on developing data refinement rules for probabilistic action systems. Our extension is interesting since some well known transformation rules that are applicable to standard programs are not applicable to probabilistic ones: we identify some of these important differences and we develop alternative rules where possible. In particular, our probabilistic action system data refinement rules are new.", "authors": ["Larissa Meinicke", "Ian J. Hayes"], "n_citation": 0, "title": "Reasoning Algebraically About Probabilistic Loops", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "23712d63-694b-4c4f-bc96-c0bd1781adbb"}
{"abstract": "To dynamically improve node selections of the waiting jobs under de-centralized scheduling, each node together with its neighbors is assumed to compose a subgrid, using distributed backfilling to optimize grid scheduling on each subgrid. Whenever a job terminates, distributed backfilling is triggered to rebackfill all waiting jobs on the corresponding subgrid in order of their submittal. Each subgrid is overlapped with some other, so the waiting jobs may be migrated around the grid. A simulated grid is established, while grid workload is modeled by extending workload models of parallel systems. Job speedup is used to evaluate scheduling strategies. Results show the dynamic optimization of node selections brought by distributed backfilling is grid-wide, and can improve scheduling performance remarkably as long as grid load is not too light and the job migration costs are not too high.", "authors": ["Qingjiang Wang", "Xiaolin Gui", "Shouqi Zheng", "Bing Xie"], "n_citation": 0, "title": "De-centralized job scheduling on computational grids using distributed backfilling", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "248059fd-4e5b-4f34-b8f7-d7fa54d8ce7b"}
{"abstract": "Many business organizations generate a huge amount of transaction data. Association rule mining is a powerful analysis tool to extract the useful meanings and associations from large databases and many automated systems have been developed for mining association rules. However, most of these systems usually mine many association rules from large databases and it is not easy for a user to extract meaningful rules. Visualization has become an important tool in the data mining process for extracting meaningful knowledge and information from large data sets. Though there are several techniques for visualizing mined association rules, most of these techniques visualize the entire set of discovered association rules on a single screen. Such a dense display can overwhelm analysts and reduce their capability of interpretation. In this paper we present a novel technique called VisAR for visualizing mined association rules. VisAR consists of four major stages for visualizing mined association rules. These stages include managing association rules, filtering association rules of interest, visualizing selected association rules, and interacting with the visualization process. Our technique allows an analyst to view only a particular subset of association rules which contain selected items of interest. VisAR is able to display not only many-to-one but also many-to-many association rules. Moreover, our technique can overcome problems of screen clutter and occlusion.", "authors": ["Kesaraporn Techapichetvanich", "Amitava Datta"], "n_citation": 50, "title": "VisAR : A new technique for visualizing mined association rules", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "26b0750b-ecf4-4370-85d0-882f74d0f825"}
{"abstract": "Recently rule based languages focussed on the use of rewriting as a modeling tool which results in making specifications executable. To extend the modeling capabilities of rule based languages, we explore the possibility of making the rule applications subject to probabilistic choices. We propose an extension of the ELAN strategy language to deal with randomized systems. We argue through several examples that we propose indeed a natural setting to model systems with randomized choices. This leads us to interesting new problems, and we address the generalization of the usual concepts in abstract reduction systems to randomized systems.", "authors": ["Olivier Bournez", "Claude Kirchner"], "n_citation": 0, "title": "Probabilistic rewrite strategies. Applications to ELAN", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "28c57ea4-41b6-48c5-9e90-1a224109528d"}
{"abstract": "The possibility to share content easily among e-business partners constitutes one of the Web's potentials. Especially XML facilitates interoperability between different e-business partners by providing a standard representation of content. But the support of reacting automatically to changes in related documents is still weak. Active XML Schemas extend XML Schemas in that respect. Active XML Schemas employ concepts from active database systems, from conceptual modeling of business rules, and from event-based systems to provide for automatic and asynchronous management of distributed web content. Different to previous approaches of employing the event-condition-action (ECA) paradigm in a web-based setting, active XML Schemas integrate passive and active behavior smoothly into document schemas in an object-oriented way, treat events as first class elements that can be stored and queried just as other document data, and provide for logical events that are defined upon querying past and scheduled events.", "authors": ["Michael Schrefl", "Martin Bernauer"], "n_citation": 0, "title": "Active XML Schemas", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2c98fa28-b71e-4dc8-8470-4106b2fb4232"}
{"abstract": "Lazy proof explication is a theorem-proving architecture that allows a combination of Nelson-Oppen-style decision procedures to leverage a SAT solver's ability to perform propositional reasoning efficiently. The SAT solver finds ways to satisfy a given formula propositionally, while the various decision procedures perform theory reasoning to block propositionally satisfied instances that are not consistent with the theories. Supporting quantifiers in this architecture poses a challenge as quantifier instantiations can dynamically introduce boolean structure in the formula, requiring a tighter interleaving between propositional and theory reasoning. This paper proposes handling quantifiers by using two SAT solvers, thereby separating the propositional reasoning of the input formula from that of the instantiated formulas. This technique can then reduce the propositional search space, as the paper demonstrates. The technique can use off-the-shelf SAT solvers and requires only that the theories are checkpointable.", "authors": ["K. Rustan M. Leino", "Madan Musuvathi", "Xinming Ou"], "n_citation": 50, "title": "A two-tier technique for supporting quantifiers in a lazily proof-explicating theorem prover", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2d4c5123-2435-4631-b25c-2c08199204b9"}
{"abstract": "Employing computer techniques to visualize the color changing of Dunhuang Frescoes is of great significance since the audience can re-experience this process and the protectors can discuss the protection measures according to the simulated video clips and thus help them make their decision more scientifically. But previously, because colorimetric and chemical modeling was not introduced to simulate the color changing of frescoes, there was a lack of objectivity and sense of reality. In this paper, however, two kinds of color changing, namely, color changing due to chemical changing of the pigment as well as dust accumulation, are modeled. Since this modeling describes the natural behavior of the color changing process, the simulation result is quite striking.", "authors": ["Xifan Shi", "Dongming Lu", "Jianming Liu", "Yunhe Pan"], "n_citation": 50, "title": "An integrated color changing simulation system based on colorimetric and chemical modeling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d8cfa63-5d2d-4450-b2e5-229ebc40ec37"}
{"abstract": "Variational Bayesian learning is proposed for approximation method of Bayesian learning. In spite of efficiency and experimental good performance, their mathematical property has not yet been clarified. In this paper we analyze variational Bayesian Stochastic Context Free Grammar which includes the true distribution thus the model is non-identifiable. We derive their asymptotic free energy. It is shown that in some prior conditions, the free energy is much smaller than identifiable models and satisfies eliminating redundant non-terminals.", "authors": ["Tikara Hosino", "Kazuho Watanabe", "Sumio Watanabe"], "n_citation": 50, "title": "Free Energy of Stochastic Context Free Grammar on Variational Bayes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3040a9bf-1380-497c-acd0-b148100754dc"}
{"abstract": "In this paper we present an original approach for personalizing complex Web applications, in particular e-commerce applications. This approach is based on a clear separation of concerns, namely: base application functionality, user profile management, and personalization rules, and supports seamless addition of personalization features (such as recommendations, special offers, individual interfaces, etc). We first explain our view of e-commerce applications as views on application models, and briefly explain why personalization functionality should be dealt by separating concerns. We next introduce a simple example and focus on different personalization patterns, emphasizing on behavior personalization. We show which design structures are the most appropriated for obtaining seamless extensions to existing software. We finally discuss some further aspects in building customized e-commerce software.", "authors": ["Juan Cappi", "Gustavo Rossi", "Andres Fortier", "Daniel Schwabe"], "n_citation": 0, "title": "Seamless personalization of E-commerce applications", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "30aa79f9-beb9-431f-942e-73ff1949a302"}
{"abstract": "Recently, a lot of research efforts in software engineering have focused on integrating business modeling as a key piece in requirements engineering. In these research works, the business models are proposed as the source of the software requirements specification process. However, the majority of these works focus only on the definition of notations that permit the representation of the semantics of the organizational context, and only a few works define processes to generate business models and to use these to generate a requirements model. This lack of both generation methods and traceability relationships between models makes practical application in software development enterprises difficult. The objective of this paper is to define a goal-based methodological approach for the generation of business models and to use these models as the starting point for the process of software requirements specification. This will enable us to develop information systems that integrate the necessary functionality so that the business actors perform their tasks and fulfill their goals.", "authors": ["Hugo Estrada", "Alicia Mart\u00ednez", "Oscar Pastor"], "n_citation": 0, "title": "Goal-based business modeling oriented towards late requirements generation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "3b72582d-beeb-48f2-a158-a70974586cad"}
{"authors": ["Gabriele Costa", "Pierpaolo Degano", "Fabio Martinelli"], "n_citation": 0, "title": "Modular Plans for Secure Service Composition", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "3c102ef3-9509-447d-98ff-939ee37a110f"}
{"abstract": "An XML algebra supporting the XQuery query language is presented. The usage of expression constructing operators instead of high-order operations using functions as parameters has permitted us to remain in the limits of first-order structures whose instance is a many-sorted algebra. The set of operators of the presented algebra substantially differs from the set of operators of relation algebra. It is caused by the complex nature of the XML data model comparing with relational one. Actually, only predicative selection is more or less same in both algebra. Yet, the XML algebra in addittion permits selection by node test. The relational projection operator is replaced by the path expression and navigating functions; the join operator is replaced by unnesting join expressions. In addition, a number of node constructing expressions permitting update of the algebra state are defined.", "authors": ["Leonid Novak", "Alexandre V. Zamulin"], "n_citation": 0, "title": "An XML algebra for XQuery", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c5702aa-6f23-4e13-948d-a77fb9370ff5"}
{"abstract": "Recently, the research of E-learning system part applies item analysis method for efficient study. Item analysis method used question analysis for increasing of learners study ability. That is, when compiling the data for local programs, item difficulty, one of item analysis, is used for the Intermediate-Level technology education test. But guessing factor for items in learning results has to be considered to apply the relative item difficulty more precisely. In this paper, I have implemented component based E-learning system which learners are able to select study step by item difficulty. This system was implemented by CBD(Component-Based Development) for efficient development. In this applied result, I have shown efficiency of development in E-learning system by CBD and learning operation in this system by item difficulty.", "authors": ["Hwa-Young Jeong"], "n_citation": 0, "title": "Component based E-learning system using item analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3fa3741d-d4ea-494d-a055-d1592b598deb"}
{"abstract": "We present a method that automatically partitions a single image into non-overlapping regions coherent in texture and colour. An assumption that each textured or coloured region can be represented by a small template, called the seed, is used. Positioning of the seed across the input image gives many possible sub-segmentations of the image having same texture and colour property as the pixels behind the seed. A probability map constructed during the sub-segmentations helps to assign each pixel to just one most probable region and produce the final pyramid representing various detailed segmentations at each level. Each sub-segmentation is obtained as the min-cut/max-flow in the graph built from the image and the seed. One segment may consist of several isolated parts. Compared to other methods our approach does not need a learning process or a priori information about the textures in the image. Performance of the method is evaluated on images from the Berkeley database.", "authors": ["Branislav Micusik", "Allan Hanbury"], "n_citation": 0, "title": "Automatic Image Segmentation by Positioning a Seed", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "40153f01-8ad3-446a-8f04-10819b7a8460"}
{"abstract": "The unlabeled document or text collections are becoming larger and larger which is common and obvious; mining such data sets are a challenging task. Using the simple word-document frequency matrix as feature space the mining process is becoming more complex. The text documents are often represented as high dimensional about few thousand sparse vectors with sparsity about 95 to 99% which significantly affects the efficiency and the results of the mining process. In this paper, we propose the two-stage Non-negative Matrix Factorization (NMF): in the first stage we tried to extract the uncorrelated basis probabilistic document feature vectors by significantly reducing the dimension of the feature vectors of the word-document frequency from few thousand to few hundred, and in the second stage for clustering or classification. In our propose approach it has been observed that the clustering or classification performance with more than 98.5% accuracy. The dimension reduction and classification performance has observed for the Classic3 dataset.", "authors": ["Paresh Chandra Barman", "Nadeem Iqbal", "Soo-Young Lee"], "n_citation": 0, "title": "Non-negative Matrix Factorization Based Text Mining : Feature Extraction and Classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4092071a-3d81-45f3-819b-e0a691835349"}
{"abstract": "Specifications are critical to companies involved in complex manufacturing. The constant reading, reviewing, and analysis of materials and process specifications is extremely labor-intensive, quality-impacting, and time-consuming. A conceptual design for a tool that provides computer-assistance in the interpretation of specification requirements has been created and a strategy for semantic-markup, which is the overlaying of abstract syntax (the essence) on the text, has been developed. The solution is based on the techniques for Information Extraction and the XML technology, and it captures the specification content within a semantic ontology. The working prototype of the tool being built will serve as the foundation for potential full-scale commercialization.", "authors": ["Krishnaprasad Thirunarayan", "Aaron Berkovich", "Dan Z. Sokol"], "n_citation": 0, "title": "Semi-automatic content extraction from specifications", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4208140a-5810-4bc4-a197-163f3d0f8d26"}
{"authors": ["Heye Zhang", "Chun Lok Wong", "Pengcheng Shi"], "n_citation": 50, "title": "Estimation of cardiac electrical propagation from medical image sequence", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "42983a71-8da4-4967-81b9-6621b4a4707a"}
{"abstract": "Defeasible reasoning is a simple but efficient approach to nonmonotonic reasoning that has recently attracted considerable interest and that has found various applications. Defeasible logic and its variants are an important family of defeasible reasoning methods. So far no relationship has been established between defeasible logic and mainstream nonmonotonic reasoning approaches. In this paper we establish close links to known semantics of extended logic programs. In particular, we give a translation of a defeasible theory D into a program P(D). We show that under a condition of decisiveness, the defeasible consequences of D correspond exactly to the sceptical conclusions of P(D) under the stable model semantics. Without decisiveness, the result holds only in one direction (all defeasible consequences of D are included in all stable models of P(D)). If we wish a complete embedding for the general case, we need to use the Kunen semantics of P(D), instead.", "authors": ["Grigoris Antoniou", "Michael J. Maher"], "n_citation": 0, "title": "Embedding defeasible logic into logic programs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "42c501ee-59ca-4e60-a40a-8c697c7ac6c6"}
{"abstract": "This paper proposed a new method of extracting texture features based on Gabor wavelet. In addition, the application of these features for bark classification applying radial basis probabilistic network (RBPNN) has been introduced. In this method, the bark texture feature is firstly extracted by filtering the image with different orientations and scales filters, then the mean and standard deviation of the image output are computed, the image which have been filtered in the frequency domain. Finally, the obtained Gabor feature vectors are fed up into RBPNN for classification. Experimental results show that, first, features extracted using the proposed approach can be used for bark texture classification. Second, compared with radial basis function neural network (RBFNN), the RBPNN achieves higher recognition rate and better classification efficiency when the feature vectors have low-dimensions.", "authors": ["Zhi-Kai Huang", "De-Shuang Huang", "Ji-Xiang Du", "Zhong-Hua Quan", "Shen-Bo Guo"], "n_citation": 50, "title": "Bark Classification Based on Gabor Filter Features Using RBPNN Neural Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "431498ea-feb8-4be4-b1af-ea5fad4abae5"}
{"abstract": "We study an online job scheduling problem arising in networks with aggregated links. The goal is to schedule n jobs, divided into k disjoint chains, on m identical machines, without preemption, so that the jobs within each chain complete in the order of release times and the maximum flow time is minimized. We present a deterministic online algorithm Block with competitive ratio O(\u221an/m), and show a matching lower bound, even for randomized algorithms. The performance bound for Block we derive in the paper is, in fact, more subtle than a simple competitive analysis, and it shows that in overload conditions (when many jobs are released in a short amount of time), Block's performance is close to the optimum. We also show efficient offline algorithms to minimize maximum flow time and makespan in our model for k =1, and prove that minimizing the maximum flow time and makespan for k, m \u2265 2 is NP-hard.", "authors": ["Wojciech Jawor", "Marek Chrobak", "Christoph D\u00fcrr"], "n_citation": 0, "title": "Competitive analysis of scheduling algorithms for aggregated links", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "434f67af-33f6-4488-b48d-594576f50aae"}
{"abstract": "One fundamental problem that confronts information retrieval is to efficiently support query with higher accuracy and less logic hops. This paper presents HSPIR (Hierarchical Semantic P2P-based Information Retrieval) that distributes document indices through the P2P network hierarchically based on documents semantics generated by Latent Semantic Indexing (LSI) [1]. HSPIR uses CAN [2] and Range Addressable network organize [3] nodes into a hierarchical overlay network. Comparing with other P2P search techniques [4, 5] those are based on simple keyword matching, HSPIR has better accuracy for it considers the advanced relevance among documents. We use Agglomerative Information Bottleneck (AIB) [6] to cluster documents and train Directed Acyclic Graph Support Vector Machines (DAGSVM) based on these clustered documents. Owning to the hierarchical overlay network, the average number of logical hops per query is smaller than other flat architectures.", "authors": ["Fei Liu", "Fanyuan Ma", "Minglu Li", "Linpeng Huang"], "n_citation": 0, "title": "Distributed information retrieval based on Hierarchical Semantic overlay network", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4372c3d9-c85d-4934-a5f8-8a489491df23"}
{"abstract": "This paper proposes the InstantGrid framework for on-demand construction of grid points. In contrast to traditional approaches, InstantGrid is designed to substantially simplify software management in grid systems, and is able to instantly turn any computer into a grid-ready platform with the desired execution environment. Experimental results demonstrate that a 256-node grid point with commodity grid middleware can be constructed in five minutes from scratch.", "authors": ["Roy S. C. Ho", "K. K. Yin", "David C. M. Lee", "Daniel H. F. Hung", "Cho-Li Wang", "Francis C. M. Lau"], "n_citation": 0, "title": "InstantGrid: A framework for on-demand grid point construction", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4385c9c1-2677-4191-ba5c-afb4c96f2cb1"}
{"abstract": "We give fast filtering algorithms to search for a 2-dimensional pattern in a 2-dimensional text allowing any rotation of the pattern. We consider the cases of exact and approximate matching under several matching models, improving the previous results. For a text of size n x n characters and a pattern of size m x m characters, the exact matching takes average time O(n 2  log m/m 2 ), which is optimal. If we allow k mismatches of characters, then our best algorithm achieves O(n 2 k log m/m 2 ) average time, for reasonable k values. For large k, we obtain an O(n 2 k 3/2 /m) average time algorithm. We generalize the algorithms for the matching model where the sum of absolute differences between characters is at most k. Finally, we show how to make the algorithms optimal in the worst case, achieving the lower bound \u03a9(n 2 m 3 ).", "authors": ["Kimmo Fredriksson", "Gonzalo Navarro", "Esko Ukkonen"], "n_citation": 0, "title": "Optimal exact and fast approximate two dimensional pattern matching allowing rotations", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "458b4581-04a8-4c9f-95cd-1ff345528f88"}
{"abstract": "The p - hub median problem is an NP hard location - allocation problem, that consists of finding p points to establish facilities and the assignment of the users to these points. In the capacitated version of this problem, each hub has got a maximum capacity limiting the traffic to be assigned. A new evolutionary approach that has been very effective for solving optimization problems is Path Relinking, an extension of Scatter Search that links solutions over neighborhood spaces. GRASP is a well-known randomized multistart metaheuristic. In this paper, we present a hybrid GRASP-Path Relinking for the capacitated p - hub median problem where the GRASP is used to construct the population of the Path Relinking. Computational results demonstrate that the hybrid GRASP-Path Relinking provides better solutions, in terms of both running times and solution quality.", "authors": ["Melqu\u00edades P\u00e9rez P\u00e9rez", "Francisco Almeida", "J. Marcos Moreno-Vega"], "n_citation": 50, "title": "A hybrid GRASP-Path Relinking algorithm for the capacitated p- hub median problem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "45aadb56-772f-40bf-a4a5-61e8b6fc78a2"}
{"abstract": "In this paper, we introduce a new type of reducts called the A-Fuzzy-Reduct, where the fuzzy similarity relation is constructed by means of cosine-distances of decision vectors and the parameter A is used to tune the similarity precision level. The A-Fuzzy-Reduct can eliminate harsh requirements of the distribution reduct, and it is more flexible than the maximum distribution reduct, the traditional reduct, and the generalized decision reduct. Furthermore, we prove that the distribution reduct, the maximum distribution reduct, and the generalized decision reduct can be converted into the traditional reduct. Thus in practice the implementations of knowledge reductions for the three types of reducts can be unified into efficient heuristic algorithms for the traditional reduct. We illustrate concepts and methods proposed in this paper by an example.", "authors": ["Qihe Liu", "Leiting Chen", "Jianzhong Zhang", "Fan Min"], "n_citation": 0, "title": "Knowledge Reduction in Inconsistent Decision Tables", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "45e00ab5-2038-401a-8080-3ee407469eb4"}
{"abstract": "We propose a remedy to that part of the state-explosion problem for timed automata which is due to interleaving of actions. We prove the following quite surprising result: the union of all zones reached by different interleavings of the same set of transitions is convex. Consequently we can improve the standard reachability computation for timed automata by merging such zones whenever they are encountered. Since passage of time distributes over union, we can continue the successor computation from the new zone and eliminate completely the explosion due to interleaving.", "authors": ["Ramzi Ben Salah", "Marius Bozga", "Oded Maler"], "n_citation": 0, "title": "On interleaving in timed automata", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "461d017b-3990-4a77-a4e8-df1aaccdb1d7"}
{"abstract": "In this paper, we present the architecture and function of a Web-based Intelligent Tutoring System (ITS), which incorporates decision tree and fuzzy matching for instruction strategy. ITS contains following components: information collecting system, learner profile analysis system, strategy matching system and strategy updating system; using domain knowledge base, web log base, learner model base and instruction strategy base. The system analyzes learning models of different users and applies different instruction strategies to meet personalized requirements.", "authors": ["Yongjun Jing", "Shaochun Zhong", "Xin Li", "Jinan Li", "Xiaochun Cheng"], "n_citation": 50, "title": "Using instruction strategy for a web-based intelligent tutoring system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4879279b-00f3-4c46-8aa6-edd81a0f4008"}
{"abstract": "Animation is the validation of formal software specifications by means of automatically generated prototypes. Animation is an effective way of validating a specification against the stakeholder requirements. OASIS is a formal language for conceptual modeling. We have carried out experiments animating OASIS specifications. In this work we take advantage of our experience animating OASIS specifications to provide animation support for UML classes. We illustrate that it is possible to use OASIS as a formal semantic framework for UML specifications. Obviously, OASIS does not cover as many model aspects as UML. We will concentrate on some UML diagrams, particularly in classes modeled in class diagrams to validate them through animation. To explain our approach, we give an example of a bank account and we show its UML model and its corresponding OASIS specification. Our aim is to build a module for animation and validation of specifications integrated in a CASE tool. We present a prototype of this module.", "authors": ["Patricio Letelier", "Pedro S\u00e1nchez"], "n_citation": 0, "title": "Validation of UML classes through animation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "49a0613e-3f6b-426c-82a4-ad076b01306a"}
{"abstract": "Time synchronization is necessary in many distributed systems, but achieving synchronization in sensornets, which combine stringent precision requirements with severe resource constraints, is particularly challenging. This challenge has been met by the recent Reference-Broadcast Synchronization (RBS) proposal, which provides on-demand pairwise synchronization with low overhead and high precision. In this paper we introduce a model of the basic RBS synchronization paradigm. Within the context of this model we characterize the optimally precise clock synchronization algorithm and establish its global consistency. In the course of this analysis we point out unexpected connections between optimal clock synchronization, random walks, and resistive networks, and present a polynomial-time approximation scheme for the problem of calculating the effective resistance in a network based on min-cost flow. We also sketch a polynomial-time algorithm for finding a schedule of data acquisition giving the optimal trade-off between energy consumption and precision of clock synchronization. We also discuss synchronization in the presence of clock skews. In ongoing work we are adapting our synchronization algorithm for execution in a network of seismic sensors that requires global clock consistency.", "authors": ["Jeremy Elson", "Richard M. Karp", "Christos H. Papadimitriou", "Scott Shenker"], "n_citation": 0, "title": "Global synchronization in sensornets", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4dcfd42d-36f1-453b-b915-b0f36fbdc582"}
{"abstract": "In this paper we present several lower bounds on the approximation of the exemplar conserved interval distance problem of genomes. We first prove that the exemplar conserved interval distance problem cannot be approximated within a factor of c log n for some constant c > 0 in polynomial time, unless P=NP. We then prove that it is NP-complete to decide whether the exemplar conserved interval distance between any two sets of genomes is zero or not. This result implies that the exemplar conserved interval distance problem does not admit any approximation in polynomial time, unless P=NP. In fact, this result holds even when a gene appears in each of the given genomes at most three times. Finally, we strengthen the second result under a weaker definition of approximation (which we call weak approximation). We show that the exemplar conserved interval distance problem does not admit a weak approximation within a factor of m, where m is the maximum length of the given genomes.", "authors": ["Zhixiang Chen", "Richard H. Fowler", "Bin Fu", "Binhai Zhu"], "n_citation": 0, "title": "Lower Bounds on the Approximation of the Exemplar Conserved Interval Distance Problem of Genomes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f09890a-6984-4c00-bab6-41c2df4efb7e"}
{"abstract": "A novel digital historical heritage restoration approach is presented in this paper, which is based on photo-realistic 3D reconstruction from uncalibrated image sequences. Original images of the targets, such as statues, buildings, or culture relics, are captured with a hand-held camera and processed in three steps, which consists of preprocessing, sparse reconstruction and dense reconstruction. The reconstructed surface models can be preserved or exhibited as digital copies of the heritage; experimental results show the effectiveness of the proposed approach and its potential to be applied in the field of historical heritage restoration.", "authors": ["Dan Hou", "Xia Shen", "Xiaowei Li", "Yue Liu", "Yongtian Wang"], "n_citation": 0, "title": "Digital restoration of historical heritage by reconstruction from uncalibrated images", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "510a2720-d8f9-4e16-ba31-953d23f85444"}
{"abstract": "Image splicing is a commonly used technique in image tampering. This paper presents a novel approach to passive detection of image splicing. In the proposed scheme, the image splicing detection problem is tackled as a two-class classification problem under the pattern recognition framework. Considering the high non-linearity and non-stationarity nature of image splicing operation, a recently developed Hilbert-Huang transform (HHT) is utilized to generate features for classification. Furthermore, a well established statistical natural image model based on moments of characteristic functions with wavelet decomposition is employed to distinguish the spliced images from the authentic images. We use support vector machine (SVM) as the classifier. The initial experimental results demonstrate that the proposed scheme outperforms the prior arts.", "authors": ["Dongdong Fu", "Yun Q. Shi", "Wei Su"], "n_citation": 0, "title": "Detection of image splicing based on hilbert-huang transform and moments of characteristic functions with wavelet decomposition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5264056f-d833-4a0d-a6d2-6cac32acaa9d"}
{"abstract": "We propose a novel algorithm for automata-based LTL model checking that interleaves the construction of the generalized Buchi automaton for the negation of the formula and the emptiness check. Our algorithm first converts the LTL formula into a linear weak alternating automaton; configurations of the alternating automaton correspond to the locations of a generalized Buchi automaton, and a variant of Tarjan's algorithm is used to decide the existence of an accepting run of the product of the transition system and the automaton. Because we avoid an explicit construction of the Btichi automaton, our approach can yield significant improvements in runtime and memory, for large LTL formulas. The algorithm has been implemented within the SPIN model checker, and we present experimental results for some benchmark examples.", "authors": ["Moritz Hammer", "Alexander Knapp", "Stephan Merz"], "n_citation": 0, "title": "Truly on-the-fly LTL model checking", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "530effa4-6efc-4ac0-a2fe-6f75210175ae"}
{"abstract": "This paper presents a new proposal for reducing bloat in Genetic Programming. This proposal is based in a well-known parallel evolutionary model: the island model. We firstly describe the theoretical motivation for this new approach to the bloat problem, and then we present a set of experiments that gives us evidence of the findings extracted from the theory. The experiments have been performed on a representative problem extracted from the GP field: the even parity 5 problem. We analyse the evolution of bloat employing different settings for the parameters employed. The conclusion is that the Island Model helps to prevent the bloat phenomenon.", "authors": ["Francisco Fern\u00e1ndez de Vega", "Germ\u00e1n Galeano Gil", "Juan Antonio G\u00f3mez Pulido", "Jose T. Guisadol"], "n_citation": 15, "title": "Control of bloat in genetic programming by means of the island model", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "586d2fa3-a4f1-433b-9e6a-723b2677a6a4"}
{"abstract": "Steganography is often combined with cryptographic mechanisms. This enhances steganography by valuable properties that are originally left to cryptographic systems. However, new problems for cryptographic mechanisms arise from the context of steganography. There are two sorts of steganographic tools: commercial tools with insecure or badly implemented cryptography and academic proof-of-concepts that abstain from the actual implementation of the cryptographic part. Comparably to cryptography, steganography evolves in an iterative process of designing and breaking new methods. In this paper we examine the encoding properties and cryptographic functionality of steganographic tools to enable the detection of embedded information in steganograms even if the embedding part was otherwise secure.", "authors": ["Andreas Westfeld"], "n_citation": 0, "title": "Steganalysis in the presence of weak cryptography and encoding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "58b939d5-8431-4f95-abd2-f134c1a26c22"}
{"abstract": "The needs for run-time data storage in modern wired and wireless network applications are increasing. Additionally, the nature of these applications is very dynamic, resulting in heavy reliance to dynamic memory allocation. The most significant problem in dynamic memory allocation is fragmentation, which can cause the system to run out of memory and crash, if it is left unchecked. The available dynamic memory allocation solutions are provided by the real time Operating Systems used in embedded or general-purpose systems. These state-of-the-art dynamic memory allocators are designed to satisfy the run-time memory requests of a wide range of applications. Contrary to most applications, network applications need to allocate too many different memory sizes (e.g. hundreds different sizes for packets) and have an extremely dynamic allocation and de-allocation behavior (e.g. unpredictable web-browsing activity). Therefore, the performance and the de-fragmentation efficiency of these allocators is limited. In this paper, we analyze all the important issues of fragmentation and the ways to reduce it in network applications, while keeping the performance of the dynamic memory allocator unaffected or even improving it. We propose highly customized dynamic memory allocators, which can be configured for specific network needs. We assess the effectiveness of the proposed approach in two representative real-life case studies of wired and wireless network applications. Finally, we show very significant reduction in memory fragmentation and increase in performance compared to state-of-the-art dynamic memory allocators utilized by real-time Operating Systems.", "authors": ["Stylianos Mamagkakis", "Christos Baloukas", "David Atienza", "Francky Catthoor", "Dimitrios Soudris", "Jos\u00e9 M. Mend\u00edas", "Antonios Thanailakis"], "n_citation": 50, "title": "Reducing memory fragmentation with performance-optimized dynamic memory allocators in network applications", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5ac81848-561b-47df-a86f-6ba6f5c028ec"}
{"authors": ["Mariangiola Dezani", "Silvia Ghilezan"], "n_citation": 0, "title": "Preciseness of Subtyping on Intersection and Union Types", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "5b2721d8-360e-48e3-b231-f42be1558606"}
{"abstract": "This paper presents a platform for interactive VR storytelling with supporting the development of system and content. This platform enables to build more complex and dynamic virtual environments by specifying the spatial and temporal relationship between not only 3D model and multimedia but also human-computer interface.", "authors": ["Changhoon Park", "Michitaka Hirose", "Heedong Ko"], "n_citation": 50, "title": "A platform for interactive VR storytelling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5bf47b11-0b6e-42bf-bc35-e922a51f9eb6"}
{"abstract": "Application of sensor networks in different fields is an interesting area to work with and has already drawn widespread attention. Since sensors have limited supply of on-board energy, efficient management of network is a compulsion in extending life of the sensor. At the same time, frequent damage to sensors and link failure occur because of the adverse environment in which they are deployed. A sensor network has to tolerate and recover from these failures themselves with no external help. In this respect, we have designed a self-stabilizing energy-aware routing protocol in a sensor network. Our protocol ensures the sensor network, starting from an arbitrary state, eventually set up reliable communication in network with minimum energy consumption and in a finite number of steps.", "authors": ["Smruti Padhy", "Diganta Goswami"], "n_citation": 0, "title": "Self-stabilizing energy-aware routing algorithm in wireless sensor network with limited mobility", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5e739dca-2cc6-4ce2-b87f-f33938b08ddb"}
{"abstract": "In many applications numeric as well as categorical features describe the data objects. A variety of algorithms have been proposed for clustering if fuzzy partitions and descriptive cluster prototypes are desired. However, most of these methods are designed for data sets with variables measured in the same scale type (only categorical, or only numeric). We have developed probabilistic distance measure to compute significance of attributes for numeric data, and distance between two categorical values. We used this distance measure with the cluster center definition proposed by Yasser El-Sonbaty and M. A. Ismail [26] to propose Fuzzy-c mean type clustering algorithm for mixed attributes data. The results of the application of the new algorithm show that new technique is quite encouraging.", "authors": ["Amir Ahmad", "Lipika Dey"], "n_citation": 0, "title": "Algorithm for fuzzy clustering of mixed data with numeric and categorical attributes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5f0b1458-1bd1-45d7-8b36-7b2ebd7f051b"}
{"abstract": "We show herein that a pattern based on FGLM techniques can be used for computing Grobner bases, or related structures, associated to linear codes. This Grobner bases setting turns out to be strongly related to the combinatorics of the codes.", "authors": ["Mijail Borges-Quintana", "Miguel A. Borges-Trenard", "Edgar Mart\u00ednez-Moro"], "n_citation": 0, "title": "A General Framework for Applying FGLM Techniques to Linear Codes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "61dbfd4b-da55-4764-94ad-3b9d0f9e4210"}
{"abstract": "Recent advances suggest that a wide range of computer vision problems can be addressed more appropriately by considering non-Euclidean geometry. This paper tackles the problem of sparse coding and dictionary learning in the space of symmetric positive definite matrices, which form a Riemannian manifold. With the aid of the recently introduced Stein kernel (related to a symmetric version of Bregman matrix divergence), we propose to perform sparse coding by embedding Riemannian manifolds into reproducing kernel Hilbert spaces. This leads to a convex and kernel version of the Lasso problem, which can be solved efficiently. We furthermore propose an algorithm for learning a Riemannian dictionary (used for sparse coding), closely tied to the Stein kernel. Experiments on several classification tasks (face recognition, texture classification, person re-identification) show that the proposed sparse coding approach achieves notable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as tensor sparse coding, Riemannian locality preserving projection, and symmetry-driven accumulation of local features.", "authors": ["Mehrtash Tafazzoli Harandi", "Conrad Sanderson", "Richard I. Hartley", "Brian C. Lovell"], "n_citation": 0, "title": "Sparse coding and dictionary learning for symmetric positive definite matrices: a kernel approach", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "62345c90-3735-488f-ba34-4aa6e928388f"}
{"abstract": "In the relational data model, the problem of data redundancy has been successfully tackled via decomposition. In advanced data models, decomposition by pivoting provides a similar concept. Pivoting has been introduced by Biskup et al. [5], and used for decomposing relationship types according to a unary nonkey functional dependency. Our objective is to study pivoting in the presence of cardinality constraints which are commonly used in semantic data models. In order to ensure the equivalence of the given schema and its image under pivoting, the original application-dependent constraints have to be preserved. We discuss this problem for sets of participation and co-occurrence constraints. In particular, we prove the necessity of path cardinality constraints, and give an appropriate foundation for this concept.", "authors": ["Sven Hartmann"], "n_citation": 0, "title": "Decomposition by pivoting and path cardinality constraints", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "6432ea1d-9d91-4db6-a3a8-6ae226d1c164"}
{"abstract": "We present a new design principle for building a batch processing protocol for interactive proofs. First, a generic method to achieve batch processing is proposed when dealing with an NP-relation with certain homomorphicity. It is shown that the method preserves zero-knowledgeness and knowledge-soundness. Second, for some NP-relation that has no such homomorphicity, we illustrate that the relation can be decomposed into a homomorphic relation(hence we have a batch process) and another NP-relation that is proven using an efficient protocol. Such a decomposition provides an advantage in terms of efficiency.", "authors": ["Koji Chida", "Go Yamamoto"], "n_citation": 0, "title": "Batch processing of interactive proofs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6640fb31-d2c0-48c7-bfd5-3690b27a963e"}
{"abstract": "To establish a semantic foundation for the synthesis of executable programs from timed models, we study in what sense the timed language (i.e. sequences of events with real-valued time-stamps) of a timed automaton is recognized by a digital machine. Based on the non-instant observability of events, we propose an alternative semantics for timed automata. We show that the new semantics gives rise to a natural notion of digitalization for timed languages. As a model for digital machines we use time-triggered automata - a subclass of timed automata with simplified syntax accepting digitalized timed languages. A time-triggered automaton is essentially a time table for a digital machine (or a digital controller), describing what the machine should do at a given time point, and it can be easily transformed to an executable program. Finally, we present a method to check whether a time-triggered automaton recognizes the language of a timed automaton according to the new semantics.", "authors": ["Pavel Krcal", "Leonid Mokrushin", "P. S. Thiagarajan", "Wang Yi"], "n_citation": 0, "title": "Timed vs. time-triggered automata", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "66973e31-b646-4839-86be-9dc0e126c558"}
{"abstract": "Vision systems for various tasks are increasingly being deployed. Although significant effort has gone into improving the algorithms for such tasks, there has been relatively little work on determining optimal sensor configurations. This paper addresses this need. We specifically address and enhance the state-of-the-art in the analysis of scenarios where there are dynamically occuring objects capable of occluding each other. The visibility constraints for such scenarios are analyzed in a multi-camera setting. Also analyzed are other static constraints such as image resolution and field-of-view, and algorithmic requirements such as stereo reconstruction, face detection and background appearance. Theoretical analysis with the proper integration of such visibility and static constraints leads to a generic framework for sensor planning, which can then be customized for a particular task. Our analysis can be applied to a variety of applications, especially those involving randomly occuring objects, and include surveillance and industrial automation. Several examples illustrate the wide applicability of the approach.", "authors": ["Anurag Mittal"], "n_citation": 50, "title": "Generalized Multi-sensor Planning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "672196bb-8806-407c-98f4-6ba5549f467d"}
{"authors": ["Shilong Ma", "Yuefei Sui", "Ke Xu"], "n_citation": 0, "title": "The limits of horn logic programs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "67ccfdca-2a9c-459f-8ca3-c4d555bcc6ce"}
{"abstract": "This paper introduces an elevator group control system based on bi-objective optimisation. The two conflicting objectives are passenger waiting times and energy consumption. Due to the response time requirements the powerful but computationally demanding Pareto-dominance based Evolutionary Multiobjective Optimisers cannot be used in this real-world-real-time control application. Instead, an evolutionary variant of the modest Weighted Aggregation method has been applied without prejudice. The presented approach solves the weight-scaling problem of the Weighted Aggregation method in dynamically changing environment. In addition, the method does not solve, but copes with the disability of the WA-method to reach the concave Pareto-front regions in the fitness space. A dedicated controller acts as a Decision Maker guiding the optimiser to produce solutions that fulfil the specified passenger waiting times over a longer period of time with minimum consumption of energy. Simulation results show that the control principle is able to regulate the service level of an elevator group and at the same time decrease the consumption of energy and system wearing.", "authors": ["Tapio Tyni", "Jari Ylinen"], "n_citation": 0, "title": "Evolutionary bi-objective controlled elevator group regulates passenger service level and minimises energy consumption", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "684a2fd0-134b-439f-b956-83e73e378402"}
{"authors": ["Markus Moll", "Luc Van Gool"], "n_citation": 0, "title": "Optimal templates for nonrigid surface reconstruction", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "68b5a4b3-ff99-424d-aef9-bbc6a3078510"}
{"abstract": "How to deal with the heterogeneous structures of XML documents, identify XML data instances, solve conflicts, and effectively merge XML documents to obtain complete information is a challenge. In this paper, we define a merging operation over XML documents that can merge two XML documents with different structures. It is similar to a full outer join in relational algebra. We design an algorithm for this operation. In addition, we propose a method for merging XML elements and handling typical conflicts. Finally, we present a merge template XML file that can support recursive processing and merging of XML elements.", "authors": ["Wanxia Wei", "Mengchi Liu", "Shijun Li"], "n_citation": 0, "title": "Merging of XML documents", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "72e3f1f2-b3bc-4bbe-b030-700ed7247c9b"}
{"abstract": "Sequential pattern mining is an important problem for data mining with broad applications. This paper presents a first-Horizontal-last-Vertical scanning database Sequential pattern Mining algorithm (HVSM). HVSM considers a database as a vertical bitmap. The algorithm first extends itemsets horizontally, and digs out all one-large-sequence itemsets. It then extends the sequence vertically and generates candidate large sequence. The candidate large sequence is generated by taking brother-nodes as child-nodes. The algorithm counts the support by recording the first TID mark (1 st -TID). Experiments show that HVSM algorithm can find frequent sequences faster than SPAM algorithm in mining the large transaction databases.", "authors": ["Shijie Song", "Huaping Hu", "Shiyao Jin"], "n_citation": 0, "title": "HVSM : A new sequential pattern mining algorithm using bitmap representation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7351740f-5f1e-4464-9aef-c8c1ec511c3f"}
{"abstract": "For many medical segmentation tasks, the contrast along most of the boundary of the target object is high, allowing simple thresholding or region growing approaches to provide nearly sufficient solutions for the task. However, the regions recovered by these techniques frequently leak through bottlenecks in which the contrast is low or non-existent. We propose a new approach based on a novel speed-up of the isoperimetric algorithm [1] that can solve the problem of leaks through a bottleneck. The speed enhancement converts the isoperimetric segmentation algorithm to a fast, linear-time computation by using a tree representation as the underlying graph instead of a standard lattice structure. In this paper, we show how to create an appropriate tree substrate for the segmentation problem and how to use this structure to perform a linear-time computation of the isoperimetric algorithm. This approach is shown to overcome common problems with watershed-based techniques and to provide fast, high-quality results on large datasets.", "authors": ["Leo Grady"], "n_citation": 50, "title": "Fast, Quality, Segmentation of Large Volumes : Isoperimetric Distance Trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7351fa23-e972-4ec3-8892-50bb6a3f67d0"}
{"abstract": "A hybrid learning approach named confusion-cross-based support vector machine tree (CSVMT) has been proposed in our current work. It is developed to achieve a better performance for complex distribution problems even when the two parameters of SVM are not appropriately selected. One problem remained is that the trained internal nodes may be high complex for those high-dimensional feature space problems due to undesirable complexity added to the underlying probability distribution of the concept label for learning algorithm to capture -thus, learning models with high complexity are likely to depress the test efficiency and performance. In this paper, we proposed a feature selection based CSVMT (FS-SCVMT) learning approach in which the input space for each internal node is adaptively dimensionality reduced by sensitivity based feature selection. Experimental results showed that FS-SCVMT approach performed well.", "authors": ["Qinzhen Xu", "Wenjiang Pei", "Luxi Yang", "Zhenya He"], "n_citation": 0, "title": "Support Vector Machine Tree Based on Feature Selection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "742198e2-729d-4fd3-927a-0daa7f161647"}
{"abstract": "We consider the problem that, given a graph G and a parameter k, asks whether the edit distance of G and a rectangular grid is at most k. We examine the general case where the edit operations are vertex/edge removals and additions. If the dimensions of the grid are given in advance, we give a parameterized algorithm that runs in 2 o(logk.k)  + O( n  3 ) steps. In the case where the dimensions of the grid are not given we give a parameterized algorithm that runs in 2 o ( log  k.k)  + O(k 2  .n 3 ) steps. We insist on parameterized algorithms with running times where the relation between the polynomial and the non-polynomial part is additive. Our algorithm is based on the technique of kernelization. In particular we prove that for each version of the above problem there exists a kernel of size O(k 4 ).", "authors": ["Josep D\u00edaz", "Dimitrios M. Thilikos"], "n_citation": 50, "title": "Fast FPT-algorithms for cleaning grids", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "75125745-f0b3-4815-b45a-a59034f82d0d"}
{"abstract": "The Campus Grid Services Environment provides many services such as the computing service, storage service, communication service, and research and education services. For the storage service, the multilevel linked lists are built to organize the storage resources contributed by the nodes of the campus network for effective utilization of the storage resources. Additionally, the mechanisms of nodes' joining and leaving, nodes' storage resource scheduling and allocation, and nodes' unallocated storage spaces management algorithms are presented in detail. The total nodes number aggregated is 16,843,261. The storage space aggregated is attained to 17 PB, much larger than those of other current campus grids. This architecture is scalable and more nodes can join this storage pool to contribute a more huge storage space.", "authors": ["Zhiqun Deng", "Zhicong Liu", "Hong Luo", "Guanzhong Dai", "Xinjia Zhang", "Dejun Mu", "Hongji Tang"], "n_citation": 0, "title": "Nodes' organization mechanisms on Campus Grid Services Environment", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "778bbb4f-73f9-4397-a91c-089bd93fc8c4"}
{"abstract": "This paper presents a novel solution to the difficult task of both detecting and estimating the 3D pose of humans in monoscopic images. The approach consists of two parts. Firstly the location of a human is identified by a probabalistic assembly of detected body parts. Detectors for the face, torso and hands are learnt using adaBoost. A pose likliehood is then obtained using an a priori mixture model on body configuration and possible configurations assembled from available evidence using RANSAC. Once a human has been detected, the location is used to initialise a matching algorithm which matches the silhouette and edge map of a subject with a 3D model. This is done efficiently using chamfer matching, integral images and pose estimation from the initial detection stage. We demonstrate the application of the approach to large, cluttered natural images and at near framerate operation (16fps) on lower resolution video streams.", "authors": ["Antonio S. Micilotta", "Eng-Jon Ong", "Richard Bowden"], "n_citation": 67, "title": "Real-Time Upper Body Detection and 3D Pose Estimation in Monoscopic Images", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7810c903-bad6-428c-b72f-3bf8754efbd2"}
{"abstract": ", Sensor networks are expected to be used for spatial cognition in harsh environments. When an event happens, there will be several sensors detect it and send their reports to a sink, but these data are neither integrated nor reliable. Therefore, it is reasonable to make use of data mining on intermediate nodes to acquire deeper knowledge on an event and cut down the total traffic at the same time. Furthermore, the QoS requests should be considered too. In this paper, we propose a centralized algorithm to achieve optimal traffic management on sensor networks with considering QoS and data fusion. Its efficiency is shown by experiments.", "authors": ["Yantao Pan", "Wei Peng", "Xicheng Lu"], "n_citation": 50, "title": "Traffic Management Genetic Algorithm Supporting Data Mining and QoS in Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7b432627-69e8-4df5-97d3-ecb313d2726a"}
{"abstract": "Generating effective tests and inferring likely program specifications are both difficult and costly problems. We propose an approach in which we can mutually enhance the tests and specifications that are generated by iteratively applying each in a feedback loop. In particular, we infer likely specifications from the executions of existing tests and use these specifications to guide automatic test generation. Then the existing tests, as well as the new tests, are used to infer new specifications in the subsequent iteration. The iterative process continues until there is no new test that violates specifications inferred in the previous iteration. Inferred specifications can guide test generation to focus on particular program behavior, reducing the scope of analysis; and newly generated tests can improve the inferred specifications. During each iteration, the generated tests that violate inferred specifications are collected to be inspected. These violating tests are likely to have a high probability of exposing faults or exercising new program behavior. Our hypothesis is that such a feedback loop can mutually enhance test generation and specification inference.", "authors": ["Tao Xie", "David Notkin"], "n_citation": 0, "title": "Mutually enhancing test generation and specification inference", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7c6e429e-22a8-474f-b110-d08865cfaabf"}
{"abstract": "Monadic Second-Order Unification (MSOU) is Second-Order Unification where all function constants occurring in the equations are unary. Here we prove that the problem of deciding whether a set of monadic equations has a unifier is NP-complete. We also prove that Monadic Second-Order Matching is also NP-complete.", "authors": ["Jordi Levy", "Manfred Schmidt-Schauss", "Mateu Villaret"], "n_citation": 0, "title": "Monadic Second-Order Unification is NP-complete", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7ded6b8e-de19-4c52-a1a2-362ad092434e"}
{"abstract": "Depicting dynamics offers manifold ways to visualize dynamics in static media, to understand dynamics in the whole, and to relate dynamics of the past and the future with the current state of a 3D scene. The depiction strategy we propose is based on visual elements, called dynamic glyphs, which are integrated in the 3D scene as additional 2D and 3D geometric objects. They are derived from a formal specification of dynamics based on acyclic, directed graphs, called behavior graphs. Different types of dynamics and corresponding mappings to dynamic glyphs can be identified, for instance, scene events at a discrete point in time, transformation processes of scene objects, and activities of scene actors. The designer or the application can control the visual mapping of dynamics to dynamic glyphs, and, thereby, create own styles of dynamic depiction. Applications of dynamic glyphs include the automated production of instruction manuals, illustrations, and storyboards.", "authors": ["Marc Nienhaus", "J\u00fcrgen D\u00f6llner"], "n_citation": 0, "title": "Dynamic glyphs - depicting dynamics in images of 3D scenes", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7f566e72-f80f-4d78-9bbf-d09ad7c5dae6"}
{"abstract": "We extend the formal developments for message sequence charts (MSCs) to support scenarios with lost and found messages. We define a notion of extended compositional message sequence charts (ECMSCs) which subsumes the notion of compositional message sequence charts in expressive power but additionally allows to define lost and found messages explicitly. As usual, ECMSCs can be combined by means of choice and repetition to (extended) compositional message sequence graphs. We show that-despite extended expressive power-model checking of monadic second-order logic (MSO) for this framework remains to be decidable. The key technique to achieve our results is to use an extended notion for linearizations.", "authors": ["Benedikt Bollig", "Martin Leucker", "Philipp Lucas"], "n_citation": 50, "title": "Extending compositional message sequence graphs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8042a17f-a034-4b10-8748-fd8213d80ec9"}
{"abstract": "Robustness and security are the key issues in the development of image watermarking algorithm. A new rotation invariant security image watermarking algorithm based on steerable pyramid transform is proposed in this paper. The algorithm is characterized as follows: (1) the rotation invariance and robust watermarking are achieved concurrently on the same transform domain; (2) the rotation synchronization is obtained through template matching by using steerable pyramid transform, which satisfies the shiftability in orientation condition; (3) the watermarks are embedded into an oriented subband at angle 0, which can be interpolated with base filter kernels and used as a key in watermark detection to increase the security of watermark; (4) the watermark detector is designed based on the steerable vector HMM model. High robustness is observed against StirMark attacks and their joint attacks.", "authors": ["Jiangqun Ni", "Rongyue Zhang", "Jiwu Huang", "Chuntao Wang", "Quanbo Li"], "n_citation": 0, "title": "A rotation-invariant secure image watermarking algorithm incorporating steerable pyramid transform", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8185e6bd-8946-44e0-9b9c-8e43fa894d15"}
{"abstract": "Grid Network applications that need to achieve reliable end-to-end performance typically make use of either reservations or adaptation. However, both techniques also have serious limitations, particularly when dealing with high-bandwidth, dynamic flow as like Grid. Fixed-capability reservations tend to be wasteful of resources and it cannot permit dynamic adaptation in application performance when resource management policies are changed. Adaptation scheme inevitably fails when congestion reduces available resources below acceptable limits. In this paper, we describe an approach to QoS that combines features of both reservation and adaptation to address the difficulties above noted. In addition, we propose extended GARA and advanced reservation and reserved resource saving scheme that provide more effective QoS.", "authors": ["Jonghyoun Choi", "Ki-Sung Yu", "Jong-Jin Park", "Youngsong Mun"], "n_citation": 50, "title": "Design of Network Aware Resource Allocation System for Grid Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "84211d90-008f-48bf-8b1b-7043b4dd4fa8"}
{"abstract": "We introduce distributed games over asynchronous transition systems to model a distributed controller synthesis problem. A game involves two teams and is not turn-based: several players of both teams may simultaneously be enabled. We define distributed strategies based on the causal view that players have of the system. We reduce the problem of finding a winning distributed strategy with a given memory to finding a memoryless winning distributed strategy in a larger distributed game. We reduce the latter problem to finding a strategy in a classical 2-players game. This allows to transfer results from the sequential case to this distributed setting.", "authors": ["Paul Gastin", "Benjamin Lerman", "Marc Zeitoun"], "n_citation": 0, "title": "Distributed games and distributed control for asynchronous systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8486aced-055a-4d64-89f8-03c24e9e8bde"}
{"abstract": "Internet Personalized services are irresistible developing trend for e-commerce. More and more researchers are committed to personalization field. Many personalization approaches are static and lack of means to improve the personalized tasks. This paper proposes an adaptive e-commerce personalization framework using traditional data mining techniques and agent technology as well as user feedback optimisation mechanism to improve the personalized services to the e-commerce customer. The behaviours of all the agents in the framework are carefully considered and the framework has been applied to an online banking system.", "authors": ["Qiubang Li", "Rajiv Khosla"], "n_citation": 0, "title": "An adaptive E-commerce personalization framework with application in E-banking", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "84f5ef4d-84d3-4373-99c9-15846c3434fd"}
{"abstract": "This paper proposes a MetaService-based architecture for dealing with the problem of services management in grid systems. We introduce the basic idea of MetaService, and then discuss MetaService classification, new MetaService construction, and the MetaService working mechanism. A graph and a tree can be used for the optimization function of MetaService. An example from real life is used to explain the role of MetaService. We give an overview of a grid prototype system which has incorporated some of the ideas presented. A brief comparison is given between MetaService and other related work.", "authors": ["Du Zhi-hui", "Francis C. M. Lau", "Cho-Li Wang", "Wai-kin Lam", "Chuan He", "Xiaoge Wang", "Yu Chen", "Sanli Li"], "n_citation": 0, "title": "Design of an OGSA-based MetaService architecture", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8bb00bed-4e1f-4b7e-b0d1-55d3d7070539"}
{"abstract": "In this paper, we discuss a lightweight approach to eliminate the overhead due to implicit type arguments during higher-order unification of dependently-typed terms. First, we show that some implicit type information is uniquely determined, and can therefore be safely skipped during higher-order unification. Second, we discuss its impact in practice during type reconstruction and during proof search within the logical framework Twelf. Our experimental results show that implicit type arguments are numerous and large in size, but their impact on run-time is between 10% and 20%. On the other hand optimizations such as eliminating the occurs check are shown to be crucial to achieve significant performance improvements.", "authors": ["Brigitte Pientka"], "n_citation": 0, "title": "Eliminating redundancy in higher-order unification : A lightweight approach", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8bcb0f2f-3271-47cd-9e47-189da9aab6e4"}
{"abstract": "This paper presents an automatic acquisition process to acquire the semantic meaning for the words. This process obtains the representation vectors for stemmed words by iteratively improving the vectors, using a trained Elman network [4]. Experiments performed on a corpus composed of Shakespeare's writings show its linguistic analysis and categorization abilities.", "authors": ["Cheng-Yuan Liou", "Jau-Chi Huang", "Wen-Chie Yang"], "n_citation": 50, "title": "Semantic Addressable Encoding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8e1ab6bf-ae87-4ae1-9746-8e7571155062"}
{"abstract": "Given a list of d-dimensional cuboid items with associated profits, the orthogonal knapsack problem asks for a packing of a selection with maximal profit into the unit cube. We restrict the items to hypercube shapes and derive a (5 4 + e)-approximation for the two-dimensional case. In a second step we generalize our result to a (2 d +1 2d+\u2208)-approximation for d-dimensional packing.", "authors": ["Rolf Harren"], "n_citation": 0, "title": "Approximating the Orthogonal Knapsack Problem for Hypercubes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8e30d4c4-760f-419b-8c2e-27f23e3e20fb"}
{"abstract": "In this poster, we show how the fuzzy answer set semantics, i.e. a combination of answer set programming and fuzzy logic, can be mapped onto the semantics for HEX-programs.", "authors": ["Davy Van Nieuwenborgh", "Martine De Cock", "Dirk Vermeir"], "n_citation": 0, "title": "Computing fuzzy answer sets using DLVHEX", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "9104d65f-84ba-4fee-9f33-8ff562a85f74"}
{"abstract": "We describe a dependent type theory with proof irrelevance. Within this framework, we give a representation of a form of Mac Lane set theory and discuss automated support for constructing proofs within this set theory. One of the novel aspects of the representation is that one is allowed to use any class (in the set theory) as a type (in the type theory). Such class types allow a natural way of representing partial functions (e.g., the first and second operators on the class of Kuratowski ordered pairs). We also discuss how automated search can be used to construct proofs. In particular, the first-order prover Vampire can be called to solve a challenge problem (the injective Cantor Theorem) which is notoriously difficult for higher-order automated provers.", "authors": ["Chad E. Brown"], "n_citation": 0, "title": "Combining type theory and untyped set theory", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "92ee33ad-e11d-41b0-913b-1b7a87464007"}
{"abstract": "Hopfield introduced the neural network for linear programming with linear constraints. In this paper, Hopfield neural network has been generalized to solve the optimization problems including nonlinear constraints. The proposed neural network can solve a nonlinear cost function with nonlinear constraints. Also, methods have been discussed to reconcile optimization problems with neural networks and implementation of the circuits. Simulation results show that the computational energy function converges to stable point by decreasing the cost function as the time passes.", "authors": ["Min-Jae Kang", "Ho-Chan Kim", "Farrukh Aslam Khan", "Wang-Cheol Song", "Sang-Joon Lee"], "n_citation": 0, "title": "Neural Networks for Optimization Problem with Nonlinear Constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "96d9a9da-1c84-4e34-aed3-b2ca13d62b6b"}
{"abstract": "Effective database applications, business rules management, data warehousing, enterprise modeling and re-engineering all depend on the quality of the underlying data model. To properly exploit relational, object-relational or object database technology, a clear understanding is needed as to how to create conceptual business models, transform them to logical database models for implementation on the chosen platform, and query the populated models. Fact-orientation provides a truly conceptual way to accomplish these tasks, facilitating communication between the modeler, the domain expert and the application. This presentation provides insights into the fact-oriented approach for modeling and querying information systems, focusing on verbalization and instantiation of data use cases for capturing business rules, including recent work on negative and default rule verbalizations.", "authors": ["Terry A. Halpin"], "n_citation": 50, "title": "A fact-oriented approach to business rules", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "9887cc08-8b79-4783-a97e-e7bdeeff1ca9"}
{"abstract": "This paper describes an automated, formal and rigorous analysis of the Ad hoc On-Demand Distance Vector (AODV) routing protocol, a popular protocol used in wireless mesh networks. We give a brief overview of a model of AODV implemented in the UPPAAL model checker. It is derived from a process-algebraic model which reflects precisely the intention of AODV and accurately captures the protocol specification. Furthermore, we describe experiments carried out to explore AODV's behaviour in all network topologies up to 5 nodes. We were able to automatically locate problematic and undesirable behaviours. This is in particular useful to discover protocol limitations and to develop improved variants. This use of model checking as a diagnostic tool complements other formal-methods-based protocol modelling and verification techniques, such as process algebra.", "authors": ["Ansgar Fehnker", "Rob J. van Glabbeek", "Peter H\u00f6fner", "Annabelle McIver", "Marius Portmann", "Wee Lum Tan"], "n_citation": 0, "title": "Automated analysis of AODV using UPPAAL", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "9909413f-8e87-4e5f-ab73-bdc3026e02cc"}
{"abstract": "In this paper, we address the problem of Part-Of-Speech lagging of Arabic texts with vowel marks. After the description of the specificities of Arabic language and the induced difficulties on the task of POS-tagging, we propose an approach combining several methods. One of these methods, based on sentences patterns, is original and very attractive. We present, afterward, the multi-agent architecture that we adopted for the conception and the realization of our POS-tagging system. The multi-agent architecture is justified by the need for collaboration, parallelism and competition between the different agents. Finally, we expose the implementation and the evaluation of the system implemented.", "authors": ["Chiraz Ben Othmane Zribi", "Aroua Torjmen", "Mohamed Ben Ahmed"], "n_citation": 0, "title": "An efficient multi-agent system combining POS-taggers for arabic texts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "990c240c-6486-419c-be31-fb3d223cd81d"}
{"authors": ["Werner Damm"], "n_citation": 0, "title": "Challenges in the verification of electronic control units", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "9b93edd5-f304-4b8c-a341-e0f1f17b241a"}
{"authors": ["Y. van der Laan", "Jan Treur", "Vu1010095", "Faculteit der Exacte Wetenschappen"], "n_citation": 0, "title": "An Agent Model for Computational Analysis of Mirroring Dysfunctioning in Autism Spectrum Disorders", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "9c031477-30e1-4026-a231-be968f51a937"}
{"abstract": "This paper gives a complete and tight mathematical analysis on the performance of the Most-Requested-First algorithm for on-demand data broadcast. The algorithm is natural and simple, yet its performance is surprisingly good in practice. We derive tight upper and lower bounds on MRF's competitiveness and thus reveal the exact competitive ratios of the algorithm under different system configurations. We prove that the competitive ratio of MRF is exactly 3 - l d when the start-up delay d is a multiple of the page length l; otherwise the ratio is 3.", "authors": ["Regant Y. S. Hung", "H. F. Ting"], "n_citation": 0, "title": "A Tight Analysis of Most-Requested-First for On-Demand Data Broadcast", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9d009367-2df3-4c54-a2a0-08c7023c27a5"}
{"abstract": "Given that applications and services for evolving mobile ad hoc networks (MANETs) have diverse quality of service requirements in a similar fashion to fixed networks, this paper proposes a proportional service differentiation (PSD) model. This model is highly scalable and simple to adopt in MANETs because it does not require explicit admission control or maintenance of state information in any intermediate node. It relies instead on localized scheduling and buffer management to achieve a desired global objective. Motivated by this aspect of the PSD model, we propose to combine it with a location-based forwarding strategy as a way to facilitate cross-layer optimization. This association is performed with a view to improve end-to-end service differentiation, although no other explicit mechanisms are used to achieve end-to-end guarantees. This model takes also into consideration the time-varying nature of available bandwidth in MANETs, and tries to calculate it dynamically. Simulation results confirm the per-hop performance improvement.", "authors": ["Sivapathalingham Sivavakeesar", "George Pavlou"], "n_citation": 0, "title": "Rate allocation and buffer management for proportional service differentiation in location-aided ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9d84f4e7-9356-4763-ae53-3f7a40d78afd"}
{"abstract": "3D image data provide several advantages than 2D data for face recognition and overcome many problems with 2D intensity images based methods. In this paper, we propose a novel approach to 3D-based face recognition. First, a novel representation, called intrinsic features, is presented to encode local 3D shapes. It describes complementary non-relational features to provide an intrinsic representation of faces. This representation is extracted after alignment, and is invariant to translation, rotation and scale. Without reduction, tens of thousands of intrinsic features can be produced for a face, but not all of them are useful and equally important. Therefore, in the second part of the work, we introduce a learning method for learning most effective local features and combining them into a strong classifier using an AdaBoost learning procedure. Experimental results are performed on a large 3D face database obtained with complex illumination, pose and expression variations. The results demonstrate that the proposed approach produces consistently better results than existing methods.", "authors": ["Chenghua Xu", "Tieniu Tan", "Stan Z. Li", "Yunhong Wang", "Cheng Zhong"], "n_citation": 50, "title": "Learning Effective Intrinsic Features to Boost 3D-Based Face Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9ebc9ca6-b2f0-4862-8b19-a9b74d98320b"}
{"abstract": "The estimation of evolutionary history from biomolecular sequences is a major intellectual project in systematic biology and many methods are used to reconstruct phylogenetic (i.e. evolutionary) trees from sequence data. In this paper, we report on an extensive performance analysis of parsimony and two distance-based methods, a popular method called neighbor joining, and a new method developed by Agarwala et al. which approximates the L\u221e-nearest tree, on more than 260,000 sequence data sets simulated on approximately 500 model trees. Our experiments indicate a decrease in statistical power of the two distance methods as the diameter grows, but also show that parsimony is not as badly affected by the diameter as the distance methods. More generally, the experiments indicate that parsimony is almost always more accurate than the other two methods on reasonable length sequences even under adverse conditions, such as having sites that evolve quickly within the tree, pairs of taxa with large evolutionary distances between them, or large ratios between the highest and the lowest substitution rates on the edges.", "authors": ["Kenneth Rice", "Tandy J. Warnow"], "n_citation": 0, "title": "Parsimony is hard to beat", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "a07bdcd9-a48f-4e3c-843a-07c4d417a404"}
{"abstract": "A planar polyomino of size n is an edge-connected set of n squares on a rectangular 2-D lattice. Similarly, a d-dimensional poly-cube (for d > 2) of size n is a connected set of n hypercubes on an orthogonal d-dimensional lattice, where two hypercubes are neighbors if they share a (d - 1)-dimensional face. There are also two-dimensional polyominoes that lie on a triangular or hexagonal lattice. In this paper we describe a generalization of Redelmeier's algorithm for counting two-dimensional rectangular polyominoes [Re81], which counts all the above types of polyominoes. For example, our program computed the number of distinct 3-D polycubes of size 18. To the best of our knowledge, this is the first tabulation of this value.", "authors": ["Gadi Aleksandrowicz", "Gill Barequet"], "n_citation": 0, "title": "Counting d-Dimensional Polycubes and Nonrectangular Planar Polyominoes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a1cc18f7-4495-4967-af0c-b68077ad20be"}
{"abstract": "In evolutionary continuous optimization by building and using probabilistic models, the multivariate Gaussian distribution and their variants or extensions such as the mixture of Gaussians have been used popularly. However, this Gaussian assumption is often violated in many real problems. In this paper, we propose a new continuous estimation of distribution algorithms (EDAs) with the variational Bayesian independent component analyzers mixture model (vbICA-MM) for allowing any distribution to be modeled. We examine how this sophisticated density estimation technique has influence on the performance of the optimization by employing the same selection and population alternation schemes used in the previous EDAs. Our experimental results support that the presented EDAs achieve better performance than previous EDAs with ICA and Gaussian mixture- or kernel-based approaches.", "authors": ["Dong-Yeon Cho", "Byoung-Tak Zhang"], "n_citation": 0, "title": "Evolutionary continuous optimization by distribution estimation with variational Bayesian independent component analyzers mixture model", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a2e7d451-95f9-4352-a940-676f9a124cb9"}
{"abstract": "In this workshop paper an experiment is designed which evaluates the use of an enterprise modeling language that was developed with the Resource Event Agent enterprise ontology and the Unified Foundational ontology as a theoretical base. The effect of using the ontology-driven modeling language is analyzed using Moody's Method Evaluation Model which contains both actual and perception-based variables for measuring the efficiency and effectiveness of the used method.", "authors": ["Frederik Gailly", "Geert Poels"], "n_citation": 0, "title": "Experimental evaluation of an ontology-driven enterprise modeling language", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "a3522aaa-d150-43ca-8ce8-ce27b19e45d5"}
{"abstract": "Storing, querying, and updating XML documents in multi-user environments requires data processing guarded by a transactional context to assure the well-known ACID properties, particularly with regard to isolate concurrent transactions. In this paper, we introduce the taDOM tree, an extended data model which considers organization of both attribute nodes and node values in a new way and allows fine-grained lock acquisition for XML documents. For this reason, we design a tailored lock concept using a combination of node locks, navigation locks, and logical locks in order to synchronize concurrent accesses to XML documents via the DOM API. Our synchronization concept supports user-driven tunable lock granularity and lock escalation to reduce the frequency of lock requests both aiming at enhanced transaction throughput. Therefore, the taDOM tree and the related lock modes are adjusted to the specific properties of the DOM API.", "authors": ["Michael Peter Haustein", "Theo H\u00e4rder"], "n_citation": 0, "title": "taDOM: A tailored synchronization concept with tunable lock granularity for the DOM API", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "a4d586c1-52a4-456f-aabf-e00d25de4fb5"}
{"abstract": "We propose a hybrid geometry-image representation of trees, which consists of three hierarchies. In the lowest hierarchy, a collection of triangles are used to represent the trunk and salient branches. The finer hierarchy for the small branches is represented by Frond Map, a special type of textures. As the finest hierarchy, the leaves are clustered first and represented as a set of billboards. For each hierarchy, we generate a sequence of LODs (Level of Details). The appropriate level is selected on-the-fly. Smooth transitions between successive levels are accomplished using an image-space voting technique. Our representation facilitates the rapid modeling of trees, the dynamic simulation of trees swaying under wind as well as realistic simulation of large-scale forest scenes. Experimental results demonstrate the efficiency and high quality of our approaches.", "authors": ["Chengfang Song", "Qifeng Tan", "Long Zhang", "Wei Chen", "Yi Gong", "Yu Guan", "Qunsheng Peng"], "n_citation": 0, "title": "Interactive simulation of large-scale forests in virtual reality applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a4d8602d-9fb1-456a-9c53-7fb63cb4d601"}
{"abstract": "A social choice function A is implementable with verification if there exists a payment scheme P such that (A, P) is a truthful mechanism for verifiable agents [Nisan and Ronen, STOC 99]. We give a simple sufficient condition for a social choice function to be implementable with verification for comparable types. Comparable types are a generalization of the well-studied one-parameter agents. Based on this characterization, we show that a large class of objective functions \u03bc admit social choice functions that are implementable with verification and minimize (or maximize) \u03bc. We then focus on the well-studied case of one-parameter agents. We give a general technique for constructing efficiently computable social choice functions that minimize or approximately minimize objective functions that are non-increasing and neutral (these are functions that do not depend on the valuations of agents that have no work assigned to them). As a corollary we obtain efficient online and offline mechanisms with verification for some hard scheduling problems on related machines.", "authors": ["Vincenzo Auletta", "Roberto De Prisco", "Paolo Penna", "Giuseppe Persiano", "Carmine Ventre"], "n_citation": 50, "title": "New Constructions of Mechanisms with Verification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a5528504-3e91-4b16-8b90-25e891383bbf"}
{"authors": ["Jonathan P. Bowen"], "n_citation": 0, "title": "Comp.specification.z and Z FORUM frequently asked questions", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "a8960efe-2fca-4d1e-950c-a1305010ea98"}
{"abstract": "In pervasive (ubiquitous) environments, context-aware agents are used to obtain, understand, and share local contexts with each other so that all resources in the environments could be integrated seamlessly. Context exchanging should be made privacy-conscious, which is generally controlled by users' privacy preferences. Besides who has rights to get what true information about him, a user's privacy preference could also designate who should be given obfuscated information. By obfuscation, people could present their private information in a coarser granularity, or simply in a falsified manner, depending on the specific situations. Nevertheless, obfuscation cannot be done randomly because by reasoning the receiver could know the information has been obfuscated. An obfuscated context can not only be inferred from its dependencies with other existing contexts, but could also be derived from its dependencies with the vanished ones. In this paper, we present a dynamic Bayesian network (DBN)-based method to reason about the obfuscated contexts in pervasive environments, where the impacts of the vanished historical contexts are properly evaluated. On the one hand, it can be used to detect obfuscations, and may further find the true information; on the other hand, it can help reasonably obfuscate information.", "authors": ["Xiangdong An", "Dawn N. Jutla", "Nick Cercone"], "n_citation": 0, "title": "Temporal context lie detection and generation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa76eb93-b304-4fca-b764-d92c8d98934c"}
{"abstract": "Among the keystones of interactive education are learners' motivation and visual tools that support learners' study at their convenient. In this paper we consider the finite state machines (automata) as an example of an interactive e-learning. Because of its abstract nature, automata seem to be less motivating to study despite its importance as a model of several software and hardware applications. This paper tackles the motivation problem and the design of supporting visual tools. We have three goals: motivating learners through several visual examples, introducing interactive visual tools to support interactive e-learning; and to evaluate the performance of several existing tools to Figure out which is practically more useful. To achieve these goals, a set of visual finite state machines examples was designed, an interactive automata simulator is introduced, and a performance evaluation is carried out. The visual examples and the tools are designed as Java applets, using Java 2D interface, so that they can run on any environment. It also enables the learner to easily access the tools through the web which is a support for interactive e-learning of finite state machines.", "authors": ["Mohamed Hamada"], "n_citation": 50, "title": "Visual tools and examples to support active E-learning and motivation with performance evaluation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aae09d1a-7fdd-4bf3-8996-182564d6a3f0"}
{"abstract": "This paper describes a new static analysis to show the absence of memory errors, especially string buffer overflows in C programs. The analysis is specifically designed for the subset of C that is found in critical embedded software. It is based on the theory of abstract interpretation and relies on an abstraction of stores that retains the length of string buffers. A transport structure allows to change the granularity of the abstraction and to concisely define several inherently complex abstract primitives such as destructive update and string copy. The analysis integrates several features of the C language such as multi-dimensional arrays, structures, pointers and function calls. A prototype implementation produces encouraging results in early experiments.", "authors": ["Xavier Allamigeon", "Wenceslas Godard", "Charles Hymans"], "n_citation": 0, "title": "Static analysis of string manipulations in critical embedded C programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ab177c02-3192-4c90-b0e4-48394097be4a"}
{"abstract": "The nominal datatype package implements an infrastructure in Isabelle/HOL for defining languages involving binders and for reasoning conveniently about alpha-equivalence classes. Pitts stated some general conditions under which functions over alpha-equivalence classes can be defined by a form of structural recursion and gave a clever proof for the existence of a primitive-recursion combinator. We give a version of this proof that works directly over nominal datatypes and does not rely upon auxiliary constructions. We further introduce proving tools and a heuristic that made the automation of our proof tractable. This automation is an essential prerequisite for the nominal datatype package to become useful.", "authors": ["Christian Urban", "Stefan Berghofer"], "n_citation": 0, "title": "A recursion combinator for nominal datatypes implemented in isabelle/HOL", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "acf89d08-0898-45b2-b5a0-18556b864486"}
{"abstract": "We present a shared memory approach to the parallelization of the Ant Colony Optimization (ACO) metaheuristic and a performance comparison with an existing message passing implementation. Our aim is to show that the shared memory approach is a competitive strategy for the parallelization of ACO algorithms. The sequential ACO algorithm on which are based both parallelization schemes is first described, followed by the parallelization strategies themselves. Through experiments, we compare speedup and efficiency measures on four TSP problems varying from 318 to 657 cities. We then discuss factors that explain the difference in performance of the two approaches. Further experiments are presented to show the performance of the shared memory implementation when varying numbers of ants are distributed among the available processors. In this last set of experiments, the solution quality obtained is taken into account when analyzing speedup and efficiency measures.", "authors": ["Pierre Delisle", "Marc Gravel", "Micha\u00ebl Krajecki", "Caroline Gagn\u00e9", "Wilson L. Price"], "n_citation": 0, "title": "Comparing parallelization of an ACO : Message passing vs. shared memory", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "adddf06e-579b-41d7-9969-393ce7a5f0dd"}
{"abstract": "We construct a boosting algorithm, which is the first both smooth and adaptive booster. These two features make it possible to achieve performance improvement for many learning tasks whose solution use a boosting technique. Originally, the boosting approach was suggested for the standard PAC model; we analyze possible applications of boosting in the model of agnostic learning (which is more realistic than PAC). We derive a lower bound for the final error achievable by boosting in the agnostic model; we show that our algorithm actually achieves that accuracy (within a constant factor of 2): When the booster faces distribution D, its final error is bounded above by 1/ -\u03b2 err D (F) + (, where err D' (F) + \u03b2 is an upper bound on the error of a hypothesis received from the (agnostic) weak learner when it faces distribution D' and \u03b6 is any real, so that the complexity of the boosting is polynomial in 1/\u03b6. We note that the idea of applying boosting in the agnostic model was first suggested by Ben-David, Long and Mansour and the above accuracy is an exponential improvement w.r.t. \u03b2 over their result (1/ -\u03b2 err D (F) 2(1/2-\u03b2)2/ln(1/\u03b2-1)  + \u03b6). Eventually, we construct a boosting tandem, thus approaching in terms of O the lowest number of the boosting iterations possible, as well as in terms of O the best possible smoothness. This allows solving adaptively problems whose solution is based on smooth boosting (like noise tolerant boosting and DNF membership learning), preserving the original solution's complexity.", "authors": ["Dmitry Gavinsky"], "n_citation": 50, "title": "Optimally-smooth adaptive boosting and application to agnostic learning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "af074db9-2883-4df5-a7dd-d4b16448513e"}
{"abstract": "The paper presents a method for improving text classification by using examples that are difficult to classify. Generally, researches to improve the text categorization performance are focused on enhancing existing classification models and algorithms itself, but the range of which has been limited by the feature-based statistical methodology. In this paper, we propose a new method to improve the accuracy and the performance using refinement training and post-processing. Especially, we focused on complex documents that are generally considered to be hard to classify. Our proposed method has a different style from traditional classification methods, and take a data mining strategy and fault tolerant system approaches. In experiments, we applied our system to documents which usually get low classification accuracy because they are laid on a decision boundary. The result shows that our system has high accuracy and stability in actual conditions.", "authors": ["Yun Jeong Choi", "Seung Soo Park"], "n_citation": 50, "title": "Refinement Method of Post-processing and Training for Improvement of Automated Text Classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b0b0c956-3ea6-49d6-9ecc-bbb10cb7efa8"}
{"abstract": "A system that retrieves problem reports from a NASA database is described. The database is queried with natural language questions. Part-of-speech tags are first assigned to each word in the question using a rule-based tagger. A partial parse of the question is then produced with independent sets of deterministic finite state automata. Using partial parse information, a look up strategy searches the database for problem reports relevant to the question. A bigram stemmer and irregular verb conjugates have been incorporated into the system to improve accuracy. The system is evaluated by a set of fifty five questions posed by NASA engineers. A discussion of future research is also presented.", "authors": ["Sebastian van Delden", "Fernando Gomez"], "n_citation": 0, "title": "Retrieving NASA problem reports with natural language", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b2829846-27c3-4af0-94ff-b527dddd4818"}
{"abstract": "The Java Modeling Language (JML) is used to specify detailed designs for Java classes and interfaces. It has a particularly rich set of features for specifying methods. This paper describes those features, with particular emphasis on the features related to specification inheritance. It shows how specification inheritance in JML forces behavioral subtyping, through a discussion of semantics and examples. It also describes a notion of modular reasoning based on static type information, supertype abstraction, which is made valid in JML by methodological restrictions on invariants, history constraints, and initially clauses and by behavioral subtyping.", "authors": ["Gary T. Leavens"], "n_citation": 0, "title": "JML's Rich, Inherited Specifications for Behavioral Subtypes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b757d815-7900-438b-9f54-5276aae80021"}
{"abstract": "In this paper, we focus on a new spatio-temporal representation scheme which can efficiently model multiple trajectories based on several moving objects in video databases. The traditional methods only consider direction property, time interval property, and spatial relations property for modeling moving objects' trajectories. But, our method also takes into account on distance property, conceptual location information, and related object information (e.g. player name having a soccer ball). In addition, we propose a similarity measure function that improves a retrieval accuracy to measure a similarity among multiple trajectories. The proposed scheme supports content-based retrieval using moving objects' trajectories and supports semantics-based retrieval using concepts which are acquired through the location information of moving objects. Finally, from the experimental results using real trajectories extracted from soccer video data with soccer ball and player, the performance of our scheme achieves about 15-20% performance improvement against existing schemes when the weights of angle and topological relation are over two times than that of distance.", "authors": ["Choon-Bo Shim", "John Kim"], "n_citation": 0, "title": "A Modeling and Similarity Measure Function for Multiple Trajectories in Moving Databases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b7ec3209-c33d-499e-98bf-d9a3de8b86e6"}
{"abstract": "For years, the cryptographic community has searched for good nonlinear functions. Bent functions, almost perfect nonlinear functions, and similar constructions have been suggested as a good base for cryptographic applications due to their highly nonlinear nature. In the first part of this paper we study these functions as block ciphers, and present several distinguishers between almost perfect nonlinear permutations and random permutations. The data complexity of the best distinguisher is O(2 n/3 ) and its time complexity is O(2 2n/3 ) for an n-bit block size, independent of the key size. In the second part of the paper we suggest a criterion to measure the effective linearity of a given block cipher. We devise a distinguisher for general block ciphers based on their effective linearity. Finally, we show that for several constructions, our distinguishing attack is better than previously known techniques.", "authors": ["Orr Dunkelman", "Nathan Keller"], "n_citation": 0, "title": "A new criterion for nonlinearity of block ciphers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b8a49df1-35cc-4017-b762-a79e8f7dc7d9"}
{"abstract": "We propose a hybrid estimation of distribution algorithm (MOHEDA) for solving the multiobjective 0/1 knapsack problem (MOKP). Local search based on weighted sum method is proposed, and random repair method (RRM) is used to handle the constraints. Moreover, for the purpose of diversity preservation, a new and fast clustering method, called stochastic clustering method (SCM), is also introduced for mixture-based modelling. The experimental results indicate that MOHEDA outperforms several other state-of-the-art algorithms.", "authors": ["Hui Li", "Qingfu Zhang", "Edward P. K. Tsang", "John A. Ford"], "n_citation": 0, "title": "Hybrid estimation of distribution algorithm for multiobjective Knapsack problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b91e1069-d3d2-494e-a264-b33206a454eb"}
{"abstract": "Given a sequence, the problem studied in this paper is to find a set of k disjoint continuous subsequences such that the total sum of all elements in the set is maximized. This problem arises naturally in the analysis of DNA sequences. The previous best known algorithm requires \u0398(n log n) time in the worst case. For a given sequence of length n, we present an almost linear-time algorithm for this problem. Our algorithm uses a disjoint-set data structure and requires O(n\u03b1(n, n)) time in the worst case, where \u03b1(n, n) is the inverse Ackermann function.", "authors": ["Fredrik Bengtsson", "Jingsen Chen"], "n_citation": 50, "title": "Computing Maximum-Scoring Segments in Almost Linear Time", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bba33fa2-05b0-4424-ac0b-39702faecef4"}
{"abstract": "This paper studies some basic problems in a multiple-object auction model using methodologies from theoretical computer science. We are especially concerned with situations where an adversary bidder knows the bidding algorithms of all the other bidders. In the two-bidder case, we derive an optimal randomized bidding algorithm, by which the disadvantaged bidder can procure at least half of the auction objects despite the adversary's a priori knowledge of his algorithm. In the general k-bidder case, if the number of objects is a multiple of k, an optimal randomized bidding algorithm is found. If the k - 1 disadvantaged bidders employ that same algorithm, each of them can obtain at least 1/k of the objects regardless of the bidding algorithm the adversary uses. These two algorithms are based on closed-form solutions to certain multivariate probability distributions. In situations where a closed-form solution cannot be obtained, we study a restricted class of bidding algorithms as an approximation to desired optimal algorithms.", "authors": ["Ming-Yang Kao", "Junfeng Qi", "Lei Tan"], "n_citation": 0, "title": "Optimal bidding algorithms against cheating in multiple-object auctions", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "bbf2f3ce-8709-4b5f-a911-2a8ed128b500"}
{"abstract": "It is well-known that the connection refinement of clause tableaux with paramodulation is incomplete (even with weak connections). In this paper, we present a new connection tableau calculus for logic with equality. This calculus is based on a lazy form of paramodulation where parts of the unification step become auxiliary subgoals in a tableau and may be subjected to subsequent paramodulations. Our calculus uses ordering constraints and a certain form of the basicness restriction.", "authors": ["Andrei Paskevich"], "n_citation": 2, "title": "Connection tableaux with lazy paramodulation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bde4b275-5244-43b3-9805-ecf5a123148d"}
{"abstract": "In this paper, we propose a query language and data model for spatio-temporal information, including objects of time-changing geometry. Our objective is to minimize the extensions required in SQL, or other relational languages, to support spatio-temporal queries. We build on the model proposed by Worboys where each state of a spatial object is captured as a snapshot of time; then, we use a directed-triangulation model to represent spatial data, and a point-based model to represent time at the conceptual level. Spatio-temporal reasoning and queries can be fully expressed with no new constructs, but user-defined aggregates, such as AREA and INSIDE for spatial relationships, DURATION and CONTAIN for temporal ones, and MOVING\u2015DISTANCE for spatio-temporal ones. We also consider the implementation problem under the assumption that, for performance reasons, the representation at the physical level can be totally different from the conceptual one. Thus, alternative physical representations, and mappings between conceptual and physical representations are discussed.", "authors": ["Cindy Xinmin Chen", "Carlo Zaniolo"], "n_citation": 87, "title": "SQLST: A spatio-temporal data model and query language", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "bfaea6d7-e08e-4e00-b9be-f1cc3f627820"}
{"abstract": "Considerable research exists on providing structured access to unstructured information sources, primarily for search and query. Little attention has been placed on (1) keeping the by-products of the process (e.g., connections between structured and unstructured data are typically forgotten and context is lost), (2) developing a rich, conceptual structure for this new layer of information (e.g., the structured data is often represented in simple relational tables), and (3) further elaborating and linking the new, structured information. We propose superimposed schematics to address these issues. Superimposed schematics offer E-R modeling constructs integrated with marks, where a mark holds an address to an information element in an underlying source. A superimposed schematic enables a conceptual addressing scheme for the information it contains (directly) and for the information it references (through marks). This addressing scheme can then be used for marking from additional layers of superimposed information. We consider schematics as a useful model for superimposed information and discuss how it fits into our general research on superimposed information.", "authors": ["Shawn Bowers", "Lois M. L. Delcambre", "David Maier"], "n_citation": 0, "title": "Superimposed schematics: Introducing E-R structure for in-situ information selections", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c0e0b542-6dd7-48bf-9158-74102b725f18"}
{"abstract": "Artificial Immune Systems are a new class of algorithms inspired by how the immune system recognizes, attacks and remembers intruders. This is a fascinating idea, but to be accepted for mainstream data mining applications, extensive benchmarking is needed to demonstrate the reliability and accuracy of these algorithms. In our research we focus on the AIRS classification algorithm. It has been claimed previously that AIRS consistently outperforms other algorithms. However, in these papers AIRS was compared to benchmark results from literature. To ensure consistent conditions we carried out benchmark tests on all algorithms using exactly the same set up. Our findings show that AIRS is a stable and robust classifier that produces around average results. This contrasts with earlier claims but shows AIRS is mature enough to be used for mainstream data mining.", "authors": ["Lingjun Meng", "Peter van der Putten", "Haiyang Wang"], "n_citation": 50, "title": "A comprehensive benchmark of the artificial immune recognition system (AIRS)", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c0f48e08-3b04-4845-bebd-145c0c9996a0"}
{"abstract": "In this paper we propose an approach to automatically produce test cases allowing to check the satisfiability of a linear property on a given implementation. Linear properties can be expressed by formulas of temporal logic. An observer is built from each formula. An observer is a finite automaton on infinite sequences. Of course, testing the satisfiability of an infinite sequence is not possible. Thus, we introduce the notion of bounded properties. Test cases are generated from a (possibly partial) specification of the IUT and the property to validate is expressed by a parameterised automaton on infinite words. This approach is formally defined, and a practical test generation algorithm is sketched.", "authors": ["Jean-Claude Fernandez", "Laurent Mounier", "Cyril Pachon"], "n_citation": 0, "title": "Property oriented test case generation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c11e4292-b9db-4d4c-baf6-317a529d439c"}
{"abstract": "The bilinear mapping technique that uses the (Weil and Tate) pairings over elliptic (or hyperelliptic) curves represents a great breakthrough in cryptography. This paper surveys this new trend in cryptography, and emphasizes the design of efficient cryptographic primitives that are provably secure in the standard model (i.e., without the random oracle model).", "authors": ["Tatsuaki Okamoto"], "n_citation": 50, "title": "Cryptography Based on Bilinear Maps", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c2a14b6d-dabf-4de8-a02a-468f93d94ffa"}
{"abstract": "Many scientific databases need to manage complex 3D geometric data such as models of the cerebral cortex. Often the complexity of the data forces users to construct muliple, simpler representations, which cover the real, complete model only partially and approximately. In order to recover the original information one needs to integrate these partial and incomplete models. In this paper, we first develop a conceptual model for objects and relationships in 3D geometric data, as well as their partial and approximate representations. We then establish mapping relationships between different approximations of the same data. Finally, we present a geometric information integration technique that will perform the integration where possible, and determine, for some cases, when the integration cannot be performed.", "authors": ["Simone Santini", "Amarnath Gupta"], "n_citation": 0, "title": "Conceptual integration of multiple partial geometric models", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c435d74d-67c0-4ae6-8501-283557ad50d9"}
{"abstract": "As Extensible Markup Language (XML) [5] is emerging as the data format of the internet era, there are increasing needs to efficiently store and query XML data. One way towards this goal is using relational database by transforming XML data into relational format. In this paper, we argue that existing transformation algorithms are not complete in the sense that they focus only on structural aspects and ignoring semantic aspects. We present the semantic knowledge that needs to be captured during the transformation to ensure a correct relational schema. Further, we show a simple algorithm that can 1) derive such semantic knowledge from the given XML Document Type Definition (DTD) and 2) preserve the knowledge by representing them in terms of semantic constraints in relational database terms. By combining the existing transformation algorithms and our constraints-preserving algorithm, one can transform XML DTD to relational schema where correct semantics and behaviors are guaranteed by the preserved constraints. Experimental results are also presented.", "authors": ["Dongwon Lee", "Wesley W. Chu"], "n_citation": 0, "title": "Constraints-preserving transformation from XML Document Type Definition to relational schema", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "c5531c1d-3a47-433d-9e6d-f4b25e4ac5f7"}
{"abstract": "Traditional Internet services provide the same level of service to all users without considering about their applications' requirements. However, new applications demand better quality of service (QoS) and host mobility according to wireless and Internet technologies are more widely available. To satisfy QoS and mobility of communication, we deploy differentiated services (Diffserv) model to provide differentiated QoS in mobile IPv6 networks. The operational procedures and cost analyzing models are suggested for an MN (mobile node)'s mobility and its performance is presented by cost analysis.", "authors": ["Misun Kim", "Youngsong Mun"], "n_citation": 0, "title": "Cost Evaluation of Differentiated QoS Model in Mobile IPv6 Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c68f6340-9ce2-4672-97c6-86616c7e3678"}
{"abstract": "In this study, we first analyze the log data of the web-server system at a Korean company and confirm strong self-similarities that have been reported in a wide range of internet data. Then, we propose a simulation approach to generate arrivals of web page requests based on the conventional familiar probability distributions. We also present a forecasting model to generate future arrivals and test the validity of our approach through real data. Finally, we present our software tool that we developed to analyze web log data.", "authors": ["Ho Woo Lee", "Yun Bae Kim", "Chang Hun Lee", "Won Joo Seo", "Jin Soo Park", "Seunghyun Yoon"], "n_citation": 0, "title": "An empirical study on a web server queueing system and traffic generation by simulation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c6ecffd1-7150-4d47-bb92-252ad924ce4d"}
{"abstract": "Ontology is a core component in semantic Web applications. The employment of an ontology building method affects the quality of ontology and the applicability of ontology language. An evaluation approach for ontology building guidelines is presented. The evaluation is based on semiotic quality framework, an evaluation scheme frequently applied for evaluating the quality of conceptual models. The framework is extended with situational and computational capabilities. A sample of ontology building method guidelines is analyzed in general and evaluated comparatively in a case study at an oil company in particular. Directions for further refinement of ontology building methods are discussed.", "authors": ["Sari Hakkarainen", "Darijus Strasunskas", "Lillian Hella", "Stine Tuxen"], "n_citation": 50, "title": "Choosing appropriate method guidelines for web-ontology building", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cb5d35c4-6174-4686-8833-bce85d3f8f18"}
{"abstract": "Since Miller and Koblitz applied elliptic curves to cryptographic system in 1985[3,6], a lot of researchers have been interested in this field and various speedup techniques for the scalar multiplication have been developed. Recently, Gallant et al. published a method that accelerates the scalar multiplication and is applicable to a larger class of curves[4]. In the process of their method, they assumed the existence of a special pair of two short linearly independent vectors. Once a pair of such vectors exists, their decomposition method improves the efficiency of the scalar multiplication roughly about 50%. In this paper, we state and prove a necessary condition for the existence of a pair of desired vectors and we also present an algorithm to find them.", "authors": ["Dongryeol Kim", "Seongan Lim"], "n_citation": 50, "title": "Integer decomposition for fast scalar multiplication on elliptic curves", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "cd6d07cd-aacb-4e7a-9547-4c0792db8a7d"}
{"abstract": "Abstract interpretation techniques prove properties of programs by computing abstract fixpoints. All such analyses suffer from the possibility of false errors. We present a new counterexample driven refinement technique to reduce false errors in abstract interpretations. Our technique keeps track of the precision losses during forward fixpoint computation, and does a precise backward propagation from the error to either confirm the error as a true error, or identify a refinement so as to avoid the false error. Our technique is quite simple, and is independent of the specific abstract domain used. An implementation of our technique for affine transition systems is able to prove invariants generated by the StInG tool [19] without doing any specialized analysis for linear relations. Thus, we hope that the technique can work for other abstract domains as well. We sketch how our technique can be used to perform shape analysis by simply defining an appropriate widening operator over shape graphs.", "authors": ["Bhargav S. Gulavani", "Sriram K. Rajamani"], "n_citation": 0, "title": "Counterexample driven refinement for abstract interpretation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cde10d43-842b-49a6-8b12-f8b889f9c0f8"}
{"abstract": "The adoption of high level models has been advocated by many authors as instrumental in Web site development as a strong support to both design and maintenance. We will discuss how the specification of Web sites at the logical level can greatly benefit from the introduction of specific features for the representation of time, which could also support the notions of versions and editions of objects in the site. Moreover, time can be seen as a coordinate of Web models, a more general notion that includes various forms of specializations and variations, such as those related to location, language, user preferences, device type.", "authors": ["Paolo Atzeni"], "n_citation": 0, "title": "Time: A coordinate for Web site modelling", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "cece408f-5453-4b74-a3f6-ea38e688d0a8"}
{"abstract": "Formal semantics of OQL in terms of object algebra, such as quantification, mapping, selection, unnesting or partitioning, developed by the author is defined, and it is shown in multiple examples that OQL queries can be easily expressed by means of this algebra. As a result, an OQL query can be mechanically translated into the corresponding object algebra expression, which can be further optimized and executed.", "authors": ["Alexandre V. Zamulin"], "n_citation": 0, "title": "Formal semantics of the ODMG 3.0 object query language", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d1d16cfb-f591-43f0-993f-8b97c10bc3a5"}
{"abstract": "This paper proposes a new support vector machine (SVM) with a robust loss function for data mining. Its dual optimal formation is also constructed. A gradient based algorithm is designed for fast and simple implementation of the new support vector machine. At the same time it analyzes algorithm's convergence condition and gives a formula to select learning step size. Numerical simulation results show that the new support vector machine performs significantly better than a standard support vector machine.", "authors": ["Haoran Zhang", "Xiaodong Wang", "Changjiang Zhang", "Xiuling Xu"], "n_citation": 0, "title": "A new support vector machine for data mining", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d31e7d28-2025-41f5-b34d-b6a3465cbb8b"}
{"abstract": "Customer Chum Prediction is an increasingly pressing issue in today's ever-competitive commercial arena. Although there are several researches in chum prediction, but the accuracy rate, which is very important to business, is not high enough. Recently, Support Vector Machines (SVMs), based on statistical learning theory, are gaining applications in the areas of data mining, machine learning, computer vision and pattern recognition because of high accuracy and good generalization capability. But there has no report about using SVM to Customer Churn Prediction. According to churn data set characteristic, the number of negative examples is very small, we introduce an improved one-class SVM. And we have tested our method on the wireless industry customer chum data set. Our method has been shown to perform very well compared with other traditional methods, ANN, Decision Tree, and Naive Bays.", "authors": ["Yu Zhao", "Bing Li", "Xiu Li", "Wenhuang Liu", "Shouju Ren"], "n_citation": 0, "title": "Customer churn prediction using improved one-class support vector machine", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d37b037a-b32e-4708-95af-74687fc627a6"}
{"abstract": "This paper introduces a generator that creates problem instances for the Euclidean symmetric travelling salesman problem. To fit real world problems, we look at maps consisting of clustered nodes. Uniform random sampling methods do not result in maps where the nodes are spread out to form identifiable clusters. To improve upon this, we propose an evolutionary algorithm that uses the layout of nodes on a map as its genotype. By optimising the spread until a set of constraints is satisfied, we are able to produce better clustered maps, in a more robust way. When varying the number of clusters in these maps and, when solving the Euclidean symmetric travelling salesman person using Chained Lin-Kernighan, we observe a phase transition in the form of an easy-hard-easy pattern.", "authors": ["Jano van Hemert", "Neil B Urquhart"], "n_citation": 0, "title": "Phase transition properties of clustered travelling salesman problem instances generated with evolutionary computation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d3b8800a-748a-4fe4-9796-6da86051af3b"}
{"abstract": "We present a domain-independent method for generation of natural language explanations of rules in expert systems. The method is based on explanatory rules written in a procedural formal language, which build the explanation from predefined natural language texts fragments. For better style, a specific text fragment is randomly selected from a group of synonymous expressions. We have implemented 16 groups of explanatory rules and 74 groups of explanatory texts containing about 200 text fragments.", "authors": ["Mar\u00eda de los \u00c1ngeles Alonso-Lavernia", "Argelio V\u00edctor De-la-Cruz-Rivera", "Grigori Sidorov"], "n_citation": 50, "title": "Generation of natural language explanations of rules in an expert system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d438aae7-2f42-48f4-9288-9419fe41dc16"}
{"abstract": "The development of masking schemes to secure AES implementations against side channel attacks is a topic of ongoing research. Many different approaches focus on the AES S-box and have been discussed in the previous years. Unfortunately, to our knowledge most of these countermeasures only address first-order DPA. In this article, we discuss the theoretical background of higher order DPA. We give the expected measurement costs an adversary has to deal with for different hardware models. Moreover, we present a masking scheme which protects an AES implementation against higher order DPA. We have implemented this masking scheme for various orders and present the corresponding performance details implementors will have to expect.", "authors": ["Kai Schramm", "Christof Paar"], "n_citation": 199, "title": "Higher order masking of the AES", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d4dbf772-2fe1-4c55-ae9b-a5f2f2fa9356"}
{"abstract": "The success of web services has influenced the way in which Grid applications are being written. Web services are increasingly used as a means to realise service-oriented distributed computing. Grid users often submit their applications in the form of workflows with certain Quality of Service (QoS) requirements imposed on the workflows. These workflows detail the composition of web services and the level of service required from the Grid. This paper addresses workload allocation techniques for Grid workflows. We model a web service as an M/M/k queue and obtain a numerical solution for missed deadlines (failures) of Grid workflow tasks. The approach is evaluated through an experimental simulation and the results confirm that the proposed workload allocation strategy performs considerably better in terms of satisfying QoS requirements of Grid workflows than scheduling algorithms that don't employ such workload allocation techniques.", "authors": ["Yash Patel", "John Darlington"], "n_citation": 0, "title": "Allocating QOS-constrained applications in a web service-oriented grid", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d74bb572-40cc-4833-b384-779c735ffb02"}
{"abstract": "Proof planning is an automated theorem proving technique which encodes meaningful blocks of proof as planning operators called methods. Methods often encode complex control strategies, and a language of methodicals, similar to tacticals, has been developed to allow methods to be expressed in a modular way. Previous work has demonstrated that proof planning can be effective for interactive theorem proving, but it has not been clear how to reconcile the complex control encoded by methodicals with the needs of interactive theorem proving. In this paper we develop an operational semantics for methodicals which allows reasoning about proof plans in the abstract, without generating object-level proofs, and facilitates interactive planning. The semantics is defined by a handful of deterministic transition rules, represents disjunction and backtracking in the planning process explicitly, and handles the cut methodical correctly.", "authors": ["Julian Richardson"], "n_citation": 0, "title": "A semantics for proof plans with applications to interactive proof planning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "da9cf240-4e47-4bfd-9bf8-6641b9260d3e"}
{"abstract": "This paper presents a simulation preorder for continuous-time Markov chains (CTMCs). The simulation preorder is a conservative extension of a weak variant of probabilistic simulation on fully probabilistic systems, i.e., discrete-time Markov chains. The main result of the paper is that the simulation preorder preserves safety and liveness properties expressed in continuous stochastic logic (CSL), a stochastic branching-time temporal logic interpreted over CTMCs.", "authors": ["Christel Baier", "Joost-Pieter Katoen", "Holger Hermanns", "Boudewijn R. Haverkort"], "n_citation": 50, "title": "Simulation for continuous-time Markov chains", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "dd32edf7-2682-49bb-bb8f-9d91109532e8"}
{"abstract": "Solomonoff's inductive learning model is a powerful, universal and highly elegant theory of sequence prediction. Its critical flaw is that it is incomputable and thus cannot be used in practice. It is sometimes suggested that it may still be useful to help guide the development of very general and powerful theories of prediction which are computable. In this paper it is shown that although powerful algorithms exist, they are necessarily highly complex. This alone makes their theoretical analysis problematic, however it is further shown that beyond a moderate level of complexity the analysis runs into the deeper problem of Godel incompleteness. This limits the power of mathematics to analyse and study prediction algorithms, and indeed intelligent systems in general.", "authors": ["Shane Legg"], "n_citation": 0, "title": "Is there an elegant universal theory of prediction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e2990126-ef03-4cd5-aaf8-1cb9854d37e3"}
{"abstract": "There is much current interest in publishing and viewing database-resident data as XML documents. In fact, such XML views of the database can be easily visualized on web browsers and processed by web languages, including powerful query languages such as XQuery. As the database is updated, its external XML view also evolves. In this paper, we investigate the problem of representing the evolution history of such a view as yet another XML document, whereby the complete history of the database can also be visualized on web browsers, processed by web languages, and queried using powerful query languages such as XQuery. We investigate various approaches used for publishing relational data, and identify and select those which are best for representing and querying database histories. We show that the selected representations make it easy to formulate in XQuery temporal queries that are difficult to express using SQL on database relations. Finally, we discuss briefly the storage organization that can be used to support these queries efficiently.", "authors": ["Fusheng Wang", "Carlo Zaniolo"], "n_citation": 50, "title": "Preserving and querying histories of XML-published relational databases", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e2de8fa8-5a73-42ac-974d-19b132207100"}
{"abstract": "We propose an extension of the join-calculus with pattern matching on algebraic data types. Our initial motivation is twofold: to provide an intuitive semantics of the interaction between concurrency and pattern matching; to define a practical compilation scheme from extended join-definitions into ordinary ones plus ML pattern matching. To assess the correctness of our compilation scheme, we develop a theory of the applied join-calculus, a calculus with value-passing and value matching.", "authors": ["Qin Ma", "Luc Maranget"], "n_citation": 0, "title": "Compiling pattern matching in join-patterns", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e47f10f0-35af-4fe5-aa91-0c51c2e4fe9b"}
{"authors": ["Roberto Javier L\u00f3pez Sastre", "Tinne Tuytelaars", "Saturnino Maldonado Basc\u00f3n"], "n_citation": 0, "title": "Class representative visual words for category-level object recognition", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "e5a3c834-215a-40b0-a078-57a0d55e0283"}
{"abstract": "Snoop is an event specification language developed for expressing primitive and composite events that are part of Event-Condition-Action (or ECA) rules. In Snoop, an event was defined to be an instantaneous, atomic (happens completely or not at all) occurrence of interest and the time of occurrence of the last event in an event expression was used as the time of occurrence for the entire event expression. The above detection-based semantics does not recognize multiple compositions of some operators - especially Sequence - in the intended way. In order to recognize all event operators, in all contexts, in the intended way, operator semantics need to include start time as well as end time for an event expression (i.e., interval-based semantics). In this paper, we formalize Snoop Interval-Based (SnoopIB), the occurrence of Snoop event operators and expressions using interval-based semantics. The algorithms for the detection of events using interval-based semantics introduce some challenges, as not all the events are known (especially their starting points).", "authors": ["Raman Adaikkalavan", "Sharma Chakravarthy"], "n_citation": 0, "title": "SnoopIB: Interval-Based event specification and detection for active databases", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e6c6a4e4-1fe7-4c1e-b596-44724626ec95"}
{"authors": ["Lois M. L. Delcambre", "Vijay Khatri", "Yair Wand", "Barbara Williams", "Carson C. Woo", "Mark Zozulia"], "n_citation": 0, "title": "Eliciting Data Semantics Via Top-Down and Bottom-Up Approaches : Challenges and Opportunities", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e721df34-49bf-4308-9453-1587ae26ffb3"}
{"abstract": "We present a non-interactive chosen ciphertext secure threshold encryption system. The proof of security is set in the standard model and does not use random oracles. Our construction uses the recent identity based encryption system of Boneh and Boyen and the chosen ciphertext secure construction of Canetti, Halevi, and Katz.", "authors": ["Dan Boneh", "Xavier Boyen", "Shai Halevi"], "n_citation": 0, "title": "Chosen ciphertext secure public key threshold encryption without random oracles", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e780e0bb-0ac9-4288-87ec-bed212915bbf"}
{"abstract": "Finding a good pattern which discriminates one set of strings from the other set is a critical task in knowledge discovery. In this paper, we review a series of our works concerning with the string pattern discovery. It includes theoretical analyses of learnabilities of some pattern classes, as well as development of practical data structures which support efficient string processing.", "authors": ["Ayumi Shinohara"], "n_citation": 0, "title": "String pattern discovery", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e7d91429-c84f-417d-9ae6-ab432c67f549"}
{"abstract": "An important step in automatic fingerprint recognition systems is the segmentation of fingerprint images. In this paper, we present an adaptive algorithm based on Gaussian-Hermite moments for non-uniform background removing in fingerprint image segmentation. Gaussian-Hermite moments can better separate image features based on different modes. We use Gaussian-Hermite moments of different orders to separate background and foreground of fingerprint image. Experimental results show that the use of Gaussian-Hermite moments makes a significant improvement for the segmentation of fingerprint images.", "authors": ["Lin Wang", "Hongmin Suo", "Mo Dai"], "n_citation": 0, "title": "Fingerprint image segmentation based on Gaussian-Hermite moments", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e8ee6637-4e44-4ebd-8e6b-7c292bb1e219"}
{"abstract": "Higher-order representations of objects such as programs, specifications and proofs are important to many metaprogramming and symbolic computation tasks. Systems that support such representations often depend on the implementation of an intensional view of the terms of suitable typed lambda calculi. Refined lambda calculus notations have been proposed that can be used in realizing such implementations. There are, however, choices in the actual deployment of such notations whose practical consequences are not well understood. Towards addressing this lacuna, the impact of three specific ideas is examined: the de Bruijn representation of bound variables, the explicit encoding of substitutions in terms and the annotation of terms to indicate their independence on external abstractions. Qualitative assessments are complemented by experiments over actual computations. The empirical study is based on AProlog programs executed using suitable variants of a low level, abstract machine based implementation of this language.", "authors": ["Chuck Liang", "Gopalan Nadathur"], "n_citation": 50, "title": "Tradeoffs in the intensional representation of lambda terms", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e9984a69-d857-4932-b879-9d5d6c4500a8"}
{"abstract": "The Curry form of a term, like f(a,b), allows us to write it, using just a single binary function symbol, as @(@(f,a),b). Using this technique we prove that the signature is not relevant in second-order unification, and conclude that one binary symbol is enough. By currying variable applications, like X(a), as @(X,a), we can transform second-order terms into first-order terms, but we have to add betareduction as a theory. This is roughly what it is done in explicit unification. We prove that by currying only constant applications we can reduce second-order unification to second-order unification with just one binary function symbol. Both problems are already known to be undecidable, but applying the same idea to context unification, for which decidability is still unknown, we reduce the problem to context unification with just one binary function symbol. We also discuss about the difficulties of applying the same ideas to third or higher order unification.", "authors": ["Jordi Levy", "Mateu Villaret"], "n_citation": 0, "title": "Currying second-order unification problems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ef9df4b4-4470-4977-a018-e39e7a606ca6"}
{"authors": ["J\u00fcrgen Zimmer", "Serge Autexier"], "n_citation": 50, "title": "The mathserve system for semantic web reasoning services", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f04c193e-1e0e-479d-83bc-09e9bc32173c"}
{"authors": ["Jens Hermans", "Frederik Vercauteren", "Bart Preneel"], "n_citation": 1, "title": "Speed records for NTRU", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "f16e5113-468b-4871-8c1c-11eb63848fa4"}
{"abstract": "In recent years the use of graph-cuts has become quite popular in computer vision. However, researchers have repeatedly asked the question whether it might be possible to compute a measure of uncertainty associated with the graph-cut solutions. In this paper we answer this particular question by showing how the min-marginals associated with the label assignments in a MRF can be efficiently computed using a new algorithm based on dynamic graph cuts. We start by reporting the discovery of a novel relationship between the min-marginal energy corresponding to a latent variable label assignment, and the flow potentials of the node representing that variable in the graph used in the energy minimization procedure. We then proceed to show how the min-marginal energy can be computed by minimizing a projection of the energy function defined by the MRF. We propose a fast and novel algorithm based on dynamic graph cuts to efficiently minimize these energy projections. The min-marginal energies obtained by our proposed algorithm are exact, as opposed to the ones obtained from other inference algorithms like loopy belief propagation and generalized belief propagation. We conclude by showing how min-marginals can be used to compute a confidence measure for label assignments in labelling problems such as image segmentation.", "authors": ["Pushmeet Kohli", "Philip H. S. Torr"], "n_citation": 55, "title": "Measuring Uncertainty in Graph Cut Solutions : Efficiently Computing Min-marginal Energies Using Dynamic Graph Cuts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f3794bf5-732d-4137-b1ec-c7363362ac14"}
{"abstract": "We formulate a robust method using Expectation Maximization (EM) to address the problem of dense photometric stereo. Previous approaches using Markov Random Fields (MRF) utilized a dense set of noisy photometric images for estimating an initial normal to encode the matching cost at each pixel, followed by normal refinement by considering the neighborhood of the pixel. In this paper, we argue that they had not fully utilized the inherent data redundancy in the dense set and that its full exploitation leads to considerable improvement. Using the same noisy and dense input, this paper contributes in learning relevant observations, recovering accurate normals and very good surface albedos, and inferring optimal parameters in an unifying EM framework that converges to an optimal solution and has no free user-supplied parameter to set. Experiments show that our EM approach for dense photometric stereo outperforms the previous approaches using the same input.", "authors": ["W. Tai-Pang", "Chi-Keung Tang"], "n_citation": 50, "title": "Dense Photometric Stereo by Expectation Maximization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f6023ce8-4328-4faf-b565-991d80049ed9"}
{"abstract": "Hospitals, organizations and companies are responsible keeping data and information about their customers private even if many internal employees have access to this data or information. When accused of an unauthorized disclosure of private information, it is important for the hospital to know which employees had the opportunity to disclose concrete private information. Our approach describes secret information in form of a secret query and performs two steps to detect which employees have used 'suspicious' queries, i.e., queries the result of which allows the user to derive secret information. First, we analyze the structure of queries and of the secret query to exclude non-suspicious queries. Second, we derive a formula from user query, query result and secret query, which is satisfiable if and only if the query is non-suspicious.", "authors": ["Stefan B\u00f6ttcher", "Rita Steinmetz"], "n_citation": 0, "title": "Information disclosure by XPath queries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f6728ef3-b488-43d2-8d68-7410e8ce00a5"}
{"abstract": "In this paper, an efficient access method for integrating multi-scale geometric data is proposed. Previous access methods do not access multi-scale geometric data efficiently. To solve it, a few access methods for multi-scale geometric data, are known. However these methods do not support all types of multi-scale geometric data, because they support only a selection operation and a simplification operation of all map generalization operations. We propose a new method for integrating multi-scale geometric data. In the proposed method, collections of indexes in its own scale are integrated into a single index structure. By the integration, not only does the proposed method offers fast search, but also the proposed method does not introduce data redundancy. Moreover, the proposed method supports all types of multi-scale geometric data. The experimental results show that our method is an efficient method for integrating multi-scale geometric data.", "authors": ["Joonhee Kwon", "Yong-Ik Yoon"], "n_citation": 0, "title": "An access method for integrating multi-scale geometric data", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f765359f-8af8-4905-ada3-32db1364d6a1"}
{"abstract": "Ordinary decision tree classifiers are used to classify data with single-valued attributes and single-class labels. This paper develops a new decision tree classifier SSC for multi-valued and multi-labeled data, on the basis of the algorithm MMDT, improves on the core formula for measuring the similarity of label-sets, which is the essential index in determining the goodness of splitting attributes, and proposes a new approach of measuring similarity considering both same and consistent features of label-sets, and together with a dynamic approach of adjusting the calculation proportion of the two features according to current data set. SSC makes the similarity of label-sets measured more comprehensive and accurate. The empirical results prove that SSC indeed improves the accuracy of MMDT, and has better classification efficiency.", "authors": ["Hong Li", "Rui Zhao", "Jianer Chen", "Yao Xiang"], "n_citation": 0, "title": "Research on Multi-valued and Multi-labeled Decision Trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f9994d5c-60f4-4462-a12d-8d5fdf354c6a"}
{"abstract": "Multi-parent crossovers have shown their superiority over classic two-parent crossovers in several problems. However, they still lack theoretical foundation to support the advantages of using more than two parents. In this paper we propose a uniform population model that helps analyze the behavior of crossover beyond the influence of selection process and the number of parents. An analysis of the probability for multi-parent diagonal crossover to obtain the optimal solution is derived accordingly. Analytical results demonstrate the advantage and limitation of multi-parent crossovers over two-parent crossover.", "authors": ["Chuan-Kang Ting"], "n_citation": 0, "title": "An analysis of the effectiveness of multi-parent crossover", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fa302d1e-0c63-4244-b4f3-1b49ff2dc451"}
{"abstract": "In this paper we present a new density estimation algorithm using mixtures of mixtures of Gaussians. The new algorithm overcomes the limitations of the popular Expectation Maximization algorithm. The paper first introduces a new model selection criterion called the Penalty-less Information Criterion, which is based on the Jensen-Shannon divergence. Mean-shift is used to automatically initialize the means and covariances of the Expectation Maximization in order to obtain better structure inference. Finally, a locally linear search is performed using the Penalty-less Information Criterion in order to infer the underlying density of the data. The validity of the algorithm is verified using real color images.", "authors": ["Wael Abd-Almageed", "Larry S. Davis"], "n_citation": 0, "title": "Density Estimation Using Mixtures of Mixtures of Gaussians", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fad63faf-6dd9-4cd5-8620-06c2078f9274"}
{"abstract": "This paper presents a novel genetic algorithm (GA) for multiple sequence alignment in protein analysis. The most significant improvement afforded by this algorithm results from its use of segment profiles to generate the diversified initial population and prevent the destruction of conserved regions by crossover and mutation operations. Segment profiles contain rich local information, thereby speeding up convergence. Secondly, it introduces the use of the norMD function in a genetic algorithm to measure multiple alignment Finally, as an approach to the premature problem, an improved progressive method is used to optimize the highest-scoring individual of each new generation. The new algorithm is compared with the ClustalX and T-Coffee programs on several data cases from the BAliBASE benchmark alignment database. The experimental results show that it can yield better performance on data sets with long sequences, regardless of similarity.", "authors": ["Yanping Lv", "Shaozi Li", "Changle Zhou", "Wenzhong Guo", "Zhengming Xu"], "n_citation": 0, "title": "Improved Genetic Algorithm for Multiple Sequence Alignment Using Segment Profiles (GASP)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fd237d23-2647-4248-aa7b-32fd9576ec56"}
{"abstract": "We propose a new algorithm called SCD for learning the structure of a Bayesian network. The algorithm is a kind of constraint-based algorithm. By taking advantage of variable ordering, it only requires polynomial time conditional independence tests and learns the exact structure theoretically. A variant which adopts the Bayesian Dirichlet scoring function is also presented for practical purposes. The performance of the algorithms are analyzed in several aspects and compared with other existing algorithms. In addition, we define a new evaluation metric named EP power which measures the proportion of errors caused by previously made mistakes in the learning sequence, and use the metric for verifying the robustness of the proposed algorithms.", "authors": ["Sanghack Lee", "Jihoon Yang", "Sungyong Park"], "n_citation": 0, "title": "A New Polynomial Time Algorithm for Bayesian Network Structure Learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "01def5c5-6462-47ef-8e83-e8ec6ec2950a"}
{"abstract": "Roles are meant to capture dynamic and temporal aspects of real-world objects. The role concept has been used with many semantic meanings: dynamic class, aspect, perspective, interface or mode. This paper identifies common semantics of different role models found in the literature. Moreover, it presents a conceptual modelling pattern for the role concept that includes both the static and dynamic aspects of roles. A conceptual modelling pattern is aimed at representing a specific structure of knowledge that appears in different domains. In particular, we adapt the pattern to UML. The use of this pattern eases the definition of roles in conceptual schemas. In addition, we describe the design of schemas defined using our pattern in order to implement them in any object-oriented language. We also discuss the advantages of our approach over previous ones.", "authors": ["Jordi Cabot", "Ruth Ravent\u00f3s"], "n_citation": 0, "title": "Roles as entity types: A conceptual modelling pattern", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "03ebd87a-6d74-49ba-8b60-403f688068db"}
{"authors": ["Shaoze Cheng", "Jie-Qiong Jin", "Antoine Vigneron", "Y. Wang"], "n_citation": 0, "title": "Approximate shortest homotopic paths in weighted regions", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "0763d9f9-1d1e-4b22-b832-a92f18c1836c"}
{"abstract": "A low-rate DoS attack to iterative servers has recently appeared as a new approach for defeating services using rates of traffic that could be adjusted to bypass security detection mechanisms. Although the fundamentals and effectiveness of these kind of attacks are known, it is not clear how to design the attack to achieve specific constraints based on the used rate and the efficiency in denial of service obtained. In this paper 1 , a comprehensive mathematical framework that models the behaviour of the attack is presented. The main contribution of this model is to give a better understanding of the dynamics of these kind of attacks, in order to facilitate the development of detection and defense mechanisms.", "authors": ["Gabriel Maci\u00e1-Fern\u00e1ndez", "Jes\u00fas E. D\u00edaz-Verdejo", "Pedro Garc\u00eda-Teodoro"], "n_citation": 0, "title": "Mathematical foundations for the design of a low-rate DoS attack to iterative servers (short paper)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "08b09d7a-d2e3-41a2-a5a4-6c7be43fca58"}
{"abstract": "Novel Independent Component analysis(ICA) algorithm for hybrid sources separation based on constrained optimization-exterior penalty function method is proposed. The proposed exterior penalty ICA algorithm is under the framework of constrained ICA(cICA) method to solve the constrained optimization problem by using the exterior penalty function method. In order to choose nonlinear functions as the probability density function (PDF) estimation of the source signals, generalized k-nearest neighbor(GKNN) PDF estimation is proposed which can separate the hybrid mixtures of source signals using only a flexible model and more important it is completely blind to the sources. The proposed EX-cICA algorithm provides the way to wider applications of ICA methods to real world signal processing. Simulations confirm the effectiveness of the proposed algorithm.", "authors": ["Fasong Wang", "Hongwei Li", "Rui Li"], "n_citation": 0, "title": "Exterior Penalty Function Method Based ICA Algorithm for Hybrid Sources Using GKNN Estimation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "098309e0-03a5-497f-99d5-068a09a0e373"}
{"abstract": "This paper proposes and evaluates the use of linguistic information in the pre-processing phase of text classification. We present several experiments evaluating the selection of terms based on different measures and linguistic knowledge. To build the classifier we used Support Vector Machines (SVM), which are known to produce good results on text classification tasks. Our proposals were applied to two different datasets written in the Portuguese language: articles from a Brazilian newspaper (Folha de Sao Paulo) and juridical documents from the Portuguese Attorney General's Office. The results show the relevance of part-of-speech information for the pre-processing phase of text classification allowing for a strong reduction of the number of features needed in the text classification.", "authors": ["Teresa Gon\u00e7alves", "Cassiana Fagundes da Silva", "Paulo Quaresma", "Renata Vieira"], "n_citation": 50, "title": "Analysing part-of-speech for portuguese text classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "09cb767d-558b-47ac-a220-3999242f6e31"}
{"abstract": "In this study, a first comprehensive text classification using n-gram model has been realized for Turkish. We worked in 3 different areas such as determining the identification of a Turkish document's author, classifying documents according to text's genre and identifying a gender of an author, automatically. Naive Bayes, Support Vector Machine, C 4.5 and Random Forest were used as classification methods and the results were given comparatively. The success in determining the author of the text, genre of the text and gender of the author was obtained as 83%, 93% and 96%, respectively.", "authors": ["M. Fatih Amasyali", "Banu Diri"], "n_citation": 50, "title": "Automatic turkish text categorization in terms of author, genre and gender", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0a5b3eb8-1db8-4348-9229-f1588876024d"}
{"abstract": "For artificial entities to achieve true autonomy and display complex life-like behaviour they will need to exploit appropriate adaptable learning algorithms. In this sense adaptability implies flexibility guided by the environment at any given time and an open-ended ability to learn appropriate behaviours. This paper examines the use of constructivism-inspired mechanisms within a neural learning classifier system architecture which exploits parameter self-adaptation as an approach to realise such behaviour. The system uses a rule structure in which each is represented by an artificial neural network. It is shown that appropriate internal rule complexity emerges during learning at a rate controlled by the learner and that the structure indicates underlying features of the task. Results are presented from using a mobile robot platform.", "authors": ["Jacob Hurst", "Larry Bull"], "n_citation": 0, "title": "A self-adaptive neural learning classifier system with constructivism for mobile robot control", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0a768ab6-34fb-467d-9e32-20b64a67ee73"}
{"abstract": "A circular-arc model (C, A) is a circle C together with a collection A of arcs of C. If A satisfies the Helly Property then (C, A) is a Helly circular-arc model. A (Helly) circular-arc graph is the intersection graph of a (Helly) circular-arc model. Circular-arc graphs and their subclasses have been the object of a great deal of attention, in the literature. Linear time recognition algorithm have been described both for the general class and for some of its subclasses. However, for Helly circular-arc graphs, the best recognition algorithm is that by Gavril, whose complexity is O(n 3 ). In this article, we describe different characterizations for Helly circular-arc graphs, including a characterization by forbidden induced subgraphs for the class. The characterizations lead to a linear time recognition algorithm for recognizing graphs of this class. The algorithm also produces certificates for a negative answer, by exhibiting a forbidden subgraph of it, within this same bound.", "authors": ["Min Chih Lin", "Jayme Luiz Szwarcfiter"], "n_citation": 0, "title": "Characterizations and Linear Time Recognition of Helly Circular-Arc Graphs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0daa80de-a5e3-4707-9644-9d02bc9ea988"}
{"abstract": "The use of the WWW as a communication medium for software engineers is limited by the lack of tools for writing, sharing, and verifying formal notations. For instance, the Z specification language has a a rich set of mathematical characters, and requires graphic-rich boxes and schemas for its specifications. It is difficult to integrate Z specifications and text on WWW pages written with the current versions of HTML, and traditional tools are not suited for the task. We present a Java-based tool for rendering Z specifications within HTML documents that can be shown on every WWW browser with Java capabilities. Being a complete rendering engine, text parts and Z specifications can be freely intermixed, and all the standard features of HTML (such as links, etc.) are available outside and inside Z specifications. Furthermore, the cxtensibility of our engine allows additional notations to be supported and integrated with current ones.", "authors": ["P. Ciancarini", "Cecilia Mascolo", "Fabio Vitali"], "n_citation": 50, "title": "Visualizing Z notation in HTML documents", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "0f1c0ff2-12a1-4f04-b4fe-0d8b5496f0ee"}
{"authors": ["Celso M. de Melo", "Jonathan Gratch"], "n_citation": 0, "title": "Evolving Expression of Emotions in Virtual Humans Using Lights and Pixels", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "0f7ac0c4-7df9-4ffc-8e4a-5ee2c70e6235"}
{"abstract": "With evolving modem IT technology, one desirable characteristic of distributed of applications is self-healing, or the ability to reconfigure themselves on the fly to circumvent failure. Thus, the goal is to avoid catastrophic failure through prompt execution of remedial actions. This paper proposes a self-healing system that monitors, diagnoses and heals its own internal problems using self-awareness as contextual information. The proposed system consists of multi agents that analyze the log context, error events and resource status in order to perform self-diagnosis and self-healing. For rapid and efficient self-healing, for developing the proposed system, we use a 6-step process: monitoring, filtering, translation, diagnosis, decision and feedback. Our experiments conducted with a prototype system confirm the effectiveness of the proposed system.", "authors": ["Jeongmin Park", "Giliong Yoo", "Chulho Jeong", "Eunseok Lee"], "n_citation": 50, "title": "Proactive Self-healing System for Application Maintenance in Ubiquitous Computing Environment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "106a79f1-3f76-43e8-96d0-18117970289b"}
{"abstract": "We present work on a tool environment for model-based testing with the Abstract State Machine Language (AsmL). Our environment supports semiautomatic parameter generation, call sequence generation and conformance testing. We outline the usage of the environment by an example, discuss its underlying technologies, and report on some applications conducted in the Microsoft environment.", "authors": ["Mike Barnett", "Wolfgang Grieskamp", "Lev Nachmanson", "Wolfram Schulte", "Nikolai Tillmann", "Margus Veanes"], "n_citation": 0, "title": "Towards a tool environment for model-based testing with AsmL", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "110bdbf3-8773-4053-a187-af8cf1399839"}
{"abstract": "Materialized views in data warehouses are typically complicated, making the maintenance of such views difficult. However, they are also very important for improving the speed of access to the information in the data warehouse. So, the selection of materialized views is crucial to the operation of the data warehouse both with respect to maintenance and speed of access. Most research to date has treated the selection of materialized views as an optimization problem with respect to the cost of view maintenance and/or with respect to the cost of queries. In this paper, we consider practical aspects of data warehousing. We identify problems with the star and snowflake schema and suggest solutions. We also identify practical problems that may arise during view selection and suggest heuristics based on data dependencies and access patterns that can be used to measure if one set of views is better than another set of views, or used to improve a set of views.", "authors": ["Gillian Dobbie", "Tok Wang Ling"], "n_citation": 50, "title": "Practical approach to selecting data warehouse views using data dependencies", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "15a04045-271a-40c3-afa5-09ae3ecd5cbd"}
{"abstract": "We investigate the coded model of fault-tolerant computations introduced by D. Spielman. In this model the input and the output of a computational circuit is treated as words in some error-correcting code. A circuit is said to compute some function correctly if for an input which is a encoded argument of the function, the output, been decoded, is the value of the function on the given argument. We consider two models of faults. In the first one we suppose that an elementary processor at each step can be corrupted with some small probability, and faults of different processors are independent. For this model, we prove that a parallel computation running on n elementary non-faulty processors in time t = poly(n) can be simulated on O(n log n/ log log n) faulty processors in time O(t log log n). Note that we get a sub-logarithmic blow up of the memory, which cannot be achieved in the classic model of faulty boolean circuit, where the input is not encoded. In the second model, we assume that at each step some fixed fraction of elementary processors can be corrupted by an adversary, who is free to chose these processors arbitrarily. We show that in this model any computation can be made reliable with an exponential blow up of the memory. Our method employs a sort of mixing mappings, which enjoy some properties of expanders. Based on mixing mappings, we implement an effective self-correcting procedure for an array of faulty processors.", "authors": ["Andrei E. Romashchenko"], "n_citation": 50, "title": "Reliable computations based on locally decodable codes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "17607d19-9eb2-4029-a387-9fd586bdd0ee"}
{"abstract": "A general linear iterative cryptanalysis method for solving binary systems of approximate linear equations which is also applicable to keystream generators producing short keystream sequences is proposed. A linear cryptanalysis method for reconstructing the secret key in a general type of initialization schemes is also developed. A large class of linear correlations in the Bluetooth combiner, unconditioned or conditioned on the output or on both the output and one input, are found and characterized. As a result, an attack on the Bluetooth stream cipher that can reconstruct the 128-bit secret key with complexity about 2 70  from about 45 initializations is proposed. In the precomputation stage, a database of about 2 80 103-bit words has to be sorted out.", "authors": ["Jovan Dj. Golic", "Vittorio Bagini", "Guglielmo Morgari"], "n_citation": 0, "title": "Linear cryptanalysis of Bluetooth stream cipher", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "17a51224-4bef-43a3-a2e0-a8f34d932930"}
{"abstract": "As demand for digital fulltext-image with fast development of super high speed communication network is increasing in digital contents industry, its scale is increasing exponentially every year. However, this fulltext-image is illegally reprinting and distributing widely, and this brings about the problems of its copyrights and proprietary rights. To solve this problems, many techniques such as DRM(Digital Right Management) or digital watermarking is receiving study. The digital fingerprinting is one method of right protection of new multimedia based on digital watermarking. Consequently, we design robust watermarking algorithm, which can resist a various attack, construct digital fulltext-image framework for copyright service, and apply them in our digital fulltext-image protection system.", "authors": ["Sangkuk Kim", "Heejun Yoon", "Hwa-Mook Yoon", "Won-Goo Lee"], "n_citation": 0, "title": "A Robust Digital Fingerprinting Mechanism for Digital Copyright Protection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1966bd47-2f06-45a6-aea4-60b93603d287"}
{"abstract": "In this paper we propose interactive visualization techniques which support the spatial comprehension of angiogram images by emphasizing depth information and introducing combined depth cues. In particular, we propose a depth based color encoding, two variations of edge enhancement and the application of a modified depth of field effect in order to enhance depth perception of complex blood vessel systems. All proposed techniques have been developed to improve the human depth perception and have been adapted with special consideration of the spatial comprehension of blood vessel structures. To evaluate the presented techniques, we have conducted a user study, in which users had to accomplish certain depth perception tasks.", "authors": ["Timo Ropinski", "Frank Steinicke", "Klaus H. Hinrichs"], "n_citation": 0, "title": "Visually supporting depth perception in angiography imaging", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1a7bb0ba-b2a6-4bc4-b1a7-56bd82415f59"}
{"abstract": "Reuse is as an important approach to conceptual object-oriented design. A number of reusable artifacts and methodologies to use these artifacts have been developed that require the designer to select a certain level of granularity and a certain paradigm. This makes retrieval and application of these artifacts difficult and prevents the simultaneous reuse of artifacts at different levels of granularity. A specific kind of artifact, analysis pattern, spans these levels of granularity. Patterns, which represent groups of objects, facilitate further assembly into what we call design fragments. Design fragments can then be used as reusable artifacts in their own right. A methodology for building a repository of design fragments is presented that consists of core and variant design fragments. The effectiveness of the methodology is assessed by verifying the appropriateness of the design fragments generated through a clustering process.", "authors": ["Taedong Han", "Sandeep Purao", "Veda C. Storey"], "n_citation": 50, "title": "A methodology for building a repository of object-oriented design fragments", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "1b4d3332-ec26-4457-96e9-2ee884deec53"}
{"abstract": "Understanding a large schema without the assistance of persons already familiar with it (and its associated applications), is a hard and very time consuming task that occurs very frequently in reverse engineering and in information integration. In this paper we describe a novel method that can aid the understanding and the visualization of very large ER diagrams that is inspired by the link analysis techniques that are used in Web Searching. Specifically, this method takes as input an ER diagram and returns a smaller (top-k) diagram that consists of the major entity and relationship types of the initial diagram. Concerning the drawing of the resulting top-k graphs in the 2D space, we propose a force-directed placement algorithm especially adapted for ER diagrams. Specifically, we describe and analyze experimentally two different force models and various configurations. The experimental evaluation on large diagrams of real world applications proved the effectiveness of this technique.", "authors": ["Yannis Tzitzikas", "Jean-Luc Hainaut"], "n_citation": 26, "title": "How to tame a very large ER diagram (using link analysis and force-directed drawing algorithms)", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1c5c4091-b065-4e31-9a1a-0b622a788617"}
{"abstract": "Data imbalance occurs when the number of patterns from a class is much larger than that from the other class. It often degenerates the classification performance. In this paper, we propose an Ensemble of Under-Sampled SVMs or BUS SVMs. We applied the proposed method to two synthetic and six real data sets and we found that it outperformed other methods, especially when the number of patterns belonging to the minority class is very small.", "authors": ["Pilsung Kang", "Sungzoon Cho"], "n_citation": 0, "title": "EUS SVMs: Ensemble of Under-Sampled SVMs for Data Imbalance Problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1cbfea45-ff7b-47ae-8559-c3483b76fa0f"}
{"abstract": "Bayesian networks (BN) constitute a useful tool to model the joint distribution of a set of random variables of interest. To deal with the problem of learning sensible BN models from data, we have previously considered various evolutionary algorithms for searching the space of BN structures directly. In this paper, we explore a simple evolutionary algorithm designed to search the space of BN equivalence classes. We discuss a number of issues arising in this evolutionary context and provide a first assessment of the new class of algorithms.", "authors": ["Jorge Muruz\u00e1bal", "Carlos Cotta"], "n_citation": 0, "title": "A primer on the evolution of equivalence classes of Bayesian-network structures", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1f5aad53-4309-497f-8a18-a97a49bf9f44"}
{"abstract": "Multi-source information systems, such as data warehouses, are composed of a set of heterogeneous and distributed data sources. The relevant information is extracted from these sources, cleaned, transformed and then integrated. The confrontation of two different data sources may reveal different kinds of heterogeneities: at the intensional level, the conflicts are related to the structure of the data. At the extensional level, the conflicts are related to the instances of the data. The process of detecting and solving the conflicts at the extensional level is known as data cleaning. In this paper, we will focus on the problem of differences in terminologies and we propose a solution based on linguistic knowledge provided by a domain ontology. This approach is well suited for application domains with intensive classification of data such as medicine or pharmacology. The main idea is to automatically generate some correspondence assertions between instances of objects. The user can parametrize this generation process by defining a level of accuracy expressed using the domain ontology.", "authors": ["Zoubida Kedad", "Elisabeth M\u00e9tais"], "n_citation": 50, "title": "Ontology-based data cleaning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "20784b1c-29ec-4962-9a2e-f3e6c4dee920"}
{"abstract": "Automatic ontology building is a vital issue in many fields where they are currently built manually. This paper presents a user-centred methodology for ontology construction based on the use of Machine Learning and Natural Language Processing. In our approach, the user selects a corpus of texts and sketches a preliminary ontology (or selects an existing one) for a domain with a preliminary vocabulary associated to the elements in the ontology (lexicalisations). Examples of sentences involving such lexicalisation (e.g. ISA relation) in the corpus are automatically retrieved by the system. Retrieved examples are validated by the user and used by an adaptive Information Extraction system to generate patterns that discover other lexicalisations of the same objects in the ontology, possibly identifying new concepts or relations. New instances are added to the existing ontology or used to tune it. This process is repeated until a satisfactory ontology is obtained. The methodology largely automates the ontology construction process and the output is an ontology with an associated trained learner to be used for further ontology modifications.", "authors": ["Christopher Brewster", "Fabio Ciravegna", "Yorick Wilks"], "n_citation": 112, "title": "User-centred ontology learning for knowledge management", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2107b774-47ac-403b-b5f9-9515f4bc23c5"}
{"abstract": "The authorization mechanisms in existing Grids are relatively static, so we present a dynamic authorization mechanism. Through the negotiation between users and resources, the Grid security management entities match the requests of the user and resource according to corresponding stipulations in security policies, form security contracts, and make authorization decisions. In order to reflect whether the security contracts have been well observed, we introduce the trust parameter as the feedback of our authorization mechanism.", "authors": ["Feng Li", "Junzhou Luo", "Yinying Yang", "Ye Zhu", "Teng Ma"], "n_citation": 0, "title": "A dynamic Grid authorization mechanism with result feedback", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2145d8b0-f7a8-4f27-a952-67ef3de5ce60"}
{"abstract": "Routing schemes often rely on local tables that associate with each destination node v a parent link on which to forward messages to v. The set of parent links form a directed tree to v. Such tables suffer from the fact that they do not provide a full representation of the entire network, and therefore are vulnerable to dynamic network changes and cannot tolerate faults. In this paper we propose a model based on a multi-parent representation which associates with each vertex a set of parent links. By choosing the multi-parents as the out-neighborhood for an acyclic orientation (acorn) of the underlying network, this representation acts as a multi-tree generator in the sense that a directed tree to r is obtained wben each vertex w chooses arbitrarily a single parent link. The capacity of an acorn is the largest integer k such that each vertex w outside the neighborhood of r has out-degree at least k. The depth of an acorn is the longest path in the acorn. An acorn with capacity k and depth d has the property that each vertex outside the neighborhood of r has at least k alternative parent choices, and for any combination of choices the depth of the resulting tree is at most d. We present a greedy algorithm that finds an acorn representation of optimal capacity. Further, we give an efficient algorithm for finding a minimum depth acorn of capacity k. We study the problem of increasing the acorn capacity by adding a server node and connections of this node to the existing network. We show that the problem of finding the minimum size set of server connections is NP-hard, and prove that any minimal set of connections has size at most k(V(\u03b4 + 1), where \u03b4 is the minimum degree. This bound is best possible over all graphs of minimum degree 6. Finally, we consider the problem of identifying collections of independent spanning trees, an important problem in network reliability. For an acorn representation with capacity k 1  we show that the parent links at each vertex w outside the neighborhood of r can be k-colored, so that any set of k monochromatic trees generated are independent. This result generalizes a recent result of Huck [5].", "authors": ["Fred S. Annexstein", "Kenneth A. Berman", "Ramjee P. Swaminathan"], "n_citation": 0, "title": "A multi-tree generating routing scheme using acyclic orientations", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "221fe0f7-00d6-453c-80bd-fbc70953da40"}
{"abstract": "Collaborative work requires, more than ever, access to data located on multiple autonomous and heterogeneous data sources. The development of these novel information platforms, referred to as information or data grids, and the evolving databases based on P2P concepts, need appropriate modeling and description mechanisms. In this paper we propose the Link Pattern Catalog as a modeling guideline for recurring problems appearing during the design or description of information grids and P2P networks. For this purpose we introduce the Data Link Modeling Language, a language for describing and modeling virtually any kind of data flows in information sharing environments.", "authors": ["Christopher Popfinger", "Cristian P\u00e9rez de Laborda", "Stefan Conrad"], "n_citation": 0, "title": "Link patterns for modeling information grids and P2P networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "227012dc-3d43-4d82-b5bd-6fdebeacfee6"}
{"abstract": "Online reinforcement learning achieves learning after update estimation value for (state, action) pairs selecting in present state before do state transition by next state. Therefore, online reinforcement learning needs polynomial search time to find most optimal value-function. But, a lots of reinforcement learning that are proposed for online reinforcement learning update estimation value for (state, action) pairs that agents select in present state, and because estimation value for unselected (state, action) pairs is evaluated in other episodes, perfect online reinforcement learning is not. Therefore, in this paper, we propose online ant reinforcement learning method using Ant-Q and eligibility trace to solve this problem. The eligibility trace is one of the basic mechanisms in reinforcement learning to handle delayed reward. The traces are said to indicate the degree to which each state is eligible for undergoing learning changes should a reinforcing event occur. Formally, there are two kinds of eligibility traces(accumulating trace or replacing traces). In this paper, we propose online ant reinforcement learning algorithms using an eligibility traces which is called replace-trace methods. This method is a hybrid of Ant-Q and eligibility traces. Although replacing traces are only slightly different from accumulating traces, it can produce a significant improvement in optimization. We could know through an experiment that proposed reinforcement learning method converges faster to optimal solution than Ant Colony System and Ant-Q.", "authors": ["Seunggwan Lee"], "n_citation": 0, "title": "A Cooperation Online Reinforcement Learning Approach in Ant-Q", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "22d6dc73-f9d4-4df2-afba-cd2a4bc5930d"}
{"abstract": "Software Engineering is still lacking methods which are capable of properly capturing the relevant System properties in the problem space and at the same time generating a correct representation (a final software product) in the solution space. This product must be functionally equivalent to the system specification and obtained in an automated way. The design of methods of this kind and the development of associated support tools is one of the current challenges in the field. This leads us to real CARE Computer-Aided Requirements Engineering environments, where the replacement of the S (of CASE) by the R is very meaningful: we want to focus on the Requirements Engineering process to capture properly the relevant system properties. Our CARE advanced tools will convert these patterns of behavior into the most convenient representation in a given software development environment, by properly compiling the system specification following a set of mappings between conceptual patterns and software representations. To do it, a formal basis and a model with the necessary expressiveness are needed in order to do this. The contribution of this paper is the object-oriented OASIS model as the basic model for such a CARE environment, together with a method for software production -the OO-Method- as an operational implementation of these ideas.", "authors": ["Oscar Pastor", "Jos\u00e9 H. Can\u00f3s", "Isidro Ramos"], "n_citation": 50, "title": "From CASE to CARE (computer-aided Requirements engineering)", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "238f2697-0748-4be5-8d50-b0b0bdd4c82f"}
{"authors": ["Edgar T. Irons"], "n_citation": 1, "title": "A Syntax Directed Compiler for ALGOL 60 (Reprint).", "venue": "Communications of The ACM", "year": 1983, "id": "2821fb5b-72e4-403f-b4dd-923b8c66963b"}
{"abstract": "The automatic synthesis of programs from their specifications has been a dream of many researchers for decades. If we restrict to open finite-state reactive systems, the specification is often presented as an ATL or LTL formula interpreted over a finite-state game. The required program is then a strategy for winning this game. A theoretically optimal solution to this problem was proposed by Pnueli and Rosner, but has never given good results in practice. This is due to the 2EXPTIME-complete complexity of the problem, and the intricate nature of Pnueli and Rosner's solution. A key difficulty in their procedure is the determinisation of Biichi automata. In this paper we look at an alternative approach which avoids determinisation, using instead a procedure that is amenable to symbolic methods. Using an implementation based on the BDD package CuDD, we demonstrate its scalability in a number of examples. Furthermore, we show a class of problems for which our algorithm is singly exponential. Our solution, however, is not complete; we prove a condition which guarantees completeness and argue by empirical evidence that examples for which it is not complete are rare enough to make our solution a useful tool.", "authors": ["Aidan Harding", "Mark Ryan", "Pierre-Yves Schobbens"], "n_citation": 0, "title": "A new algorithm for strategy synthesis in LTL games", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2903812b-19bb-41e2-aa2c-fec35e4b90a5"}
{"abstract": "Using a single confidence threshold will result in a dilemmatic situation when simultaneously studying positive and negative association rule (PNAR), i.e., the forms A=>B, A=>\u00acB, \u00acA\u21d2B and \u00acA\u21d2\u00acB. A method based on four confidence thresholds for the four forms of PNARs is proposed. The relationships among the four confidences, which show the necessity of using multiple confidence thresholds, are also discussed. In addition, the chi-squared test can avoid generating misleading rules that maybe occur when simultaneously studying the PNARs. The method of how to apply chi-squared test in mining association rules is discussed. An algorithm PNARMC based on the chi-squared test and the four confidence thresholds is proposed. The experimental results demonstrate that the algorithm can not only generate PNARs rightly, but can also control the total number of rules flexibly.", "authors": ["Xiangjun Dong", "Fengrong Sun", "Xiqing Han", "Ruilian Hou"], "n_citation": 0, "title": "Study of Positive and Negative Association Rules Based on Multi-confidence and Chi-Squared Test", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2a3c8ea6-ad15-4631-80df-e69edbd8c79d"}
{"abstract": "This paper extends Reynolds' flocking algorithm to incorporate the effects of emotions, in this case fear. Sheep are used as a typical example of flocking mammals and an autonomous agent architecture with an action selection mechanism incorporating the effects of emotion is linked to the standard flocking rules. Smell is used to transmit emotion between animals, through pheromones modelled as particles in a free expansion gas. Preliminary results show that more realistic flocking behaviour is produced.", "authors": ["Carlos Delgado-Mata", "Jes\u00fas Ibanez", "Ruth Aylett"], "n_citation": 0, "title": "Let's run for it!: Conspecific emotional flocking triggered via virtual pheromones", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2a6ed314-d1fc-495a-a858-a25deb3c0d2d"}
{"abstract": "Ground decision procedures for combinations of theories are used in many systems for automated deduction. There are two basic paradigms for combining decision procedures. The Nelson-Oppen method combines decision procedures for disjoint theories by exchanging equality information on the shared variables. In Shostak's method, the combination of the theory of pure equality with canonizable and solvable theories is decided through an extension of congruence closure that yields a canonizer for the combined theory. Shostak's original presentation, and others that followed it, contained serious errors which were corrected for the basic procedure by the present authors. Shostak also claimed that it was possible to combine canonizers and solvers for disjoint theories. This claim is easily verifiable for canonizers, but is unsubstantiated for the case of solvers. We show how our earlier procedure can be extended to combine multiple disjoint canonizable, solvable theories within the Shostak framework.", "authors": ["Natarajan Shankar", "Harald Ruess"], "n_citation": 0, "title": "Combining Shostak theories", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2aed18b0-a105-4194-be82-b1ef7b27ac29"}
{"abstract": "An exportable and robust system using only camera images#R##N#is proposed for path execution in robot navigation. Motion information#R##N#is extracted in the form of optical flow from SURF robust descriptors of#R##N#consecutive frames, so the method is called SURF flow. This information#R##N#is used to correct robot displacement when a straight forward path#R##N#command is sent to the robot, but it is not really executed due to several#R##N#robot and environmental concerns. The proposed system has been#R##N#successfully tested on the legged robot Aibo.", "authors": ["Xavier Perez-Sala", "Cecilio Angulo Bah\u00f3n", "Sergio Escalera"], "n_citation": 0, "title": "Biologically inspired path execution using SURF flow in robot navigation", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "2b7e6fb5-96b1-4f9c-bd15-dceb2c4de0b4"}
{"abstract": "Many experimental results suggest that more precise spike timing is significant in neural information processing. From this point of view, we construct a self-organization model using the spatiotemporal patterns, where Spike-Timing Dependent. Plasticity (STDP) tunes the conduction delays between neurons. STDP forms more smoothed map with the spatially random and dispersed patterns, whereas it causes spatially distributed clustering patterns from spatially continuous and synchronous inputs. These results suggest that STDP forms highly synchronous cell assemblies changing through external stimuli to solve a binding problem.", "authors": ["Toshio Akimitsu", "Akira Hirose", "Yoichi Okabe"], "n_citation": 0, "title": "Self-organization Through Spike-Timing Dependent Plasticity Using Localized Synfire-Chain Patterns", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2c4eb906-e59c-45a4-ab77-2e24c1d9fe2e"}
{"abstract": "This paper presents a smart algorithm for labeling column charts and their derivatives. To efficiently solve the problem, we separate it into two sub-problems. We first present a geometric algorithm to solve the problem of finding a good labeling for the labels of a single column, given that some other columns have already been labeled. We then present a strategy for finding a good order in which columns should be labeled, which repeatedly uses the first algorithm for some limited look-ahead. The presented algorithm is being used in a commercial product to label charts, and has shown in practice to produce satisfactory results.", "authors": ["Sebastian M\u00fcller", "Arno Sch\u00f6dl"], "n_citation": 0, "title": "A smart algorithm for column chart labeling", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2e76cfec-9fa2-4ca7-ad14-11acb213518f"}
{"abstract": "In this paper we introduce the class of semiprimitive Fermat curves, for which Weil-Serre's bound can be improved using Moreno-Moreno p-adic techniques. The basis of the improvement is a technique for giving the exact divisibility for Fermat curves, by reducing the problem to a simple finite computation.", "authors": ["Francis N. Castro", "Ernesto Gomez", "Oscar Moreno"], "n_citation": 0, "title": "A Class of Fermat Curves for which Weil-Serre's Bound Can Be Improved", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "32acf831-736a-4e70-8efb-a3258bdd4df9"}
{"abstract": "We report progress on adding affect-detection to a program for virtual dramatic improvisation, monitored by a human director. To aid the director, we have partially implemented emotion detection within users' text input. The affect-detection module has been used to help develop an automated virtual actor. The work involves basic research into how affect is conveyed through metaphor and contributes to the conference themes such as building improvisational intelligent virtual agents for interactive narrative environments.", "authors": ["Li Zhang", "John A. Barnden", "Robert J. Hendley", "Alan M. Wallington"], "n_citation": 0, "title": "Exploitation in affect detection in improvisational e-drama", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3802ab94-7b6c-437a-a28c-594543f08888"}
{"abstract": "Because the eighteen online university digital museums of China confront a problem that the multi-discipline resources at these digital museums are isolated and dispersed without sufficient interconnection, ChinaGrid (China Education & Research Grid) supports a project named UDMGrid (University Digital Museum Grid), which studies a grid application for University Digital Museums using grid technology, especially information grid technology. According to the analysis on the problem, UDMGrid focuses on the resource sharing, information management and information service about those university digital museums. This paper presents research work on UDMGrid's framework, metadata, replica management, application server, etc.", "authors": ["Xiaowu Chen", "Zhi Xu", "Zhangsheng Pan", "Xixi Luo", "Hongchang Lin", "Yingchun Huang", "Haifeng Ou"], "n_citation": 0, "title": "UDMGrid: A grid application for University digital museums", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "38260a1c-576b-4399-af10-ee18de3e3e07"}
{"authors": ["Cl\u00e1udio Silva", "Ricardo Rocha", "Ricardo Lopes"], "n_citation": 50, "title": "An external module for implementing linear tabling in prolog", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "38640d10-5062-402f-bf52-56c257aa3c3a"}
{"abstract": "In this paper, we propose a new 3-D model retrieval system using the Aspect-Transition Descriptor which is based on the aspect graph representation [1,2] approach. The proposed method differs from the conventional aspect graph representation in that we utilize transitions as well as aspects. The process of generating the Aspect-Transition Descriptor is as follows: First, uniformly sampled views of a 3-D model are separated into a stable and an unstable view sets according to the local variation of their 2-D shape. Next, adjacent stable views and unstable views are grouped into clusters and we select the characteristic aspects and transitions by finding the representative view from each cluster. The 2-D descriptors of the selected characteristic aspects and transitions are concatenated to form the 3-D descriptor. Matching the Aspect-Transition Descriptors is done using a modified Hausdorff distance. To evaluate the proposed 3-D descriptor, we have evaluated the retrieval performance on the Princeton benchmark database [3] and found that our method outperforms other retrieval techniques.", "authors": ["Soochahn Lee", "Sehyuk Yoon", "Il Dong Yun", "Duck Hoon Kim", "Kyoung Mu Lee", "Sang Uk Lee"], "n_citation": 0, "title": "A New 3-D Model Retrieval System Based on Aspect-Transition Descriptor", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3a6b87cb-b0aa-4528-8c7d-51820e27d222"}
{"abstract": "Since the early ages of artificial intelligence, associative or semantic networks have been proposed as representations that enable the storage of language units and the relationships that interconnect them, allowing for a variety of inference and reasoning processes, and simulating some of the functionalities of the human mind. The symbolic structures that emerge from these representations correspond naturally to graphs - relational structures capable of encoding the meaning and structure of a cohesive text, following closely the associative or semantic memory representations. The activation or ranking of nodes in such graph structures mimics to some extent the functioning of human memory, and can be turned into a rich source of knowledge useful for several language processing applications. In this paper, we suggest a framework for the application of graph-based ranking algorithms to natural language processing, and illustrate the application of this framework to two traditionally difficult text processing tasks: word sense disambiguation and text summarization.", "authors": ["Rada Mihalcea"], "n_citation": 0, "title": "Random walks on text structures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3a848b90-56cb-42e9-add9-dfd1b61f6a73"}
{"abstract": "In the past, most conceptual schemas of information systems have been developed essentially from scratch. Currently, however, several research projects are considering an emerging approach that tries to reuse as much as possible the knowledge included in existing ontologies. Using this approach, conceptual schemas would be developed as refinements of (more general) ontologies. However, when the refined ontology is large, a new problem that arises using this approach is the need of pruning the concepts in that ontology that are superfluous in the final conceptual schema. This paper proposes a new method for pruning ontologies in this approach. We show the advantages of our method with respect to similar pruning methods developed in other contexts. Our method is general and it can be adapted to most conceptual modeling languages. We give the complete details of its adaptation to the UML. On the other hand, the method is fully automatic. The method has been implemented. We illustrate the method by means of its application to a case study that refines the Cyc ontology.", "authors": ["Jordi Conesa", "Antoni Oliv\u00e9"], "n_citation": 0, "title": "Pruning ontologies in the development of conceptual schemas of information systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3ceb5020-e265-4bc7-9b4e-b28d04e27b3f"}
{"abstract": "The web with its rapid expansion has become an excellent resource for gathering information and people's opinion. A company owner wants to know who is the competitor, and a customer also wants to know which company provides similar product or service to what he/she is in want of. This paper proposes an approach based on mutual information, which focuses on mining competitors of the entity(such as company, product, person ) from the web. The proposed techniques first extract a set of candidates of the input entity, and then rank them according to the comparability, and finally find and organize the reviews related to both original entity and its competitors. A novel system called CoDis based upon these techniques is implemented, which is able to automate the tedious process in a domain-independent and web-scale dynamical manner. In the experiment we use 32 different entities distributed in varied domains as inputs and the CoDis discovers 143 competitors. The experimental results show that the proposed techniques are highly effective.", "authors": ["Rui Li", "Shenghua Bao", "Jin Wang", "Yuanjie Liu", "Yong Yu"], "n_citation": 50, "title": "Web Scale Competitor Discovery Using Mutual Information", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3f87ead5-4ed2-4f0a-964e-6ad6e401a74f"}
{"abstract": "We propose a requirements elicitation process for a data warehouse (DW) that identifies its information contents. These contents support the set of decisions that can be made. Thus, if the information needed to take every decision is elicited, then the total information determines DW contents. We propose an Informational Scenario as the means to elicit information for a decision. An informational scenario is written for each decision and is a sequence of pairs of the form  . A query requests for information necessary to take a decision and the response is the information itself. The set of responses for all decisions identifies DW contents. We show that informational scenarios are merely another sub class of the class of scenarios.", "authors": ["Naveen Prakash", "Yogesh Singh", "Anjana Gosain"], "n_citation": 0, "title": "Informational scenarios for data warehouse requirements elicitation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3fa4fa5a-b0f2-4a9a-a6ea-cc76013d0a9b"}
{"abstract": "Operating systems (OS) are increasingly geared towards support of diverse peripheral components, both hardware (HW) and software (SW), rather than explicitly focused on increased reliability of delivered OS services. The interface between the OS and the HW devices is provided by device drivers. Furthermore, drivers have become add-on COTS components to support the OS's capabilities of widespread device support. Unfortunately, drivers constitute a major cause of system outages, impacting overall service reliability. Consequently, the testing of drivers becomes important. However, despite the efforts to develop appropriate testing methods, the multitude of possible system configurations and lack of detailed OS specifications makes the task difficult. Not requiring access to OS source code, this paper develops novel, non-intrusive support for test methods, based on ascertaining test progress from a driver's operational state model. This approach complements existing schemes, enhancing the level of accuracy of the test process by providing test location guidance.", "authors": ["Constantin Sarbu", "Andr\u00e9as Johansson", "Falk Fraikin", "Neeraj Suri"], "n_citation": 0, "title": "Improving Robustness Testing of COTS OS Extensions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3fdf0569-9483-4ebf-a961-563b26909fb7"}
{"abstract": "We present the Mobile Resource Guarantees framework: a system for ensuring that downloaded programs are free from run-time violations of resource bounds. Certificates are attached to code in the form of efficiently checkable proofs of resource bounds; in contrast to cryptographic certificates of code origin, these are independent of trust networks. A novel programming language with resource constraints encoded in function types is used to streamline the generation of proofs of resource usage.", "authors": ["David Aspinall", "Stephen Gilmore", "Martin Hofmann", "Donald Sannella", "Ian Stark"], "n_citation": 0, "title": "Mobile Resource Guarantees for smart devices", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3fe88d82-ad35-45ab-b030-a724be6ec0bc"}
{"abstract": "We propose an efficient approach to mine frequent Itemsets on data streams. It is a memory efficient and accurate one-pass algorithm that can deal with batch updates. The proposed algorithm performers well by dividing all frequent itemsets into frequent equivalence classes and pruning all redundant itemsets except for those that represent GLB (Greatest Lower Bound) and LUB (Least Upper Bound) of the frequent equivalence classes. The number of GLB and LUB is much less than the number of frequent itemsets. The experimental evaluation on synthetic and real datasets shows that the algorithm is very accurate and requires significantly lower memory than other well-known one-pass algorithms.", "authors": ["Zhi-jun Xie", "Hong Chen", "Cuiping Li"], "n_citation": 0, "title": "MFIS-Mining Frequent Itemsets on Data Streams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "403db306-2891-4323-bf6a-5b6dca8ffa82"}
{"abstract": "In this paper, a supervised learning method of word sense disambiguation based on maximum entropy conditional probability models is presented. This system acquires the linguistic knowledge from an annotated corpus and this knowledge is represented in the form of features. Several types of features has been analyzed for a few words selected from the DSO corpus. The main contribution of this paper consists of the selection of the best sets of features for each word from the training data in order to build the classifiers. Our experimentation shows that our method reaches a good accuracy when it is compared with, for example, the systems at SENSEVAL-2.", "authors": ["Armando Su\u00e1rez", "Manuel Palomar"], "n_citation": 1, "title": "Best feature selection for maximum entropy-based word sense disambiguation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "436eb8b2-97cd-4b52-8c73-d07caa802b4c"}
{"abstract": "Reasoning on constraint sets is a difficult task. Classical database design is based on a step-wise extension of the constraint set and on a consideration of constraint sets through generation by tools. Since the database developer must master semantics acquisition, tools and approaches are still sought that support reasoning on sets of constraints. We propose novel approaches for presentation of sets of functional dependencies based on specific graphs. These approaches may be used for the elicitation of the full knowledge on validity of functional dependencies in relational schemata.", "authors": ["J\u00e1nos Demetrovics", "Andr\u00e1s Moln\u00e1r", "Bernhard Thalheim"], "n_citation": 0, "title": "Graphical reasoning for sets of functional dependencies", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4ab96101-f89f-4aa8-838d-427aa867d1cf"}
{"abstract": "Achieving secure communications in networks has been one of the most important problems in information technology. Dolev, Dwork, Waarts, and Yung have studied secure message transmission in one-way or two-way channels. They only consider the case when all channels are two-way or all channels are one-way. Goldreich, Goldwasser, and Linial, Franklin and Yung, Franklin and Wright, and Wang and Desmedt have studied secure communication and secure computation in multi-recipient (multicast) models. In a multicast channel (such as Ethernet), one processor can send the same message - simultaneously and privately - to a fixed subset of processors. In this paper, we shall study necessary and sufficient conditions for achieving secure communications against active adversaries in mixed one-way and two-way channels. We also discuss multicast channels and neighbor network channels.", "authors": ["Yvo Desmedt", "Yongge Wang"], "n_citation": 0, "title": "Perfectly secure message transmission revisited", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4ac3aaac-a4a6-4cfb-84a4-2986a2fbc504"}
{"abstract": "Web communities involve networks of loosely coupled data sources. Members in those communities should be able to pose queries and gather results from all data sources in the network, where available. At the same time, data sources should have limited restrictions on how to organize their data. If a global schema is not available for such a network, query processing is strongly based on the existence of (hard to maintain) mapping rules between pairs of data sources. If a global schema is available, local schemas of data sources have to follow strict modelling restrictions posed by that schema. In this paper, we suggest an architecture to provide better support for distributed data management in loosely coupled data sources. In our approach, data sources can maintain diverse schemas. No explicit mapping rules between data sources are needed to facilitate query processing. Data sources can join and leave the network any time, at no cost for the community. We demonstrate our approach, describing SDQNET, a prototype platform to support semantic query processing in loosely coupled data sources.", "authors": ["Eirini Spyropoulou", "Theodore Dalamagas"], "n_citation": 0, "title": "SDQNET : Semantic distributed querying in loosely coupled data sources", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4e247df5-0f00-4f92-8d7f-e32ad5467a81"}
{"abstract": "Quality is currently one of the main research topics in conceptual modeling. In this paper nine new research contributions are organized using a classification framework that is based on the well-known framework for conceptual modeling quality of Lindland, Sindre, and Solvberg. The aim of this work is to identify new directions in conceptual modeling quality research.", "authors": ["Geert Poels", "J. K. Nelson", "Marcela Genero", "Mario Piattini"], "n_citation": 0, "title": "Quality in conceptual modeling -new research directions", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "4ed0ea88-aca7-4425-ac4a-d198feb43b5a"}
{"abstract": "We consider k-tape 1-way alternating finite automata (k-tape lafa). We say that an alternating automaton accepts a language L \u2282 (\u03a3 * ) k  with f(n)-bounded maximal (respectively, minimal) leaf-size if arbitrary (respectively, at least one) accepting tree for any (w 1 , w 2 ,...,w k ) \u2208 L has no more than f( max |w i |) leaves. The main results of the paper are the following. If k-tape 1afa accepts language L over one-letter alphabet with o(log n)-bounded maximal leaf-size or o(log log n)-bounded minimal leaf-size then the language L is semilinear. Moreover, if a language L is accepted with o(loglog(n))-bounded minimal (respectively, o(log(n))-bounded maximal) leaf-size then it is accepted by constant-bounded minimal (respectively, maximal) leaf-size by the same automaton. To show that this bound is optimal we prove that 4-tape 1afa can accept a non-semilinear languages over one-letter alphabet with O(log log n)-bounded minimal leaf-size. For maximal leaf-size our bound is optimal due to King's results.", "authors": ["Dainis Geidmanis", "Janis Kaneps", "Kalvis Apsitis", "Daina Taimina", "Elena Calude"], "n_citation": 0, "title": "Tally languages accepted by alternating multitape finite automata", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "4f402d3f-b857-43fd-bdb5-0f2b4a794d4c"}
{"abstract": "This paper presents a method for cross-language question answering. The method combines multiple query translations in order to improve the answering precision. The combination of translations is based on their pertinence to the target document collection rather than on their grammatical correctness. The pertinence is measured by the translation perplexity with respect to the collection language model. Experimental evaluation on question answering demonstrates that the proposed approach outperforms the results obtained by the best translation machine.", "authors": ["Rita M. Aceves-P\u00e9rez", "Luis Villase\u00f1or-Pineda", "Manuel Montes-y-G\u00f3mez"], "n_citation": 9, "title": "Using N-gram models to combine query translations in cross-language question answering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f8f65a9-a630-4cd3-b234-a527533ae459"}
{"abstract": "We present an approach for camera calibration from the image of at least two circles arranged in a coaxial way. Such a geometric configuration arises in static scenes of objects with rotational symmetry or in scenes including generic objects undergoing rotational motion around a fixed axis. The approach is based on the automatic localization of a surface of revolution (SOR) in the image, and its use as a calibration artifact. The SOR can either be a real object in a static scene, or a virtual surface obtained by frame superposition in a rotational sequence. This provides a unified framework for calibration from single images of SORs or from turntable sequences. Both the internal and external calibration parameters (square pixels model) are obtained from two or more imaged cross sections of the SOR, whose apparent contour is also exploited to obtain a better calibration accuracy. Experimental results show that this calibration approach is accurate enough for several vision applications, encompassing 3D realistic model acquisition from single images, and desktop 3D object scanning.", "authors": ["Carlo Colombo", "Dario Comanducci", "Alberto Del Bimbo"], "n_citation": 50, "title": "Camera Calibration with Two Arbitrary Coaxial Circles", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f8fe2da-4a6c-403f-8cbb-c5764df65c74"}
{"abstract": "Generalized cylinder (GC) has played an important role in computer vision since it was introduced in the 1970s. While studying GC models in human visual perception of shapes from contours, Marr assumed that GC's limbs are planar curves. Later, Koenderink and Ponce pointed out that this assumption does not hold in general by giving some examples. In this paper, we show that straight homogeneous generalized cylinders (SHGCs) and tori (a kind of curved GCs) have planar limbs when viewed from points on specific straight lines. This property leads us to the definition and investigation of a new class of GCs, with the help of the surface model proposed by Degen for geometric modeling. We call them Degen generalized cylinders (DGCs), which include SHGCs, tori, quadrics, cyclides, and more other GCs into one model. Our rigorous discussion is based on projective geometry and homogeneous coordinates. We present some invariant properties of DGCs that reveal the relations among the planar limbs, axes, and contours of DGCs. These properties are useful for recovering DGC descriptions from image contours as well as for some other tasks in computer vision.", "authors": ["Liangliang Cao", "Jianzhuang Liu", "Xiaoou Tang"], "n_citation": 0, "title": "Degen Generalized Cylinders and Their Properties", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "50c066b4-30df-4014-be40-0012488122bc"}
{"abstract": "We show how a fuzzy answer set program can be compiled to an equivalent fuzzy propositional theory whose models correspond to the answer sets of the program. This creates a basis for constructing fuzzy answer set solvers, such as solvers based on fuzzy SAT-solvers or on linear programming.", "authors": ["Jeroen Janssen", "Stijn Heymans", "Dirk Vermeir", "Martine De Cock"], "n_citation": 0, "title": "Compiling fuzzy answer set programs to fuzzy propositional theories", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "51853646-c68d-484e-839a-54dfed045ef1"}
{"abstract": "U-shaped learning behaviour in cognitive development involves learning, unlearning and relearning. It occurs, for example, in learning irregular verbs. The prior cognitive science literature is occupied with how humans do it, for example, general rules versus tables of exceptions. This paper is mostly concerned with whether U-shaped learning behaviour may be necessary in the abstract mathematical setting of inductive inference, that is, in the computational learning theory following the framework of Gold. All notions considered are learning from text, that is, from positive data. Previous work showed that U-shaped learning behaviour is necessary for behaviourally correct learning but not for syntactically convergent, learning in the limit (= explanatory learning). The present paper establishes the necessity for the whole hierarchy of classes of vacillatory learning where a behaviourally correct learner has to satisfy the additional constraint that it vacillates in the limit between at most k grammars, where k > 1. Non U-shaped vacillatory learning is shown to be restrictive: Every non U-shaped vacillatorily learnable class is already learnable in the limit. Furthermore, if vacillatory learning with the parameter k = 2 is possible then non U-shaped behaviourally correct learning is also possible. But for k = 3, surprisingly, there is a class witnessing that this implication fails.", "authors": ["Lorenzo Carlucci", "John Case", "Sanjay Jain", "Frank Stephan"], "n_citation": 0, "title": "Non U-shaped vacillatory and team learning", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "53c476f6-7f3f-42a6-b405-e17bd1f21d7c"}
{"abstract": "In this experience paper we present a case study in using logic programming in a pervasive computing project in the healthcare domain. An expert system is used to detect healthcare activities in a pervasive hospital environment where positions of people and things are tracked. Based on detected activities an activity-driven computing infrastructure provides computational assistance to healthcare staff on mobile-and pervasive computing equipment. Assistance range from simple activities like fast log-in into the electronic patient medical record system to complex activities like signing for medicine given to specific patients. We describe the role of logic programming in the infrastructure and discuss the benefits and problems of using logic programming in a pervasive context.", "authors": ["Henrik B\u00e6rbak Christensen"], "n_citation": 55, "title": "Using logic programming to detect activities in pervasive healthcare", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "54f3ee1b-496c-4d94-b2f8-3d243b0e75f6"}
{"abstract": "Multicast is an effective means for conducting the cooperative P2P communications. This paper studies an algorithm to construct a scalable and efficient end host multicast tree - Shared Cluster Tree (SCT). Compared with the previous work, by utilizing the underlying network properties, the novelty and contributions of our multicast algorithms are: (1) Scalability: by fully utilizing the underlying network properties to layer and cluster the group members, SCT decreases the transfer of the data traffic in the backbone domain links and therefore reserves more backbone domain resources to more local domains which include many more end hosts; (2) Efficiency: by searching the cluster cores based on the cluster core selection method, packets are multicasted to the cluster members by the cluster cores in the minimum delay. Our simulation results indicate that SCT is scalable and efficient for the end host multicast in P2P systems...", "authors": ["Wanqing Tu", "Weijia Jia"], "n_citation": 0, "title": "End host multicast for peer-to-peer systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5600366e-359b-4d66-8cf1-29bd72ded77f"}
{"abstract": "We investigate a new class of metrics to find good variable orders for decision diagrams in symbolic state-space generation. Most of the previous work on static ordering is centered around the concept of minimum variable span, which can also be found in the literature under several other names. We use a similar concept, but applied to event span, and generalize it to a family of metrics parameterized by a moment, where the metric of moment 0 is the combined event span. Finding a good variable order is then reduced to optimizing one of these metrics, and we design extensive experiments to evaluate them. First, we investigate how the actual optimal order performs in state-space generation, when it can be computed by evaluating all possible permutations. Then, we study the performance of these metrics on selected models and compare their impact on two different state-space generation algorithms: classic breadth-first and our own saturation strategy. We conclude that the new metric of moment 1 is the best choice. In particular, the saturation algorithm seems to benefit the most from using it, as it achieves the better performance in nearly 80% of the cases.", "authors": ["Radu I. Siminicean", "Gianfranco Ciardo"], "n_citation": 0, "title": "New metrics for static variable ordering in decision diagrams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "56038093-45f1-4c10-8ddb-037e1172bc7b"}
{"abstract": "In this paper we introduce Associative Commutative Distributive Term Rewriting (ACDTR), a rewriting language for rewriting logical formulae. ACDTR extends AC term rewriting by adding distribution of conjunction over other operators. Conjunction is vital for expressive term rewriting systems since it allows us to require that multiple conditions hold for a term rewriting rule to be used. ACDTR uses the notion of a conjunctive context, which is the conjunction of constraints that must hold in the context of a term, to enable the programmer to write very expressive and targeted rewriting rules. ACDTR can be seen as a general logic programming language that extends Constraint Handling Rules and AC term rewriting. In this paper we define the semantics of ACDTR and describe our prototype implementation.", "authors": ["Gregory J. Duck", "Peter J. Stuckey", "Sebastian Brand"], "n_citation": 0, "title": "ACD term rewriting", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "569784a9-41e3-4c7c-89c8-9cb8f3639a99"}
{"abstract": "Micro-payment systems have the potential to provide non-intrusive, high-volume and low-cost pay-as-you-use services for a wide variety of web-based applications. We propose an extension, P2P-NetPay, a micro-payment protocol characterized by off-line processing, suitable for peer-to-peer network services sharing. Our approach provides high performance and security using one-way hashing functions for e-coin encryption. In our P2P-NetPay protocol, each peer's transaction does not involve any broker and double spending is detected during the redeeming transaction. We describe the motivation for P2P-NetPay and describe three transactions of the P2P-NetPay protocol in detail to illustrate the approach. We then discuss future research on this protocol.", "authors": ["Xiaoling Dai", "John C. Grundy"], "n_citation": 0, "title": "Off-line micro-payment system for content sharing in P2P networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "586fd7d4-33b3-45ba-9937-5476e3fbb6e6"}
{"abstract": "A Reactive Virtual Trainer (RVT) is an Intelligent Virtual Agent (IVA) capable of presenting physical exercises that are to be performed by a human, monitoring the user and providing feedback at different levels. Depending on the motivation and the application context, the exercises may be general ones of fitness to improve the user's physical condition, special exercises to be performed from time to time during work to prevent for example RSI, or physiotherapy exercises with medical indications. In the paper we discuss the functional and technical requirements of a framework which can be used to author specific RVT applications. The focus is on the reactivity of the RVT, manifested in natural language comments on readjusting the tempo, pointing out mistakes or rescheduling the exercises. We outline the components we have implemented so far: our animation engine, the composition of exercises from basic motions and the module for analysis of tempo in acoustic input.", "authors": ["Zs\u00f3fia Ruttkay", "Job Zwiers", "Herwin van Welbergen", "Dennis Reidsma"], "n_citation": 0, "title": "Towards a reactive virtual trainer", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5af475ff-679e-4ca8-afca-4ba93bdd100e"}
{"authors": ["Max Kemman", "Stef Scagliola", "F.M.G. de Jong", "Roeland Ordelman"], "n_citation": 0, "title": "Talking with Scholars: Developing a Research Environment for Oral History Collections.", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "5b8109d0-6dc5-4aef-9b0a-2278d7420486"}
{"abstract": "We suggest authentication scheme of resource group and protection scheme of RSL or resource information in Grid environment based on web services. At present, resource group composed of job request in Grid middleware has no scheme of itself to be authenticated what it is a set of legal resources. Because of using SSL, it also breaks out heavy overload on each process as the aspects of protection for RSL or resource information towards and away between each resource groups and users. In particular, implementation of Grid service based on web services works out no plan on an above essential point. This paper provides the efficient protection scheme of RSL or resource information transmitted through network and authentication scheme of resource group in web-service based Grid environment with relating XML-Signature and resource management system.", "authors": ["Seoung-Hyeon Lee", "Byung-Sun Choi", "Jae-Seung Lee", "Kiyoung Moon", "Jae-Kwang Lee"], "n_citation": 50, "title": "VO Authentication Framework in Grid Environment Using Digital Signature", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5dd645c7-ff0b-4f58-aaca-16fb1e796df5"}
{"abstract": "3D video mosaics created by 3D image mosaicking, our previous research, are for 3D visualization of the roadside standing building scene captured by a side-looking video camera as a continuous set of vertical-textured planar faces. These vertical faces were concatenated to create an approximate model on which the images could be back-projected as textures. We proposed a retrieval technique of selecting vertical-textured planar faces from archived 3D image mosaics to fast represent a 3D world, where this selection is based on proximity to the user location in the virtual world. The chosen vertical-textured planar faces are recomposed as a continuous set formed stream and are then rendered in 3D virtual space.", "authors": ["Jae-Choon Chon", "Eihan Shimizu", "Ryosuke Shibasaki"], "n_citation": 50, "title": "Retrieval of 3D video mosaics for fast 3D visualization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5e111441-7fc0-4ff5-97c4-8d69a3f65e1f"}
{"abstract": "By using the saturation linearity of the output functions of neurons in cellular neural networks, and by adopting the method of decomposing the state space to sub-regions, the mathematical equations of delayed cellular neural networks are rewritten to be the form of linear differential difference equations in the neighbourhood of each equilibrium, which is an interior point of some sub-region. Based on this linear form and by using the stability theory of linear differential difference equations and the tool of M-matrix, delay-dependent and delay-independent stability algebraic criteria are obtained. All results obtained in this paper need only to compute the eigenvalues of some matrices or to examine the matrices to be M-matrix or to verify some inequalities to be holden.", "authors": ["Wudai Liao", "Dongyun Wang", "Yulin Xu", "Xiaoxin Liao"], "n_citation": 0, "title": "Delay-Dependent and Delay-Independent Stability Conditions of Delayed Cellular Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5ff42698-359c-4afb-9945-56d957310646"}
{"abstract": "This paper discusses new challenges and possible approaches for developing and evolving mobile information systems, with focus on model-based approaches on the conceptual and logical level. We have experienced these new challenges through several research and industrial projects on mobile solutions, usability and model-based approaches over the last years. We summarize the main challenges on how model-based approaches can support the development of mobile information systems that are to be used together with other types of systems, primarily in a professional setting and indicate upcoming research issues in this very dynamic area. We argue that this research area is also timely, because the underlying technological infrastructure are just becoming sufficiently mature to make feasible research on conceptual and logical, and not only on technical issues.", "authors": ["John Krogstie", "Kalle Lyytinen", "Andreas L. Opdahl", "Barbara Pernici", "Keng Siau", "Kari Smolander"], "n_citation": 0, "title": "Mobile information systems - research challenges on the conceptual and logical level", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "603dbbc8-575e-41ec-b640-e1f5851a4676"}
{"authors": ["Aaron Bohy", "V\u00e9ronique Bruy\u00e8re", "Emmanuel Filiot", "Jean-Fran\u00e7ois Raskin"], "n_citation": 0, "title": "Synthesis from LTL specifications with mean-payoff objectives", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "60eaf72b-8293-4ae5-987a-ad95560aaa9a"}
{"abstract": "In a recent article, Nakhleh, Ringe and Warnow introduced perfect phylogenetic networks-a model of language evolution where languages do not evolve via clean speciation-and formulated a set of problems for their accurate reconstruction. Their new methodology assumes networks, rather than trees, as the correct model to capture the evolutionary history of natural languages. They proved the NP-hardness of the problem of testing whether a network is a perfect phylogenetic one for characters exhibiting at least three states, leaving open the case of binary characters, and gave a straightforward brute-force parameterized algorithm for the problem of running time O(3 k n), where k is the number of bidirectional edges in the network and n is its size. In this paper, we first establish the NP-hardness of the binary case of the problem. Then we provide a more efficient parameterized algorithm for this case running in time O(2 k n 2 ). The presented algorithm is very simple, and utilizes some structural results and elegant operations developed in this paper that can be useful on their own in the design of heuristic algorithms for the problem. The analysis phase of the algorithm is very elegant using amortized techniques to show that the upper bound on the running time of the algorithm is much tighter than the upper bound obtained under a conservative worst-case scenario assumption. Our results bear significant impact on reconstructing evolutionary histories of languages particularly from phonological and morphological character data, most of which exhibit at most. two states (i.e., are binary), as well as on the design and analysis of parameterized algorithms.", "authors": ["Iyad A. Kanj", "Luay Nakhleh", "Ge Xia"], "n_citation": 50, "title": "Reconstructing Evolution of Natural Languages : Complexity and Parameterized Algorithms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "624615d3-c8ee-4ae7-b9ec-5560379266e2"}
{"abstract": "Heterogeneous access networks in B3G networks can provide each proper network service to mobile users. Furthermore, mobile users can use not only a unique access network service, but also different hetero network services by connecting another access network. For using another access network service, mobile terminals must support hetero network system techniques. Hence, by this multiple system modes, mobile terminals are able to implement ISHO (Inter System Handover) for changing the network service. In the heterogeneous access network environment, new resource management can be adopted to control QoS management. In this paper, we propose a resource balancing management scheme by the ISHO to increase the network availability. From the numeric analysis, we compare the proposed scheme with a current scheme.", "authors": ["Sangjoon Park", "Youngchul Kim", "Hyungbin Bang", "Kwanjoong Kim", "Youngsong Mun", "Byunggi Kim"], "n_citation": 50, "title": "A Resource Balancing Scheme in Heterogeneous Mobile Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "62e95f1d-cfb9-4c36-8894-832bdf3586fd"}
{"abstract": "Run-time checks are often assumed to be a cost-effective way of improving the dependability of software components, by checking required properties of their outputs and flagging an output as incorrect if it fails the check. Run-time checks' main point of attractiveness is that they are supposed to be easy to implement. Also, they are implicitly assumed to be effective in detecting incorrect outputs. This paper reports the results of an experiment designed to challenge these assumptions about run-time checks. The experiment uses a subset of 196 of 867 programs (primaries) solving a problem called Make Palindrome. This is an existing problem on the On-Line Judge website of the university of Valladolid. We formulated eight run-time checks, and posted this problem on the same website. This resulted in 335 programs (checkers) implementing the run-time checks, 115 of which are used for the experiment. In this experiment: (1) the effectiveness of the population of possibly faulty checkers is very close to the effectiveness of a correct checker; (2) the reliability improvement provided by the run-time checks is relatively small, between a factor of one and three; (3) The reliability improvement gained by using multiple-version redundancy is much higher. Given the fact that this experiment only considers one primary/Run-Time Check combination, it is not yet possible to generalise the results.", "authors": ["Meine J. P. van der Meulen", "Miguel Revilla"], "n_citation": 50, "title": "Experiences with the Design of a Run-Time Check", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "63b1c8e4-dd2f-48ab-b43a-21c1661622ed"}
{"abstract": "In this paper, we show how the problem of verifying liveness properties is related to termination of term rewrite systems (TRSs). We formalize liveness in the framework of rewriting and present a sound and complete transformation to transform particular liveness problems into TRSs. Then the transformed TRS terminates if and only if the original liveness property holds. This shows that liveness and termination are essentially equivalent. To apply our approach in practice, we introduce a simpler sound transformation which only satisfies the 'only if'-part. By refining existing techniques for proving termination of TRSs we show how liveness properties can be verified automatically. As examples, we prove a liveness property of a waiting line protocol for a network of processes and a liveness property of a protocol on a ring of processes.", "authors": ["J\u00fcrgen Giesl", "Hans Zantema"], "n_citation": 0, "title": "Liveness in rewriting", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "68a43f27-fcc0-4bf2-b56b-7fb0cc2cb8d6"}
{"abstract": "In formal testing, the assumption of input enabling is typically made. This assumption requires all inputs to be enabled anytime. In addition, the useful concept of quiescence is sometimes applied. Briefly, a system is in a quiescent state when it cannot produce outputs. In this paper, we relax the input enabling assumption, and allow some input sets to be enabled while others remain disabled. Moreover, we also relax the general bound M used in timed systems to detect quiescence, and allow different bounds for different sets of outputs. By considering the tioco M  theory, an enriched theory for timed testing with repetitive quiescence, and allowing the partition of input sets and output sets, we introduce the mtioco M  relation. A test derivation procedure which is nondeterministic and parameterized is further developed, and shown to be sound and complete wrt mtioco M .", "authors": ["Laura Brandan Briones", "Ed Brinksma"], "n_citation": 0, "title": "Testing real-time multi input-output systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6b3679d0-888a-480d-b650-c21f3fb2725f"}
{"authors": ["Ir\u00e8ne Durand"], "n_citation": 0, "title": "Autowrite: A tool for checking properties of term rewriting systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6eccc9fd-7830-4a23-8a8a-bfd5dc5dcf4d"}
{"abstract": "We have shown [8] that LZ78 parse length can be used effectively for a music classification task. The parse length is used to compute a normalized information distance [6,7] which is then used to drive a simple classifier. In this paper we explore a more subtle use of the LZ78 parsing algorithm. Instead of simply counting the parse length of a string, we use the coding dictionary constructed by LZ78 to derive a valid string kernel for a Support Vector Machine (SVM). The kernel is defined over a feature space indexed by all the phrases identified by our (modified) LZ78 compression algorithm. We report experiments with our kernel approach on two datasets: (i) a collection of MIDI files and (ii) Reuters-21578. We compare our technique with an n-gram based kernel. Our results indicate that the LZ78 kernel technique has a performance similar to that obtained with the best n-gram performance but with significantly lower computational overhead, and without requiring a search for the optimal value of n.", "authors": ["Ming Li", "Ronan Sleep"], "n_citation": 0, "title": "An LZ78 based string kernel", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "706fae52-60e9-4865-9fb5-11fea6389ad3"}
{"abstract": "We consider equational theories of binary relations, in a language expressing composition, converse, and lattice operations. We treat the equations valid in the standard model of sets and also define a hierarchy of equational axiomatisations stratifying the standard theory. By working directly with a presentation of relation-expressions as graphs we are able to define a notion of reduction which is confluent and strongly normalising, in sharp contrast to traditional treatments based on first-order terms. As consequences we obtain unique normal forms, decidability of the decision problem for equality for each theory. In particular we show a non-deterministic polynomial-time upper bound for the complexity of the decision problems.", "authors": ["Daniel J. Dougherty", "Claudio Gutierrez"], "n_citation": 0, "title": "Normal forms and reduction for theories of binary relations", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "70af4633-3011-4eb9-88ad-4b057bfb5c36"}
{"abstract": "Relational databases get more and more employed in order to store the content of a web site. At the same time, XML is fast emerging as the dominant standard at the hypertext level of web site management describing pages and links between them. Thus, the integration of XML with relational database systems to enable the storage, retrieval and update of XML documents is of major importance. This paper presents X-Ray, a generic approach for integrating XML with relational database systems. The key idea is that mappings may be defined between XML DTDs and relational schemata while preserving their autonomy. This is made possible by introducing a meta schema and meta knowledge for resolving data model heterogeneity and schema heterogeneity. Since the mapping knowledge is not hard-coded but rather reified within the meta schema, maintainability and changeability is enhanced. The meta schema provides the basis for X-Ray to automatically compose XML documents out of the relational database when requested and decompose them when they have to be stored.", "authors": ["Gerti Kappel", "Elisabeth Kapsammer", "Stefan Rausch-Schott", "Werner Retschitzegger"], "n_citation": 0, "title": "X-Ray : Towards integrating XML and relational database systems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "71c90fc0-273d-45b2-8171-e88fb03e271b"}
{"abstract": "Data futures in a metacomputing system refer to data products that have not yet been created but which can be uniquely named and manipulated. We employ data flow mechanisms expressed as high level task graphs in our DISCWorld metacomputing system. Nodes in these task graphs can themselves be expanded into further task graphs or can be represented as futures in the processing schedule. We find this this a generally useful approach to the dynamic and optimal execution of task graphs. In this paper we describe our DISCWorld Remote Access Mechanism for Futures (DRAMFs) and its implementation using Java technology. DRAMFs embody these ideas and allow data products to be lazily de-referenced and to be manipulated as first class objects in the object space of the metacomputing system. Our system aids in the provision against partial failure or network disruptions in a distributed system such as DISCWorld.", "authors": ["Heath A. James", "Kenneth A. Hawick"], "n_citation": 0, "title": "Data futures in DISCWorld", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "71f74a93-3213-4220-b0c9-84c2d1305acb"}
{"abstract": "The next phase envisioned for the World Wide Web is automated ad-hoc interaction between intelligent agents, web services, databases and semantic web enabled applications. Although at present this appears to be a distant objective, there are practical steps that can be taken to advance the vision. We propose an extension to classical conceptual models to allow the definition of application components in terms of public standards and explicit semantics, thus building into web-based applications, the foundation for shared understanding and interoperability. The use of external definitions and the need to store outsourced type information internally, brings to light the issue of object identity in a global environment, where object instances may be identified by multiple externally controlled identification schemes. We illustrate how traditional conceptual models may be augmented to recognise and deal with multiple identities.", "authors": ["Phillipa Oaks", "Arthur H. M. ter Hofstede", "David Edmond", "Murray Spork"], "n_citation": 0, "title": "Extending conceptual models for Web based applications", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "749b3d01-15c0-4fc9-81a0-283bb9a379b0"}
{"abstract": "Transparent checkpointing is a well known method to increase the dependability of long running applications. However, most known implementations concentrate on applications that do not use graphical user interfaces. In this paper we describe common problems arising with transparent checkpointing of applications including their graphical user interfaces. We present a proxy that is able to store the window session of an application and compare our approach with an existing X-Server extension that serves the same purpose. We also discuss the performance impact of both solutions and present performance and latency measurements that demonstrate the usability of the proxy.", "authors": ["Jan-Thomas Czomack", "Carsten Trinitis", "Max Walter"], "n_citation": 0, "title": "Transparent Checkpointing for Applications with Graphical User Interfaces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "754e8790-9f74-44b3-aed1-cb051a8633d2"}
{"authors": ["Aleksandra Jovanovic", "Didier Lime", "Olivier H. Roux"], "n_citation": 66, "title": "Integer Parameter Synthesis for Timed Automata", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "78292753-9ae6-437d-a460-eebd15322e7a"}
{"abstract": "In Japanese language, the speaker must choose suitable honorific expressions depending on many factors. The computer system should imitate this mechanism to make a natural Japanese sentence. We made a system to determine a suitable expression and named it honorific expression determining system (HEDS). It generates a set of rules to determine suitable honorific expression automatically, by decision tree learning. The system HEDS determines one out of the three classes for an input sentence: the respect expression, the modesty expression and the non-honorific expression and determines what expression the verb is. We calculated the accuracy of HEDS using the cross validation method and it was up to 74.88%.", "authors": ["Kanako Komiya", "Yasuhiro Tajima", "Nobuo Inui", "Yoshiyuki Kotani"], "n_citation": 0, "title": "Generating a set of rules to determine honorific expression using decision tree learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7876cf5f-e50b-47e2-ad35-9a6193c00973"}
{"authors": ["Siu Wing Cheng", "Naoki Katoh", "Manabu Sugai"], "n_citation": 0, "title": "A Study of the LMT-Skeleton", "venue": "Lecture Notes in Computer Science", "year": 1996, "id": "79284dbb-b409-4d8a-8c68-5be685e97448"}
{"abstract": "A business process model represents the basic building block for a workflow-enabled enterprise information system. Generally, a process model evolves through numerous changes during its lifetime to meet dynamic and changing business requirements. It is essential that such changes are introduced systematically and their impact is clearly understood. Process model transformation is a suitable approach for this purpose. Applying pre-defined transformation operations can ensure that the modified process conforms to a given class of constraints specified in the original model. Using a generic process modelling language, we identify three classes of transformation principles -equivalent, imply, and subsume - to manage changes in process models. A simple algebraic notation for representing process graphs is also presented that can be used to reason about transformation operations.", "authors": ["Wasim Sadiq", "Maria E. Orlowska"], "n_citation": 0, "title": "On business process model transformations", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "7b38980a-abd9-46b5-a9b9-fceb550d462e"}
{"abstract": "In this paper we consider the following question: how many vertices of the discrete torus must be deleted so that no topologically nontrivial cycles remain? We look at two different edge structures for the discrete torus. For (Z d  m ) 1 , where two vertices in Z m  are connected if their   1  distance is 1, we show a nontrivial upper bound of d log 2 (3/2) m d-1  \u2243 d 0.6 m d-1  on the number of vertices that must be deleted. For (Z d  m )\u221e, where two vertices are connected if their  \u221e distance is 1, Saks, Samorodnitsky and Zosin [8] already gave a nearly tight lower bound of d(m-1) d-1  using arguments involving linear algebra. We give a more elementary proof which improves the bound to m d -(m-1) d , which is precisely tight.", "authors": ["B\u00e9la Bollob\u00e1s", "Guy Kindler", "Imre Leader", "Ryan O'Donnell"], "n_citation": 50, "title": "Eliminating cycles in the discrete torus", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7b9a736c-cfa1-4abc-acc3-dcf40d874c8c"}
{"abstract": "Given a graph G and degree bound B on its nodes, the bounded-degree minimum spanning tree (BDMST) problem is to find a minimum cost spanning tree among the spanning trees with maximum degree B. This bi-criteria optimization problem generalizes several combinatorial problems. including the Traveling Salesman Path Problem (TSPP). An (\u03b1, f(B))-approximation algorithm for the BDMST problem produces a spanning tree that has maximum degree f(B) and cost within a factor \u03b1 of the optimal cost. Konemann and Ravi [13,14] give a polynomial-time (1 + 1 \u03b2,bB(1 + \u03b2) + log b  n)-approximation algorithm for any  b  > 1, \u03b2 > 0. In a recent paper [2], Chaudhuri et al. improved these results with a (1. bB +\u221ab log b  n)-approximation for any b > 1. In this paper, we present a (1 +1 \u03b2 2B(1 + \u03b2 + o(B(1 + \u03b2) ))-approximation polynomial-time algorithm. That is, we give the first algorithm that approximates both degree and cost to within a constant factor of the optimal. These results generalize to the case of non-uniform degree bounds. The crux of our solution is an approximation algorithm for the related problem of finding a minimum spanning tree (MST) in which the maximum degree of the nodes is minimized, a problem we call the minimum-degree MST (MDMST) problem. Given a graph G for which the degree of the MDMST solution is \u0394 OPT : our algorithm obtains in polynomial time an MST of G of degree at most 2\u0394 o PT + fi(A o p T ). This result improves on a previous result of Fischer [4] that finds an MST of G of degree at most b\u0394 OPT  + log b  n for any b > 1, and on the improved quasipolynomial algorithm of [2]. Our algorithm uses the push-relabel framework developed by Goldberg [7] for the maximum flow problem. To our knowledge, this is the first instance of a push-relabel approximation algorithm for an NP-hard problem, and we believe these techniques may have larger impact. We note that for B = 2. our algorithm gives a tree of cost within a (1 + e)-factor of the optimal solution to TSPP and of maximum degree 0(1 e) for any e > 0, even on graphs not satisfying the triangle inequality.", "authors": ["Karnalika Chaudhuri", "Satish Rao", "Samantha Riesenfeld", "Kunal Talwar"], "n_citation": 0, "title": "A Push-Relabel Algorithm for Approximating Degree Bounded MSTs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7e51f333-23af-4a1a-a446-46f1307544b1"}
{"abstract": "This paper addresses the following combination problem: given two equational theories E 1  and E 2  whose positive theories are decidable, how can one obtain a decision procedure for the positive theory of E 1  \u222a E 2 ? For theories over disjoint signatures, this problem was solved by Baader and Schulz in 1995. This paper is a first step towards extending this result to the case of theories sharing constructors. Since there is a close connection between positive theories and unification problems, this also extends to the non-disjoint case the work on combining decision procedures for unification modulo equational theories.", "authors": ["Franz Baader", "Cesare Tinelli"], "n_citation": 50, "title": "Combining decision procedures for positive theories sharing constructors", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7ecb61b2-cf93-4fe3-920a-619b9c94c1a0"}
{"abstract": "In a client-server relational database system the response time and server throughput can be improved by outsourcing workload to clients. As extention of client-side caching techniques, we propose to preprocess database transactions at the client-side. A client operates on secondary data and supports only a low degree of isolation. The main objective is to provide a framework where the amount of preprocessing at clients is variable and adapts dynamically at run-time. Thereby, the overall goal is to maximize the systems performance, e.g. response time and throughput. We make use of a two-phase transaction protocol that verifies and reprocesses client computations if necessary. By using execution statistics we show how the amount of preprocessing can be partially predicted for each client. Within an experiment we show the correspondence between amount of preprocessing, update frequency and response time.", "authors": ["Steffen Jurk", "Mattis Neiling"], "n_citation": 0, "title": "Client-side dynamic preprocessing of transactions", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7fe9979b-122c-4dea-8e6c-e9ca5bb96045"}
{"abstract": "The purpose of this paper is to design and implement a mobile class Web site for elementary schools using intelligent user interface. Especially, our mobile class Web site encourages parents to participate in various school matters including their children's scholastic achievements. Traditional class Web sites provide information to parents through wired networks. Recently, with the development of wireless data transmission technologies, traditional class Web sites can be linked with wireless networks. Armed with wireless transmission handset devices such as cellular phones and PDA (personal digital assistants), students' parents can get class information directly through those devices. This work is to develop class Web sites that can be synergistically linked between wired and wireless networks. The communication through both wired and wireless networks enables students' parents to overcome the limitation of space and time and further promotes better collaboration with teachers. The survey results show that our class Web site promotes active participation of parents.", "authors": ["Yeonho Hong", "Woochun Jun", "Byeong Heui Kwak"], "n_citation": 0, "title": "Design and implementation of a mobile class web site using intelligent user interface", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "81555e71-8a7e-4ba3-aa20-5ced6f6d0a9d"}
{"abstract": "Information hiding capacity of digital image is the maximum information that can be hidden in an image. But the lower limit of information hiding, the minimum detectable information capacity is also an interesting problem. This paper proposes new method of the information hiding capacity bounds analysis that is based on the theories of attractors and attraction basin of neural network. The upper limit and lower limit of information hiding, namely the maximum information capacity and the minimum detectable information capacity are unified in a same theory frame. The results of research show that the attraction basin of neural network decides the upper limit of information hiding, and the attractors of neural network decide the lower limit of information hiding.", "authors": ["Fan Zhang", "Xianxing Liu", "Jie Li", "Xinhong Zhang"], "n_citation": 0, "title": "The Maximum Capacity and Minimum Detectable Capacity of Information Hiding in Digital Images", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "843a379f-8262-4b9e-ad17-2915d4756fec"}
{"abstract": "The goal of this note is to compare two notions, one coming from the theory of rewrite systems and the other from proof theory: confluence and cut elimination. We show that to each rewrite system on terms, we can associate a logical system: asymmetric deduction modulo this rewrite system and that the confluence property of the rewrite system is equivalent to the cut elimination property of the associated logical system. This equivalence, however, does not extend to rewrite systems directly rewriting atomic propositions.", "authors": ["Gilles Dowek"], "n_citation": 0, "title": "Confluence as a cut elimination property", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "84f0cf3f-2a96-4db0-ac05-d34169daa841"}
{"abstract": "Motivated by the perspective that animals' rhythmic movements such as locomotion are controlled by neural circuits called central pattern generators (CPGs), motor control mechanisms by CPG have been studied. As an autonomous learning framework for a CPG controller, we previously proposed a reinforcement learning (RL) method called the CPG-actor-critic method. In this article, we propose a natural policy gradient learning algorithm for the CPG-actor-critic method, and applied our RL to an automatic control problem by a biped robot simulator. Computer simulations show that our RL makes the biped robot walk stably on various terrain.", "authors": ["Yutaka Nakamura", "Takeshi Mori", "Shin Ishii"], "n_citation": 0, "title": "Natural policy gradient reinforcement learning for a CPG control of a biped robot", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "86745eaf-bf34-45f3-9005-d3a7be19219a"}
{"abstract": "Feature selection is an important data preprocessing step in data mining and pattern recognition. Many algorithms have been proposed in the past for simple patterns that can be characterised by a single feature vector. Unfortunately, these algorithms are hardly applicable to what are referred as complex patterns that have to be described by a finite set of feature vectors. This paper addresses the problem of feature selection for the complex patterns. First, we formulated the calculation of mutual information for complex patterns based on Gaussian mixture model. A hybrid feature selection algorithm is then proposed based on the formulated mutual information calculation (filter) and Baysian classification (wrapper). Experimental results on XM2VTS speaker recognition database have not only verified the performance of the proposed algorithm, but also demonstrated that traditional feature selection algorithms designed for simple patterns would perform poorly for complex patterns.", "authors": ["Peter Sehenkel", "Wanqing Li", "Wanquan Liu"], "n_citation": 0, "title": "Feature Selection for Complex Patterns", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "87dbf7c6-e973-4acf-9b88-86cfdb32feb4"}
{"abstract": "A hybrid approach that combines the (1 + 1)-ES and threshold selection methods is developed. The framework of the new experimentalism is used to perform a detailed statistical analysis of the effects that are caused by this hybridization. Experimental results on the sphere function indicate that hybridization worsens the performance of the evolution strategy, because evolution strategies are well-scaled hill-climbers: the additional threshold disturbs the self-adaptation process of the evolution strategy. Theory predicts that the hybrid approach might be advantageous in the presence of noise. This effect could be observed-however, a proper fine tuning of the algorithm's parameters appears to be advantageous.", "authors": ["Thomas Bartz-Beielstein"], "n_citation": 50, "title": "Evolution strategies and threshold selection", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8938f7c0-6aba-4cc6-8973-f73f487e69c7"}
{"abstract": "In order to analyze shapes of continuous curves in R 3 , we parameterize them by arc-length and represent them as curves on a unit two-sphere. We identify the subset denoting the closed curves, and study its differential geometry. To compute geodesics between any two such curves, we connect them with an arbitrary path, and then iteratively straighten this path using the gradient of an energy associated with this path. The limiting path of this path-straightening approach is a geodesic. Next, we consider the shape space of these curves by removing shape-preserving transformations such as rotation and re-parametrization. To construct a geodesic in this shape space, we construct the shortest geodesic between the all possible transformations of the two end shapes; this is accomplished using an iterative procedure. We provide step-by-step descriptions of all the procedures, and demonstrate them with simple examples.", "authors": ["Eric Klassen", "Anuj Srivastava"], "n_citation": 0, "title": "Geodesics Between 3D Closed Curves Using Path-Straightening", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "89533343-45b9-4755-ae77-c9c2ec3d508f"}
{"abstract": "The UK Defence Standard for developing safety-related software, [16], requires that a safety analysis be performed on the process used to develop safety-related software. This paper describes the experience of performing such a safety analysis, and reflects upon the lessons learnt. It discusses the issues involved in defining the process at the appropriate level of abstraction, and it evaluates the difficulties and benefits of performing Function Failure Analysis and Fault-Tree Analysis on a development process. It concludes that the benefits of performing safety-analysis of a software development process are limited, but if such an analysis must be performed, it is best done to develop a qualitative understanding of the ways the process may fail, rather than to develop a quantitative understanding of the likelihood of the process failing.", "authors": ["Stephen E. Paynter", "Bob W. Born"], "n_citation": 50, "title": "Analysing the safety of a software development process", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8b6be302-27a1-4e76-8baa-679992d0bb01"}
{"abstract": "The multidimensional modeling of data is steadily gaining popularity, finding adoption not only for business but for scientific applications as well. Data Warehousing is the most prominent example of multidimensional data usage. In parallel, wireless networks, with their rapid growth, already play a fundamental role in facilitating time critical decision-making. Nevertheless, their inherent shortcomings, but also those of the mobile devices operating within their proximity, introduce additional complexity. Access time and energy consumption become, among others, factors that should be taken into consideration. This paper deals with the efficient dissemination of multidimensional data into wireless networks. In this context, a new family of scheduling algorithms, which simultaneously exploits various characteristics both of OLAP data and wireless networks, is introduced. These algorithms clearly outperform existing proposals, on all counts: average access time, energy consumption and network utilization.", "authors": ["Ilias Michalarias", "Hans-J. Lenz"], "n_citation": 9, "title": "Dissemination of multidimensional data using broadcast clusters", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8bb633f1-f366-4920-b5cc-bb5e215168dd"}
{"authors": ["Bipin C. Sakamuri", "Sanjay Kumar Madria", "Kalpdrum Passi", "Eric Chaudhry", "Mukesh K. Mohania", "S. Bhowlnick"], "n_citation": 0, "title": "AXIS: A XML schema integration system", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "8c78acf1-3c7c-4fb1-bc38-af226af527e5"}
{"abstract": "Data Warehouses (DW), Multidimensional (MD) Databases, and OnLine Analytical Processing Applications are used as a very powerful mechanism for discovering crucial business information. Considering the extreme importance of the information managed by these kinds of applications, it is essential to specify security measures from early stages of the DW design in the MD modeling process, and enforce them. In the past years, there have been some proposals for representing main MD modeling properties at the conceptual level. Nevertheless, none of these proposals considers security measures as an important element in their models, so they do not allow us to specify confidentiality constraints to be enforced by the applications that will use these MD models. In this paper, we discuss the confidentiality problems regarding DW's and we present an extension of the Unified Modeling Language (UML) that allows us to specify main security aspects in the conceptual MD modeling, thereby allowing us to design secure DW's. Then, we show the benefit of our approach by applying this extension to a case study. Finally, we also sketch how to implement the security aspects considered in our conceptual modeling approach in a commercial DBMS.", "authors": ["Eduardo Pernandez-Medina", "Juan Trujillo", "Rodolfo Villarroel", "Mario Piattini"], "n_citation": 0, "title": "Extending UML for designing secure Data Warehouses", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8ecf277b-7dce-4e76-be9e-35fe26a0d3ea"}
{"abstract": "Quantum-behaved Particle Swarm Optimization (QPSO) is a novel optimization algorithm proposed in the previous work. Compared to the original Particle Swarm Optimization (PSO), QPSO is global convergent, while the PSO is not. This paper focuses on exploring the applicability of the QPSO to data clustering. Firstly, we introduce the K-means clustering algorithm and the concepts of PSO and QPSO. Then we present how to use the QPSO to cluster data vectors. After that, experiments are implemented to compare the performance of various clustering algorithms. The results show that the QPSO can generate good results in clustering data vectors with tolerable time consumption.", "authors": ["Jun Sun", "Wenbo Xu", "Bin Ye"], "n_citation": 0, "title": "Quantum-Behaved Particle Swarm Optimization Clustering Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8f71ab6d-7239-4e24-8bd8-314de8b59735"}
{"abstract": "In real-world data mining applications, an accurate ranking is as important as an accurate classification. Naive Bayes has been widely used in data mining as a simple and effective classification and ranking algorithm. Since its conditional independence assumption is rarely true, numerous algorithms have been proposed to improve naive Bayes, for example, SBC[1] and TAN[2]. Indeed, the experimental results show that SBC and TAN achieve a significant improvement in term of classification accuracy. However, unfortunately, our experiments also show that SBC and TAN perform even worse than naive Bayes in ranking measured by AUC[3,4] (the area under the Receiver Operating Characteristics curve). This fact raises the question of whether we can improve Naive Bayes with both accurate classification and ranking? In this paper, responding to this question, we present a new learning algorithm called One Dependence Augmented Naive Bayes(ODANB). Our motivation is to develop a new algorithm to improve Naive Bayes' performance not only on classification measured by accuracy but also on ranking measured by AUC. We experimentally tested our algorithm, using the whole 36 UCI datasets recommended by Weka[5], and compared it to Naive Bayes, SBC and TAN. The experimental results show that our algorithm outperforms all the other algorithms significantly in yielding accurate ranking, yet at the same time outperforms all the other algorithms slightly in terms of classification accuracy.", "authors": ["Liangxiao Jiang", "Harry Zhang", "Zhihua Cai", "Jiang Su"], "n_citation": 50, "title": "One dependence augmented naive bayes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9138e47f-f4f3-4824-b83d-08fe3401a742"}
{"abstract": "Model management is a framework for supporting meta-data related applications where models and mappings are manipulated as first class objects using operations such as Match, Merge, ApplyFunction, and Compose. To demonstrate the approach, we show how to use model management in two scenarios related to loading data warehouses. The case study illustrates the value of model management as a methodology for approaching meta-data related problems. It also helps clarify the required semantics of key operations. These detailed scenarios provide evidence that generic model management is useful and, very likely, implementable.", "authors": ["Philip A. Bernstein", "Erhard Rahm"], "n_citation": 165, "title": "Data warehouse scenarios for model management", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "913a7082-f2c2-421f-a493-ecdca4aa7e22"}
{"abstract": "Based on the simply typed term rewriting framework, inductive reasoning in higher-order rewriting is studied. The notion of higher-order inductive theorems is introduced to reflect higher-order feature of simply typed term rewriting. Then the inductionless induction methods in first-order term rewriting are incorporated to verify higher-order inductive theorems. In order to ensure that higher-order inductive theorems are closed under contexts, the notion of higher-order sufficient completeness is introduced. Finally, the decidability of higher-order sufficient completeness is discussed.", "authors": ["Takahito Aoto", "Toshiyuki Yamada", "Yoshihito Toyama"], "n_citation": 0, "title": "Inductive theorems for higher-order rewriting", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "927aa7f3-ec84-41e0-9270-9bda8366039e"}
{"abstract": "We revisit the following open problem in information-theoretic cryptography: Does the communication complexity of unconditionally secure computation depend on the computational complexity of the function being computed? For instance, can computationally unbounded players compute an arbitrary function of their inputs with polynomial communication complexity and a linear threshold of unconditional privacy? Can this be done using a constant number of communication rounds? We provide an explanation for the difficulty of resolving these questions by showing that they are closely related to the problem of obtaining efficient protocols for (information-theoretic) private information retrieval and hence also to the problem of constructing short locally-decodable error-correcting codes. The latter is currently considered to be among the most intriguing open problems in complexity theory.", "authors": ["Yuval Ishai", "Eyal Kushilevitz"], "n_citation": 0, "title": "On the hardness of information-theoretic multiparty computation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "934a0e0e-f16e-4f37-8549-ed9bd58f434c"}
{"abstract": "The rapid growth of the Internet and increased demand to use the Internet for voice and video applications necessitate the design and utilization of new Internet architectures with effective congestion control algorithms. As a result, the Differentiated Service (Diff-Serv) architectures were proposed to deliver Quality of Service (QoS) in TCP/IP networks. Network congestion control remains a critical and high priority issue, even for the present Internet architecture. The aim of this paper is to design a robust active queue management system to secure high utilization, bounded delay and loss, while the network complies with the demands each traffic class sets. To this end, variable structure control theory is used and a sliding mode congestion controller is designed. Simulation results of the proposed control action demonstrate the effectiveness of the controller in providing robust queue management system.", "authors": ["Hassan Ebrahimirad", "M.J. Yazdanpanah"], "n_citation": 0, "title": "Sliding mode congestion control in Differentiated Service communication networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "94a8fadf-3d6e-43f0-a626-bdcaf5bdbe6d"}
{"abstract": "This paper is devoted to investigation of the global asymptotic stability for Bidirectional associative memory (BAM) neural networks with variable coefficient and S-type distributed signal transmission delays along the axon of a neuron. Some sufficient conditions for global asymptotic stability of the networks was obtained, in which the boundedness and differentiability of the signal functions in some papers are deleted. Some examples are also presented to show that our results are new and improve the previous results.", "authors": ["Yonggui Kao", "Cunchen Gao", "Lu Wu", "Qinghe Ming"], "n_citation": 50, "title": "Global Stability of Bidirectional Associative Memory Neural Networks with Variable Coefficients and S-Type Distributed Delays", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "94c43ba7-f071-4647-9400-8091597504b7"}
{"abstract": "Object-Z is strong in modeling the data and operations of complex systems. However, it is weak in specifying real-time and concurrent systems. Timed Communicating Object-Z (TCOZ) extends the Object-Z notation with Timed CSP constructs. TCOZ is particularly well suited for specifying complex systems whose components have their own thread of control. This paper demonstrates expressiveness of the TCOZ notation through a case study on specifying a multi-lift system that operates in real-time.", "authors": ["Brendan P. Mahony", "Jin Song Dong"], "n_citation": 0, "title": "Network topology and a case study in TCOZ", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "95eb079d-80a9-4c41-a8e2-ef20789fa434"}
{"abstract": "Conventional frequent pattern mining algorithms require users to specify some minimum support threshold. If that specified-value is large, users may lose interesting information. In contrast, a small minimum support threshold results in a huge set of frequent patterns that users may not be able to screen for useful knowledge. To solve this problem and make algorithms more user-friendly, an idea of mining the k-most interesting frequent patterns has been proposed. This idea is based upon an algorithm for mining frequent patterns without a minimum support threshold, but with a k number of highest frequency patterns. In this paper, we propose an explorative mining algorithm, called ExMiner, to mine k-most interesting (i.e. top-k) frequent patterns from large scale datasets effectively and efficiently. The ExMiner is then combined with the idea of build once mine anytime to mine top-k frequent patterns sequentially. Experiments on both synthetic and real data show that our proposed methods are more efficient compared to the existing ones.", "authors": ["Tran Minh Quang", "Shigeru Oyanagi", "Katsuhiro Yamazaki"], "n_citation": 0, "title": "ExMiner : An Efficient Algorithm for Mining Top-K Frequent Patterns", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9605dd51-00e8-4f0e-a5a8-cb642e66acbc"}
{"abstract": "We present an approach to support incremental navigation of structured information, where the structure is introduced by the data model and schema (if present) of a data source. Simple browsing through data values and their connections is an effective way for a user or an automated system to access and explore information. We use our previously defined Uni-Level Description (ULD) to represent an information source explicitly by capturing the source's data model, schema (if present), and data values. We define generic operators for incremental navigation that use the ULD directly along with techniques for specifying how a given representation scheme can be navigated. Because our navigation is based on the ULD, the operations can easily move from data to schema to data model and back, supporting a wide range of applications for exploring and integrating data. Further, because the ULD can express a broad range of data models, our navigation operators are applicable, without modification, across the corresponding model or schema. In general, we believe that information sources may usefully support various styles of navigation, depending on the type of user and the user's desired task.", "authors": ["Shawn Bowers", "Lois M. L. Delcambre"], "n_citation": 0, "title": "Incremental navigation: Providing simple and generic access to heterogeneous structures", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "963b16cc-9588-43c1-a2aa-94e94d55deb9"}
{"abstract": "We present a modular approach to defining logics for a wide variety of state-based systems. We use coalgebras to model the behaviour of systems, and modal logics to specify behavioural properties of systems. We show that the syntax, semantics and proof systems associated to such logics can all be derived in a modular way. Moreover, we show that the logics thus obtained inherit soundness, completeness and expressiveness properties from their building blocks. We apply these techniques to derive sound, complete and expressive logics for a wide variety of probabilistic systems.", "authors": ["Corina C\u00eerstea", "Dirk Pattinson"], "n_citation": 0, "title": "Modular construction of modal logics", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "967bbe3d-99db-450e-90f7-4441b3308273"}
{"abstract": "We consider the problem of computing a Nash equilibrium in multiple-player games. It is known that there exist games, in which all the equilibria have irrational entries in their probability distributions [19]. This suggests that either we should look for symbolic representations of equilibria or we should focus on computing approximate equilibria. We show that every finite game has an equilibrium such that all the entries in the probability distributions are algebraic numbers and hence can be finitely represented. We also propose an algorithm which computes an approximate equilibrium in the following sense: the strategies output by the algorithm are close with respect to l\u221e-norm to those of an exact Nash equilibrium and also the players have only a negligible incentive to deviate to another strategy. The running time of the algorithm is exponential in the number of strategies and polynomial in the digits of accuracy. We obtain similar results for approximating market equilibria in the neoclassical exchange model under certain assumptions.", "authors": ["Richard J. Lipton", "Evangelos Markakis"], "n_citation": 0, "title": "Nash equilibria via polynomial equations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "96872a35-035d-491b-9560-20452ae49976"}
{"abstract": "Mining association rules and correlation relationships have been studied in the data mining field for many years. However, the rules mined only indicate association relationships among variables in an interested system. They do not specify the essential underlying mechanism of the system that describe causal relationships. In this paper, we present an approach for mining causal relationships among attributes and propose a potential application in the field of bioinformatics. Based on the theory of causal diagram, we show the properties of our approach.", "authors": ["Yang Bo He", "Zhi Geng", "Xun Liang"], "n_citation": 0, "title": "An approach to mining local causal relationships from databases", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "99fcb71a-1695-46c0-8838-1bed9c648eee"}
{"abstract": "Filtering based algorithms have become popular in tracking human body pose. Such algorithms can suffer the curse of dimensionality due to the high dimensionality of the pose state space; therefore, efforts have been dedicated to either smart sampling or reducing the dimensionality of the original pose state space. In this paper, a novel formulation that employs a dimensionality reduced state space for multi-hypothesis tracking is proposed. During off-line training, a mixture of factor analyzers is learned. Each factor analyzer can be thought of as a local dimensionality reducer that locally approximates the pose manifold. Global coordination between local factor analyzers is achieved by learning a set of linear mixture functions that enforces agreement between local factor analyzers. The formulation allows easy bidirectional mapping between the original body pose space and the low-dimensional space. During on-line tracking, the clusters of factor analyzers are utilized in a multiple hypothesis tracking algorithm. Experiments demonstrate that the proposed algorithm tracks 3D body pose efficiently and accurately, even when self-occlusion, motion blur and large limb movements occur. Quantitative comparisons show that the formulation produces more accurate 3D pose estimates over time than those that can be obtained via a number of previously-proposed particle filtering based tracking algorithms.", "authors": ["Rui Li", "Ming-Hsuan Yang", "Stan Sclaroff", "Tai-Peng Tian"], "n_citation": 73, "title": "Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9c154dbf-76fe-4939-b736-3f7a6e258d42"}
{"abstract": "3d visualisation of real spaces in the Internet is nowadays getting ready for all day usage. Because of higher complexity of 3d- compared to 2d visualisation, it is mostly used for high-end visualisations. The strategy of integrating them into existing spatial data infrastructures and making reuse of existing resources that are so far mostly used for 2d visualisation, created the opportunity to build 3d web-applications as an add-on. The recurring problems of web-based 3d visualisation raised the question, how special modules for 3d visualisation could also be made reuseable. We took a closer look especially on the so-called mapping-level, which is essential for the type of visualisation, interactivity and some special analysis-tasks.", "authors": ["Torsten Heinen", "Martin May", "Benno Schmidt"], "n_citation": 0, "title": "3d visualisation in spatial data infrastructures", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9c27a648-9ea7-4bb4-a7c8-32fe76338cd5"}
{"abstract": "The problem of global state observation is fundamental to distributed systems. All interactions in distributed systems can be analyzed in terms of the building block formed by the pairwise interactions of intervals between two processes. Considering causality-based pairwise interactions by which two intervals at different processes may interact with each other, there are 40 possible orthogonal interactions. This paper examines the problem: If a global state of interest to an application is specified in terms of the pairwise interaction types between each pair of processes, how can such a global state be detected? A solution identifies a global state in which the relation specified for each process pair is satisfied. This paper formulates the specific conditions on the exact communication structures to determine which of the intervals being examined at any time may never satisfy the stipulated relation for that pair of processes, and therefore that interval must be deleted.", "authors": ["Punit Chandra", "Ajay D. Kshemkalyani"], "n_citation": 50, "title": "Analysis of interval-based global state detection", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9c436696-5d95-4a19-9c93-6b21c0ae0664"}
{"abstract": "Co-referential relations between textual and visual elements in illustrations can be encoded efficiently through textual labels. The labels support students to learn unknown terms and focus their attention on important aspects of the illustration; while a functional and aesthetic label layout aims at guaranteeing the readability of text strokes as well as preventing the referential mismatches. By analyzing a corpus of complex label layouts in hand-drawn illustrations, a classification of label layout styles and several metrics for functional requirements and aesthetic attributes were extracted. As the choice of a specific layout style seems largely determined by individual preferences, a real-time layout algorithm for internal and external labels balances conflicting user-specific requirements, functional and aesthetic attributes.", "authors": ["Knut Hartmann", "Timo G\u00f6tzelmann", "Kamran Ali", "Thomas Strothotte"], "n_citation": 57, "title": "Metrics for functional and aesthetic label layouts", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9eca9d02-f8a1-46fe-9ca5-b52c7a525537"}
{"authors": ["Pawe\u0142 G\u00f3recki", "Oliver Eulenstein"], "n_citation": 50, "title": "Deep Coalescence Reconciliation with Unrooted Gene Trees: Linear Time Algorithms", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "a18ae186-ac63-4fe5-9b6b-c29df815fe45"}
{"abstract": "Given a set. of images of scenes containing multiple object categories (e.g. grass, roads, buildings) our objective is to discover these objects in each image in an unsupervised manner, and to use this object distribution to perform scene classification. We achieve this discovery using probabilistic Latent Semantic Analysis (pLSA), a generative model from the statistical text literature, here applied to a bag of visual words representation for each image. The scene classification on the object distribution is carried out by a k-nearest neighbour classifier. We investigate the classification performance under changes in the visual vocabulary and number of latent topics learnt, and develop a novel vocabulary using colour SIFT descriptors. Classification performance is compared to the supervised approaches of Vogel & Schiele [19] and Oliva & Torralba [11], and the semi-supervised approach of Fei Fei & Perona [3] using their own datasets and testing protocols. In all cases the combination of (unsupervised) pLSA followed by (supervised) nearest neighbour classification achieves superior results. We show applications of this method to image retrieval with relevance feedback and to scene classification in videos.", "authors": ["Anna Bosch", "Andrew Zisserman", "Xavier Mu\u00f1oz"], "n_citation": 0, "title": "Scene Classification Via pLSA", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a1be0952-6084-4c4e-ac8b-228022f67934"}
{"abstract": "Semantics of logic programs has been given by proof theory, model theory and by fixpoint of the immediate-consequence operator. If clausal logic is a programming language, then it should also have a compositional semantics. Compositional semantics for programming languages follows the abstract syntax of programs, composing the meaning of a unit by a mathematical operation on the meanings of its constituent units. The procedural interpretation of logic has only yielded an incomplete abstract syntax for logic programs. We complete it and use the result as basis of a compositional semantics. We present for comparison Tarski's algebraization of first-order predicate logic, which is in substance the compositional semantics for his choice of syntax. We characterize our semantics by equivalence with the immediate-consequence operator.", "authors": ["Maarten H. van Emden"], "n_citation": 0, "title": "Compositional semantics for the procedural interpretation of logic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a23410e5-0593-406d-a7e5-9dc08f6900ed"}
{"abstract": "This paper deals with test data set selection from algebraic specifications. Test data sets are generated from selection criteria which are usually defined to cover specification axioms. The unfolding selection criterion consists in covering the input domain of an operation using case analysis. The unfolding procedure can be iterated in order to split input domains of operations into finer subdomains. In this paper we propose to extend an unfolding procedure previously developed in [5,19] that could only be performed on very low level, i.e. executable specifications. On the contrary, our new unfolding procedure can be applied to any positive conditional specification. We show that our unfolding procedure is sound (no test is added) and complete (no test is lost) with respect to the starting reference test data set.", "authors": ["Marc Aiguier", "Agn\u00e8s Arnould", "Cl\u00e9ment Boin", "Pascale Le Gall", "Bruno Marre"], "n_citation": 0, "title": "Testing from Algebraic Specifications : Test Data Set Selection by Unfolding Axioms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a317eec1-f136-41fb-b430-10e6257799b5"}
{"abstract": "Data grids provide researching groups with data management services such as Grid-Ftp, Reliable File Transfer Service (RFP), and Replica Location Service (RLS). Especially, Replica Location Service could improve data availability by storing multiple data to the distributed location. In this paper, we propose Tree-based Replica Location Scheme (TRLS) that could support users to decide optimally the location of multiple replicas as well as the number of replicas. In addition to, we could decide the number of replicas to satisfy user's requirements by minimizing cost.", "authors": ["Dong Su Nam", "Eung Ki Park", "Sangjin Jeong", "Byungsang Kim", "Chan-Hyun Youn"], "n_citation": 0, "title": "Tree-based Replica Location Scheme (TRLS) for data grids", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a679c2e0-9704-49c6-8de7-31a02f906118"}
{"abstract": "How to exploit current information techniques for rapidly and accurately building a fittest neural network becomes increasingly significant for flood peak forecasting. This paper firstly designs a distributed computing architecture and builds a computing environment based on Grid technologies. Then a distributed computing service for neural networks based on a genetic algorithm and a modified BP algorithm is designed and developed to rapidly and accurately building a fittest neural network for flood peak forecasting. Finally, a distributed computing prototype system is developed and implemented on a case study of the flood prevention in Shenzhen city, China. The experiment result shows that the scheme addressed in the paper is efficient and feasible.", "authors": ["Jun Zhu", "Chunbo Liu", "Jianhua Gong", "Daojun Wang", "Tao Song"], "n_citation": 50, "title": "A Distributed Computing Service for Neural Networks and Its Application to Flood Peak Forecasting", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a7461a72-580e-4f1d-8944-57a891aa7be1"}
{"abstract": "With the increasing use of Grid computing, it is growing to concern a computing power and infrastructure problems of Grid environments. But, it is also important to consider its security problems because of joining many different users. In this paper we first consider satellite environments having a variety of users like Grid environments. We proposed two kinds of key distribution models to achieve a security between satellite terminals. At last, through performance analysis of proposed models, we considered their suitability to satellite environments and Grid computing.", "authors": ["Taeshik Shon", "Jungtaek Seo", "Jongsub Moon"], "n_citation": 0, "title": "Simple key agreement and its efficiency analysis for Grid computing environments", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a7da68d3-584b-4541-8b80-61e3c2cf28e9"}
{"abstract": "In this tutorial we describe general approaches to deciding bisimilarity between vertices of (infinite) directed edge-labelled graphs. The approaches are based on a systematic search following the definition of bisimilarity. We outline (in decreasing levels of detail) how the search is modified to solve the problem for finite graphs, BPP graphs, BPA graphs, normed PA graphs, and normed PDA graphs. We complete this by showing the technique used in the case of graphs generated by one-counter machines. Finally, we demonstrate a general reduction strategy for proving undecidability, which we apply in the case of graphs generated by state-extended BPP (a restricted form of labelled Petri nets).", "authors": ["P. Jancar", "Faron Moller"], "n_citation": 0, "title": "Techniques for decidability and undecidability of bisimilarity", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "a85dd9b2-7ad8-4f9d-8185-c1281c27b6e2"}
{"abstract": "Privacy preserving data mining is to discover accurate patterns without precise access to the original data. In this paper, we combine the two strategies of data transform and data hiding to propose a new randomization method, Randomized Response with Partial Hiding (RRPH), for distorting the original data. Then, an effective naive Bayes classifier is presented to predict the class labels for unknown samples according to the distorted data by RRPH. Shown in the analytical and experimental results, our method can obtain significant improvements in terms of privacy, accuracy, and applicability.", "authors": ["Peng Zhang", "Yunhai Tong", "Shiwei Tang", "Dongqing Yang"], "n_citation": 50, "title": "Privacy preserving naive bayes classification", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a878daff-eabb-46b8-865b-ac37948d5f61"}
{"abstract": "This paper describes the design of a system, which facilitates Accessing and Interconnecting heterogeneous data sources. Data sources can be static or active: static data sources include structured or semistructured data like databases, XML and HTML documents; active data sources include services which are localised on one or several servers including web services. The main originality of this work is to make interoperability between actives and/or static data sources based on XQuery language. As an example of using our approach, we'll give a scenario for analyzing log files basing on OLAP (On Line Analytical Processing) literature.", "authors": ["Gilles Nachouki", "Marie-Pierre Chastang"], "n_citation": 0, "title": "On-line analysis of a web data warehouse", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "a8b6d74e-f71a-4b98-8f70-46789b3a330a"}
{"abstract": "Abstraction-Carrying Code (ACC) has recently been proposed as a framework for Proof-Carrying Code (PCC) in which the code supplier provides a program together with an abstraction (or abstract model of the program) whose validity entails compliance with a predefined safety policy. Existing approaches for PCC are developed under the assumption that the consumer reads and validates the entire program w.r.t. the full certificate at once, in a non incremental way. In the context of ACC, we propose an incremental approach to PCC for the generation of certificates and the checking of untrusted updates of a (trusted) program, i.e., when a producer provides a modified version of a previously validated program. Our proposal is that, if the consumer keeps the original (fixed-point) abstraction, it is possible to provide only the program updates and the incremental certificate (i.e., the difference of abstractions). Furthermore, it is now possible to define an incremental checking algorithm which, given the new updates and its incremental certificate, only re-checks the fixpoint for each procedure affected by the updates and the propagation of the effect of these fixpoint changes. As a consequence, both certificate transmission time and checking time can be reduced significantly.", "authors": ["Elvira Albert", "Puri Arenas", "Germ\u00e1n Puebla"], "n_citation": 50, "title": "An incremental approach to abstraction-carrying code", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a9c2e3fe-1df4-40ee-96f7-ab89f391f8ba"}
{"abstract": "In this paper, we study learning-related complexity of linear ranking functions from n-dimensional Euclidean space to {1, 2,..., k}. We show that their graph dimension, a kind of measure for PAC learning complexity in the multiclass classification setting, is \u03b8(n+k). This graph dimension is significantly smaller than the graph dimension Q(nk) of the class of {1, 2,..., k}-valued decision-list functions naturally defined using k - 1 linear discrimination functions. We also show a risk bound of learning linear ranking functions in the ordinal regression setting by a technique similar to that used in the proof of an upper bound of their graph dimension.", "authors": ["Atsuyoshi Nakamura"], "n_citation": 0, "title": "Learning-related complexity of linear ranking functions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aabc5e5e-ba39-464c-8225-9afcab7a12c3"}
{"abstract": "In this paper, we propose a study of co-verbal gesture properties that could enhance the animation of an Embodied Conversational Agent and their communicative performances. This work is based on the analysis of gesture expressivity over time that we have study from a corpus of 2D animations. First results point out two types of modulations in gesture expressivity that are evaluated on their communicative performances. A model of these modulations is proposed.", "authors": ["Nicolas Ech Chafai", "Catherine Pelachaud", "Danielle Pel\u00e9", "Gaspard Breton"], "n_citation": 0, "title": "Gesture expressivity modulations in an ECA application", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "abb5fb2a-4bc2-4038-a185-378d1eb21988"}
{"abstract": "Online mining of changes from data streams is an important problem in view of growing number of applications such as network flow analysis, e-business, stock market analysis etc. Monitoring of these changes is a challenging task because of the high speed, high volume, only-one-look characteristics of the data streams. User subjectivity in monitoring and modeling of the changes adds to the complexity of the problem. This paper addresses the problem of i) capturing user subjectivity and ii) change modeling, in applications that monitor frequency behavior of item-sets. We propose a three stage strategy for focusing on item-sets, which are of current interest to the user and introduce metrics that model changes in their frequency (support) behavior.", "authors": ["Vasudha Bhatnagar", "Sarabjeet Kaur Kochhar"], "n_citation": 0, "title": "User subjectivity in change modeling of streaming itemsets", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ac02f9bf-8998-43f1-a428-511a7fdaed57"}
{"abstract": "Modelling a finite population genetic algorithm (GA) as a Markov chain can quickly become unmanageable since the number of population states increases rapidly with the population size and search space size. One approach to resolving this issue is to lump similar states together, so that a tractable Markov chain can be produced. A paper by Spears and De Jong in [1] presents an algorithm that can be used to lump states together, thus compressing the probability transition matrix. However, to obtain a lumped model, one needs to calculate the exact transition matrix before the algorithm can be applied to it. In this paper, we explore the possibility of producing a reduced Markov model without the need to first produce the exact model. We illustrate this approach using the Vose model and Spears lumping algorithm on the Onemax problem with a selection-mutation GA.", "authors": ["Cheah C. J. Moey", "Jonathan E. Rowe"], "n_citation": 0, "title": "A reduced Markov model of gas without the exact transition matrix", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "acc66574-3470-433f-934f-5a81b5a69058"}
{"abstract": "Propositional canonical Gentzen-type systems, introduced in [1], are systems which in addition to the standard axioms and structural rules have only logical rules in which exactly one occurrence of a connective is introduced and no other connective is mentioned. [1] provides a constructive coherence criterion for the non-triviality of such systems and shows that a system of this kind admits cut-elimination iff it is coherent. The semantics of such systems is provided using two-valued nondeterministic matrices (2Nmatrices). [14] extends these results to systems with unary quantifiers of a very restricted form. In this paper we substantially extend the characterization of canonical systems to (n,k)-ary quantifiers, which bind k distinct variables and connect n formulas. We show that the coherence criterion remains constructive for such systems, and that for the case of k \u2208 {0,1}: (i) a canonical system is coherent iff it has a strongly characteristic 2Nmatrix, a.nd (ii) if a canonical system is coherent, then it admits cut-elimination.", "authors": ["Anna Zamansky", "Arnon Avron"], "n_citation": 0, "title": "Canonical gentzen-type calculi with (n,k)-ary quantifiers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "acf6ecc5-13f1-4c6b-b4bf-752bb818c35c"}
{"abstract": "From the analysis of some hard drawbacks faced by content delivery and service system today, such as heterogeneous, isolated CDN systems and difficult integration, a novel OGSA-based open content alliance scheme, is introduced in this paper. OGSA (Open Grid Services Architecture) is a new architecture of service-oriented grid middleware platform different from computational grid. We study and discuss the key modules for integrating heterogeneous content service systems based on OGSA standard. And then we describe the main functions realized by the alliance platform, including integration and registration service, content discovery service, content cooperating service and so forth. From our research experiences and related survey, we analyze the prospective research direction and challenges in this field.", "authors": ["Zhihui Lv", "Yiping Zhong", "Shiyong Zhang"], "n_citation": 0, "title": "Research and design on a novel OGSA-based open content alliance scheme", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ad273e78-16ea-4d63-ab0d-643facc3376a"}
{"abstract": "The set Z \u03b2  of \u03b2-integers is a Meyer set when \u03b2 is a Pisot number, and thus there exists a finite set F such that Z \u03b2 -Z \u03b2  \u2282 Z \u03b2  + F. We give finite automata describing the expansions of the elements of Z \u03b2  and of Z \u03b2  - Z \u03b2 . We present a construction of such a finite set F, and a method to minimize the size of F. We obtain in this way a finite transducer that performs the decomposition of the elements of Z \u03b2  - Z \u03b2  as a sum belonging to Z \u03b2  + F.", "authors": ["Shigeki Akiyama", "Fr\u00e9d\u00e9rique Bassino", "Christiane Frougny"], "n_citation": 0, "title": "Automata for arithmetic Meyer sets", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "adb5d4d5-e72f-4309-8857-d05f9367c50d"}
{"abstract": "In this paper, we prove two general theorems on monotone Boolean functions which are useful for constructing an learning algorithm for monotone Boolean functions under the uniform distribution. A monotone Boolean function is called fair if it takes the value 1 on exactly half of its inputs. The first result proved in this paper is that the single variable function f(x) = x i  has the minimum correlation with the majority function among all fair monotone functions (Theorem 1). This proves the conjecture by Blum, Burch and Langford (FOCS '98) and improves the performance guarantee of the best known learning algorithm for monotone Boolean functions under the uniform distribution proposed by them. Our second result is on the relationship between the influences and the average sensitivity of a monotone Boolean function. The influence of variable x i  on f is defined as the probability that f(x) differs from f(x\u25cb+ e i ) where x is chosen uniformly from {0, 1} n  and x \u25cb+ e i  means x with its i-th bit flipped. The average sensitivity of f is defined as the sum of the influences over all variables x i . We prove that a somewhat unintuitive result which says if the influence of every variable on a monotone Boolean function is small, i.e., O(1/n c ) for some constant c > 0, then the average sensitivity of the function must be large, i.e., \u03a9(log n) (Theorem 11). We also discuss how to apply this result to construct a new learning algorithm for monotone Boolean functions.", "authors": ["Kazuyuki Amano", "Akira Maruoka"], "n_citation": 18, "title": "On learning monotone Boolean functions under the uniform distribution", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "afc88afa-cfc6-4e06-b3b0-5b430e9e3984"}
{"abstract": "Sometimes called the smoothing assumption, the prior model of a stereo matching algorithm is the algorithm's expectation on the surfaces in the world. Any stereo algorithm makes assumptions about the probability to see each surface that can be represented in its representation system. Although the past decade has seen much continued progress in stereo matching algorithms, the prior models used in them have not changed much in three decades: most algorithms still use a smoothing prior that minimizes some function of the difference of depths between neighboring sites, sometimes allowing for discontinuities. However, one system seems to use a very different prior model from all other systems: the human vision system. In this paper, we first report the observations we made in examining human disparity interpolation using stereo pairs with sparse identifiable features. Then we mathematically analyze the implication of using current prior models and explain why the human system seems to use a model that is not only different but in a sense diametrically opposite from all current models. Finally, we propose two candidate models that reflect the behavior of human vision. Although the two models look very different, we show that they are closely related.", "authors": ["Hiroshi Ishikawa", "Davi Geiger"], "n_citation": 0, "title": "Rethinking the Prior Model for Stereo", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b05ef9c1-310c-4ff5-a098-9858a3b2ccd4"}
{"abstract": "Current user interfaces of CAD systems are still not suited to the initial stages of product development, where freehand drawings are used by engineers and designers to express their visual thinking. In order to exploit these sketching skills, we present a sketch based modeling system, which provides a reduced instruction set calligraphic interface to create orthogonal polyhedra, and an extension of them we named quasi-normalons. Our system allows users to draw lines on free-hand axonometric-like drawings, which are automatically tidied and beautified. These line drawings are then converted into a three-dimensional model in real time because we implemented a fast reconstruction process, suited for quasi-normalon objects and so-called axonometric inflation method, providing in this way an innovative integrated 2D sketching and 3D view visualization work environment.", "authors": ["Ferran Naya", "Julian Conesa", "Manuel Contero", "Joaquim A. Jorge"], "n_citation": 50, "title": "Smart sketch system for 3D reconstruction based modeling", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b2edb385-1832-4c0b-ba57-aef686f50951"}
{"authors": ["Johan Borst", "Lars R. Knudsen", "Vincent Rijmen"], "n_citation": 54, "title": "Two attacks on reduced IDEA", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "b44759d0-c4e1-431d-bafc-78fa6c9f1211"}
{"abstract": "A multi-objective evolutionary algorithm is applied to optimize the design of a helical spring made out of a composite material. The criteria considered are the minimization of the mass along with the maximization of the stiffness of the spring. Considering the computation time required for finite element analyses, the optimization is performed using approximate relations between design parameters. Dual kriging interpolation allows improving the accuracy of the classical model of spring stiffness by estimating the error between the model and the results of finite element analyses. This error is taken into account by adding a correction function to the stiffness function. The NSGA-II algorithm is applied and shows satisfactory results, while using the correction function induces a displacement of the Pareto front.", "authors": ["Fr\u00e9d\u00e9ric Ratle", "Beno\u00eet Lecarpentier", "Richard Labib", "Fran\u00e7ois Trochu"], "n_citation": 0, "title": "Multi-objective optimization of a composite material spring design using an evolutionary algorithm", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b68141b1-9b28-4eeb-bf79-449e6ee83694"}
{"abstract": "Web services have become a new wave for Internet-based business applications. Through compositions of Web services available, value-added services can be built. More and more enterprises have ventured into this area staking a claim in their future. However, there still exists a great challenge to perform transactions in the loosely coupled environment. Traditional ACID transaction model is not suitable and failure recovery algorithm ensuring semantic atomicity is needed. In this paper, we propose a process-oriented transactional business coordination model with hierarchical structure. It allows dependencies across hierarchies and supports various transactional behaviors of different services. Based on this model, a novel failure recovery algorithm is presented. It can ensure semantic atomicity for long running Web services composition. Analysis shows it can effectively reduce compensation sphere and thus decrease costs. The correctness and implementation issues of this algorithm are also presented.", "authors": ["Yi Ren", "Quanyuan Wu", "Yan Jia", "Jianbo Guan", "Weihong Han"], "n_citation": 0, "title": "Transactional business coordination and failure recovery for Web services composition", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b719eed9-a946-4c4a-8bf3-f401e732f6af"}
{"authors": ["Dong-Kyu Kim", "Yuan-Chi Chang", "Juhnyoung Lee", "Sang-goo Lee"], "n_citation": 0, "title": "Ontological approaches to enterprise applications", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b8bfc725-4f4f-489b-a70d-1da63527b953"}
{"abstract": "This paper presents a novel steganalysis scheme with high-dimensional feature vectors derived from co-occurrence matrix in either spatial domain or JPEG coefficient domain, which is sensitive to data embedding process. The class-wise non-principal components analysis (CNPCA) is proposed to solve the problem of the classification in the high-dimensional feature vector space. The experimental results have demonstrated that the proposed scheme outperforms the existing steganalysis techniques in attacking the commonly used steganographic schemes applied to spatial domain (Spread-Spectrum, LSB, QIM) or JPEG domain (OutGuess, F5, Model-Based).", "authors": ["Guorong Xuan", "Yun Q. Shi", "Cong Huang", "Dongdong Fu", "Xiuming Zhu", "Peiqi Chai", "Jianjiong Gao"], "n_citation": 0, "title": "Steganalysis using high-dimensional features derived from co-occurrence matrix and class-wise non-principal components analysis (CNPCA)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ba33ec6b-117e-4803-b12f-79899e1b644a"}
{"abstract": "We propose a distributed computing framework for network-optimized visualization and steering of real-time scientific simulations and computations executed on a remote host, such as workstation, cluster or super-computer. Unlike the conventional batch simulations, this system enables: (i) monitoring of an on-going remote computation using visualization tools, and (ii) on-line specification of simulation parameters to interactively steer remote computations. Using performance models for transport channels and visualization modules, we develop a dynamic programming method to optimize the real| ization of the visualization pipeline over a wide-area network to maximize the frame rate. We present experimental results to illustrate the effectiveness of this system.", "authors": ["Qishi Wu", "Mengxia Zhu", "Nageswara S. V. Rao"], "n_citation": 0, "title": "System design for on-line distributed computational visualization and steering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bf267521-9d78-4ed3-b32a-53c8d5b6890c"}
{"abstract": "This paper introduces the DeltaGrid abstract execution model as a foundation for building a semantically robust execution environment for concurrent processes executing over Delta-Enabled Grid Services (DEGS). A DEGS is a Grid Service with an enhanced capability to capture incremental data changes, known as deltas, associated with service execution in the context of global processes. The abstract model contains a service composition model that provides multi-level protection against service execution failure, thus maximizing the forward recovery of a process. The model also contains a process recovery model to handle the possible impact caused by failure recovery of a process on other concurrently executing processes using data dependencies derived from a global execution history and using user-defined correctness criteria. This paper presents the abstract execution model and demonstrates its use. We also outline future directions for incorporating application exception handling and build a simulation framework for the DeltaGrid system.", "authors": ["Yang Xiao 0003", "Susan Darling Urban", "Ning Liao"], "n_citation": 0, "title": "The DeltaGrid Abstract Execution Model : Service Composition and Process Interference Handling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bf4cda08-1a02-4a45-ac9e-a70f42e58e07"}
{"abstract": "In this paper, we propose a variant of Tardos code which is practical for various applications against a small number of pirates. As an example of our results, for c = 5, the code length becomes only 1500 log(1/\u2208) bits while the conventional Tardos code requires 2500 log(1/\u2208) bits, where e is a security parameter. Furthermore our codes do not need a continuous distribution which is needed to construct the original Tardos codes. Our codes are based on a simple random variable drawn from a small set. It implies that it makes to implement and to perform a simulation extremely easier than the original one.", "authors": ["Manabu Hagiwara", "Goichiro Hanaoka", "Hideki Imai"], "n_citation": 0, "title": "A Short Random Fingerprinting Code Against a Small Number of Pirates", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c0398d28-7457-442a-8759-2f3753b9da77"}
{"abstract": "Many automotive electronic systems are safety related and therefore need to be developed using a safety process. A preliminary hazard analysis, PHA, is one of the first and vital steps in such a process. In this paper, two methods with different approaches are experimentally evaluated using an electrical steering column lock system. The two methods are an adapted FFA, functional failure analysis, method based on induction with generic failure modes and a method from ESA based on induction with generic low level hazards. In the evaluation, interviews and questionnaires are used to triangulate the results. Both methods are found to be applicable for hazard identification in the automotive system context. The experiments conducted also show, with statistical significance, that the adapted FFA method is less time consuming and easier to use than the ESA method. Hence, the FFA method is found to be more suitable for hazard identification in early phases of development in this context.", "authors": ["Fredrik Tomer", "Per Johannessen", "Peter \u00d6hman"], "n_citation": 0, "title": "Assessment of Hazard Identification Methods for the Automotive Domain", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c1441383-70b4-46b3-9423-1d5f2a800187"}
{"abstract": "In many image analysis applications there is a need to extract curves in noisy images. To achieve a more robust extraction, one can exploit correlations of oriented features over a spatial context in the image. Tensor voting is an existing technique to extract features in this way. In this paper, we present a new computational scheme for tensor voting on a dense field of rank-2 tensors. Using steerable filter theory, it is possible to rewrite the tensor voting operation as a linear combination of complex-valued convolutions. This approach has computational advantages since convolutions can be implemented efficiently. We provide speed measurements to indicate the gain in speed, and illustrate the use of steerable tensor voting on medical applications.", "authors": ["Em Erik Franken", "Markus van Almsick", "Peter M. J. Rongen", "Luc Florack", "Bart M. ter Haar Romeny"], "n_citation": 0, "title": "An Efficient Method for Tensor Voting Using Steerable Filters", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c1b22c9f-ffcb-4450-9090-76f6c48ffb60"}
{"abstract": "Model checking techniques have proven effective for checking a number of non-trivial concurrent object-oriented software systems. However, due to the high computational and memory costs, a variety of model reduction techniques are needed to overcome current limitations on applicability and scalability. Conventional wisdom holds that static program slicing can be an effective model reduction technique, yet anecdotal evidence is mixed, and there has been no work that has systematically studied the costs/benefits of slicing for model reduction in the context of model checking source code for realistic systems. In this paper, we present an overview of the sophisticated Indus program slicer that is capable of handling full Java and is readily applicable to interesting off-the-shelf concurrent Java programs. Using the Indus program slicer as part of the next generation of the Bandera model checking framework, we experimentally demonstrate significant benefits from using slicing as a fully automatic model reduction technique. Our experimental results consider a number of Java systems with varying structural properties, the effects of combining slicing with other well-known model reduction techniques such as partial order reductions, and the effects of slicing for different classes of properties. Our conclusions are that slicing concurrent object-oriented source code provides significant reductions that are orthogonal to a number of other reduction techniques, and that slicing should always be applied due to its automation and low computational costs.", "authors": ["Matthew B. Dwyer", "John Hatcliff", "Matthew Hoosier", "Venkatesh Prasad Ranganath", "Todd Wallentine"], "n_citation": 65, "title": "Evaluating the effectiveness of slicing for model reduction of concurrent object-oriented programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c28519b5-eff2-4644-8c2a-a95e1f8b2a24"}
{"abstract": "In this paper we revisit the classical regular expression matching problem, namely, given a regular expression R and a string Q consisting of m and n symbols, respectively, decide if Q matches one of tlie strings specified by R. We present new algorithms designed for a standard unit-cost RAM with word length \u03c9 > log n. We improve the best known time bounds for algorithms that use O(m) space, and whenever w > log 2  n, we obtain the fastest known algorithms, regardless of how much space is used.", "authors": ["Philip Bille"], "n_citation": 0, "title": "New Algorithms for Regular Expression Matching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c33e3fa1-5460-47ce-8fac-c68a69d15d7a"}
{"abstract": "Data quality issues have taken on increasing importance in recent years. In our research, we have discovered that many data quality problems are actually data misinterpretation problems - that is, problems with data semantics. In this paper, we first illustrate some examples of these problems and then introduce a particular semantic problem that we call corporate householding. We stress the importance of context to get the appropriate answer for each task. Then we propose an approach to handle these tasks using extensions to the COntext INterchange (COIN) technology for knowledge storage and knowledge processing.", "authors": ["Stuart E. Madnick"], "n_citation": 0, "title": "Oh, so that is what you meant! The interplay of data quality and data semantics", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c5180fc9-b279-4c28-bbed-d1da99b238df"}
{"abstract": "Xface, the new version of our open source, platform independent toolkit for developing 3D embodied conversational agents is presented. The toolkit currently incorporates four pieces of software. The core Xface library is for developers who want to embed 3D facial animation to their applications. XfaceEd editor provides an easy to use interface to generate MPEG-4 ready meshes from static 3D models. Xface-Player is a sample application that demonstrates the toolkit in action and XfaceClient is used as the communication controller over network.", "authors": ["Koray Balci"], "n_citation": 0, "title": "Xface : Open source toolkit for creating 3D faces of an embodied conversational agent", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c5733c1f-67ae-4fe1-b52f-81019402bf03"}
{"abstract": "This paper presents a design method for user interfaces based on some ideas from conversation analysis. The method uses interaction diagram and it is conceived to design the overall flow of conversation between the user and the computer system in an abstract way, as an architectural prolegomenon to the designer's choice of the actual interface elements that will be used.", "authors": ["Simone Santini"], "n_citation": 0, "title": "Notes for the Conceptual Design of Interfaces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c6a535e9-a888-4971-9630-2b22839b2e7c"}
{"abstract": "A standard model of nonlinear combiner generator for stream cipher system combines the outputs of several independent Linear Feedback Shift Register (LFSR) sequences using a nonlinear Boolean function to produce the key stream. Given such a model, cryptanalytic attacks have been proposed by finding the sparse multiples of the connection polynomials corresponding to the LFSRs. In this direction recently a few works are published on t-nomial multiples of primitive polynomials. We here provide further results on degree distribution of the t-nomial multiples. However, getting the sparse multiples of just a single primitive polynomial does not suffice. The exact cryptanalysis of the nonlinear combiner model depends on finding sparse multiples of the products of primitive polynomials. We here make a detailed analysis on t-nomial multiples of products of primitive polynomials. We present new enumeration results for these multiples and provide some estimation on their degree distribution.", "authors": ["Subhamoy Maitra", "Kishan Chand Gupta", "Ayineedi Venkateswarlu"], "n_citation": 50, "title": "Multiples of primitive polynomials and their products over GF(2)", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c7c6fd85-e2d7-49cf-90b8-54b7ceea533b"}
{"abstract": "This paper introduces a propositional encoding for lexicographic path orders in connection with dependency pairs. This facilitates the application of SAT solvers for termination analysis of term rewrite systems based on the dependency pair method. We address two main inter-related issues and encode them as satisfiability problems of propositional formulas that can be efficiently handled by SAT solving: (1) the combined search for a lexicographic path order together with an argument filtering to orient a set of inequalities; and (2) how the choice of the argument filtering influences the set of inequalities that have to be oriented. We have implemented our contributions in the termination prover AProVE. Extensive experiments show that by our encoding and the application of SAT solvers one obtains speedups in orders of magnitude as well as increased termination proving power.", "authors": ["Michael Codish", "Peter Schneider-Kamp", "Vitaly Lagoon", "Ren\u00e9 Thiemann", "J\u00fcrgen Giesl"], "n_citation": 0, "title": "SAT solving for argument filterings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c7dfb3b4-7c8a-4cf6-84e0-4cfe7dbbbe97"}
{"abstract": "We study the notion of safe realizability for high-level message sequence charts (HMSCs), which was introduced in [2]. We prove that safe realizability is EXPSPACE-complete for bounded HMSCs but undecidable for the class of all HMSCs. This solves two open problems from [2]. Moreover we prove that safe realizability is also EXPSPACE-complete for the larger class of transition-connected HMSCs.", "authors": ["Markus Lohrey"], "n_citation": 0, "title": "Safe realizability of high-level message sequence charts", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "cb993134-6d0a-487f-b43f-fa262ba71c5d"}
{"abstract": "In Asiacrypt 2003, the notion of the universal designated verifier signature (UDVS) was put forth by Steinfeld, Bull, Wang and Pieprzyk. In the new paradigm, any signature holder (not necessarily the signer) can designate the standard signature to any desired designated verifier (using the verifier's public key), such that only the designated verifier will believe that the signature holder holds a valid standard signature, and hence, believe that the signer has indeed signed the message. When the signature holder is the signer himself, the UDVS scheme can be considered as a designated verifier signature (DVS) which was proposed by Jakobsson, Sako and Impagliazzo in Eurocrypt 1996. In the recent paper published in ICALP 2005, Lipmaa, Wang and Bao introduced a new security property, called non-delegatability, as an essential property of (universal) designated verifier signature. Subsequently, Li, Lipmaa and Pei used this new property to attack four designated verifier signatures in ICICS 2005 and showed that none of them satisfy the required property. To date, there is no UDVS scheme that does not suffer from the delegatability problem. In this paper, we propose the first provably secure UDVS without delegatability, which can also be regarded as another DVS scheme without delegatability. We also refine the models of the UDVS schemes and introduce the notion of the strong universal designated verifier signature (SUDVS). We believe that the model itself is of an independent interest.", "authors": ["Xinyi Huang", "Willy Susilo", "Yi Mu", "Wei Wu"], "n_citation": 0, "title": "Universal designated verifier signature without delegatability", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cd5c31d0-d467-4548-b69c-a583846f9eef"}
{"abstract": "We address the problems of combining satisfiability procedures and consider two combination scenarios: (i) the combination within the class of rewriting-based satisfiability procedures and (ii) the Nelson-Oppen combination of rewriting-based satisfiability procedures and arbitrary satisfiability procedures. In each scenario, we use meta-saturation, which schematizes saturation of the set containing the axioms of a given theory and an arbitrary set of ground literals, to syntactically decide sufficient conditions for the combinability of rewriting-based satisfiability procedures. For (i), we give a sufficient condition for the modular termination of meta-saturation. When meta-saturation for the union of theories halts, it yields a rewriting-based satisfiability procedure for the union. For (ii), we use meta-saturation to prove the stable infiniteness of the component theories and deduction completeness of their rewriting-based satisfiability procedures. These properties are important to establish the correctness of the Nelson-Oppen combination method and to obtain an efficient implementation.", "authors": ["H\u00e9l\u00e8ne Kirchner", "Silvio Ranise", "Christophe Ringeissen", "Duc-Khanh Tran"], "n_citation": 0, "title": "Automatic combinability of rewriting-based satisfiability procedures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cf4b79d1-08a1-41d3-bdc6-110e070fe7e2"}
{"abstract": "We provide a denotational trace semantics for processes with synchronous communication and a form of weakly fair parallelism. The semantics is fully abstract: processes have the same trace sets if and only if their communication behaviors are identical in all contexts. The model can easily be adapted for asynchronously communicating processes, or for shared-memory parallel programs. We also provide a partial-order semantics, using pomsets adapted for synchronization and our form of fairness. The pomset semantics can also be adjusted to model alternative paradigms. The traces of a process can be recovered from the pomset semantics by taking all fair interleavings consistent with the partial order.", "authors": ["Stephen Brookes"], "n_citation": 50, "title": "Traces, pomsets, fairness and full abstraction for communicating processes", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "cff61b04-3aff-41c7-9633-0441ddaa22e2"}
{"abstract": "We show how game semantics, counterexample-guided abstraction refinement, assume-guarantee reasoning and the L* algorithm for learning regular languages can be combined to yield a procedure for compositional verification of safety properties of open programs. Game semantics is used to construct accurate models of subprograms compositionally. Overall model construction is avoided using assume-guarantee reasoning and the L* algorithm, by learning assumptions for arbitrary subprograms. The procedure has been implemented, and initial experimental results show significant space savings.", "authors": ["Aleksandar S. Dimovski", "Ranko Lazic"], "n_citation": 0, "title": "Assume-Guarantee Software Verification Based on Game Semantics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d1dfac33-8d10-43ea-b6f9-f27dde080c25"}
{"abstract": "HOL-TestGen is a test environment for specification-based unit testing build upon the proof assistant Isabelle/HOL. While there is considerable skepticism with regard to interactive theorem provers in testing communities, we argue that they are a natural choice for (automated) symbolic computations underlying systematic tests. This holds in particular for the development on non-trivial formal test plans of complex software, where some parts of the overall activity require inherently guidance by a test engineer. In this paper, we present the underlying methods for both black box and white box testing in interactive unit test scenarios. HOL-TestGen can also be understood as a unifying technical and conceptual framework for presenting and investigating the variety of unit test techniques in a logically consistent way.", "authors": ["Achim D. Brucker", "Burkhart Wolff"], "n_citation": 0, "title": "Interactive Testing with HOL-TestGen", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d2eac602-700c-4ad1-9437-76da33f8e997"}
{"abstract": "We give a transparent characterization, by means of a certain syntactic semigroup, of regular languages possessing the finite power property. Then we use this characterization to obtain a short elementary proof for the uniform decidability of the finite power property for rational languages in all monoids defined by a confluent regular system of deletion rules. This result in particular covers the case of free groups solved earlier by d'Alessandro and Sakarovitch by means of an involved reduction to the boundedness problem for distance automata.", "authors": ["Michal Kunc"], "n_citation": 0, "title": "Algebraic Characterization of the Finite Power Property", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d44d69f4-e34c-4240-8bed-4e19a65d3822"}
{"abstract": "One of the main limitation when accessing the web is the lack of explicit schema about the logical organization of web pages/sites, whose presence may help in understanding data semantics. Here, an approach to extract a logical schema from web pages based on HTML source code analysis is presented. We define a set of primary tags actually used to give a structural/logical backbone to the page. Primary tags are used to divide the page into collections, which represent distinct structural page sections; these are finally mapped into logical sections according to their semantics, providing a logical page schema. The structuring methodology is applied to some real web pages to test the approach.", "authors": ["Vincenza Carchiolo", "Alessandro Longheu", "Michele Malgeri"], "n_citation": 0, "title": "Hidden schema extraction in web documents", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d4fcd9b3-c967-4784-9930-e9b0f090d395"}
{"abstract": "Given a sequence of n bits with binary zero-order entropy H 0 , we present a dynamic data structure that requires nH 0  + o(n) bits of space, which is able of performing rank and select, as well as inserting and deleting bits at arbitrary positions, in O(logn) worst-case time. This extends previous results by Hon et al. [ISAAC 2003] achieving 0(log n/log log n) time for rank and select but \u03b8(polylog(n)) amortized time for inserting and deleting bits, and requiring n + o(n) bits of space; and by Raman et al. [SODA 2002] which have constant query time but a static structure. In particular, our result becomes the first entropy-bound dynamic data structure for rank and select over bit sequences. We then show how the above result can be used to build a dynamic full-text self-index for a collection of texts over an alphabet of size a, of overall length n and zero-order entropy H 0 . The index requires nH 0  + o(n log \u03c3) bits of space, and can count the number of occurrences of a pattern of length m in time O(m log n log \u03c3). Reporting the occ occurrences can be supported in O(occlog 2  n log \u03c3) time, paying O(n) extra space. Insertion of text to the collection takes O(log n log \u03c3) time per symbol, which becomes O(log 2  n log a) for deletions. This improves a previous result by Chan et al. [CPM 2004]. As a consequence, we obtain an O(n log n log \u03c3) time construction algorithm for a compressed self-index requiring nH 0  + o(n log a) bits working space during construction.", "authors": ["Veli M\u00e4kinen", "Gonzalo Navarro"], "n_citation": 0, "title": "Dynamic entropy-compressed sequences and full-text indexes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d507662d-1365-4103-a5a3-0a3e743da1ec"}
{"abstract": "The 'engineering' and 'adaptive' approaches to system production are distinguished. It is argued that producing reliable self-organised software systems (SOSS) will necessarily involve considerable use of adaptive approaches. A class of apparently simple multi-agent systems is defined, which however has all the power of a Turing machine, and hence is beyond formal specification and design methods (in general). It is then shown that such systems can be evolved to perform simple tasks. This highlights how we may be faced with systems whose workings we have not wholly designed and hence that we will have to treat them more as natural science treat the systems it encounters, namely using the classic experimental method. An example is briefly discussed. A system for annotating such systems with hypotheses, and conditions of application is proposed that would be a natural extension of current methods of open source code development.", "authors": ["Bruce Edmonds"], "n_citation": 0, "title": "Using the experimental method to produce reliable self-organised systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d5723def-893e-4130-850c-a8b40e7adf91"}
{"abstract": "The composition of elementary web services to larger-scale services has become an important means to enhance e-business collaborations. If such composite web services can also integrate legacy components that are not yet provided as web services, the number of possible compositions is increased. Following a process-oriented approach, the compositions can be described as control- and data-flow between available web services and components. This paper discusses the Business Process Execution Language for Web Services (BPEL4WS), an existing service composition language, and proposes UML-WSC as an alternative, visual language. For the advanced description of service interfaces, UML-WSC extends the type system of the established Web Service Definition Language (WSDL).", "authors": ["Sebastian Th\u00f6ne", "Ralph Depke", "Gregor Engels"], "n_citation": 0, "title": "Process-oriented, flexible composition of Web Services with UML", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d5a436fc-4330-41d3-ae1b-7228709c91dd"}
{"abstract": "Sized types provides a type-based mechanism to enforce termination of recursive definitions in typed A-calculi. Previous work has provided strong indications that type-based termination provides an appropriate foundation for proof assistants based on type theory; however, most work to date has been confined to non-dependent type systems. In this article, we introduce a variant of the Calculus of Inductive Constructions with sized types and study its meta theoretical properties: subject reduction, normalization, and thus consistency and decidability of type-checking and of size-inference. A prototype implementation has been developed alongside case studies.", "authors": ["Gilles Barthe", "Benjamin Gr\u00e9goire", "Femando Pastawski"], "n_citation": 0, "title": "CIC : Type-based termination of recursive definitions in the calculus of inductive constructions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d9696c63-a877-4735-90ca-37838b9ab797"}
{"abstract": "We study the problem of adding edges to a given arbitrary graph so that the resulting graph is a split graph, called a split completion of the input graph. Our purpose is to add an inclusion minimal set of edges to obtain a minimal split completion, which means that no proper subset of the added edges is sufficient to create a split completion. Minimal completions of arbitrary graphs into chordal graphs have been studied previously, and new results have been added continuously. There is an increasing interest in minimal completion problems, and minimal completions of arbitrary graphs into interval graphs have been studied very recently. We extend these previous results to split graphs, and we give a characterization of minimal split completions, along with a linear time algorithm for computing a minimal split completion of an arbitrary input graph. Among our results is a new way of partitioning the vertices of a split graph uniquely into three subsets.", "authors": ["Pinar Heggernes", "Federico Mancini"], "n_citation": 0, "title": "Minimal split completions of graphs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "daebb28f-0aa7-4728-88d3-5b56ba1234eb"}
{"abstract": "In this paper we propose a methodological approach for the development of XML databases. Our proposal is framed in MIDAS, a model driven methodology for the development of Web Information Systems (WISs) based on the Model Driven Architecture (MDA) proposed by the Object Management Group (OMG), So, in this framework, the proposed data Platform Independent Model (PIM) is the conceptual data model and the data Platform Specific Model (PSM) is the XML Schema model. Both of them will be represented in UML, therefore we also summarize in this work an extension to UML for XML Schema. Moreover, we define the mappings to transform the data PIM into the data PSM, which will be the XML database schema. The development process of the XML database will be shown by means of a case study: a WIS for the management of medical images stored in the XML DB of Oracle.", "authors": ["Bel\u00e9n Vela", "C\u00e9sar J. Acu\u00f1a", "Esperanza Marcos"], "n_citation": 0, "title": "A Model Driven approach for XML database development", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "db33246e-9b1f-4ed0-830f-80499bc9f5c7"}
{"abstract": "The Variational Bayesian learning, proposed as an approximation of the Bayesian learning, has provided computational tractability and good generalization performance in many applications. However, little has been done to investigate its theoretical properties. In this paper, we discuss the Variational Bayesian learning of the mixture of exponential families and derive the asymptotic form of the stochastic complexities. We show that the stochastic complexities become smaller than those of regular statistical models, which implies the advantage of the Bayesian learning still remains in the Variational Bayesian learning. Stochastic complexity, which is called the marginal likelihood or the free energy, not only becomes important in addressing the model selection problem but also enables us to discuss the accuracy of the Variational Bayesian approach as an approximation of the true Bayesian learning.", "authors": ["Kazuho Watanabe", "Sumio Watanabe"], "n_citation": 0, "title": "Stochastic complexity for mixture of exponential families in variational bayes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "de4b557b-4030-494c-9124-a064348f8bd4"}
{"abstract": "This paper focuses on the resource management technology of large-scale distributed simulation based on grid service. As a data grid, Aegis is established to provide computing service environment and uniform view of various data resources for simulation applications. This paper starts from demands of distributed simulation, introduces the architecture of Aegis simulation grid, discusses the realization of main modules, and establishes a platform of resource management oriented to distributed simulation. Finally, it describes the design and implementation of simulation computing service with the example of tactical notification based on strategy rules, and makes experiments of distributed simulation supported by Aegis simulation grid.", "authors": ["Wei Wu", "Zhong Zhou", "Shaofeng Wang", "Qinping Zhao"], "n_citation": 0, "title": "Aegis: A simulation grid oriented to large-scale distributed simulation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "de9b5a77-64af-4b8d-b08c-b8a4755cd2b6"}
{"abstract": "This paper provides an algorithm to compute the complement of tree languages recognizable with A-TA (tree automata with associativity axioms [16]). Due to this closure property together with the previously obtained results, we know that the class is boolean closed, while keeping recognizability of A-closures of regular tree languages. In the proof of the main result, a new framework of tree automata, called sequence-tree automata, is introduced as a generalization of Lugiez and Dal Zilio's multi-tree automata [14] of an associativity case. It is also shown that recognizable A-tree languages are closed under a one-step rewrite relation in case of ground A-term rewriting. This result allows us to compute an under-approximation of A-rewrite descendants of recognizable A-tree languages with arbitrary accuracy.", "authors": ["Hitoshi Ohsaki", "Hiroyuki Seki", "Toshinori Takai"], "n_citation": 0, "title": "Recognizing boolean closed A-tree languages with membership conditional rewriting mechanism", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "dff59bb5-5ac2-412e-bc7b-545e9d393c0a"}
{"abstract": "In this paper, an improved fair exchange protocol for e-commerce has been presented. The new protocol builds upon a previously published one by Ray & Ray [12]. After discussing the protocol and related fairness, four weaknesses have been found, viz., Fairness Problem: Advantage Merchant, Fairness Problem: Advantage Customer, Possibility of Man in the Middle Attack and Possibility of Malicious Bank. Our modified fair exchange e-commerce protocol is able to handle these weaknesses. The proposed protocol does not involve third party to ensure the fairness.", "authors": ["Debajyoti Konar", "Chandan Mazumdar"], "n_citation": 0, "title": "An improved e-commerce protocol for fair exchange", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e07247e1-eba0-43c3-8bed-480d66d057a6"}
{"abstract": "Active learning aims at reducing the number of training examples to be labeled by automatically processing the unlabeled examples, then selecting the most informative ones with respect to a given cost function for a human to label. The major problem is to find the best selection strategy function to quickly reach high classification accuracy. Query-by-Committee (QBC) method of active learning is less computation than other active learning approaches, but its classification accuracy can not achieve the same high as passive learning. In this paper, a new selection strategy for the QBC method is presented by combining Vote Entropy with Kullback-Leibler divergence. Experimental results show that the proposed algorithm is better than previous QBC approach in classification accuracy. It can reach the same accuracy as passive learning with few labeled training examples.", "authors": ["Yue Zhao", "Ciwen Xu", "Yongcun Cao"], "n_citation": 0, "title": "Research on Query-by-Committee Method of Active Learning and Application", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e224b84c-91ed-4878-b24e-44e361e022b6"}
{"abstract": "We compare two heuristic approaches, evolutionary computation and ant colony optimisation, and a complete tree-search approach, constraint programming, for solving binary constraint satisfaction problems. We experimentally show that, if evolutionary computation is far from being able to compete with the two other approaches, ant colony optimisation nearly always succeeds in finding a solution, so that it can actually compete with constraint programming. The resampling ratio is used to provide insight into heuristic algorithms performances. Regarding efficiency, we show that if constraint programming is the fastest when instances have a low number of variables, ant colony optimisation becomes faster when increasing the number of variables.", "authors": ["Jano van Hemert", "Christine Solnon"], "n_citation": 0, "title": "A study into ant colony optimisation, evolutionary computation and constraint programming on binary constraint satisfaction problems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e4a14280-c025-4198-b452-fed66075e648"}
{"abstract": "In the mobile web environment, multimedia contents adaptation services should be available through supporting various kinds of devices, network abilities and users' preferences. If a new user wants to stream multimedia contents in a server through a new kind of terminal device, it should be considered whether the existing transcoders are able to adapt the multimedia contents. However, the existing libraries for multimedia adaptation have heavy transcoder figures which include all adaptive functions in one library. The challenge of universal access is too complex to be solved with these all-in-one solutions. In this paper, an application-independent multimedia adaptation framework which meets the QoS of new and varied mobile devices is suggested. This framework is composed of a group of unit transcoders having only one transcoding function respectively, instead of heavy transcoders. Also, the transcoder manager supporting the dynamic connections of the unit transcoders to satisfy end to end QoS is included.", "authors": ["Sungmi Chon", "Young-Hwan Lim", "Kyujung Kim"], "n_citation": 0, "title": "An Application-Independent Multimedia Adaptation Framework for the Mobile Web", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e69c7304-b97d-4088-a2af-1b15b3be77a5"}
{"abstract": "Transitive propagation along properties can be modelled in various ways in the OWL description logic. Doing so allows existing description logic reasoners based on the tableaux algorithm to make inferences based on such transitive constructs. This is espectially useful for medical knowledge bases, where such constructs are common. This paper compares, contrasts and evaluates a variety of different methods for simulating transitive propagation: property subsumption, classic SEP triples and adapted SEP triples. These modelling techniques remove the need to extending the OWL language with additional operators in order to express the transitive propagation. Other approaches require an extended tableaux reasoner or first-order logic prover, as well as a modification of the OWL standard. The adapted SEP triples methodology is ultimately recommended as the most reliable modelling technique.", "authors": ["Julian Seidenberg", "Alan L. Rector"], "n_citation": 0, "title": "Representing Transitive Propagation in OWL", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e8bb470f-410a-4400-8d12-1d3d379676b5"}
{"abstract": "This paper examines the case for using self organizing systems in elements of infrastructures for the operation of Agents, Autonomous Agents and Service Orientated Systems. An overview of the use of self organizing systems in different elements of the infrastructures of an agent based system is provided. There is also a discussion of the ways in which these techniques can be brought into the mainstream of agent implementation and deployment.", "authors": ["Simon Thompson"], "n_citation": 0, "title": "Toward the application of self organizing systems to agent infrastructures: A critical review and prospects", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ea005b43-63f4-4b81-94a9-937d48e93cda"}
{"authors": ["Jan Treur", "Vu1010095", "Faculteit der Exacte Wetenschappen"], "n_citation": 0, "title": "A Virtual Human Agent Model with Behaviour Based on Feeling Exhaustion", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "eb1864fb-88fd-434c-96da-341417b12b97"}
{"abstract": "Formal analysis of cryptographic protocols has mainly concentrated on protocols with closed-ended data structures, where closed-ended data structure means that the messages exchanged between principals have fixed and finite format. However, in many protocols the data structures used are open-ended, i.e., messages have an unbounded number of data fields. Formal analysis of protocols with open-ended data structures is one of the challenges pointed out by Meadows. This work studies decidability issues for such protocols. We propose a protocol model in which principals are described by transducers, i.e., finite automata with output, and show that in this model security is decidable and PSPACE-hard in presence of the standard Dolev-Yao intruder.", "authors": ["Ralf K\u00fcsters"], "n_citation": 0, "title": "On the decidability of cryptographic protocols with open-ended data structures", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ef3ca702-d088-43d9-8e12-ba2dd46723dd"}
{"authors": ["Miko\u0142aj Boja\u0144czyk", "Tomasz Idziaszek"], "n_citation": 0, "title": "Algebra for Infinite Forests with an Application to the Temporal Logic EF", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "f0f5e1b0-5fa3-4e3e-a0c3-d471701c33df"}
{"abstract": "We propose a new copy-paste user interface for 3D geometry based on repetitive patterns. The system, guided by the user, analyzes patterns of repetition in the source geometry and then pastes the geometry while increasing or decreasing the number of repetitions using scaling and deformation, which is controlled by two freehand strokes called handles. The system has two main advantages over existing methods: the entire copy-paste operation is controlled by the user's stroke input and thus can be specified easily without explicitly adjusting the parameters, and splitting the shape information into source geometry and handles can not only significantly reduce the amount of data required but also quickly change a scene's appearance while keeping its structure consistent.", "authors": ["Shigeru Owada", "Frank Nielsen", "Takeo Igarashi"], "n_citation": 0, "title": "Copy-paste synthesis of 3D geometry with repetitive patterns", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f185b441-73ea-4e55-9af0-758fbedf8446"}
{"abstract": "Skyline query processing is fundamental to many applications including multi-criteria decision making. In this paper, we will present an optimal algorithm for computing skyline in the two dimensional space. The algorithm has the progressive nature and adopts the divide-conquer paradigm. It can be shown that our algorithm achieves the minimum I/O costs, and is more efficient and scalable than the existing techniques. The experiment results demonstrated that our algorithm greatly improves the performance of the existing techniques.", "authors": ["Hai-Xin Lu", "Yi Luo", "Xuemin Lin"], "n_citation": 0, "title": "An optimal divide-conquer algorithm for 2D skyline queries", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f222f643-79a0-4222-af9e-bf5010b6feb6"}
{"abstract": "With the convergence of information communication and broadcasting technology, DMB (Digital Multimedia Broadcasting) becomes the hottest issue of the next generation in the new media industry and the communications market. It is possible for people to watch seamless TV broadcast using DMB in motion, by adding video transportation specification into Eureka-147 of European digital radio broadcasting standard. DMB can select the preferred channel by using EPG (Electronic Program Guide). Since the EPG information takes a role of baseline which provides whole guidelines and services to users, its update should be performed constantly. To support scalability and compatibility on DMB, XML-based EPG and binary EPG have been proposed for the establishment of standard and these EPGs are being updated daily or weekly. However, this update period is too long to apply changes on time and also makes the update process take long time. To solve this problem, this paper proposes and implements efficient EPG system which maximizes the efficiency of EPG specification, by updating EPG of binary format per period of an hour and applying Naming Rule.", "authors": ["Minju Cho", "Jun Hwang", "Gyung-Leen Park", "Jung-Uk Kim", "Taeuk Jang", "Oh Juhyun", "Young Seok Chae"], "n_citation": 0, "title": "A Study on the Transportation Period of the EPG Data Specification in Terrestrial DMB", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f49e51e3-661b-456b-81f0-e2a9261b9bd1"}
{"abstract": "Histograms of local appearance descriptors are a popular representation for visual recognition. They are highly discriminant and have good resistance to local occlusions and to geometric and photometric variations, but they are not able to exploit spatial co-occurrence statistics at scales larger than their local input patches. We present a new multilevel visual representation, 'hyperfeatures', that is designed to remedy this. The starting point is the familiar notion that to detect object parts, in practice it often suffices to detect co-occurrences of more local object fragments - a process that can be formalized as comparison (e.g. vector quantization) of image patches against a codebook of known fragments, followed by local aggregation of the resulting codebook membership vectors to detect co-occurrences. This process converts local collections of image descriptor vectors into somewhat less local histogram vectors - higher-level but spatially coarser descriptors. We observe that as the output is again a local descriptor vector, the process can be iterated, and that doing so captures and codes ever larger assemblies of object parts and increasingly abstract or 'semantic' image properties. We formulate the hyperfeatures model and study its performance under several different image coding methods including clustering based Vector Quantization, Gaussian Mixtures, and combinations of these with Latent Dirichlet Allocation. We find that the resulting high-level features provide improved performance in several object image and texture image classification tasks.", "authors": ["Ankur Agarwal", "Bill Triggs"], "n_citation": 0, "title": "Hyperfeatures : Multilevel Local Coding for Visual Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f621ba3c-ff08-4968-8a88-edc834f82d89"}
{"abstract": "In this paper, adaptation of 3D graphics on the basis of the MPEG-21 DIA (Digital Item Adaptation) standard is presented. The MPEG-21 DIA standard covers a general usage environment description framework for adapting multimedia contents including computer graphics. The adaptation parameters for the 3D graphics consist of user characteristics and terminal capability. The user characteristics parameters represent the user's quality preferences on graphics components of geometry, material and animation as well as 3D to 2D conversion preference. The terminal capability parameters represent the graphics format and the graphics processing capacity of vertex processing rate, fill rate and memory bandwidth. MPEG-21 3D graphics DIA adaptation system adapts 3D graphics to these usage environment parameters input from the user for the best experience of the graphics contents.", "authors": ["Haekwang Kim", "Nam-Yeol Lee", "Jinwoong Kim"], "n_citation": 0, "title": "3D graphics adaptation system on the basis of MPEG-21 DIA", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f699686c-dca8-4b0b-a789-38ae8c6babb7"}
{"authors": ["Sergey V. Avgustinovich", "Anna E. Frid", "Svetlana Puzynina"], "n_citation": 0, "title": "Ergodic infinite permutations of minimal complexity", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "f709c432-591e-4191-94ab-15865ccd0dc7"}
{"abstract": "Automatic structures are countable structures finitely presentable by a collection of automata. We study questions related to properties invariant with respect to the choice of an automatic presentation. We give a negative answer to a question of Rubin concerning definability of intrinsically regular relations by showing that order-invariant first-order logic can be stronger than first-order logic with counting on automatic structures. We introduce a notion of equivalence of automatic presentations, define semi-synchronous transductions, and show how these concepts correspond. Our main result is that a one-to-one function on words preserves regularity as well as non-regularity of all relations iff it is a semi-synchronous transduction. We also characterize automatic presentations of the complete structures of Blumensath and Gradel.", "authors": ["Vince B\u00e1r\u00e1ny"], "n_citation": 0, "title": "Invariants of automatic presentations and semi-synchronous transductions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f924b6a5-dea5-4dcb-afa4-0fff88298cee"}
{"abstract": "Lossy VASS (vector addition systems with states) are defined as a subclass of VASS in analogy to lossy FIFO-channel systems. They can be used to model concurrent systems with unreliable communication. We analyze the decidability of model checking problems for lossy systems and several branching-time and linear-time temporal logics. We present an almost complete picture of the decidability of model checking for normal VASS, lossy VASS and lossy VASS with test for zero.", "authors": ["Ahmed Bouajjani", "Richard Mayr"], "n_citation": 0, "title": "Model checking lossy vector addition systems", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "f9753da2-3e54-4071-85be-91579f585a5f"}
{"abstract": "For decades, the algorithm providing the smallest proven worst-case running time (with respect to the number of terminals) for the Steiner tree problem has been the one by Dreyfus and Wagner. In this paper, a new algorithm is developed, which improves the running time from O(3 k n+2 k n 2 +n 3 ) to (2+\u03b4) k .poly(n) for arbitrary but fixed \u03b4 > 0. Like its predecessor, this algorithm follows the dynamic programming paradigm. Whereas in effect the Dreyfus-Wagner recursion splits the optimal Steiner tree in two parts of arbitrary sizes, our approach looks for a set of nodes that separate the tree into parts containing only few terminals. It is then possible to solve an instance of the Steiner tree problem more efficiently by combining partial solutions.", "authors": ["Daniel M\u00f6lle", "Stefan Richter", "Peter Rossmanith"], "n_citation": 0, "title": "A faster algorithm for the steiner tree problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fa2425c3-412c-4e6d-aa0c-b4c46cd4dfb3"}
{"abstract": "This short paper introduces the issues and challenges of next generation Java-based smart card platforms. Betting on a continuous evolution toward open computing devices, next generation cards will consist in embedded Java micro-server platforms. Those platforms will be able to serve various types of services and applications thanks to two important system features: adaptability and maintainability. Two features that have to be carefully taken into account in the research perspectives described in this paper: real Java for cards, cards integration in a networked world, and flexible and adaptable cards.", "authors": ["Jean-Jacques Vandewalle"], "n_citation": 50, "title": "Smart card research perspectives", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fb218e3c-73a5-4b1e-97bb-a4b0d9dbad29"}
{"abstract": "When using virtual characters in the human-computer interface the question arises of how useful this kind of interface is: whether the human user accepts, enjoys and profits from this form of interaction. Thorough system evaluations, however, are rarely done. We propose a post-questionnaire evaluation for a virtual character system that we apply to COHIBIT, an interactive museum exhibit with virtual characters. The evaluation study investigates the subjects' experiences with the exhibit with regard to informativeness, entertainment and virtual character perception. Our subjects rated the exhibit both entertaining and informative and gave it a good overall mark. We discuss the detailed results and identify useful factors to consider when building and evaluating virtual character applications.", "authors": ["Michael Kipp", "Kerstin H. Kipp", "Alassane Ndiaye", "Patrick Gebhard"], "n_citation": 50, "title": "Evaluating the tangible interface and virtual characters in the interactive COHIBIT exhibit", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fbe030be-cbca-4b45-9574-46f2d8948d13"}
{"abstract": "As shown in recent researches, in a distribution system, ignoring routes when locating depots may overestimate the overall system cost. The Location Routing Problem (LRP) overcomes this drawback dealing simultaneously with location and routing decisions. This paper presents a memetic algorithm with population management (MA|PM) to solve the LRP with capacitated routes and depots. MA|PM is a very recent form of memetic algorithm in which the diversity of a small population of solutions is controlled by accepting a new solution if its distance to the population exceeds a given threshold. The method is evaluated on three sets of instances, and compared to other heuristics and a lower bound. The preliminary results are quite promising since the MA|PM already finds the best results on several instances.", "authors": ["Christian Prins", "Caroline Prodhon", "Roberto Wolfler Calvo"], "n_citation": 125, "title": "A memetic algorithm with population management (MA|PM) for the capacitated location-routing problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fbfb9202-dbe4-43a7-b311-f5e2d78f154b"}
{"abstract": "This paper analyses the role of ontological commitment in structuring the requirements for the PSIM Environment. This environment aims to support (i) the sharing and the exchange of knowledge between the different actors involved in the design or redesign of a manufacturing enterprise; and (ii) the exchange of information between tools supporting enterprise analysis according to different perspectives (logistic, technologic and human). The techniques for piecemeal ontological commitment are related to two contributions from research on enterprise reference architectures: (i) the dimension of genericity of the ENV 40003 reference architecture and (ii) the relationships between lifecycles of enterprise entities as defined in GERAM. Two kinds of applications illustrate the ontological commitments: support for the interoperation and communication between applications; and the provision of task-specific interfaces to users working in an enterprise.", "authors": ["Jan Goossenaerts", "Christine Pelletier"], "n_citation": 50, "title": "Ontological commitment for participative simulation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fe2daff9-28fc-4948-8b85-6432c2cc4bae"}
{"abstract": "In presence of Grid where a huge amount of databases can be involved in sharing cycle, database tools and middlewares should be well suited for schema mediation and query processing in a semantically meaningful way. Dart is an implemented prototype system whose goal is to provide a semantic solution capable of deployment in grid settings. This paper particularly concerns the problems of semantic query answering in DartGrid. Our first contribution is a semantically enriched query language, called Q3, for specifying complex queries using RDF/OWL semantic. We describe its implementation in Dart Database Grid, and also introduce a semantic interface for user to graphically browse RDF semantic, and visually construct Q3 queries.", "authors": ["Huajun Chen", "Zhaohui Wu", "Yuxing Mao"], "n_citation": 0, "title": "Q3: A semantic query language for dart database Grid", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "feb27157-b797-49e6-9607-f9e113c26b8a"}
{"abstract": "Word frequencies play important roles in a variety of NLP-related applications. Word frequency estimation for Chinese is a big challenge due to characteristics of Chinese, in particular word-formation and word segmentation. This paper concerns the issue of word frequency estimation in the condition that we only have a Chinese wordlist and a raw Chinese corpus with arbitrarily large size, and do not perform any manual annotation to the corpus. Several realistic schemes for approximating word frequencies under the framework of STR (frequency of string of characters as an approximation of word frequency) and MM (Maximal matching) are presented. Large-scale experiments indicate that the proposed scheme, MinMaxMM, can significantly benefit the estimation of word frequencies, though its performance is still not very satisfactory in some cases.", "authors": ["Maosong Sun", "Zhengcao Zhang", "Benjamin K. Tsou", "Huaming Lu"], "n_citation": 0, "title": "Word frequency approximation for chinese without using manually-annotated corpus", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "feef73e9-acf8-45fa-93f1-f077f940368a"}
{"abstract": "The explosive growth of the Internet has fuelled the creation of new and exciting information services. Most of the current technology has been designed for desktop and larger computers with medium to high bandwidth and generally reliable data networks. On the other hand, hand-held wireless devices provide a much more constrained and poor computing environment compared to desktop computers. That is why wireless users rarely (if ever) benefit from Internet information services. Yet the trend and interest for wireless services is growing with fast pace. Personalization comes into aid by directly toning down factors that break up the functionally of the Internet services when viewed through wireless devices; factors like the click count, user response time and the size of the wireless network traffic. In this paper we present a flexible personalization system tuned for the wireless Internet. The system utilizes the various characteristics of mobile agents to support flexibility, scalability, modularity and user mobility.", "authors": ["George Samaras", "Christoforos Panayiotou"], "n_citation": 50, "title": "A flexible personalization architecture for wireless Internet based on mobile agents", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ffddaafc-fef2-4a44-9798-df2f57289281"}
{"abstract": "This paper describes the Software Criticality Analysis (SCA) approach that was developed to support the justification of commercial off-the-shelf software (COTS) used in a safety-related system. The primary objective of SCA is to assess the importance to safety of the software components within the COTS and to show there is segregation between software components with different safety importance. The approach taken was a combination of Hazops based on design documents and on a detailed analysis of the actual code (100kloc). Considerable effort was spent on validation and ensuring the conservative nature of the results. The results from reverse engineering from the code showed that results based only on architecture and design documents would have been misleading.", "authors": ["Peter G. Bishop", "Robin E. Bloomfield", "Tim Clement", "Sofia Guerra"], "n_citation": 0, "title": "Software criticality analysis of COTS/SOUP", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ffdf76f8-5b64-486e-8f2e-c401b0b0006f"}
{"abstract": "We study two-way tree automata modulo equational theories. We deal with the theories of Abelian groups (ACUM), idempotent commutative monoids (ACUI), and the theory of exclusive-or (ACUX), as well as some variants including the theory of commutative monoids (ACU). We show that the one-way automata for all these theories are closed under union and intersection, and emptiness is decidable. For two-way automata the situation is more complex. In all these theories except ACUI, we show that two-way automata can be effectively reduced to one-way automata, provided some care is taken in the definition of the so-called push clauses. (The ACUI case is open.) In particular, the two-way automata modulo these theories are closed under union and intersection, and emptiness is decidable. We also note that alternating variants have undecidable emptiness problem for most theories, contrarily to the non-equational case where alternation is essentially harmless.", "authors": ["Kumar Neeraj Verma"], "n_citation": 0, "title": "Two-way equational tree automata for AC-like theories: Decidability and closure properties", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "01d36730-07c0-402c-b438-ee9378a004b4"}
{"abstract": "For two strings a, b, the longest common subsequence (LCS) problem consists in comparing a and b by computing the length of their LCS. In a previous paper, we defined a generalisation, called the all semi-local LCS problem, for which we proposed an efficient output representation and an efficient algorithm. In this paper, we consider a restriction of this problem to strings that are permutations of a given set. The resulting problem is equivalent to the all local longest increasing subsequences (LIS) problem. We propose an algorithm for this problem, running in time O(n 1.5 ) on an input of size n. As an interesting application of our method, we propose a new algorithm for finding a maximum clique in a circle graph on n nodes, running in the same asymptotic time O(n 1.5 ). Compared to a number of previous algorithms for this problem, our approach presents a substantial improvement in worst-case running time.", "authors": ["Alexander Tiskin"], "n_citation": 0, "title": "Longest common subsequences in permutations and maximum cliques in circle graphs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "021bfd12-79e4-43df-810c-bf2f194ad72c"}
{"abstract": "Variable Neighbourhood Search (VNS) is one of the most recent metaheuristics used for solving combinatorial optimization problems in which a systematic change of neighbourhood within a local search is carried out. In this paper, a variable neighbourhood search algorithm is proposed for Job Shop Scheduling (JSS) problem with makespan criterion. The results gained by VNS algorithm are presented and compared with the best known results in literature. It is concluded that the VNS implementation is better than many recently published works with respect to the quality of the solution.", "authors": ["Mehmet Sevkli", "M. Emin Aydin"], "n_citation": 0, "title": "A variable neighbourhood search algorithm for job shop scheduling problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "02751928-b8c6-45a6-bb05-74bc0f645793"}
{"abstract": "As one of the most important services of Internet, e-mail is widely used in both commercial business and personal communication. In this paper, we explore ways to develop e-mail systems on P2P networks to overcome the weakness of traditional server-centric e-mail systems. We propose a framework of e-mail system based on hybrid P2P network, and present three implementations of the proposed framework, each of which has different capability, reliability. Our system is more flexible and reliable than the traditional e-mail systems.", "authors": ["Yue Zhao", "Shuigeng Zhou", "Aoying Zhou"], "n_citation": 0, "title": "E-mail services on hybrid P2P networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "07b93368-3894-456e-a500-fc84de319dcb"}
{"abstract": "We propose a novel approach for mining recent frequent itemsets. The approach has three key contributions. First, it is a single-scan algorithm which utilizes the special property of suffix-trees to guarantee that all frequent itemsets are mined. During the phase of itemset growth it is unnecessary to traverse the suffix-trees which are the data structure for storing the summary information of data. Second, our algorithm adopts a novel method for itemset growth which includes two special kinds of itemset growth operations to avoid generating any candidate itemset. Third, we devise a new regressive strategy from the attenuating phenomenon of radioelement in nature, and apply it into the algorithm to distinguish the influence of latest transactions from that of obsolete transactions. We conduct detailed experiments to evaluate the algorithm. It confirms that the new method has an excellent scalability and the performance illustrates better quality and efficiency.", "authors": ["Lifeng Jia", "Zhe Wang", "Chunguang Zhou", "Xiujuan Xu"], "n_citation": 0, "title": "Mining recent frequent itemsets in data streams by radioactively attenuating strategy", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "07be400f-38f4-478b-911f-aca1fe942c37"}
{"abstract": "Belief propagation (BP) has become widely used for low-level vision problems and various inference techniques have been proposed for loopy graphs. These methods typically rely on ad hoc spatial priors such as the Potts model. In this paper we investigate the use of learned models of image structure, and demonstrate the improvements obtained over previous ad hoc models for the image denoising problem. In particular, we show how both pairwise and higher-order Markov random fields with learned clique potentials capture rich image structures that better represent the properties of natural images. These models are learned using the recently proposed Fields-of-Experts framework. For such models, however, traditional BP is computationally expensive. Consequently we propose some approximation methods that make BP with learned potentials practical. In the case of pairwise models we propose a novel approximation of robust potentials using a finite family of quadratics. In the case of higher order MRFs, with 2 \u00d7 2 cliques, we use an adaptive state space to handle the increased complexity. Extensive experiments demonstrate the power of learned models, the benefits of higher-order MRFs and the practicality of BP for these problems with the use of simple principled approximations.", "authors": ["Xiangyang Lan", "Stefan Roth", "Daniel P. Huttenlocher", "Michael J. Black"], "n_citation": 0, "title": "Efficient Belief Propagation with Learned Higher-Order Markov Random Fields", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "083044cc-f835-4c37-80ad-4b0f3b317010"}
{"abstract": "Recently, grid-based virtual wargame collaborative environment has become a hot topic, in which load balance plays an important role. In this paper, we survey existing technologies in federate migration for the purpose of balancing load, and focus on the selection of destination node for a federate to migrate to. Different from traditional methods; the proposed method takes into consideration not only the load of CPU & memory, but also communications among federates when selecting the destination node. Moreover, the procedure of migration is described and a prototype is implemented on the basis of Globus Toolkit 3.0 to validate its functionality and performance.", "authors": ["Hai Huang", "Wei Wu", "Xin Tang", "Zhong Zhou"], "n_citation": 50, "title": "Federate migration in grid-based virtual wargame collaborative environment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "09ee90fd-624a-43a9-82e3-9e65d37fe9f5"}
{"abstract": "In this paper, a short-term load forecasting model using a Multiscale BiLinear Recurrent Neural Network with an adaptive learning algorithm (M-BLRNN(AL)) is proposed. The proposed M-BLRNN(AL) model is based on a wavelet-based neural network architecture formulated by a combination of several individual BLRNN models. The wavelet transform adopted in the M-BLRNN(AL) is employed to decompose the load curve into a rnutiresolution representation. Each individual BLRNN model is used to forecast the load signal at each resolution level obtained by the wavelet transform. The learning process is further improved by applying an adaptive learning algorithm at each resolution level. Experiments and results on load data from the North-American Electric Utility (NAEU) show that the proposed M-BLRNN(AL) model converges faster and archives better forecasting performance in comparison with other conventional models.", "authors": ["Chung Nguyen Tran", "Dong-Chul Park", "Hwan-Soo Choi"], "n_citation": 0, "title": "Short-Term Load Forecasting Using Multiscale BiLinear Recurrent Neural Network with an Adaptive Learning Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0adda64f-3789-4622-8542-d13395b9214e"}
{"abstract": "The main contribution of this work is a fast algorithm for checking whether a labelled transition system (LTS) is operationally deterministic. Operational determinism is a condition on the LTS designed to capture the notion of deterministic behaviour without ruling out invisible actions and divergence, and without strictly devoting oneself to any single process-algebraic semantics. Indeed, we show that in the case of operationally deterministic LTSs, all divergence-sensitive equivalences between divergence-sensitive branching bisimilarity and trace + divergence trace equivalence collapse to the same equivalence. The running time of the algorithm is linear except a term that, roughly speaking, grows as slowly as Ackermann's function grows quickly. If the original LTS is operationally deterministic, the algorithm produces as a by-product a structurally deterministic LTS that is divergence-sensitive branching bisimilar to the original one. This LTS can be minimised like a deterministic finite automaton. The overall approach is so cheap that it makes almost always sense to first try it and revert to a semantics-specific reduction or minimisation algorithm only if the LTS proves operationally nondeterministic.", "authors": ["Henri Hansen", "Antti Valmari"], "n_citation": 0, "title": "Operational determinism and fast algorithms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0b8444f1-8b53-4b1b-9304-311ea7402b70"}
{"abstract": "We study the generation of test cases for nondeterministic real-time systems. We define a class of Determinizable Timed Automata (DTA), in order to specify the system under test. The principle of our test method consists of two steps. In Step 1, we express the problem in a non-real-time form, by transforming a DTA into an equivalent finite state automaton. The latter uses two additional types of events, Set and Exp. In Step 2, we adapt a non-real-time test generation method.", "authors": ["Ahmed Khoumsi", "Thierry J\u00e9ron", "Herv\u00e9 Marchand"], "n_citation": 0, "title": "Test cases generation for nondeterministic real-time systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0bb9d087-365d-424f-9dbb-44a22a7926c3"}
{"abstract": "The use of inhabited Virtual Environments is continuously growing. People can embody a human-like avatar to participate inside these Virtual Environments or they can have personalized character acting as mediator; sometimes they can even customize it to some extent. Those Virtual Characters belong to the software owner, but they could be potentially shared, exchanged and individualized between participants, such as already proposed by Sony with Station Exchange. Technology with standards could significantly improve the exchange, the reuse and the creation of such Virtual Characters. However an optimal reuse is only possible if the main components of the characters: geometry, morphology, animation and behavior, are annotated with semantics. This may allow to users searching for specific models and customize them. Moreover search technology based on the Web Ontology Language (OWL) can be implemented to provide this type of service. In this paper we present the considerations to build an ontology that fulfills the mentioned proposes.", "authors": ["Laurent Moccozet", "Alejandra Garc\u00eda-Rojas", "F. Vexo", "Daniel Thalmann", "Nadia Magnenat-Thalmann"], "n_citation": 50, "title": "In search for your own virtual individual", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0db91677-3de3-430a-8377-c67bc542901e"}
{"authors": ["Qiao Chen", "Andrew Lim", "Wenbin Zhu"], "n_citation": 0, "title": "A greedy heuristic for airline crew rostering: Unique challenges in a large airline in China", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "0f079a4d-acf7-41c2-b959-f3e1b10c790e"}
{"abstract": "Search is until now a difficult problem in peer-to-peer (P2P) file-sharing systems. In this paper, we propose to use adaptive metadata spreading to make search in P2P networks efficient and scalable to large-scale systems. We model the search process in unstructured P2P networks obtain the optimized metadata populations for performance optimization. Based on the model, we propose adaptive metadata spreading approach which can adapt metadata populations to variational environment and achieve the optimized search performance. To implement our approach in fully decentralized P2P system, we employ self-organized fault-tolerant overlay trees, through which peers can easily cooperate with each other to perform adaptive metadata spreading with minor overhead.", "authors": ["Xuezheng Liu", "Guangwen Yang", "Ming Chen", "Yongwei Wu"], "n_citation": 0, "title": "Efficient search using adaptive metadata spreading in peer-to-peer networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "10e48a87-684a-4002-95d8-dc632b83bd4e"}
{"abstract": "Impossible futures equivalence is the semantic equivalence on labelled transition systems that identifies systems iff they have the same AGEF properties: temporal logic properties saying that reaching a desired outcome is not doomed to fail. We show that this equivalence, with an added root condition, is the coarsest congruence containing weak bisimilarity with explicit divergence that respects deadlock/livelock traces (or fair testing, or any liveness property under a global fairness assumption) and assigns unique solutions to recursive equations.", "authors": ["Rob J. van Glabbeek", "Marc Voorhoeve"], "n_citation": 0, "title": "Liveness, fairness and impossible futures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1426b8dc-0982-4cce-8121-cbe2f0c66d31"}
{"abstract": "We propose to use a low order autoregressive (AR) model to describe the temporal structure of source. Then we adopt Variational Bayesian (VB) method for source separation which models additive noise into the mixing system. The approach integrates the source probabilistic model and noise probabilistic model. Its goal is to approximate the actual probability density function of hidden variables and parameters using its approximating posterior distribution by minimizing the misfit between them. The advantage of our VB-AR algorithm is that it can exploit the temporal nature of source signals and avoid over-fitting in the separating process. This algorithm can also identify the AR order. Experiments on artifact and real-world speech signals are used to verify our proposed algorithms. As a result, the lower AR source model improves the separation. The performance of the algorithm is compared with that of i.i.d. separation algorithm and the second-order separation algorithm.", "authors": ["Qinghua Huang", "Jie Yang", "Yue Zhou"], "n_citation": 0, "title": "Variational Bayesian Method for Temporally Correlated Source Separation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1bc02bd3-4f35-4468-9869-b37a4994efac"}
{"abstract": "With the development of motion capture techniques; more and more 3D motion libraries become available. The growing amount of motion capture data requires more efficient and effective methods for indexing, searching and retrieving. In many cases, the user will only have a sketchy idea of which kind of motion to look for in the motion database. In consequence, the description about the query movement is a bottleneck for motion retrieval system. This paper presents a framework that can describe and handle the query scenes effectively. Our content-based retrieval system supports two kinds of query modes: textual query mode and query-by-example mode. By using various kinds of qualitative features and adaptive segments of motion capture data stream, our indexing and retrieval methods are carried out at the segment level rather than at the frame level, making them quite efficient. Some experimental examples are given to demonstrate the effectiveness and efficiency of proposed algorithms.", "authors": ["Yan Gao", "Lizhuang Ma", "Junfa Liu", "Xiaomao Wu", "Zhihua Chen"], "n_citation": 50, "title": "An efficient algorithm for content-based human motion retrieval", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "232bd24b-1734-46e3-850a-1429f65fbd08"}
{"abstract": "For counting points of Jacobians of genus 2 curves defined over large prime fields, the best known method is a variant of Schoof's algorithm. We present several improvements on the algorithms described by Gaudry and Harley in 2000. In particular we rebuild the symmetry that had been broken by the use of Cantor's division polynomials and design a faster division by 2 and a division by 3. Combined with the algorithm by Matsuo, Chao and Tsujii, our implementation can count the points on a Jacobian of size 164 bits within about one week on a PC.", "authors": ["Pierrick Gaudry", "\u00c9ric Schost"], "n_citation": 78, "title": "Construction of secure random curves of genus 2 over prime fields", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "25dcf612-206f-4344-a1dd-d1b527840bfa"}
{"abstract": "The rapid progress of computer and network technologies makes it easy to collect and store a large amount of unstructured or semi-structured texts such as Web pages, HTML/XML archives, E-mails, and text files. These text data can be thought of large scale text databases, and thus it becomes important to develop an efficient tools to discover interesting knowledge from such text databases. There are a large body of data mining researches to discover interesting rules or patterns from well-structured data such as transaction databases with boolean or numeric attributes [1,8,13]. However, it is difficult to directly apply the traditional data mining technologies to text or semi-structured data mentioned above since these text databases consist of (i) heterogeneous and (ii) huge collections of (iii) un-structured or semi-structured data. Therefore, there still have been a small number of studies on text mining, e.g., [4,5,12,17]. Our research goal is to devise an efficient semi-automatic tool that supports human discovery from large text databases. Therefore, we require a fast pattern discovery algorithm that can work in time, e.g., O(n) to O(n log n), to respond in real time on an unstructured data set of total size n. Furthermore, such an algorithm has to be robust in the sense that it can work on a large amount of noisy and incomplete data without the assumption of an unknown hypothesis class. To achieve this goal, we adopt the framework of optimized pattern discovery [11], also known as Agnostic PAC learning [10] in computational learning theory. In optimized pattern discovery, an algorithm tries to find a pattern from a hypothesis space that optimizes a given statistical measure, such as classification error [10], information entropy [11], and Gini index [6], to discriminate a set of interesting documents from a set of uninteresting ones. In the recent developments in computational learning theory, it is shown that such an algorithm can approximate arbitrary distributions on data within a given class of hypotheses very well in the sense of classification accuracy [6,10]. In this lecture, we present efficient and robust pattern discovery algorithms for large unstructured and semi-structured data combining the techniques from combinatorial pattern matching, computational geometry, and computational learning theory [2,3,9,16]. We then describe applications of our pattern discovery algorithms to interactive document browsing, keyword discovery from Web and structure discovery from XML archives. We also discuss applications of optimized pattern discovery to information extraction from Web [14,15].", "authors": ["Hiroki Arimura"], "n_citation": 0, "title": "Efficient text mining with optimized pattern discovery", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "25fbac4c-de25-45b0-a543-1f699b1c5427"}
{"abstract": "We define an agent to be any node in the network that is prepared to provide some services to other parties when requested. Such services may be required for various purposes and then, searching and establishing communication with the agent will be necessary. The agent search may be oriented along a particular direction to reduce network load and optimize overall performance. Such oriented algorithms already exist. In this paper we have given a communications protocol that may use such oriented algorithm, defined its request and response packet formats and shown the simulation results for overhead incurred in such communication.", "authors": ["Rajat Shuvro Roy", "M. Sohel Rahman"], "n_citation": 0, "title": "On communicating with agents on the network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "265cdb50-acdb-4147-8721-18f8c5f413f0"}
{"abstract": "We study the problem of encoding homotopy of simple paths in the plane. We show that the homotopy of a simple path with k edges in the presence of n obstacles can be encoded using O(n log(n+k)) bits. The bound is tight if k = \u03a9(n 1+e ). We present an efficient algorithm for encoding the homotopy of a path. The algorithm can be applied to find homotopic paths among a set of simple paths. We show that the homotopy of a general (not necessary simple) path can be encoded using O(k log n) bits. The bound is tight. The code is based on a homotopic minimum-link path and we present output-sensitive algorithms for computing a path and the code.", "authors": ["Sergei Bespamyatnikh"], "n_citation": 0, "title": "Encoding homotopy of paths in the plane", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "266df857-cd4d-4c6a-863e-620876536463"}
{"abstract": "In this paper, we present a novel approach for the change discovery in ontology-based knowledge management systems. It extends our previous work in the ontology evolution by taking into account the usage of an ontology in the knowledge management system. The approach is mainly based on the analysis of the user's interaction with the system in providing annotations for knowledge resources, as well as in the process of accessing the knowledge by querying the knowledge repository. We defined several assessment criteria to estimate the quality of annotations and the user's needs from the point of view of the knowledge management. These criteria result in the recommendations for the continual system improvement. Two evaluation studies illustrate the benefits of our approach.", "authors": ["Ljiljana Stojanovic", "Nenad Stojanovic", "Alexander Maedche"], "n_citation": 0, "title": "Change discovery in ontology-based knowledge management systems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2673f890-e5d3-4cec-baff-2efa17619a8f"}
{"abstract": "In this paper, we introduce a constraint-based approach to camera control. The aim is to determine the path of a camera that verifies declarative properties on the desired image, such as location or orientation of objects on the screen at a given time. The path is composed of elementary movements called hypertubes, based on established cinematographic techniques. Hypertubes are connected by relations that guarantee smooth transitions. In order to determine the path of the camera, we rely on an incremental solving process, based on recent constraint solving techniques. The hypertube approach provides a nice global control over the camera path, as well as fine tuning in the image via cinematographic properties.", "authors": ["Marc Christie", "Eric Langu\u00e9nou"], "n_citation": 50, "title": "A constraint-based approach to camera path planning", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "27343ccd-9d1b-4003-af49-436d337fb518"}
{"abstract": "Tour into the picture (TIP) is a technique that can make animation from one 2D image of a scene. Compared to traditional geometry based modeling method, it is not only realized easily, but also can provide better effects of navigation. Based on the previous research experience, this paper mainly introduces the application of TIP for digitization of Culture Heritage by using relative depth calculation and cubic panorama. Finally, by using a hybrid NPR/TIP approach, we provide stylized interactive and real-time scenes walkthrough so as to convey a kind of specific cultural spirits.", "authors": ["Ya-Ping Zhang", "Yang Zhao", "Jiaoying Shi", "Dan Xu"], "n_citation": 50, "title": "Digitization of culture heritage based on tour into the picture", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "27408e53-a78b-4d6d-b3b1-cd69dbd2d617"}
{"abstract": "Modular multiplication and exponentiation are common operations in modern cryptography. Unification problems with respect to some equational theories that these operations satisfy are investigated. Two different but related equational theories are analyzed. A unification algorithm is given for one of the theories which relies on solving syzygies over multivariate integral polynomials with noncommuting indeterminates. For the other theory, in which the distributivity property of exponentiation over multiplication is assumed, the unifiability problem is shown to be undecidable by adapting a construction developed by one of the authors to reduce Hilbert's 10th problem to the solvability problem for linear equations over semi-rings. A new algorithm for computing strong Grobner bases of right ideals over the polynomial ring Z  is proposed; unlike earlier algorithms proposed by Baader as well as by Madlener and Reinert which work only for right admissible term orderings with the boundedness property, this algorithm works for any right admissible term ordering. The algorithms for some of these unification problems are expected to be integrated into Naval Research Lab.'s Protocol Analyzer (NPA), a tool developed by Catherine Meadows, which has been successfully used to analyze cryptographic protocols, particularly emerging standards such as the Internet Engineering Task Force's (IETF) Internet Key Exchange [11] and Group Domain of Interpretation [12] protocols. Techniques from several different fields - particularly symbolic computation (ideal theory and Groebner basis algorithms) and unification theory - are thus used to address problems arising in state-based cryptographic protocol analysis.", "authors": ["Deepak Kapur", "Paliath Narendran", "Lida Wang"], "n_citation": 50, "title": "An E-unification algorithm for analyzing protocols that use modular exponentiation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "27b69123-bbf4-487b-8c27-3e7d0ded23b9"}
{"abstract": "We are interested in learning unions of the pattern languages in the limit from positive data using strategies that guarantee some form of minimality during the learning process. It is known that for any class of languages with so-called finite elasticity, any learning strategy that finds a language that is minimal with respect to inclusion (MINL) ensures identification in the limit. We consider a learning strategy via another form of minimality - the minimality in the number of elements that are shorter than a specified length. A search for languages with this minimality is possible in many cases, and the search can be adapted to identify any class where every language in the class has a characteristic set within the class. We compare solutions using this strategy to those from MINL to illustrate how we may obtain solutions that fulfill both notions of minimality. Finally, we show how the results are relevant using some subclasses of the pattern languages.", "authors": ["Yen Kaow Ng", "Takeshi Shinohara"], "n_citation": 50, "title": "Inferring unions of the pattern languages by the most fitting covers", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "29f59767-be79-48c7-a49a-d984c79e3f62"}
{"abstract": "This paper describes a simulation of the physical processes of coupled heat and moisture transfer in a clothing assembly containing phase change material (PCM). This paper focuses on the analysis of the effect of the PCM. The results of simulation show that the PCM can delay the decrease in temperature of the clothing. The experiment results are also shown in this paper. A reasonable agreement was found when comparing the results of the simulation with the experiment results.", "authors": ["Shuxiao Wang", "Yi Li", "Hiromi Tokura", "J Y Hu", "Aihua Mao"], "n_citation": 0, "title": "Computer simulation of multi-phase coupled heat and moisture transfer in clothing assembly with a phase change material in a cold environment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2b93e141-6db0-4924-b1d5-7f6ab5f80c38"}
{"abstract": "Traditional stereo algorithms implicitly use the frontal parallel plane assumption when exploiting contextual information, since the smoothness prior biases towards constant disparity (depth) over a neighborhood. For curved surfaces these algorithms introduce systematic errors to the matching process. These errors are non-negligible for detailed geometric modeling of natural objects (e.g. a human face). We propose to use contextual information geometrically. In particular, we perform a differential geometric study of smooth surfaces and argue that geometric contextual information should be encoded in Cartan's moving frame model over local quadratic approximations of the smooth surfaces. The result enforces geometric consistency for both depth and surface normal. We develop a simple stereo algorithm to illustrate the importance of using such geometric contextual information and demonstrate its power on images of the human face.", "authors": ["Gang Li", "Steven W. Zucker"], "n_citation": 50, "title": "Differential Geometric Consistency Extends Stereo to Curved Surfaces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2bb0a0be-ce4f-489e-be8d-db30bca47f81"}
{"abstract": "For Peer-2-Peer (P2P) networks to realize their full potential the nodes they are composed of need to coordinate and cooperate, to improve the performance of the network as a whole. This requires the suppression of selfish behavior (free-riding). Existing P2P systems often assume that nodes will behave altruistically, but this has been shown to be far from the case. We outline encouraging initial results from a P2P simulation that translates and applies the properties of tag models (initially developed within social simulations) [8, 9] to tackle these issues. We find that a simple node rewiring policy, based on the tag dynamics, quickly eliminates free-riding without centralized control. The process appears highly scalable and robust.", "authors": ["David Hales"], "n_citation": 0, "title": "Self-organising, open and cooperative P2P societies : From tags to networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2c8c98cb-473f-4e42-9a67-92531ca2037a"}
{"abstract": "Traditional document indexing methods, although useful, do not take into account some important aspects of language, such as syntax and semantics. Unlikely, semantic hyperspaces are mathematical and statistical-based techniques that do it. However, although they are an improvement on traditional methods, the output representation is still vector like. This paper proposes a computational model of text reading, called Cognitive Reading Indexing (CRIM), inspired by some aspects of human reading cognition, such as sequential perception, temporality, memory, forgetting and inferences. The model produces not vectors but nets of activated concepts. This paper is focused on indexing or representing documents that way so that they can be labeled or retrieved, presenting promising results. The system was applied to model human subjects as well, and some interesting results were obtained.", "authors": ["J. Ignacio Serrano", "M. Dolores del Castillo"], "n_citation": 0, "title": "Text Representation by a Computational Model of Reading", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2e2a0905-8bbc-4e02-b0af-92c350aa80b5"}
{"abstract": "The paper presents the new computational model of Herbrand engines which combines finite-state control with uninterpreted data and function registers, thus yielding a finite representation of infinite-state machines. Herbrand engines are used to provide a high-level model of out-of-order execution in the design of micro-processors. The problem of verifying that a highly parallel design for out-of-order execution correctly implements the Instruction Set Architecture is reduced to establishing the equivalence of two Herbrand engines. We show that, for a reasonably restricted class of such engines, the equivalence problem is decidable, and present two algorithms for solving this problem. Ultimately, the appropriate statement of correctness is that the out-of-order execution produces the same final state (and all relevant intermediate actions, such as writes to memory) as a purely sequential machine running the same program.", "authors": ["Werner Damm", "Amir Pnueli", "Sitvanit Ruah"], "n_citation": 0, "title": "Herbrand automata for hardware verification", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "315e5d3c-3f2a-4b5f-b6a9-482734dd5f07"}
{"abstract": "Most current conceptual modeling languages and methods do not model events as entities. We argue that, at least in Object-Oriented (O-O) languages, modeling events as entities provides substantial benefits. We show that a method for behavioral modeling that deals with event and entity types in a uniform way may yield better behavioral schemas. The proposed method makes an extensive use of language constructs such as constraints, derived types, derivation rules, type specializations and operations, which are present in all complete O-O conceptual modeling languages. The method can be adapted to most 0-0 languages. In this paper we explain its adaptation to the UML.", "authors": ["Antoni Olive"], "n_citation": 0, "title": "Definition of events and their effects in Object-Oriented conceptual modeling languages", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "318d2c0f-c6d5-418e-b89b-dc53d9af16fc"}
{"abstract": "First, a modified Neighborhood-Based Clustering (MNBC) algorithm using the directed tree for data clustering is presented. It represents a dataset as some directed trees corresponding to meaningful clusters. Governed by Neighborhood-based Density Factor (NDF), it also can discover clusters of arbitrary shape and different densities like NBC. Moreover, it greatly simplify NBC. However, a failure applying in image segmentation is due to an unsuitable use of Euclidean distance between image pixels. Second, Gray NDF (GNDF) is introduced to make MNBC suitable for image segmentation. The dataset to be segmented is all grays and thus MNBC has the constant computational complexity 0(256). The experiments on synthetic datasets and real-world images shows that MNBC outperforms some existing graph-theoretical approaches in terms of computation time as well as segmentation effect.", "authors": ["Jundi Ding", "Songcan Chen", "Runing Ma", "Bo Wang"], "n_citation": 50, "title": "A Fast Directed Tree Based Neighborhood Clustering Algorithm for Image Segmentation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "32879850-a55d-49af-aea4-0056c6a991de"}
{"abstract": "Self-organization in multi agent systems requires two main building blocks: adaptive and uncoupled interaction mechanisms and context-awareness. Here we show how the middleware TOTA (Tuples On The Air) supports self-organization by providing effective abstractions for the above two building-blocks. TOTA relies on spatially distributed tuples for both supporting adaptive and uncoupled interactions between agents, and context-awareness. Agents can inject these tuples in the network, to make available some kind of contextual information and to interact with other agents. Tuples are propagated by the middleware, on the basis of application specific patterns, defining sorts of computational fields, and their intended shape is maintained despite network dynamics, such as topological reconfigurations. Agents can locally sense these fields and rely on them for both acquiring contextual information and carrying on distributed self-organizing coordination activities. Several application examples in different scenarios show the effectiveness of our approach.", "authors": ["Marco Mamei", "Franco Zambonelli"], "n_citation": 0, "title": "Self-organization in multi agent systems: A middleware approach", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "330341e7-ad66-4c3c-90ee-264d0c802da3"}
{"abstract": "This paper presents a multi-agent conceptual model for resource allocation in a manufacturing environment. To attain this purpose a framework called M-DRAP - Multi-agent Dynamic Resources Allocation Planning - was developed. Multi-agent systems have been employed as a solution for problems that require decentralization and distribution in both decision-making and execution process. This is a premise in many information systems where (i) the domain involves intrinsic distribution of data, problem-solving capabilities and responsibilities; (ii) it is necessary to maintain the autonomy of the subparts, without lost of organizational structure; and (iii) the problem solution cannot be completely described a priori due to the possibility of real-time perturbations in the environment (equipment failures, for example) and also as a consequence of the natural dynamics of the business process. The main contribution of this work is the proposition of a set of activities and models defining a framework to represent multi-agent systems for business process under an enterprise model perspective.", "authors": ["Ricardo Melo Bastos", "Jose Palazzo Moreira De Oliveira"], "n_citation": 5, "title": "A conceptual modeling framework for multi-agent information systems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "33f002a4-6011-4bde-bb51-ae7caffa7fcf"}
{"abstract": "We are interested in the problem of qualitative, point-based temporal reasoning. That is, given a set of assertions concerning the relative positions of points in time, we are interested in compiling the assertions into a representation that supports efficient reasoning, determining consistency, computing the strongest entailed relation between any two points and updating the set of assertions. We begin by considering a general set of operations and their corresponding algorithms, applicable to general point-based temporal domains. We also consider special-purpose reasoners, which may be expected to perform well in an application domain with a restricted structure. Lastly we consider reasoners which incorporate restricted structures into a more general reasoning framework. Finally, we present instances of each such approach and give a comparison of their performance on random data sets.", "authors": ["T. Van Allen", "James P. Delgrande", "Arvind Gupta"], "n_citation": 0, "title": "Point-based approaches to qualitative temporal reasoning", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "369567f8-7d9e-442e-87a0-115592a23260"}
{"abstract": "Conventional photometric stereo has a fundamental limitation that the scale of recovered geometry is limited to the resolution of the input images. However, surfaces that contain sub-pixel geometric structures are not well modelled by a single normal direction per pixel. In this work, we propose a technique for resolution-enhanced photometric stereo, in which surface geometry is computed at a resolution higher than that of the input images. To achieve this goal, our method first utilizes a generalized reflectance model to recover the distribution of surface normals inside each pixel. This normal distribution is then used to infer sub-pixel structures on a surface of uniform material by spatially arranging the normals among pixels at a higher resolution according to a minimum description length criterion on 3D textons over the surface. With the presented method, high resolution geometry that is lost in conventional photometric stereo can be recovered from low resolution input images.", "authors": ["Ping Tan", "Stephen Lin", "Long Quan"], "n_citation": 50, "title": "Resolution-enhanced photometric stereo", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37f56bb6-d432-49b0-a2b1-ef675422d8e5"}
{"abstract": "The disambiguation of verbs is usually considered to be more difficult with respect to other part-of-speech categories. This is due both to the high polysemy of verbs compared with the other categories, and to the lack of lexical resources providing relations between verbs and nouns. One of such resources is WordNet, which provides plenty of information and relationships for nouns, whereas it is less comprehensive with respect to verbs. In this paper we focus on the disambiguation of verbs by means of Support Vector Machines and the use of WordNet-extracted features, based on the hyperonyms of context nouns.", "authors": ["Davide Buscaldi", "Paolo Rosso", "Ferran Pla", "Encarna Segarra", "Emilio Sanchis Arnal"], "n_citation": 0, "title": "Verb sense disambiguation using support vector machines : Impact of wordnet-extracted features", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "383a240e-86ba-4f72-9923-fc8514942f70"}
{"abstract": "We discuss an adaptation of the technique of saturation up to redundancy, as introduced by Bachmair and Ganzinger [1], to tableau and sequent calculi for classical first-order logic. This technique can be used to easily show the completeness of optimized calculi that contain destructive rules e.g. for simplification, rewriting with equalities, etc., which is not easily done with a standard Hintikka-style completeness proof. The notions are first introduced for Smullyan-style ground tableaux, and then extended to constrained formula free-variable tableaux.", "authors": ["Martin Giese"], "n_citation": 0, "title": "Saturation Up to redundancy for tableau and sequent calculi", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c979029-399a-4383-a96d-af421fb41523"}
{"abstract": "Automatic image annotation concerns a process of automatically labeling image contents with a pre-defined set of keywords, which are regarded as descriptors of image high-level semantics, so as to enable semantic image retrieval via keywords. A serious problem in this task is the unsatisfactory annotation performance due to the semantic gap between the visual content and keywords . Targeting at this problem, we present a new approach that tries to incorporate lexical semantics into the image annotation process. In the phase of training, given a training set of images labeled with keywords, a basic visual vocabulary consisting of visual terms, extracted from the image to represent its content, and the associated keywords is generated at first, using K-means clustering combined with semantic constraints obtained from WordNet, then the statistical correlation between visual terms and keywords is modeled by a two-level hierarchical ensemble model composed of probabilistic SVM classifiers and a co-occurrence language model. In the phase of annotation, given an unlabeled image, the most likely associated keywords are predicted by the posterior probability of each keyword given each visual term at the first-level classifier ensemble, then the second-level language model is used to refine the annotation quality by word co-occurrence statistics derived from the annotated keywords in the training set of images. We carried out experiments on a medium-sized image collection from Corel Stock Photo CDs. The experimental results demonstrated that the annotation performance of this method outperforms some traditional annotation methods by about 7% in average precision, showing the feasibility and effectiveness of the proposed approach.", "authors": ["Wei Li", "Maosong Sun"], "n_citation": 0, "title": "Automatic image annotation based on wordNet and hierarchical ensembles", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3d16f02d-3611-446d-af07-4942148efaeb"}
{"abstract": "This paper presents an improved Naive Bayes algorithm for clustering. Many researchers search for parameter values from incomplete data using EM (Expectation Maximization) algorithm. It is well-known that EM approach has a drawback - local optimal solution, so we propose a novel hybrid algorithm of the DPSO (Discrete Particle Swarm Optimization) and the EM approach to improve the global search performance. We then apply the approach to 4 real-world data sets from UCI repository and compare the performance of clustering by the new algorithm with by EM algorithm. In the comparison, the hybrid DPSO+EM algorithm exhibits more effectively and outperforms the EM approach.", "authors": ["Jing-Hua Guan", "Dayou Liu", "Si-Pei Liu"], "n_citation": 0, "title": "Discrete Particle Swarm Optimization and EM Hybrid Approach for Naive Bayes Clustering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3df63193-a9f8-4aba-b8af-21db93c56912"}
{"abstract": "Parametric CAD applications enable designers to model and maintain both objects and relationships. Selection is the first step in the process of establishing relationships in a CAD model. Because designing is characterized by a process of change and development, establishing and maintaining relationships can contribute significant overhead to a parametric design process. Selection rules can be used to reduce the overhead involved in maintaining relationships and increase the portability of model objects by pushing responsibility for maintaining relations to the computer. We report on two implemented concepts for rule based selection and discuss directions for future research.", "authors": ["Davis Marques", "Robert F. Woodbury"], "n_citation": 0, "title": "Using rule based selection to support change in parametric CAD models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "42f9e0ad-ae5d-4172-be95-0a6b3da4a44e"}
{"abstract": "In this paper, we consider source location problems and their generalizations with three connectivity requirements A, k and k. We show that the source location problem with edge-connectivity requirement A in undirected networks is strongly NP-hard, and that no source location problems with three connectivity requirements in undirected/directed networks are approximable within a ratio of O(In D), unless NP has an O(N log log N )-time deterministic algorithm. Here D denotes the sum of given demands. We also devise (1 + In D)-approximation algorithms for all the extended source location problems if we have the integral capacity and demand functions. Furthermore, we study the extended source location problems when a given graph is a tree. Our algorithms for all the extended source location problems run in pseudo-polynomial time and the ones for the source location problem with vertex-connectivity requirements k and k run in polynomial time, where pseudo-polynomiality for the source location problem with the arc-connectivity requirement A is best possible unless P=NP, since it is known to be weakly NP-hard, even if a given graph is a tree.", "authors": ["Mariko Sakashita", "Kazuhisa Makino", "Satoru Fujishige"], "n_citation": 0, "title": "Minimum cost source location problems with flow requirements", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "44c40f06-5c9b-4ff4-96ad-658eff6aad47"}
{"abstract": "In the last decade UML has emerged as the standard object-oriented conceptual modeling language. Since UML is a combination of previous languages, such as Booch, OMT, Statecharts, etc., the creation of multi-views within UML was unavoidable. These views, which represent different aspects of system structure and behavior, overlap, raising consistency and integration problems. Moreover, the object-oriented nature of UML sets the ground for several behavioral views in UML, each of which is a different alternative for representing behavior. In this paper I suggest a Top-Level Object-Oriented Framework (TLOOF) for UML models. This framework, which serves as the glue of use case, class, and interaction diagrams, enables changing the refinement level of a model without losing the comprehension of the system as a whole and without creating contradictions among the mentioned structural and behavioral views. Furthermore, the suggested framework does not add new classifiers to the UML metamodel, hence, it does not complicate UML.", "authors": ["Iris Reinhartz-Berger"], "n_citation": 50, "title": "Conceptual modeling of structure and behavior with UML : The Top Level Object-Oriented Framework (TLOOF) approach", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "44d09b76-333d-4626-92c5-89aea7b563ba"}
{"abstract": "XML is a widespread W3C standard used by several kinds of applications for data representation and exchange over the web. In the context of a system that provides semantic integration of heterogeneous XML sources, the same information at a semantic level may have different representations in XML. However, the syntax of an XML query depends on the structure of the specific XML source. Therefore, in order to obtain the same query result, one must write a specific query for each XML source. To deal with such problem, a much better solution is to state queries against a global conceptual schema and then translate them into an XML query against each specific data source. This paper presents CXPath (Conceptual XPath), a language for querying XML sources at the conceptual level, as well as a translation mechanism that converts a CXPath query to an XPath query against a specific XML source.", "authors": ["Sandro Daniel Camillo", "Carlos A. Heuser", "Ronaldo Dos Mello"], "n_citation": 0, "title": "Querying heterogeneous XML sources through a conceptual schema", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "496bc44c-b249-408e-9989-137809db5222"}
{"authors": ["Jorge Tavares", "Alexandru-Adrian Tantar", "Nouredine Melab", "El-Ghazali Talbi"], "n_citation": 0, "title": "The Influence of Mutation on Protein-Ligand Docking Optimization: A Locality Analysis", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "4b5c97f5-3599-4759-a3df-d1fb7e25d55e"}
{"abstract": "Superimposed information (SI) refers to new information such as annotations and summaries overlaid on fragments of existing base information (BI) such as web pages and PDF documents. Each BI fragment is referenced using an encapsulated address called a mark. Based on the widespread applicability of SI and wide range of superimposed applications (SAs) that can be built, we consider here how to represent marks explicitly in a conceptual model for an SA. The goal of this work is to facilitate the development of SAs by making it easy to model SI (including the marks) and to exploit the middleware and query capability that we have developed for managing marks and interacting with the base applications. The contribution of this paper is a general-purpose framework to make marks explicit in a conceptual (ER) model. We present conventions to associate marks with entities, attributes, and relationships; and to represent that an attribute's value is the same as the excerpt obtained from a mark. We also provide procedures to automatically convert ER schemas expressed using our conventions to relational schemas, and show how a resulting relational schema supports SQL queries over the combination of SI, the associated marks and the excerpts associated with the marks.", "authors": ["Sudarshan Murthy", "Lois M. L. Delcambre", "David Maier"], "n_citation": 0, "title": "Explicitly Representing Superimposed Information in a Conceptual Model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4d65721d-0a99-4138-a4ea-855f4c2bcc1e"}
{"abstract": "In this work we introduce and study the question of combining multiple heuristics. Given a problem instance, each of the multiple heuristics is capable of computing the correct solution, but has a different cost. In our models the user executes multiple heuristics until one of them terminates with a solution. Given a set of problem instances, we show how to efficiently compute an optimal fixed schedule for a constant number of heuristics, and show that in general, the problem is computationally hard even to approximate (to within a constant factor). We also discuss a probabilistic configuration, in which the problem instances are drawn from some unknown fixed distribution, and show how to compute a near optimal schedule for this setup.", "authors": ["Tzur Sayag", "Shai Fine", "Yishay Mansour"], "n_citation": 0, "title": "Combining multiple heuristics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4db8f2aa-0e52-4a17-b943-59cbeaa0d49a"}
{"abstract": "In the field of neural networks, feature selection has been studied for the last ten years and classical as well as original methods have been employed. This paper reviews the efficiency of four approaches to do a driven forward features selection on neural networks. We assess the efficiency of these methods compare to the simple Pearson criterion in case of a regression problem.", "authors": ["Vincent Lemaire", "Rapha\u00ebl F\u00e9raud"], "n_citation": 50, "title": "Driven Forward Features Selection : A Comparative Study on Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f29f2be-22b3-41c0-a9b0-cb989880cc6a"}
{"abstract": "Semi-supervised classification uses a large amount of unlabeled data to help a little amount of labeled data for designing classifiers, which has good potential and performance when the labeled data are difficult to obtain. This paper mainly discusses semi-supervised classification based on CPN (Counter-propagation Network). CPN and its revised models have merits such as simple structure, fast training and high accuracy. Especially, its training scheme combines supervised learning and unsupervised learning, which makes it very conformable to be extended to semi-supervised classification problem. According to the characteristics of CPN, we propose a semi-supervised dynamic CPN, and compare it with other two semi-supervised CPN models using Expectation Maximization and Co-Training/Self-Training techniques respectively. The experimental results show the effectiveness of CPN based semi-supervised classification methods.", "authors": ["Yao Chen", "Yuntao Qian"], "n_citation": 0, "title": "Semi-supervised Dynamic Counter Propagation Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5060b250-d0fa-4648-a7a1-dd91e374427d"}
{"abstract": "AbstracL This paper introduces an approach to generate video clips starting from an annotated commentary. The novelty of the approach lies in the use of rhetorical structure of the accompanying audio commentary in planning the video. The basic notions of cinematography are briefly introduced together with the Rhetorical Relation Theory to model the structure of a discourse. Then, the architecture of a video engine to automatically build video clips from the audio commentary annotated with respect to rhetorical relations is described. Finally, an application for a multimedia mobile guide in a museum is described.", "authors": ["Massimo Zancanaro", "Cesare Rocchi", "Oliviero Stock"], "n_citation": 0, "title": "Automatic video composition", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "50f52b2b-b313-4f27-b617-0b516feb3bf9"}
{"abstract": "We review and investigate the current status of intransitivity as a potential obstacle in coevolution. Pareto-Coevolution avoids intransitivity by translating any standard superiority relation into a transitive Pareto-dominance relation. Even for transitive problems though, cycling is possible. Recently however, algorithms that provide monotonic progress for Pareto-Coevolution have become available. The use of such algorithms avoids cycling, whether caused by intransitivity or not. We investigate this in experiments with two intransitive test problems, and find that the IPCA and LAPCA archive methods establish monotonic progress on both test problems, thereby substantially outperforming the same method without an archive.", "authors": ["Edwin D. de Jong"], "n_citation": 0, "title": "Intransitivity in coevolution", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "512b5d9c-9e7d-454f-b084-050bac22cd43"}
{"abstract": "Organizations are seeking for a special kind of browser for exchanging structured information across business entities. In this paper, we present a generic solution called as XML Schema Definition (XSD) [6] to GUI translator. This translator processes data model defined in XML schema document and then generates user interface dynamically. This translator generates user interfaces in different rendering languages such as Java swings, HTML and WML.", "authors": ["V. Radha", "S. Ramakrishna", "N. Pradeep Kumar"], "n_citation": 50, "title": "Generic XML schema definition (XSD) to GUI translator", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5372d13a-202c-4db5-abaf-552136ae5452"}
{"abstract": "The empirical risks of regression models are not accurate since they are evaluated from the finite number of samples. In this context, we investigate the confidence intervals for the risks of regression models, that is, the intervals between the expected and empirical risks. The suggested method of estimating confidence intervals can provide a tool for predicting the performance of regression models.", "authors": ["Imhoi Koo", "Rhee Man Kil"], "n_citation": 1, "title": "Confidence Intervals for the Risks of Regression Models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "555e4539-89f6-4801-ac27-f8e31966c9da"}
{"abstract": "We describe our word-based implementation of a language identifying system for the text messages written in European languages. Specifically, we use and compare linguistic (based on functional words) and statistic (based on the word frequency) approaches to construction of the identifying vocabularies. Our version of the statistic approach copes with the differences in degrees of word overlap among languages and the problem of the small-size messages. In addition, it allows an user to choose the accuracy of language identification. At present, our system identifies 8 languages (Bulgarian, English, French, German, Italian, Russian, Spanish and Swedish) in various encodings. With the identifying vocabularies of limited size (less than 1500 keys per language), the accuracy of identification attains 99% even for the messages containing only one sentence.", "authors": ["Anna V. Zhdanova"], "n_citation": 50, "title": "Automatic identification of European languages", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "55954205-0ac5-4a70-bb89-17e6804b3ce5"}
{"abstract": "We propose a general-purpose technique, called DASH (Decision Assignment Scheme Heuristic), to eliminate isomorphic subspaces when generating finite models. Like LNH, DASH is based on inherent isomorphism in first order clauses on finite domains. Unlike other methods, DASH can completely eliminate isomorphism during the search. Therefore, DASH can generate all the models none of which are isomorphic. And DASH is an efficient technique for finite model enumeration. The main idea is to cut the branch of the search tree which is isomorphic to a branch that has been searched. We present a new method to describe the class of isomorphic branches. We implemented this technique by modifying SEM1.7B, and the new tool is called SEMD. This technique proves to be very efficient on typical problems like the generation of finite groups, rings and quasigroups. The experiments show that SEMD is much faster than SEM on many problems, especially when generating all the models and when there is no model. SEMD can generate all the non-isomorphic models with little extra cost, while other tools like MACE4 will spend more time.", "authors": ["Xiangxue Jia", "Jian Zhang"], "n_citation": 0, "title": "A powerful technique to eliminate isomorphism in finite model search", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "57025fd8-48c1-40b5-bd12-95de8dfce5d8"}
{"authors": ["Mario Sznaier"], "n_citation": 0, "references": ["6cb7a19a-882e-4601-ad70-729eab912c07"], "title": "H2 control with time domain constraints: theory and an application", "venue": "advances in computing and communications", "year": 2002, "id": "574fa6b2-5082-4239-b740-4ee72de83879"}
{"abstract": "While cluster computing is well established, it is not clear how to coordinate clusters consisting of many database components in order to process high workloads. In this paper, we focus on Online Analytical Processing (OLAP) queries, i.e., relatively complex queries whose evaluation tends to be time-consuming, and we report on some observations and preliminary results of our PowerDB project in this context. We investigate how many cluster nodes should be used to evaluate an OLAP query in parallel. Moreover, we provide a classification of OLAP queries, which is used to decide, whether and how a query should be parallelized. We run extensive experiments to evaluate these query classes in quantitative terms. Our results are an important step towards a two-phase query optimizer. In the first phase, the coordination infrastructure decomposes a query into subqueries and ships them to appropriate cluster nodes. In the second phase, each cluster node optimizes and evaluates its subquery locally.", "authors": ["Fuat Akal", "Klemens B\u00f6hm", "Hans-J\u00f6rg Schek"], "n_citation": 62, "title": "OLAP query evaluation in a database cluster: A performance study on intra-query parallelism", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "57b7edd4-eaed-4e7a-880a-1a60bed31426"}
{"abstract": "Perfect decryption has been always assumed in the research of public key encryption, however, this is not true all the time. For some public key encryption primitives, like NTRU [9] or Ajtai-Dwork [1], the decryption process may not obtain the corresponding message even the encryption and decryption are run correctly. Furthermore, such a kind of decryption errors will lead to some dangerous attacks against the underlying primitive. Another interesting point is that, those primitives are not based on the factoring, nor the discrete log problem which are subject to the Shor's algorithm [18] with quantum computers. This kind of primitives may be promising in the post-quantum cryptography. Therefore, the decryption errors deserve much attention and should be coped with carefully. In this paper, our main technique is not to use any error-correcting codes to eliminate the errors, but to use some padding (transform) to hide bad errors from attacker's control. We 1) efficiently enhance these error-prone public key encryption primitives to the chosen ciphertext security, even in the presence of the decryption errors, and 2) show that the solution is more generic, rather than some specific padding methods previously presented, to thwart the decryption errors based attacks successfully.", "authors": ["Yang Cui", "Kazukuni Kobara", "Hideki Imai"], "n_citation": 50, "title": "On Achieving Chosen Ciphertext Security with Decryption Errors", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5a735876-ca1b-45cb-9029-6c8a0cb41f7d"}
{"abstract": "We define an extension of predicate logic, called Binding Logic, where variables can be bound in terms and in propositions. We introduce a notion of model for this logic and prove a soundness and completeness theorem for it. This theorem is obtained by encoding this logic back into predicate logic and using the classical soundness and completeness theorem there.", "authors": ["Gilles Dowek", "Th\u00e9r\u00e8se Hardin", "Claude Kirchner"], "n_citation": 0, "title": "Binding logic: Proofs and models", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5ac4aa80-849a-4cd3-a451-bfb6db7772ab"}
{"abstract": "Conditional schema changes change the schema of the tuples that satisfy the change condition. When the schema of a relation changes some tuples may no longer fit the current schema. Handling the mismatch between the intended schema of tuples and the recorded schema of tuples is at the core of a DBMS that supports schema evolution. We propose to keep track of schema mismatches at the level of individual tuples, and prove that evolving schemas with conditional schema changes, in contrast to database systems relying on data migration, are lossless when the schema evolves. The lossless property is a precondition for a flexible semantics that allows to correctly answer general queries over evolving schemas. The key challenge is to handle attribute mismatches between the intended and recorded schema in a consistent way. We provide a parametric approach to resolve mismatches according to the needs of the application. We introduce the mismatch extended completed schema (MECS) which records attributes along with their mismatches, and we prove that relations with MECS are lossless.", "authors": ["Ole Guttorm Jensen", "Michael H. B\u00f6hlen"], "n_citation": 0, "title": "Lossless conditional schema evolution", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5b1dfbf3-0b0b-4f12-91d6-3bce5b560161"}
{"abstract": "A scalable parallel algorithm for matrix multiplication on SISAMD computers is presented. Our method enables us to implement an efficient BLAS library on the Italian APE100/Quadrics SISAMD massively parallel computer on which hitherto scalable parallel BLAS-3 were not available. The approach proposed is based on a one-dimensional ring connectivity. The flow of data is hyper-systolic. The communication overhead is competitive with that of established algorithms for SIMD and MIMD machines. Advantages are that (i) the layout of the matrices is preserved during the computation, (ii) BLAS-2 fit well into this layout and (iii) indexed addressing is avoided, which renders the algorithm suitable for SISAMD machines and! in this way, for all other types of parallel computers. On the APE100/Quadrics, a performance of nearly 25 % of the peak performance for multiplications of complex matrices is achieved.", "authors": ["Th. Lippert", "Nicolai Petkov", "K. Schilling", "Bob Hertzberger", "Peter M. A. Sloot"], "n_citation": 0, "title": "BLAS-3 for the Quadrics parallel computer", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "5ba251f5-2174-48fc-bd44-926bbdd7c25b"}
{"abstract": "The Fiat-Shamir paradigm for transforming identification schemes into signature schemes has been popular since its introduction because it yields efficient signature schemes, and has been receiving renewed interest of late as the main tool in deriving forward-secure signature schemes. We find minimal (meaning necessary and sufficient) conditions on the identification scheme to ensure security of the signature scheme in the random oracle model, in both the usual and the forward-secure cases. Specifically we show that the signature scheme is secure (resp. forward-secure) against chosen-message attacks in the random oracle model if and only if the underlying identification scheme is secure (resp. forward-secure) against impersonation under passive (i.e., eavesdropping only) attacks, and has its commitments drawn at random from a large space. An extension is proven incorporating a random seed into the Fiat-Shamir transform so that the commitment space assumption may be removed.", "authors": ["Michel Abdalla", "Jee Hea An", "Mihir Bellare", "Chanathip Namprempre"], "n_citation": 0, "title": "From identification to signatures via the Fiat-Shamir transform: Minimizing assumptions for security and forward-security", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5eb9f57e-fdae-4fd1-903d-6b1f0d83f0db"}
{"abstract": "in 2  can be considered as an attractive and didactic tool to approach the interaction net paradigm. But it is also an implementation in C of the core of a real programming language featuring a user-friendly graphical syntax and an efficient garbage collector free execution.", "authors": ["Sylvain Lippi"], "n_citation": 50, "title": "in2: A graphical interpreter for interaction nets", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5fc1ab82-8f1b-4e3b-8670-372434033125"}
{"abstract": "In a Proportional Representation (PR) electoral system it is assumed that seats are apportioned to the different electoral districts/states according to the corresponding voters' distribution. In a previous paper we proposed a MILP (Mixed Integer Linear Programming) model to apportion the seats in the European Parliament (EP). Since the exact solution to the problem is not computationally efficient, we have designed a hybrid metaheuristic algorithm based on Variable Neighborhood Search (VNS) and Tabu Search (TS). The proposed approach takes into account the existing situation, guaranteeing a minimum number of seats, independently of the population size of each member. The model is illustrated with actual data and its results are compared with the present apportionment. The results show that the proposed approach can significantly improve the proportionality of the present apportionment.", "authors": ["Gabriel Villa", "Sebasti\u00e1n Lozano", "Jes\u00fas Racero", "David Canca"], "n_citation": 50, "title": "A hybrid VNS/Tabu search algorithm for apportioning the European parliament", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "602d92f5-ca1b-4d9f-a279-b6a8e89d412c"}
{"abstract": "This note is concerned with the executive compressed worm virus and their detection. It is difficult to detect some worm viruses recently since their file structure adopts the type of executive compression which can be run of themselves in the compressed state. In fact there are some informations about executive compression type in the sections of compressed file structure including whether it is executive compressed or not and compression method if compressed. In this research, we have adopted the reverse assembling method to investigate the effective method for detecting the many varietal malicious codes that are generated in different types by the variating the compression methods.", "authors": ["Seung-Jae Yoo", "Kuinam J. Kim"], "n_citation": 0, "title": "Detection Methods for Executive Compressed Malicious Codes in Wire/Wireless Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "61a005ea-1d0c-475e-a9c3-19acbc4085d0"}
{"abstract": "We propose a solution to the problem of robust subspace estimation using the projection based M-estimator. The new method handles more outliers than inliers, does not require a user defined scale of the noise affecting the inliers, handles noncentered data and nonorthogonal subspaces. Other robust methods like RANSAC, use an input for the scale, while methods for subspace segmentation, like GPCA, are not robust. Synthetic data and three real cases of multibody factorization show the superiority of our method, in spite of user independence.", "authors": ["Raghav Subbarao", "Peter Meer"], "n_citation": 50, "title": "Subspace Estimation Using Projection Based M-Estimators over Grassmann Manifolds", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "61f653da-1eb9-44a3-8801-18d2be4c81e6"}
{"abstract": "Recently, a number of instructional design platforms have emerged in educational informatizing process. However, teachers still can not form their individualised instructional design through them, because these platforms lack the combination with a specific subject. Thus, an informatized instructional design platform for physics is programmed in this paper to meet the needs of teachers in physics.", "authors": ["Yongjiang Zhong", "Ju Liu", "Shaochun Zhong", "Yamei Zhang", "Xiaochun Cheng"], "n_citation": 0, "title": "Programming of informatized instructional design platform for physics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "625cb1c9-7933-4435-918a-84d82f33262a"}
{"abstract": "In the high performance radio access networks, the number of random access channels can be used for mobile stations to transmit their bandwidth requests in contention mode via multiple random access. In this paper a collision reduced random access scheme based on m-art split algorithm in the centralized medium access control protocol is presented. In this method the splitting algorithm is used in two fold. On one hand the whole random access channels are exclusively separated into two parts; one is for initial contention where only initial access mobile stations can be served and the other is for retransmit contention where the mobile stations whose initial random access was not successful can join for their retransmission. On the other hand the m-ary split algorithm applied in the retransmit contention area to resolve the collisions. By doing so the proposed scheme achieves considerably greater performance in terms of maximum throughput, mean access delay, and delay jitter, which is one of the important criteria for real-time traffic. Through numerical examples and computer simulations the effect of the various parameters of the algorithm, initial number of random access report(s) and the split size m, on the system performance is examined.", "authors": ["You-Chang Ko", "Eui-Seok Hwang", "Jeong-Jae Won", "Hyong-Woo Lee", "Choong-Ho Cho"], "n_citation": 50, "title": "Collision reduction random access using m-ary split algorithm in wireless access network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "63325b28-4c1f-495b-9de6-6e4147576036"}
{"abstract": "We study exact learning of halfspaces from equivalence queries. The algorithm uses an oracle RCH that returns a random consistent hypothesis to the counterexamples received from the equivalence query oracle. We use the RCH oracle to give a new polynomial time algorithm for exact learning halfspaces from majority of halfspaces and show that its query complexity is less (by some constant factor) than the best known algorithm that learns halfspaces from halfspaces.", "authors": ["Nader H. Bshouty", "Ehab Wattad"], "n_citation": 0, "title": "On exact learning halfspaces with random consistent hypothesis oracle", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65fa1e3e-2076-4c85-b111-b5c1e0d786b7"}
{"abstract": "Integration of multiple heterogeneous data sources continues to be a critical problem for many application domains and a challenge for researchers world-wide. Schema matching, a fundamental aspect of integration, has been a well-studied problem. However researchers have, for the most part, concentrated on the development of different schema matching algorithms, and their performance with respect to the number of matches produced. To the best of our knowledge, current research in schema matching does not address the issue of quality of matching. We believe that quality of match is an important measure that can not only provide a basis for comparing multiple matches, but can also be used as a metric to compare as well as optimize existing match algorithms. In this paper, we define the Quality of Match (QoM) metric, and provide qualitative and quantitative analysis techniques to evaluate the QoM of two given schemata. In particular, we introduce a taxonomy of schema matches as a qualitative analysis technique, and a weight-based match model that in concert with the taxonomy provides a quantitative measure of the QoM. We show, via examples, how QoM can be used to distinguish the goodness of one match in comparison with other matches.", "authors": ["Naiyana Tansalarak", "Kajal T. Claypool"], "n_citation": 0, "title": "QoM: Qualitative and quantitative schema match measure", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "67b413a2-801a-4f00-b801-0c40af84a165"}
{"abstract": "We present a system using semi-autonomous agents to help artists express ideas. Agents control their representation on a canvas via interactions in agent space. They are given rules, which include the ability to form, join, leave and dissolve coalitions. While individually, agents constitute only small parts of the composition, together they form more and more complex parts until the full picture emerges. The artist can take direct control of an agent or coalition if the result is unsatisfactory. We have implemented the Surreal engine that realises these ideas and we present a case study of its usage in producing Mondriaan-like paintings.", "authors": ["Kaye Mason", "J\u00f6rg Denzinger", "Sheelagh Carpendale"], "n_citation": 50, "title": "Negotiating gestalt : Artistic expression by coalition formation between agents", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "67baee3a-ab03-4e5a-b49e-17804277af11"}
{"abstract": "Two-dimensional linear discriminant analysis (2DLDA) was recently developed for face image representation and recognition by adopting the idea of image projection in 2DPCA. 2DLDA outperforms traditional LDA mainly in terms of feature extraction speed. Unfortunately, 2DLDA needs to use large numbers of features to represent an image sample, causing storage requirements are heavy and also feature matching process is time-consuming. Against this problem, we discuss in this paper a new image representation scheme called Enhanced 2DLDA (E-2DLDA) for face recognition. The main strategy adopted in our method is that two image projections are applied to an image sample jointly, so the dimensions of extracted feature matrix along both horizontal direction and vertical direction get compressed, and finally the total number of features can be reduced to a great extent. The experimental results on ORL database show that this method remarkably outperforms existing 2DLDA in terms of speed of feature matching and storage requirements of features.", "authors": ["Long Fei", "Dong Huailin", "Fan Ling", "Chen Hai-shan"], "n_citation": 0, "title": "E-2DLDA : A New Matrix-Based Image Representation Method for Face Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "680a3405-6d46-4a76-919d-af248ea00ddb"}
{"abstract": "A source-less attitude measurement system and robust heading self-calibration method for virtual surgery navigation is presented. The proposed system includes three accelerometers and three magnetometers to measure the gravity field and the geomagnetic filed to calculate the attitude. During the heading calibration procedure, both the magnetic field and gravity field are measured in all three axes with a set of combinations of attitudes. The set of measurements of magnetometer triad are first selected by random sampling consensus (RANSAC), and then are used to fit an ellipsoid to remove the hard iron error, finally the measurements of the magnetometer triad without the hard iron error and the measurements of accelerometer triad are rearranged to a set of equations to solve the soft iron matrix. Compared with other existing calibration method, the proposed calibration method does not require heading reference. Experimental results show the effectiveness of the proposed method and its potential application in virtual surgery navigation.", "authors": ["Xiaoming Hu", "Yue Liu", "Yongtian Wang"], "n_citation": 0, "title": "Study on attitude measurement system for virtual surgery navigation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6c0536ed-4921-4df3-9b67-89b6986a696a"}
{"abstract": "Simulink has been used widely as an industry tool to model and simulate embedded systems. With increasing usage of embedded systems in real-time safety-critical situations, Simulink is deficient to cope with the requirements of high-level assurance and timing analysis. In this paper, we present a systematic approach to translate Simulink diagrams to Timed Interval Calculus (TIC), a notation extending Z to support real-time system specification and verification. This work is based on the same angle chosen by Simulink and TIC where they model systems in terms of continuous time. Translated TIC specifications preserve the functional and timing aspects of the diagrams, and cover a wide range of Simulink blocks. After the translation, we can increase the design space by specifying important requirements, especially timing constraints exactly on the system or its components. Moreover, we can take advantage of TIC reasoning rules to formally verify systems with requirements, and hence elevate the design quality of Simulink.", "authors": ["Chunqing Chen", "Jin Song Dong"], "n_citation": 50, "title": "Applying Timed Interval Calculus to Simulink Diagrams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6e4d9545-9332-4236-aee5-b26cf18134eb"}
{"authors": ["Stephen Kent"], "n_citation": 0, "title": "Rethinking PKI: What's trust got to do with it?", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6fb32eef-e409-4020-853d-5e5513147114"}
{"abstract": "The competitive associative net called CAN2 has been shown effective in many applications, such as function approximation, control, rainfall estimation, time-series prediction, and so on, but the learning method has been constructed basically for reducing the training (empirical) error. In order to reduce prediction (generalization) error, we, in this article, try to apply the ensemble scheme to the CAN2 and present a method to select an effective number of units for the ensemble. We show the result of numerical experiments and examine the effectiveness of the present method.", "authors": ["Shuichi Kurogi", "Shinya Tanaka", "Ryohei Koyama", "Takeshi Nishida"], "n_citation": 0, "title": "Ensemble of Competitive Associative Nets and a Method to Select an Effective Number of Units", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "705d3ff1-2a2e-4b70-b815-97f47452fbff"}
{"authors": ["Tibor Bosse", "Charlotte Gerritsen", "Michel C. A. Klein", "Vu1010095", "Faculteit der Exacte Wetenschappen"], "n_citation": 50, "title": "Predicting the Development of Juvenile Delinquency by Simulation", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "70a7e9dd-0d69-4d53-be3b-0e214527140c"}
{"abstract": "In this paper the performance of the CMA evolution strategy with rank-\u03bc-update and weighted recombination is empirically investigated on eight multimodal test functions. In particular the effect of the population size A on the performance is investigated. Increasing the population size remarkably improves the performance on six of the eight test functions. The optimal population size takes a wide range of values, but, with one exception, scales sub-linearly with the problem dimension. The global optimum can be located in all but one function. The performance for locating the global optimum scales between linear and cubic with the problem dimension. In a comparison to state-of-the-art global search strategies the CMA evolution strategy achieves superior performance on multimodal, non-separable test functions without intricate parameter tuning.", "authors": ["Nikolaus Hansen", "Stefan Kern"], "n_citation": 0, "title": "Evaluating the CMA evolution strategy on multimodal test functions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7102b59b-ee10-425f-8b4a-7b99883e711d"}
{"abstract": "Pulse coupled neural network (PCNN) is different from traditional artificial neural networks, models of which have biological background and are based on the experimental observations of synchronous pulse bursts in the cat visual cortex. However, it is very difficult to determine the exact relationship between the parameters of PCNN model. Focusing on the famous difficult problem of PCNN, how to determine the optimum parameters automatically, this paper proposes the definition of water valley area, establishes a modified PCNN, and puts forward an adaptive PCNN parameters determination algorithm based on water valley area. Extensive experimental results on image processing demonstrate its validity and robustness.", "authors": ["Min Li", "Wei Cai", "Zheng Tan"], "n_citation": 0, "title": "Adaptive Parameters Determination Method of Pulse Coupled Neural Network Based on Water Valley Area", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7350717f-ac9b-4eea-964c-045e24ec4004"}
{"abstract": "We consider the problem of nonrigid shape and motion recovery from point correspondences in multiple perspective views. It is well known that the constraints among multiple views of a rigid shape are multilinear on the image points and can be reduced to bilinear (epipolar) and trilinear constraints among two and three views, respectively. In this paper, we generalize this classic result by showing that the constraints among multiple views of a nonrigid shape consisting of K shape bases can be reduced to multilinear constraints among K + [(K + 1)/2], ..., 2K + 1 views. We then present a closed form solution to the reconstruction of a nonrigid shape consisting of two shape bases. We show that point correspondences in five views are related by a nonrigid quintifocal tensor, from which one can linearly compute nonrigid shape and motion. We also demonstrate the existence of intrinsic ambiguities in the reconstruction of camera translation, shape coefficients and shape bases. Examples show the effectiveness of our method on nonrigid scenes with significant perspective effects.", "authors": ["Ren\u00e9 Vidal", "Daniel Abretske"], "n_citation": 0, "title": "Nonrigid Shape and Motion from Multiple Perspective Views", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "74330e0b-e0e7-4595-89e4-e85f97b9b1d4"}
{"abstract": "We propose two efficient information hiding algorithms in the least significant bits of JPEG coefficients of images. Our algorithms embed information by modifying JPEG coefficients in such a way that the introduced distortion is minimized. We derive the expected value of the additional error due to distortion as a function of the message length and the probability distribution of the JPEG quantization errors of cover images. We have implemented our methods in Java and performed the extensive experiments with them. The experiments have shown that our theoretical predictions agree closely with the actual introduced distortions.", "authors": ["Younhee Kim", "Zoran Duric", "Dana Richards"], "n_citation": 0, "title": "Towards Lower Bounds on Embedding Distortion in Information Hiding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "76c919a5-f37a-4d04-a987-333792c57625"}
{"authors": ["Costas S. Iliopoulos", "Marcin Kubica", "Mohammad Sohel Rahman", "Tomasz Wale\u0144"], "n_citation": 50, "title": "Algorithms for Computing the Longest Parameterized Common Subsequence", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "782d22e4-d8fd-4f77-9322-283fa854f25e"}
{"abstract": "Nowadays, one of the main research issues of great interest is the efficient tracking of mobile objects that enables the effective answering of spatiotemporal queries. This line of research is relevant to a number of modern applications spanning many contexts, In this paper, we consider the organization of a moving object database by quadtree based structures (structures obeying the Embedding Space Hierarchy). In this context, we adapt an indexing method, called XBR trees, to support range queries about the history of trajectories of moving objects. The XBR tree is a quadtree like external memory, balanced and compact structure that follows regular decomposition. Apart from the presentation of the new method, we experimentally show that it outperforms the only previous Embedding Space Hierarchy approach (based on PRM quadtrees) for indexing moving objects.", "authors": ["Katerina Raptopoulou", "Michael Vassilakopoulos", "Yannis Manolopoulos"], "n_citation": 0, "title": "Towards quadtree-based moving objects databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "78a5dc9a-0dff-4846-a8d4-e2d45d5f100f"}
{"abstract": "Radio Frequency Identification (RFID) systems promise large scale, automated tracking solutions but also pose a threat to customer privacy. The tree-based hash protocol proposed by Molnar and Wagner presents a scalable, privacy-preserving solution. Previous analyses of this protocol concluded that an attacker who can extract secrets from a large number of tags can compromise privacy of other tags. We propose a new metric for information leakage in RFID protocols along with a threat model that more realistically captures the goals and capabilities of potential attackers. Using this metric, we measure the information leakage in the tree-based hash protocol and estimate an attacker's probability of success in tracking targeted individuals, considering scenarios in which multiple information sources can be combined to track an individual. We conclude that an attacker has a reasonable chance of tracking tags when the tree-based hash protocol is used.", "authors": ["Karsten Nohl", "David Evans"], "n_citation": 0, "title": "Quantifying information leakage in tree-based hash protocols (short paper)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ab4402f-fdb3-4ba6-9542-e9d8f5fd9eec"}
{"abstract": "This main target of this paper is to present an interaction model allowing the user to feel immersed to the VR content with a storytelling factor. The storytelling factor allows the user to be presented with a series of interaction environment in a narrative form about the task domain. Finding out about how the VR content can be delivered in a more narrative form has produced a method of 'controlling the time factor' within the VR application. This interaction model allows the player action to result in the change of the scenery of the virtual environment as well as the progression of the environment in time. By adding the time controlling factor, a more complex environment can be presented where a novice user may not be overwhelmed by the complexity of the virtual environment. The content designer can maximize the storytelling factor within the VR to gracefully acquaint the novice user and provide a compelling and effective content for the task domain of the VR contents.", "authors": ["Seulki Kang", "Kang Heedong", "Yoon-Chul Choy"], "n_citation": 0, "title": "4-Dimensional context management for interactive virtual storytelling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7bc8365f-8a76-4a2e-bbf0-b70a13b8c81e"}
{"abstract": "For several graph theoretic parameters such as vertex cover and dominating set, it is known that if their values are bounded by k then the treewidth of the graph is bounded by some function of k. This fact is used as the main tool for the design of several fixed-parameter algorithms on minor-closed graph classes such as planar graphs, single-crossing-minor-free graphs, and graphs of bounded genus. In this paper we examine the question whether similar bounds can be obtained for larger minor-closed graph classes, and for general families of parameters including all the parameters where such a behavior has been reported so far. Given a graph parameter P, we say that a graph family F has the parameter-treewidth property for P if there is a function f(p) such that every graph G \u2208 F with parameter at most p has treewidth at most f(p). We prove as our main result that, for a large family of parameters called contraction-bidimensional parameters, a minor-closed graph family F has the parameter-treewidth property if F has bounded local treewidth. We also show if and only if for some parameters, and thus this result is in some sense tight. In addition we show that, for a slightly smaller family of parameters called minor-bidimensional parameters, all minor-closed graph families F excluding some fixed graphs have the parameter-treewidth property. The bidimensional parameters include many domination and covering parameters such as vertex cover, feedback vertex set, dominating set, edge-dominating set, q-dominating set (for fixed q). We use these theorems to develop new fixed-parameter algorithms in these contexts.", "authors": ["Erik D. Demaine", "Fedor V. Fomin", "Mohammad Taghi Hajiaghayi", "Dimitrios M. Thilikos"], "n_citation": 0, "title": "Bidimensional parameters and local treewidth", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7cc7c664-aa3b-4c50-8bd0-45d2ccd29c20"}
{"abstract": "In this paper, we present a method for the automatic acquisition of semantic-based reformulations from natural language questions. Our goal is to find useful and generic reformulation patterns, which can be used in our question answering system to find better candidate answers. We used 1343 examples of different types of questions and their corresponding answers from the TREC-8, TREC-9 and TREC-10 collection as training set. The system automatically extracts patterns from sentences retrieved from the Web based on syntactic tags and the semantic relations holding between the main arguments of the question and answer as defined in WordNet. Each extracted pattern is then assigned a weight according to its length, the distance between keywords, the answer sub-phrase score, and the level of semantic similarity between the extracted sentence and the question. The system differs from most other reformulation learning systems in its emphasis on semantic features. To evaluate the generated patterns, we used our own Web QA system and compared its results with manually created patterns and automatically generated ones. The evaluation on about 500 questions from TREC-11 shows comparable results in precision and MRR scores. Hence, no loss of quality was experienced, but no manual work is now necessary.", "authors": ["Jamileh Yousefi", "Leila Kosseim"], "n_citation": 0, "title": "Automatic acquisition of semantic-based question reformulations for question answering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ce90c9e-a54a-4361-a9ac-dcc67269e7a5"}
{"abstract": "Next generation network faces many challenges. In this paper, the new mechanism based on mobile agent is focused and the core architecture called common agent request broker architecture (CARBA) is established in order to embody intelligent collaboration and high performance in the next generation network. Then mobile agent based computing architecture (MABCA), extension of MABCA in nomadic environment and validation of MABCA openness is discussed. In the end, experiment results are given out.", "authors": ["Yun-Yong Zhang", "Zhi-Jiang Zhang", "Fan Zhang", "Yunjie Liu"], "n_citation": 0, "title": "Mobile computing architecture in next generation network", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7d203373-6146-41ae-b0f1-0ea48ce7016e"}
{"abstract": "Association redefinition is a new concept in UML 2.0 that makes it possible to impose additional constraints on some instances of associations. In this paper, we describe how to use association redefinition to declare additional referential integrity and cardinality constraints for associations. We also analyze the interactions between taxonomic constraints and association redefinitions and their impact on the satisfaction of taxonomic constraints. Finally, we establish several conditions that are necessary to guarantee well-formed association redefinitions.", "authors": ["Dolors Costal", "Cristina G\u00f3mez"], "n_citation": 0, "title": "On the Use of Association Redefinition in UML Class Diagrams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7f6b19b9-760e-4085-8c52-78bc87cff3ac"}
{"abstract": "In logic programs, negation-as-failure has been used both for representing negative information and for providing default nonmonotonic inference. In this paper we argue that this twofold role is not only unnecessary for the expressiveness of the language, but it also plays against declarative programming, especially if further negation symbols such as strong negation are also available. We therefore propose a new logic programming approach in which negation and default inference are independent, orthogonal concepts. Semantical characterization of this approach is given in the style of answer sets, but other approaches are also possible. Finally, we compare them with the semantics for logic programs with two kinds of negation.", "authors": ["Pablo Rub\u00e9n Fillottrani", "Guillermo Ricardo Simari"], "n_citation": 0, "title": "Representing defaults and negative information without negation-as-failure", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "80163b83-2414-44ba-bbd8-de2db8e992ad"}
{"abstract": "Nested logic programs have recently been introduced in order to allow for arbitrarily nested formulas in the heads and the bodies of logic program rules under the answer sets semantics. Previous results show that nested logic programs can be transformed into standard (unnested) disjunctive logic programs in an elementary way, applying the negation-as-failure operator to body literals only. This is of great practical relevance since it allows us to evaluate nested logic programs by means of off-the-shelf disjunctive logic programming systems, like DLV. However, it turns out that this straightforward transformation results in an exponential blow-up in the worst-case, despite the fact that complexity results indicate that there is a polynomial translation among both formalisms. In this paper, we take up this challenge and provide a polynomial translation of logic programs with nested expressions into disjunctive logic programs. Moreover, we show that this translation is modular and (strongly) faithful. We have implemented both the straightforward as well as our advanced transformation; the resulting compiler serves as a front-end to DLV and is publicly available on the Web.", "authors": ["David Pearce", "Vladimir Sarsakov", "Torsten Schaub", "Hans Ibmpits", "Stefan Woltran"], "n_citation": 0, "title": "A polynomial translation of logic programs with nested expressions into disjunctive logic programs: Preliminary report", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "84606942-0048-437e-b576-e22d2c35ed66"}
{"abstract": "We consider the problem of computing the intersection of private datasets of two parties, where the datasets contain lists of elements taken from a large domain. This problem has many applications for online collaboration. We present protocols, based on the use of homomorphic encryption and balanced hashing, for both semi-honest and malicious environments. For lists of length k, we obtain O(k) communication overhead and O(k In In k) computation. The protocol for the semi-honest environment is secure in the standard model, while the protocol for the malicious environment is secure in the random oracle model. We also consider the problem of approximating the size of the intersection, show a linear lower-bound for the communication overhead of solving this problem, and provide a suitable secure protocol. Lastly, we investigate other variants of the matching problem, including extending the protocol to the multi-party setting as well as considering the problem of approximate matching.", "authors": ["Michael J. Freedman", "Kobbi Nissim", "Benny Pinkas"], "n_citation": 0, "title": "Efficient private matching and set intersection", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "85f77b34-7037-474a-862f-790f8c682d36"}
{"abstract": "We study the underapproximation of the predicate transformers used to give semantics to the modalities in dynamic and temporal logic. Because predicate transformers operate on state sets, we define appropriate powerdomains for sound approximation. We study four such domains - two are based on set inclusion approximation, and two are based on quantification approximation - and we apply the domains to synthesize the most precise, underapproximating pre and pre transformers, in the latter case, introducing a focus operation. We also show why the expected abstractions of post and post are unsound, and we use the powerdomains to guide us to correct, sound underapproximations.", "authors": ["David A. Schmidt"], "n_citation": 0, "title": "Underapproximating predicate transformers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "87e32780-8dac-445b-a3f3-1e6f3f8956e0"}
{"abstract": "The bisection method is the consecutive bisection of a triangle by the median of the longest side. This paper introduces a taxonomy of triangles that precisely captures the behavior of the bisection method. Our main result is an asymptotic upper bound for the number of similarity classes of triangles generated on a mesh obtained by iterative bisection, which previously was known only to be finite. We also prove that the number of directions on the plane given by the sides of the triangles generated is finite. Additionally, we give purely geometric and intuitive proofs of classical results for the bisection method.", "authors": ["Claudio Gutierrez", "Flavio Gutierrez", "Maria-Cecilia Rivara"], "n_citation": 0, "title": "A geometric approach to the bisection method", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8955483b-bbec-4399-99fa-e23f9588614f"}
{"abstract": "In this paper we describe a model in which artificial evolution is employed to design neural mechanisms that control the motion of two autonomous robots required to communicate through sound to perform a common task. The results of this work are a \"proof-of-concept\": they demonstrate that evolution can exploit a very simple sound communication system, to design the mechanisms that allow the robots cooperate by employing acoustic interactions. The analysis of the evolved strategies uncover the basic properties of the communication protocol. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "authors": ["Elio Tuci", "Christos Ampatzis"], "n_citation": 0, "title": "Evolution of acoustic communication between two cooperating robots", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "8a55466c-d9ef-4d10-8266-1154cdb59712"}
{"abstract": "Leo Harrington surprisingly constructed a machine which can learn any computable function f according to the following criterion (called Bc * -identification). His machine, on the successive graph points of f, outputs a corresponding infinite sequence of programs p 0 ,p 1 ,p 2 ., and, for some i, the programs p i ,p i+1 ,p i+2  each compute a variant of f which differs from f at only finitely many argument places. A machine with this property is called general purpose. The sequence p i ,p i+1 ,p i+2  is called a final sequence. For Harrington's general purpose machine, for distinct m and n, the finitely many argument places where p i+m  fails to compute f can be very different from the finitely many argument places where p i+n  fails to compute f. One would hope though, that if Harrington's machine, or an improvement thereof, inferred the program p i+m  based on the data points f(0), f(1) f(k), then p i+m  would make very few mistakes computing f at the near future' arguments k + 1, k + 2 k +  , where   is reasonably large. Ideally, p i+m 'S finitely many mistakes or anomalies would (mostly) occur at arguments x k, i.e., ideally, its anomalies would be well placed beyond near future arguments. In the present paper, for general purpose learning machines, it is analyzed just how well or badly placed these anomalies may be with respect to near future arguments and what are the various tradeoffs. In particular, there is good news and bad. Bad news is that, for any learning machine M (including general purpose M), for all m, there exist infinitely many computable functions f such that, infinitely often M incorrectly predicts f's next m near future values. Good news is that, for a suitably clever general purpose learning machine M, for each computable f, for M on f, the density of any such associated bad prediction intervals of size m is vanishingly small.", "authors": ["John Case", "Keh Jiann Chen", "Sanjay Jain"], "n_citation": 0, "title": "Costs of general purpose learning", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "8a99c9b3-607e-4084-a8d1-985ab797b703"}
{"abstract": "Object- oriented frameworks are designed to provide functionality common to a variety of applications. Developers use these frameworks in building their own specialized applications, often without having the source code of the original framework. Unfortunately, the interactions between the framework components and the new application code can lead to behaviors that could not be predicted even if valid black-box specifications were provided for the framework components. What is needed are grey-box specifications that include information about sequences of method calls made by the original framework code. Our focus is on how to test frameworks against such specifications, which requires the ability to monitor such method calls made by the framework during testing. The problem is that without the source code of the framework, we cannot resort to code instrumentation to track these calls. We develop an approach that allows us to do this, and demonstrate it on a simple case study.", "authors": ["Benjamin Tyler", "Neelam Soundarajan"], "n_citation": 0, "title": "Black-box testing of Grey-box behavior", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8c3d6e75-76e8-4867-804e-de44c3795002"}
{"abstract": "We consider the problem of designing error correcting codes (ECC), a hard combinatorial optimization problem of relevance in the field of telecommunications. This problem is tackled here with two related techniques, scatter search and memetic algorithms. The instantiation of these techniques for ECC design will be discussed. Specifically, the design of the local improvement strategy and the combination method will be treated. The empirical evaluation will show that these techniques can dramatically outperform previous approaches to this problem. Among other aspects, the influence of the update method, or the use of path relinking is also analyzed on increasingly large problem instances.", "authors": ["Carlos Cotta"], "n_citation": 0, "title": "Scatter search and memetic approaches to the error correcting code problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8e6d34fd-7748-484e-bcc5-18d1487b862f"}
{"abstract": "Abbreviated words carry critical information in the literature of many special domains. This paper reports our research in recognizing dotted abbreviations with MaxEnt model. The key points in our work include: (1) allowing the model to optimize with as many features as possible to capture the text characteristics of context words, and (2) utilizing simple lexical information such as sentence-initial words and candidate word length for performance enhancement. Experimental results show that this approach achieves impressive performance on the WSJ corpus.", "authors": ["Chunyu Kit", "Xiaoyue Liu", "Jonathan J. Webster"], "n_citation": 0, "title": "Abbreviation recognition with MaxEnt model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "90ebbf69-3224-4f2e-babd-eb8ad8b1c3ee"}
{"abstract": "The nature of construction claims is highly complicated and the cost involved is high. It will be advantageous if the parties to a dispute may know with some certainty how the case would be resolved if it were taken to court. The recent advancements in artificial neural networks may render a cost-effective technique to help to predict the outcome of construction claims, on the basis of characteristics of cases and the corresponding past court decisions. In this paper, a split-step particle swarm optimization (PSO) model is applied to train perceptrons in order to predict the outcome of construction claims in Hong Kong. It combines the advantages of global search capability of PSO algorithm in the first step and the local convergence of back-propagation algorithm in the second step. It is shown that, through a real application case, its performance is much better than the benchmark backward propagation algorithm and the conventional PSO algorithm.", "authors": ["Kwok-wing Chau"], "n_citation": 0, "title": "Prediction of Construction Litigation Outcome Using a Split-Step PSO Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "92279f1f-bb73-4f39-b2a2-c1a815950cd8"}
{"abstract": "Clustering is a major technique in data mining. However the numerical feedback of clustering algorithms is difficult for user to have an intuitive overview of the dataset that they deal with. Visualization has been proven to be very helpful for high-dimensional data analysis. Therefore it is desirable to introduce visualization techniques with user's domain knowledge into clustering process. Whereas most existing visualization techniques used in clustering are exploration oriented. Inevitably, they are mainly stochastic and subjective in nature. In this paper, we introduce an approach called HOV 3  (Hypothesis Oriented Verification and Validation by Visualization), which projects high-dimensional data on the 2D space and reflects data distribution based on user hypotheses. In addition, HOV 3  enables user to adjust hypotheses iteratively in order to obtain an optimized view. As a result, HOV 3  provides user an efficient and effective visualization method to explore cluster information.", "authors": ["Ke-Bing Zhang", "Mehmet A. Orgun", "Kang Zhang"], "n_citation": 15, "title": "HOV3 : An Approach to Visual Cluster Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "92eeeead-f7e3-41a5-94c1-bedd6ef9e8ff"}
{"abstract": "At Eurocrypt '96, Coppersmith proposed an algorithm for finding small roots of bivariate integer polynomial equations, based on lattice reduction techniques. But the approach is difficult to understand. In this paper, we present a much simpler algorithm for solving the same problem. Our simplification is analogous to the simplification brought by Howgrave-Graham to Coppersmith's algorithm for finding small roots of univariate modular polynomial equations. As an application, we illustrate the new algorithm with the problem of finding the factors of n = pq if we are given the high order 1/4 log 2  n bits of p.", "authors": ["Jean-S\u00e9bastien Coron"], "n_citation": 0, "title": "Finding small roots of bivariate integer polynomial equations revisited", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "960afbb1-9f4a-47a2-9af3-97ecaab43866"}
{"abstract": "The design and development of hypermedia applications that are deployed on the Wireless World Wide Web is a daunting task because of the various available platforms and the restrictions and capabilities imposed by the lack of established standards and the exponentially increasing number of emerging platforms. There are now justifiable research and development efforts that attempt to formalize the engineering process of such applications in order to achieve certain quality attributes like modifiability, maintainability and portability. This paper presents such an attempt for designing a conceptual model for hypermedia applications that allows for easy update and alteration of its content as well as its presentation and also allows for deployment in various mobile platforms. In specific this model explicitly separates the hypermedia content from its presentation to the user, by employing XML content storage and XSL transformations. Our work is based upon the empirical results of designing, developing and deploying hypermedia applications for mobile platforms, and on the practices of well-established hypermedia engineering techniques.", "authors": ["Dionysios G. Synodinos", "Paris Avgeriou"], "n_citation": 0, "title": "m-WOnDA: The Write Once 'n' Deliver Anywhere model for mobile users", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "97086613-98d4-4aed-b211-45d01bc6d024"}
{"abstract": "Causal perception is an important cognitive phenomenon, which plays a central role in our understanding of the world. As such, its study is also relevant to interactive graphics systems. In this paper, we introduce a virtual reality system which can elicit causal perception. The system operates by intercepting the consequences of subjects' actions and modifying them to generate new event co-occurrence. This process is based on an explicit representation of action structure, which supports the generation of event co-occurrences on a principled basis. We present results from a user experiment, which confirm a high level of causal perception in subjects having used the system.", "authors": ["Jean-Luc Lugrin", "Marc Cavazza", "Marc J. Buehner"], "n_citation": 0, "title": "Causal perception in virtual environments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "98da55c8-34d5-4a5a-b66e-aeee59dfa84e"}
{"abstract": "We give a complete characterization for all those morphisms on a two-letter alphabet that are repetitive. Based on this characterization we describe those morphisms on a two-letter alphabet that generate an ultimately periodic infinite word, and those morphisms f for which the languages L(f) or SL(f) are context-free or even regular.", "authors": ["Yuji Kobayashi", "Friedrich Otto", "Patrice S\u00e9\u00e9bold"], "n_citation": 0, "title": "A complete characterization of repetitive morphisms over the two-letter alphabet", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "99225ca9-d959-4df6-b800-e9c0c2135be6"}
{"abstract": "Cryptographic computations (decryption, signature generation, etc.) are often performed on a relatively insecure device (e.g., a mobile device or an Internet-connected host) which cannot be trusted to maintain secrecy of the private key. We propose and investigate the notion of key-insulated security whose goal is to minimize the damage caused by secret-key exposures. In our model, the secret key(s) stored on the insecure device are refreshed at discrete time periods via interaction with a physically-secure - but computationally-limited - device which stores a master key. All cryptographic computations are still done on the insecure device, and the public key remains unchanged. In a (t, N)-key-insulated scheme, an adversary who compromises the insecure device and obtains secret keys for up to t periods of his choice is unable to violate the security of the cryptosystem for any of the remaining N - t periods. Furthermore, the scheme remains secure (for all time periods) against an adversary who compromises only the physically-secure device. We focus primarily on key-insulated public-key encryption. We construct a (t, N)-key-insulated encryption scheme based on any (standard) public-key encryption scheme, and give a more efficient construction based on the DDH assumption. The latter construction is then extended to achieve chosen-ciphertext security.", "authors": ["Yevgeniy Dodis", "Jonathan Katz", "Shouhuai Xu", "Moti Yung"], "n_citation": 415, "title": "Key-insulated public key cryptosystems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "99a82f5b-aa75-418c-bb46-3774a3e1199f"}
{"abstract": "We describe an interprocedural shape analysis that makes use of spatial locality (i.e. the fact that most procedures modify only a small subset of the heap) in its representation of abstract states. Instead of tracking reachability information directly and aliasing information indirectly, our representation tracks reachability indirectly and aliasing directly. Computing the effect of procedure calls and returns on an abstract state is easy because the representation exhibits spatial locality mirroring the locality that is present in the concrete semantics. The benefits of this approach include improved speed, support for programs that deallocate memory, the handling of bounded numbers of heap cutpoints, and support for cyclic and shared data structures.", "authors": ["Alexey Gotsman", "Josh Berdine", "Byron Cook"], "n_citation": 0, "title": "Interprocedural shape analysis with separated heap abstractions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9c5a0153-cb67-4ce8-8194-2af31552a0ae"}
{"abstract": "In the information retrieval and document categorization context, both Euclidean distance- and cosine-based similarity models are based on the assumption that term vectors are orthogonal. But this assumption is not true. Term associations are ignored in such similarity models. This paper analyzes the properties of term-document space, term-category space and category-document space. Then, without the assumption of term independence, we propose a new mathematical model to estimate the association between terms and define a e-similarity model of documents. Here we make best use of existing category membership represented by corpus as much as possible, and the objective is to improve categorization performance. The empirical results been obtained by k-NN classifier over Reuters-21578 corpus show that utilization of term association can improve the effectiveness of categorization system and e-similarity model outperforms than ones without term association.", "authors": ["Huaizhong Kou", "Georges Gardarin"], "n_citation": 0, "title": "Similarity model and term association for document categorization", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9e2db932-4a48-4340-87e5-5ea34e5585e0"}
{"abstract": "This paper describes how the concept of four-dimensional world can be introduced to players through series of carefully sequenced games. Starting from playing games in one-dimensional world, and then followed by games in two-dimensional and three-dimensional ones, the players are progressively led to generalized rules that can be applied to the four-dimensional world, in which the generalized rule is tested. Different series of games were designed for players to learn the different features of four-dimensional world and were implemented into a web-based learning system which is now undergone field tests.", "authors": ["Fong Lok Lee", "J. H. M. Lee", "Marti K. H. Wong", "Le Wang"], "n_citation": 0, "title": "Exploring the fourth dimension : The design of a multimedia learning system for generalization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9ea7e096-0d45-4ba6-9399-d5fca803e39e"}
{"abstract": "We propose a methodological framework for building a statistical integration model for heterogeneous data sources. We apply the latent class analysis, a well-established statistical method, to investigate the relationships between entities in data sources as relationships among dependent variables, with the purpose of discovering the latent factors that affect them. The latent factors are associated with the real world entities which are unobservable in the sense that we do not know the real world class memberships, but only the stored data. The approach provides the evaluation of uncertainties which aggregate in the integration process. The key parameter evaluated by the method is the probability of the real world class membership. Its value varies depending on the selection criteria applied in the pre-integration stages and in the subsequent integration steps. By adjusting selection criteria and the integration strategies the proposed framework allows to improve data quality by optimizing the integration process.", "authors": ["Evguenia Altareva", "Stefan Conrad"], "n_citation": 0, "title": "Statistical analysis as methodological framework for data(base) integration", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9ec3c1f1-6e56-4e30-b589-80ce74b27863"}
{"abstract": "A fundamental phenomenon in Natural Language Processing concerns the semantic variability of expressions. Identifying that two texts express the same meaning with different words is a challenging problem. We discuss the role of entailment for various Natural Language Processing applications and develop a machine learning system for their resolution. In our system, text similarity is based on the number of consecutive and non-consecutive word overlaps between two texts. The system is language and resource independent, as it does not use external knowledge resources such as WordNet, thesaurus, semantic, syntactic or part-of-speech tagging tools. In this paper all tests were done for English, but our system can be used with no restrains by other languages.", "authors": ["Zornitsa Kozareva", "Andr\u00e9s Montoyo"], "n_citation": 0, "title": "The role and resolution of textual entailment in natural language processing applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a0a630a8-3924-4a8d-a46b-9eeb8416f3bf"}
{"abstract": "Refinement is a key concept in the B-Method. While refinement is at the heart of the B Method, so far no automatic refinement checker has been developed for it. In this paper we present a refinement checking algorithm and implementation for B. It is based on using an operational semantics of B, obtained in practice by the PROB animator. The refinement checker has been integrated into PROB toolset and we present various case studies and empirical results in the paper, showing the algorithm to be surprisingly effective. The algorithm checks that a refinement preserves the trace properties of a specification. We also compare our tool against the refinement checker FDR for CSP and discuss an extension for singleton failure refinement.", "authors": ["Michael Leuschel", "Michael J. Butler"], "n_citation": 0, "title": "Automatic refinement checking for B", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a1cd48c6-cdb9-4b4e-9421-bafd4e5be644"}
{"abstract": "Imputing missing values in high dimensional time series is a difficult problem. There have been some approaches to the problem [11,8] where neural architectures were trained as probabilistic models of the data. However, we argue that this approach is not optimal. We propose to view temporal neural networks with latent variables as energy-based models and train them for missing value recovery directly. In this paper we introduce two energy-based models. The first model is based on a one dimensional convolution and the second model utilizes a recurrent neural network. We demonstrate how ideas from the energy-based learning framework can be used to train these models to recover missing values. The models are evaluated on a motion capture dataset.", "authors": ["Phil\u00e9mon Brakel", "Benjamin Schrauwen"], "n_citation": 0, "title": "Energy-based temporal neural networks for imputing missing values", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "a25e993c-9399-423c-b2f1-bc30219e437b"}
{"abstract": "We develop a primal heuristic based on a genetic algorithm for the minimum graph bisection problem and incorporate it in a branch-and-cut framework. The problem concerns partitioning the nodes of a weighted graph into two subsets such that the total weight of each set is within some lower and upper bounds. The objective is to minimize the total cost of the edges between both subsets of the partition. We formulate the problem as an integer program. In the genetic algorithm the LP-relaxation of the IP-formulation is exploited. We present several ways of using LP information and demonstrate the computational success.", "authors": ["Michael Armbruster", "Marzena Fiigenschuh", "Christoph Helmberg", "Nikolay Jetchev", "Alexander Martin"], "n_citation": 0, "title": "Hybrid genetic algorithm within branch-and-cut for the minimum graph bisection problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a3212842-776a-490d-ae66-ac964d1eeee4"}
{"abstract": "Lazy algorithms for Satisfiability Modulo Theories (SMT) combine a generic DPLL-based SAT engine with a theory solver for the given theory T that can decide the T-consistency of conjunctions of ground literals. For many theories of interest, theory solvers need to reason by performing internal case splits. Here we argue that it is more convenient to delegate these case splits to the DPLL engine instead. The delegation can be done on demand for solvers that can encode their internal case splits into one or more clauses, possibly including new constants and literals. This results in drastically simpler theory solvers. We present this idea in an improved version of DPLL(T), a general SMT architecture for the lazy approach, and formalize and prove it correct in an extension of Abstract DPLL Modulo Theories, a framework for modeling and reasoning about lazy algorithms for SMT. A remarkable additional feature of the architecture, also discussed in the paper, is that it naturally includes an efficient Nelson-Oppen-like combination of multiple theories and their solvers.", "authors": ["Clark Barrett", "Robert Nieuwenhuis", "Albert Oliveras", "Cesare Tinelli"], "n_citation": 0, "title": "Splitting on demand in SAT modulo theories", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a4299d86-a592-44b7-805b-4d55fcfdebd3"}
{"abstract": "As document collections accummulate over time, some of the discussion subjects in them become outfashioned, while new ones emerge. Then, old classification schemes should be updated. In this paper, we address the challenge of finding emerging and persistent themes, i.e. subjects that live long enough to be incorporated into a taxonomy or ontology describing the document collection. We focus on the identification of cluster labels that survive changes in the constitution of the underlying population of documents, including changes in the feature space of dominant words, because the terminology of the document archive also changes over time. We have conducted a set of promising experiments on the identification of themes that manifested themselves in section H2.8 of the ACM digital library and juxtapose them with the classes foreseen in the ACM taxonomy for this section.", "authors": ["Rene Schult", "Myra Spiliopoulou"], "n_citation": 0, "title": "Discovering emerging topics in unlabelled text collections", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a9f08cb6-e7dd-4a24-8538-29823cb1ae24"}
{"abstract": "We show the existence of an infinitary confluent and normalising extension of the finite extensional lambda calculus with beta and eta. Besides infinite beta reductions also infinite eta reductions are possible in this extension, and terms without head normal form can be reduced to bottom. As corollaries we obtain a simple, syntax based construction of an extensional Bohm model of the finite lambda calculus; and a simple, syntax based proof that two lambda terms have the same semantics in this model if and only if they have the same eta-Bohm tree if and only if they are observationally equivalent wrt to beta normal forms. The confluence proof reduces confluence of beta, bottom and eta via infinitary commutation and postponement arguments to confluence of beta and bottom and confluence of eta. We give counterexamples against confluence of similar extensions based on the identification of the terms without weak head normal form and the terms without top normal form (rootactive terms) respectively.", "authors": ["Paula Severi", "Fer-Jan de Vries"], "n_citation": 0, "title": "An extensional B\u00f6hm model", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "aaeb299e-3069-4a99-a5dc-f1e95b689fce"}
{"abstract": "The characteristics of the search space (its size and shape as well as solution density) are key issues in the application of evolutionary algorithms to real-world problems. Often some regions are crowded and other regions are almost empty. Therefore, some techniques must be used to avoid solutions too close (which the decision maker is indifferent to) and to allow all the regions of interest to be adequately represented in the population. In this paper the concept of \u03b4-non-dominance is introduced which is based on indifference thresholds. Experiments dealing with the use of this technique in the framework of an evolutionary approach are reported to provide decision support in the identification and selection of electric load control strategies.", "authors": ["\u00c1lvaro Gomes", "Carlos Henggeler Antunes", "Ant\u00f3nio Martins"], "n_citation": 0, "title": "Dealing with solution diversity in an EA for multiple objective decision support: A case study", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "abb95cc7-9221-42f0-808b-ff2cc4c9311b"}
{"abstract": "We study the isomorphism problem of two natural algebraic structures - F-algebras and cubic forms. We prove that the F-algebra isomorphism problem reduces in polynomial time to the cubic forms equivalence problem. This answers a question asked in [AS05]. For finite fields of the form 3 (#F - 1), this result implies that the two problems are infact equivalent. This result also has the following interesting consequence: formula chim.", "authors": ["Manindra Agrawal", "Nitin Saxena"], "n_citation": 50, "title": "Equivalence of F-algebras and cubic forms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "adb83ff7-d38a-48fe-bfac-c01b3f310826"}
{"abstract": "We show that RL C L/O(n), i.e., any language computable in randomized logarithmic space can be computed in deterministic logarithmic space with a linear amount of non-uniform advice. To prove our result we use an ultra-low space walk on the Gabber-Galil expander graph due to Gutfreund and Viola.", "authors": ["Lance Fortnow", "Adam R. Klivans"], "n_citation": 0, "title": "Linear advice for randomized logarithmic space", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b094ba70-4c8c-4d49-9a5c-4ef384cebf0e"}
{"abstract": "The objective of this paper is to define an ontology language to support multiple representations of ontologies. In our research, we focus on the logic-based ontology languages. As a matter of fact, we will consider only languages that are based on description logics (DLs). At first, we propose a sub-language of DL as an ontology language. Furthermore we achieve multiple representations of ontological concepts by extending such sub-language through the use of stamping mechanism proposed in the context of multiple representation of spatial databases. The proposed language should offer a modest solution to the problem of multirepresentation ontologies.", "authors": ["Djamal Benslimane", "Christelle Vangenot", "Catherine Roussey", "Ahmed Arara"], "n_citation": 0, "title": "Multirepresentation in ontologies", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b1f662ac-7591-4e84-863d-a37ac3a671cf"}
{"authors": ["G. van Lankveld", "S. Schreurs", "Pieter Spronck", "H.J. van den Herik", "H.J. van den Herik", "H. Lida", "A. Plaat"], "n_citation": 0, "title": "Extraversion in Games", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "b9431d3e-08cb-4a76-a51a-21b08ca1bcfe"}
{"abstract": "This paper focuses on the use of sense clusters for classification and clustering of very short texts such as conference abstracts. Common keyword-based techniques are effective for very short documents only when the data pertain to different domains. In the case of conference abstracts, all the documents are from a narrow domain (i.e., share a similar terminology), that increases the difficulty of the task. Sense clusters are extracted from abstracts, exploiting the WordNet relationships existing between words in the same text. Experiments were carried out both for the categorization task, using Bernoulli mixtures for binary data, and the clustering task, by means of Stein's MajorClust method.", "authors": ["Davide Buscaldi", "Paolo Rosso", "Mikhail Alexandrov", "Alfons Juan C\u00edscar"], "n_citation": 0, "title": "Sense cluster based categorization and clustering of abstracts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bb1859c0-a6f1-4289-9e93-1138546df373"}
{"abstract": "In this paper, we propose a new variant of the breadth-first shortest path search called Markov Cluster Shortest Path (MCSP). This is applied to the associative semantic network to show us the flow of association between two very different concepts, by providing the shortest path of them. MCSP is obtained from the virtual adjacency matrix of the hard clusters taken as vertices after MCL process. Since each hard cluster grouped by concepts as a result of MCL has no overlap with others, we propose a method called Alibi-breaking algorithm, which calculates the adjacency matrix of them in a way of collecting their past overlapping information by tracing back to the on-going MCL loops. The comparison is made between MCSP and the ordinary shortest paths to know the difference in quality.", "authors": ["Jaeyoung Jung", "Maki Miyake", "Hiroyuki Akama"], "n_citation": 50, "title": "Markov cluster shortest path founded upon the alibi-breaking algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bb460241-fc54-4a04-9774-64f2cfff6238"}
{"abstract": "The automata-theoretic approach to LTL verification relies on an algorithm for finding accepting cycles in a Biichi automaton. Explicit-state model checkers typically construct the automaton on the fly and explore its states using depth-first search. We survey algorithms proposed for this purpose and identify two good algorithms, a new algorithm based on nested DFS, and another based on strongly connected components. We compare these algorithms both theoretically and experimentally and determine cases where both algorithms can be useful.", "authors": ["Stefan Schwoon", "Javier Esparza"], "n_citation": 122, "title": "A note on on-the-fly verification algorithms", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bc2c4c21-a9f8-4493-9b5c-c7fe559561ca"}
{"abstract": "There currently exist many geographical databases that represent a same part of the world, each with its own levels of detail and points of view. The use and management of these databases sometimes requires their integration into a single database. One important issue in this integration process is the ability to analyse and understand the differences among the multiple representations. These differences can of course be explained by the various specifications but can also be due to updates or errors during data capture. In this paper, after describing the overall process of integrating spatial databases, we propose a process to interpret the differences between two representations of the same geographic phenomenon. Each step of the process is based on the use of an expert system. Rules guiding the process are either introduced by hand from the analysis of specifications, or automatically learnt from examples. The process is illustrated through the analysis of the representations of traffic circles in two actual databases.", "authors": ["David Sheeren", "S\u00e9bastien Musti\u00e8re", "Jean-Daniel Zucker"], "n_citation": 0, "title": "How to integrate heterogeneous spatial databases in a consistent way", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "bdcafd0f-7a7d-4cb7-a52f-ac197876fb05"}
{"abstract": "This paper presents a methodology for the assessment of Web site quality with a particular emphasis public sector Web sites. In order to rate a site, a hierarchical model is proposed, which comprises several quality attributes. The methodology is designed for use by independent analysts who may have no knowledge of the technology underlying the site nor any contact with the site managers. We used the methodology to assess the quality of several Web sites of the Italian Public Administration. The results of the analysis are presented as a case study.", "authors": ["Paolo Atzeni", "Paolo Merialdo", "Giuseppe Sindoni"], "n_citation": 50, "title": "Web Site Evaluation: Methodology and Case Study", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "be777562-4fd0-4ef9-9a22-22601b1f29e6"}
{"abstract": "Variants of Herbelin's A-calculus, here collectively named Herbelin calculi, have proved useful both in foundational studies and as internal languages for the efficient representation of A-terms. An obvious requirement of both these two kinds of applications is a clear understanding of the relationship between cut-elimination in Herbelin calculi and normalisation in the A-calculus. However, this understanding is not complete so far. Our previous work showed that A is isomorphic to a Herbelin calculus, here named AP, only admitting cuts that are both left- and right-permuted. In this paper we consider a generalisation APh admitting any kind of right-permuted cut. We show that there is a natural deduction system \u03bbNh which conservatively extends A and is isomorphic to APh. The idea is to build in the natural deduction system a distinction between applicative term and application, together with a distinction between head and tail application. This is suggested by examining how natural deduction proofs are mapped to sequent calculus derivations according to a translation due to Prawitz. In addition to \u03b2, \u03bbNh includes a reduction rule that mirrors left permutation of cuts, but without performing any append of lists/spines.", "authors": ["Jos\u00e9 Esp\u00edrito Santo"], "n_citation": 0, "title": "An isomorphism between a fragment of sequent calculus and an extension of natural deduction", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "befc8eb3-c202-4cfb-b9b3-fde2d724543a"}
{"abstract": "We present the tool MSCAN, which supports MSC-based system development. In particular, it automatically checks high-level MSC specifications for implementability.", "authors": ["Benedikt Bollig", "Carsten Kern", "Markus Schl\u00fctter", "Volker Stolz"], "n_citation": 0, "title": "MSCAN : A tool for analyzing MSC specifications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bf921762-73ab-45f7-b7fc-717b8ca13d12"}
{"abstract": "Service choreography has become an emerging and promising technology to design and build complex cross-enterprise business applications. Dynamic composition of services on the fly requires mechanisms for ensuring that the component services in the composition are compatible with each other. Current service composition languages provide notations for describing the interactions among component services. However, they focus only on the compatibility at the syntax and semantic level in an informal way, yet ignoring the dynamic behavior within services. This paper emphasizes the importance of the behavior in the compatibility verification between services and utilizes the -calculus to model the service behavior and the interaction in a formal way. Based on the formalization, it proposes a method based on the operational semantics of the -calculus to automate the verification of compatibility between two services and presents an algorithm to measure the compatibility degree quantitatively.", "authors": ["Shuiguang Deng", "Zhaohui Wu", "MengChu Zhou", "Ying Li", "Jian Wu"], "n_citation": 0, "title": "Modeling Service Compatibility with Pi-calculus for Choreography", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c3faa29e-eb82-4c67-9fe7-dd6e176ea850"}
{"abstract": "This paper presents a completeness result for a first-order interval temporal logic, called Neighbourhood Logic (NL) which has two neighbourhood modalities. NL cap support the specification of liveness and fairness properties of computing systems as well as formalisation of many concepts of real analysis. These two modalities are also adequate in the sense that they can derive other important unary and binary modalities of interval temporal logic. We prove the completeness result for NL by giving a Kripke model semantics and then mapping the Kripke models to the interval models for NL.", "authors": ["Rana Barua", "S. M. Roy", "Zhou Chaochen"], "n_citation": 50, "title": "Completeness of Neighbourhood Logic", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "c46e91da-e358-412a-b7a0-bd6fb4279e13"}
{"abstract": "In this paper we study the learnability of a mixture of lines model which is of great importance in machine vision, computer graphics, and computer aided design applications. The mixture of lines is a partially-probabilistic model for an image composed of line-segments. Observations are generated by choosing one of the lines at random and picking a point at random from the chosen line. Each point is contaminated with some noise whose distribution is unknown, but which is bounded in magnitude. Our goal is to discover efficiently and rather accurately the line-segments that generated the noisy observations. We describe and analyze an efficient probably approximately correct (PAC) algorithm for solving the problem. Our algorithm combines techniques from planar geometry with simple large deviation tools and is simple to implement.", "authors": ["Sanjoy Dasgupta", "Elan Pavlov", "Yoram Singer"], "n_citation": 50, "title": "An efficient PAC algorithm for reconstructing a mixture of lines", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c5660d97-b8d4-4937-bd1b-dab0ea811177"}
{"abstract": "As increasingly complex computer systems have started playing a controlling role in all aspects of modern life, system availability and associated downtime of technical systems have acquired critical importance. Losses due to system downtime have risen manifold and become wide-ranging. Even though the component level availability of hardware and software has increased considerably, system wide availability still needs improvement as the heterogeneity of components and the complexity of interconnections has gone up considerably too. As systems become more interconnected and diverse, architects are less able to anticipate and design for every interaction among components, leaving such issues to be dealt with at runtime. Therefore, in this paper, we propose an approach for autonomic management of system availability, which provides real-time evaluation, monitoring and management of the availability of systems in critical applications. A hybrid approach is used where analytic models provide the behavioral abstraction of components/subsystems, their interconnections and dependencies and statistical inference is applied on the data from real time monitoring of those components and subsystems, to parameterize the system availability model. The model is solved online (that is, in real time) so that at any instant of time, both the point as well as the interval estimates of the overall system availability are obtained by propagating the point and the interval estimates of each of the input parameters, through the system model. The online monitoring and estimation of system availability can then lead to adaptive online control of system availability.", "authors": ["Kesari Mishra", "Kishor S. Trivedi"], "n_citation": 0, "title": "Model Based Approach for Autonomic Availability Management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c7703594-eee1-4ef3-89db-b0dbf607e7c0"}
{"abstract": "The last few years, research in wireless multi-hop networks is mainly driven by a search for efficient routing protocols. From an application point of view, nodes (users) will only setup connections with a specific goal, i.e. in order to use services and resources available in or reachable through the ad hoc network. Consequently, resource and service discovery (R&SD) protocols that allow nodes to learn available services in the network are indispensable. In this paper we compare the performance of two basic decentralized R&SD techniques, proactive and reactive discovery, through simulations and theoretical analysis. Our results show that the choice between them is not straightforward. It highly depends on the network and service characteristics and on the interaction with the underlying routing protocols. Therefore, our analysis provides some guidelines for developing new or extending existing R&SD protocols for operation in mobile ad hoc networks.", "authors": ["Jeroen Hoebeke", "Ingrid Moerman", "Bart Dhoedt", "Piet Demeester"], "n_citation": 0, "title": "Analysis of decentralized resource and service discovery mechanisms in wireless multi-hop networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c9872cf5-5687-44fd-9cd0-98113779f454"}
{"abstract": "Control-Flow Integrity (CFI) means that the execution of a program dynamically follows only certain paths, in accordance with a static policy. CFI can prevent attacks that, by exploiting buffer overflows and other vulnerabilities, attempt to control program behavior. This paper develops the basic theory that underlies two practical techniques for CFI enforcement, with precise formulations of hypotheses and guarantees.", "authors": ["Mart\u00edn Abadi", "Mihai Budiu", "\u00dalfar Erlingsson", "Jay Ligatti"], "n_citation": 0, "title": "A theory of secure control flow", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "caf9d73f-130a-4be7-b273-7bdad40a9671"}
{"abstract": "DHT systems like Chord, Pastry, CAN and Tapstry can only handle semantics-free, large-granularity requests for objects by identifier (typically a name). How to implement the content-based query in DHT system is a challenge. Query the XML data using XPath language can provide expressiveness for the DHT system. In this paper, we propose an XML-based content query system, termed as XPeer, built on DHT systems like Chord, Pastry or Tapstry. Besides the inherent properties provided by DHT, XPeer has several unique features. First, XPeer can utilize XML to implement content-based query using XPath as the query language; Second, XML data in XPeer can be totally heterogeneous, without conforming to the same XML schema or DTD. Third, XPeer can support range query over DHT. Finally, the XPeer can be easily extended to real P2P application like Napster, Gnutella and Freenet.", "authors": ["Weixiong Rao", "Hui Song", "Fanyuan Ma"], "n_citation": 0, "title": "Querying XML data over DHT system using XPeer", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cb7749ac-1d18-4149-a61e-8783f10feab3"}
{"abstract": "In this paper we address the modelling of humans in a context of ubiquitous information services. First we present the apparently conflicting requirements of, on the one hand avoiding data inconsistencies while reducing burdensome aspects of information technologies, and on the other hand, the respect for a person's privacy. These problems are analysed in the context of current ICT developments towards a ubiquitous information infrastructure. The architecture of a Human Data Manager Service (HDMS) is proposed and illustrated. It offers a solution to information modelling problems that originate from the current shift in the scope of information systems, from the enterprise to the enterprise network in the market.", "authors": ["Frank Berkers", "Jan Goossenaerts", "Dieter K. Hammer", "Hans Wortmann"], "n_citation": 50, "title": "Human models and data in the ubiquitous information infrastructure", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "cbee639f-b11f-4ae1-a15d-4dfdb7845938"}
{"abstract": "We propose a descriptive high-level language XDTrans devoted to specify transformations over XML data. The language is based on unranked tree automata approach. In contrast to W3C's XQuery or XSLT which require programming skills, our approach uses high-level ions reflecting intuitive understanding of tree oriented nature of XML data. XDTrans specifies transformations by means of rules which involve XPath expressions, node variables and non-terminal symbols denoting fragments of a constructed result. We propose syntax and semantics for the language as well as algorithms translating a class of transformations into XSLT.", "authors": ["Tadeusz Pankowski"], "n_citation": 0, "title": "A high-level language for specifying XML data transformations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ccb4e621-12e9-4356-bfdd-090982acc373"}
{"abstract": "In this paper we discuss some issues in implementing a model checker for the Z specification language. In particular, the language design of Z and its semantics, raises some challenges for efficient model checking, and we discuss some of these issues here. Our approach to model checking Z specifications involves implementing a translation from Z into the SAL input language, upon which the SAL toolset can be applied. In this paper we discuss issues in the implementation of this translation algorithm and illustrate them by looking at how the mathematical toolkit is encoded in SAL and the resultant efficiency of the model checking tools.", "authors": ["John Derrick", "Siobh\u00e1n North", "Tony Simons"], "n_citation": 0, "title": "Issues in Implementing a Model Checker for Z", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cd5f71fe-4c52-4733-ac00-9b288e3fb0d6"}
{"abstract": "We introduce the (b, n)- Committee Decision Problem (CD) - a generalization of the consensus problem. While set agreement generalizes consensus in terms of the number of decisions allowed, the CD problem generalizes consensus in the sense of considering many instances of consensus and requiring a processor to decide in at least one instance. In more detail, in the CD problem each one of a set of n processes has a (possibly distinct) value to propose to each one of a set of b consensus problems, which we call committees. Yet a process has to decide a value for at least one of these committees, such that all processes deciding for the same committee decide the same value. We study the CD problem in the context of a wait-free distributed system and analyze it using a combination of distributed algorithmic and topological techniques, introducing a novel reduction technique. We use the reduction technique to obtain the following results. We show that the (2,3)-CD problem is equivalent to the musical benches problem introduced by Gafni and Rajsbaum in [10], and both are equivalent to (2, 3)-set agreement, closing an open question left there. Thus, all three problems are wait-free unsolvable in a read/write shared memory system, and they are all solvable if the system is enriched with objects capable of solving (2, 3)-set agreement. While the previous proof of the impossibility of musical benches was based on the Borsuk-Ulam (BU) Theorem, it now relies on Sperner's Lemma, opening intriguing questions about the relation between BU and distributed computing tasks.", "authors": ["Eli Gafni", "Sergio Rajsbaum", "Michel Raynal", "Corentin Travers"], "n_citation": 0, "title": "The committee decision problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ce82a730-a765-4a55-9380-7517ead7048d"}
{"abstract": "This paper reviews the principles behind the paradigm of abstract interpretation via 3-valued logic, discusses recent work to extend the approach, and summarizes on-going research aimed at overcoming remaining limitations on the ability to create program-analysis algorithms fully automatically.", "authors": ["Thomas W. Reps", "Mooly Sagiv", "Reinhard Wilhelm"], "n_citation": 0, "title": "Static program analysis via 3-valued logic", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cfeb8463-b423-44a7-98b5-07b982e1ff91"}
{"abstract": "This paper proposes a new fuzzy modeling method, which involves the Minimum Cluster Volume clustering algorithm. The cluster centers founded are naturally considered to be the centers of Gaussian membership functions. Covariance matrix obtained from the result of cluster method is made use to estimate the parameters \u03c3 for Gaussian membership functions. A direct result of this method are compared in our simulations with published methods, which indicate that our method is powerful so that it solves the multi-dimension problems more accurately even with less complexity of our fuzzy model structure.", "authors": ["Can Yang", "Jun Meng"], "n_citation": 50, "title": "Optimal fuzzy modeling based on minimum cluster volume", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d0033fd8-bacb-40cd-b185-5b473be62f1e"}
{"abstract": "It is well known that there exists a universal (i.e., optimal to within an additive constant if allowed to work infinitely long) algorithm for lossless data compression (Kolmogorov, Levin). The game of lossless compression is an example of an on-line prediction game; for some other on-line prediction games (such as the simple prediction game) a universal algorithm is known not to exist. In this paper we give an analytic characterisation of those binary on-line prediction games for which a universal prediction algorithm exists.", "authors": ["Yuri Kalnishkan", "Vladimir Vovk", "Michael V. Vyugin"], "n_citation": 0, "title": "A criterion for the existence of predictive complexity for binary games", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d1d1300b-76d3-4581-8cdd-fac5cd64f02e"}
{"abstract": "Model Based Development (MBD) using Mathworks tools like Simulink, Stateflow etc. is being pursued in Honeywell for the development of safety critical avionics software. Formal verification techniques are well-known to identify design errors of safety critical systems reducing development cost and time. As of now, formal verification of Simulink design models is being carried out manually resulting in excessive time consumption during the design phase. We present a tool that automatically translates certain Simulink models into input language of a suitable model checker. Formal verification of safety critical avionics components becomes faster and less error prone with this tool. Support is also provided for reverse translation of traces violating requirements (as given by the model checker) into Simulink notation for playback.", "authors": ["B. Meenakshi", "A. Bhatnagar", "Sudeepa Roy"], "n_citation": 0, "title": "Tool for Translating Simulink Models into Input Language of a Model Checker", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d1ea2403-304f-4c01-a485-a5837cb25064"}
{"abstract": "Job finding process is a high human-resource utilization process. The applicant needs to search for all the jobs that he is interested in. After the searching and filtering process, the applicant's qualification and personal information, such as working experience, fields interested in and so on, are necessary to be matched with the basic requirements of the job so as to see whether he is suitable for applying the job. To enhance this process, an Intelligent Job Counseling System (IJCS) is developed in the present paper. The IJCS aims at performing job finding and matching process. It is designed to be an expert system with rule-based reasoning.", "authors": ["James N. K. Liu", "Thomas Mak", "Brian Tang", "Katie Ma", "Zikey Tsang"], "n_citation": 0, "title": "An intelligent job Counseling System", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "d4094df5-39a4-46f8-a7fb-c5b18f56f17c"}
{"abstract": "Answer set programming has emerged as a new important paradigm for declarative problem solving. It relies on algorithms that compute the stable models of a logic program, a problem that is, in the worst-case, intractable. Although, local search procedures have been successfully applied to a variety of hard computational problems, the idea of employing such procedures in answer set programming has received very limited attention. This paper presents several local search algorithms for computing the stable models of a normal logic program. They are all based on the notion of a conflict set, but use it in different ways, resulting in different computational behaviors. The algorithms are inspired from related work in solving propositional satisfiability problems, suitably adapted to the stable model semantics. The paper also discusses how the heuristic equivalence method, that has been proposed in the context of propositional satisfiability, can be used in systematic search procedures that compute the stable models of logic programs.", "authors": ["Yannis Dimopoulos", "Andreas Sideris"], "n_citation": 0, "title": "Towards local search for answer sets", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d40f9e99-e937-4231-bc69-d870231e30f6"}
{"abstract": "Since the sensor network nodes have a small size and limited battery power, there have been many studies for reducing their energy consumption. Each sensor node can show different energy usage according to the frequency of event sensing and data transmission, and thus they have different lifetime. So, some nodes may run out of energy that causes disconnection of paths and reduction of network lifetime. In this paper, we propose a new energy-efficient routing algorithm for sensor networks called Maximum remaining energy constrained directed diffusion routing (MRE-DD), that selects a least energy-consuming path among the paths formed by nodes with highest remaining energy, and thus provides long network lifetime and more uniform energy consumption by nodes. Simulation results show that our algorithm extends the network lifetime and enhances the network reliability by maintaining remaining energy distribution relatively uniform among sensor nodes.", "authors": ["An Kyu Hwang", "Jae Yong Lee", "Byung Chul Kim"], "n_citation": 0, "title": "Design of Maximum Remaining Energy Constrained Directed Diffusion Routing for Wireless Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d6eb3592-fd97-4653-bf05-9ca970c9604d"}
{"abstract": "A monitoring system for the security is increasingly demanded in line with the issue of the infringement of privacy. This paper has conducted the study on the monitoring technique via the Internet using PC or PDA. This is a monitoring system for web-camera based home server, which transmits the images and sound data captured by mike-embedded USB camera, through multiplexer of H.263 and internet. It integrates the operating processors of each device into a unified drive module and provides it the operator to facilitate the system operation and maintenance.", "authors": ["Jong-Geun Jeong", "Byung-Rae Cha"], "n_citation": 0, "title": "A Study for Monitoring Technique for Home Server Based on Web Camera", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d7776c16-2b0d-4dfe-bade-6181cfb55b6e"}
{"abstract": "In this paper, we study the minimum degree minimum spanning tree problem: Given a graph G = (V, E) and a non-negative cost function  c  on the edges, the objective is to find a minimum cost spanning tree T under the cost function c such that the maximum degree of any node in T is minimized. We obtain an algorithm which returns an MST of maximum degree at most\u0394* + k where \u0394* is the minimum maximum degree of any MST and k is the distinct number of costs in any MST of G. We use a lower bound given by a linear programming relaxation to the problem and strengthen known graph-theoretic results on minimum degree subgraphs [3,5] to prove our result. Previous results for the problem [1,4] used a combinatorial lower bound which is weaker than the LP bound we use.", "authors": ["R. Ravi", "Mohit Singh"], "n_citation": 50, "title": "Delegate and Conquer : An LP-Based Approximation Algorithm for Minimum Degree MSTs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "da3b6a85-fc8a-49a2-99f2-a730b49e4ef0"}
{"abstract": "It is well-known that the class of sets that can be computed by polynomial size circuits is equal to the class of sets that are polynomial time reducible to a sparse set. It is widely believed, but unfortunately up to now unproven, that there are sets in EXP NP , or even in EXP that are not computable by polynomial size circuits and hence are not reducible to a sparse set. In this paper we study this question in a more restricted setting: what is the computational complexity of sparse sets that are selfreducible? It follows from earlier work of Lozano and Toran [10] that EXP NP  does not have sparse selfreducible hard sets. We define a natural version of selfreduction, tree-selfreducibility, and show that NEXP does not have sparse tree-selfreducible hard sets. We also show that this result is optimal with respect to relativizing proofs, by exhibiting an oracle relative to which all of EXP is reducible to a sparse tree-selfreducible set. These lower bounds are corollaries of more general results about the computational complexity of sparse sets that are selfreducible, and can be interpreted as super-polynomial circuit lower bounds for NEXP.", "authors": ["Harry Buhrman", "Leen Torenvliet", "Falk Unger"], "n_citation": 0, "title": "Sparse selfreducible sets and polynomial size circuit lower bounds", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dd502c7a-b6d2-459f-b3d7-37444f55ac45"}
{"abstract": "The problem of Minimum Broadcast Time (MBT) seeks to find the minimal number of times steps required to broadcast a message through a communication network. We describe a novel ant algorithm for solving this NP-Complete problem and compare it to three other known algorithms, one of which is genetic. Through experimentation on randomly generated graphs we show that our algorithm finds the best solutions.", "authors": ["Yehudit Hasson", "Moshe Sipper"], "n_citation": 0, "title": "A novel ant algorithm for solving the Minimum Broadcast Time problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ddb346e8-c5f0-42f6-9a1d-6d50d5854845"}
{"abstract": "We advocate to analyze the average complexity of learning problems. An appropriate framework for this purpose is introduced. Based on it we consider the problem of learning monomials and the special case of learning monotone monomials in the limit and for on-line predictions in two variants: from positive data only, and from positive and negative examples. The well-known Wholist algorithm is completely analyzed, in particular its average-case behavior with respect to the class of binomial distributions. We consider different complexity measures: the number of mind changes, the number of prediction errors, and the total learning time. Tight bounds are obtained implying that worst case bounds are too pessimistic. On the average learning can be achieved exponentially faster. Furthermore, we study a new learning model, stochastic finite learning, in which, in contrast to PAC learning, some information about the underlying distribution is given and the goal is to find a correct (not only approximatively correct) hypothesis. We develop techniques to obtain good bounds for stochastic finite learning from a precise average case analysis of strategies for learning in the limit and illustrate our approach for the case of learning monomials.", "authors": ["R. Reischuk", "Thomas Zeugmann"], "n_citation": 50, "title": "A complete and tight average-case analysis of learning monomials", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "e12ba341-ce42-4e6a-bfa3-e627a4f4266f"}
{"abstract": "Spatio-temporal fitness landscapes that are constructed from Coupled Map Lattices (CML) are introduced. These landscapes are analyzed in terms of modality and ruggedness. Based on this analysis, we study the relationship between landscape measures and the performance of an evolutionary algorithm used to solve the dynamic optimization problem.", "authors": ["Hendrik Richter"], "n_citation": 0, "title": "Evolutionary optimization in spatio-temporal fitness landscapes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e13ed373-e31c-44c2-9704-d184d48496a6"}
{"abstract": "We review the definition of entity types derived by symbol-generating rules. These types appear frequently in conceptual schemas. However, up to now they have received very little attention in the field of conceptual modeling of information systems. Most conceptual modeling languages, like the UML and ORM, do not allow their formal definition. In this paper, we propose a new method for the definition of entity types derived by symbol-generating rules. Our method is based on the fact that these types can always be expressed as the result of the reification of a derived relationship type. Many languages, like the UML and ORM, allow defining derived relationship types and, at the same time, provide support for reification. Using our method, these languages can directly deal with those derived entity types.", "authors": ["Jordi Cabot", "Antoni Oliv\u00e9", "Ernest Teniente"], "n_citation": 0, "title": "Entity types derived by symbol-generating rules", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e3739e0c-2096-45d7-ab77-1a9b62055d6f"}
{"abstract": "This paper proposes a novel active contour model for image object recognition using neural networks as a dynamic information fusion kernel. It first learns feature fusion strategies from training data by searching for an optimal fusion model at each marching step of the active contour model. A recurrent neural network is then employed to learn the fusion strategy knowledge. The learned knowledge is then applied to guide another linear neural network to fuse the features, which determine the marching procedures of an active contour model for object recognition. We test our model on both artificial and real image data sets and compare the results to those of a standard active model, with promising outcomes.", "authors": ["Xiongcai Cai", "Arcot Sowmya"], "n_citation": 50, "title": "Active Contour with Neural Networks-Based Information Fusion Kernel", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e52948db-94db-49f7-a944-6eb924616ecb"}
{"abstract": "This paper evaluates performance of scale-free networks in case of intentional removal of their nodes. The distinguishing feature of this kind of networks (Internet is an excellent example) is the power law distribution of node degrees. An interesting behavior of scale-free networks, if node removal process is performed sufficiently long, is manifested by their migration to random networks. The main idea of our research is to quantify this process. In contrast to well explored parameters like: characteristic path length or clustering coefficient, we propose the new ones: mean maximum flow, centre of gravity of node degree distribution and other. To the best of our knowledge, these measures are proposed for the first time. Our results confirm that the migration process steps relatively fast.", "authors": ["T. Gierszewski", "Wojciech Molisz", "Jacek Rak"], "n_citation": 0, "title": "On Certain Behavior of Scale-Free Networks Under Malicious Attacks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e77b8e0b-1208-4a3f-bd3d-ced86df3e24f"}
{"abstract": "We provide an optimally mixing Markov chain for 6-colorings of the square grid. Furthermore, this implies that the uniform distribution on the set of such colorings has strong spatial mixing. Four and five are now the only remaining values of k for which it is not known whether there exists a rapidly mixing Markov chain for k-colorings of the square grid.", "authors": ["Dimitris Achlioptas", "Michael Molloy", "Cristopher Moore", "Frank Van Bussel"], "n_citation": 0, "title": "Sampling grid colorings with fewer colors", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e7d10fe1-bf91-4568-992d-20ce489eb4f7"}
{"abstract": "With the development of information-communication infrastructure such as Internet, multimedia teachwares in good use of this infrastructure have been developed and serviced in all subjects of all school years in Korea. But there are some drawbacks. Using these multimedia teachwares requires studying and training. Even after that, it is still sophisticated to search information and make it one's own useful materials. For purpose of overcoming these challenges and maximizing the access and applicability of multimedia teachware, a new system was suggested in this study which combined 2D barcode of ubiquitous technology and the URL(Uniform Resource Location) of multimedia teachware. Performance analysis demonstrates that this system is superior to existing one, and that the minimization of time for bridging the multimedia contents to 2D barcode enables the optimization of performance of the suggested system.", "authors": ["Duckki Kim", "Youngsong Mun"], "n_citation": 50, "title": "Design and Performance Analysis of Multimedia Teachware Making System. Using 2D Barcode", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e7d70cd3-fe4e-4128-94d6-3da861c2d3bd"}
{"abstract": "The essence of grid computing lies in the efficient utilization of a wide range of heterogeneous, loosely coupled resources in an organization. In a computational grid environment, regular monitoring of the execution of applications and taking actions for improving their performance in real time can achieve this. This paper presents the design of a multiagent framework for performance monitoring and tuning of an application executing in a Grid environment.", "authors": ["Sarbani Roy", "Nandini Mukherjee"], "n_citation": 0, "title": "Towards an agent-based framework for monitoring and tuning application performance in grid environment", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e9099a03-974d-4122-a7fa-16048192e4ea"}
{"abstract": "In pre-Gridservice times, conventional way of pooling domain expertise is by sharing legacy codes. With the introduction and wide adoption of OGSA framework and OGSI specification, Grid computing has entered a new era of service-orientation, in which implementation of sharing domain expertise resorts to portable, interoperable Grid services. So comes the challenge to gridify existing scientific computing legacy codes (to convert to OGSI-compliant Grid services). This paper analyzes features of scientific legacy codes and proposes a design scheme of gridificaiton. The design scheme consists of three parts; gridified computing services; scientific computing Operation Providers(OPs); on-demand scientific computing factories. The design solution has been successfully applied to Bioinformatics field. Application scope tests show that the design solution applies to most types of scientific computing legacy codes. Performance experiments find that the proposed gridification mechanisms bring about only slight efficiency loss.", "authors": ["Bin Wang", "Zhuoqun Xu", "Cheng Xu", "Yanbin Yin", "Wenkui Ding", "Huashan Yu"], "n_citation": 0, "title": "A study of gridifying scientific computing legacy codes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e99a0911-98d2-4ae9-9ec8-12d2d8b2b38a"}
{"abstract": "The mechanical behaviour of the human skin shows a non-linear stress-strain relationship. Based on the behaviour of human dermis proposed by Hendriks, we develop a system to draw a specific skin incision shape which consists of four spine curves and two lines. Users can firstly define the two incision points on the skin, our system can superimpose the shape on the surface and show the skin incision cutaway. OBBTree analysis for the skin surface is used to put the incision shape. Followed by shape shown on the surface and finally a cutaway is generated to show the cut. Jittering problem generated by the above steps is also resolved.", "authors": ["Zhongyu Chen", "Guohua Wu", "Ronghua Liang", "Guofeng Zhang"], "n_citation": 0, "title": "Skin incision using real-time cutaway based on FE analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ea2f12f4-80c8-4d3d-bf14-fc7e78db7e20"}
{"abstract": "A modeling and development methodology is a combination of a language for expressing the universal or domain ontology and an approach for developing systems using that language. A common way for building, comparing, and evaluating methodologies is metamodeling, i.e., the process of modeling the methodology. Most of the methodology metamodels pertain only to the language part of the methodologies, leaving out the description of the system development processes or describing them informally. A major reason for this is that the methods used for metamodeling are structural- or object-oriented, and, hence, are less expressive in modeling the procedural aspects of a methodology. In this paper we apply Object-Process Methodology (OPM) to specify a generic OPM-based system development process. This metamodel is made possible due to OPM's view of objects and processes as being on equal footing rather than viewing object classes as superiors to and owners of processes. This way, OPM enables specifying both the structural (ontological constructs) and behavioral (system development) aspects of a methodology in a single, unified view.", "authors": ["Dov Dori", "Iris Reinhartz-Berger"], "n_citation": 0, "title": "An OPM-based metamodel of system development process", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "ec4c03db-b429-4b35-805e-61f9b01a321c"}
{"abstract": "Different credit assignment strategies are investigated in a two level co-evolutionary model which involves a population of Gaussian neurons and a population of radial basis function networks consisting of neurons from the neuron population. Each individual in neuron population can contribute to one or more networks in network population, so there is a two-fold difficulty in evaluating the effectiveness (or fitness) of a neuron. Firstly, since each neuron only represents a partial solution to the problem, it needs to be assigned some credit for the complete problem solving activity. Secondly, these credits need to be accumulated from different networks the neuron participates in. This model, along with various credit assignment strategies, is tested on a classification (Heart disease diagnosis problem from UCI machine learning repository) and a regression problem (Mackey-Glass time series prediction problem).", "authors": ["Vineet R. Khare", "Xin Yao", "Bernhard Sendhoff"], "n_citation": 0, "title": "Credit assignment among neurons in co-evolving populations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ef202e7b-9006-4183-bbe0-8232b21f9a57"}
{"abstract": "Frequent changes of software requirements imply changes of the underlying database, like database schema, integrity constraints, as well as database transactions and programs. Tools like ERWin, DBMain and Silverrun help developers in applying these changes. Yet, the automatic derivation might pose a problem: Since the developer is not aware of the details of the derivation applications, the resulting programs might include contradictory actions. That is, intentions of programs might be reversed by the automatic derivation, resulting a different behavior than expected by the developer. In this paper, a compile-time algorithm that achieves preservation of intentions is suggested. The algorithm revises a composite program into a program without contradictory actions. It is based on a fine analysis of effects, that is sensitive to computation paths. The output program is expressive and efficient since it interleaves run-time sensitive analysis of already reduced effects within the input program. The compile-time reduction of effects accounts for the efficiency; the run-time sensitivity of effects accounts for the expressiveness. The novelty of the proposed approach is in combining static and dynamic analysis in a way that run-time overhead is minimized without sacrificing the expressivity of the resulting program.", "authors": ["Mira Balaban", "Steffen Jurk"], "n_citation": 0, "title": "Intentions of operations -characterization and preservation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "ef47564f-41fc-4cf5-8f04-2c411482c40b"}
{"abstract": "We study the design of structured Tanner codes with low error-rate floors on the AWGN channel. The design technique involves the doping of standard LDPC (proto-)graphs, by which we mean Hamming or recursive systematic convolutional (RSC) code constraints are used together with single-parity-check (SPC) constraints to construct a code's protograph. We show that the doping of a good graph with Hamming or RSC codes is a pragmatic approach that frequently results in a code with a good threshold and very low error-rate floor. We focus on low-rate Tanner codes, in part because the design of low-rate, low-floor LDPC codes is particularly difficult. Lastly, we perform a simple complexity analysis of our Tanner codes and examine the performance of lower-complexity, suboptimal Hamming-node decoders.", "authors": ["Shadi Abu-Surra", "Gianluigi Liva", "William E. Ryan"], "n_citation": 0, "title": "Low-Floor Tanner Codes Via Hamming-Node or RSCC-Node Doping", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f0feaa0a-11d4-4810-8562-35a19e280b94"}
{"abstract": "We show decidability of the satisfiability problem for an extension of the modal \u03bc-calculus with event-recording clocks. Based on techniques for deciding the untimed \u03bc-calculus, we present a complete set of reduction rules for constructing tableaux for formulas of this event-recording logic. To keep track of the actual value of the clocks, the premises and conclusions of our tableau rules are augmented with timing contexts, which are sets of timing constraints satisfied by the actual value of the clocks. The decidability problem is shown to be EXPTIME complete. In addition, we address the problem of model synthesis, that is, given a formula \u03c6, we construct an event-recording automaton that satisfies \u03c6.", "authors": ["Maria Sorea"], "n_citation": 0, "title": "A decidable fixpoint logic for time-outs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f3e0d79e-0c6a-42d2-884b-e630b681e2ca"}
{"abstract": "In this paper, we introduce an information hiding technique into remote sensing area. We develop its connotation that the secret information is still hidden in the original remote sensing image. We propose a practical information hiding technique and a novel wavelet information hiding algorithm which is able to adapt to features of a remote sensing image. The technique is based on the embedding strategy of Discrete Wavelet Transform and HVS (Human Visual System) character. The algorithm is a blind one and has no influence on applied value of a remote sensing image.", "authors": ["Xianmin Wang", "Zequn Guan", "Chenhan Wu"], "n_citation": 50, "title": "A novel information hiding technique for remote sensing image", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f3e6060c-8b3f-42a5-898a-1c7d8460134e"}
{"abstract": "Many valuable Web documents have not been indexed by general search engines and are only accessible through specific search interfaces. Metasearching groups of specialty search engines is one possible way to gain access to large amount of such hidden Web resources. One of the key issues for returning quality metasearch results is how to select the most relevant specialty search engines for a given query. We introduce a method for categorizing specialty search engines automatically into a hierarchical directory for metasearching. By utilizing the directory, specialty search engines that have a high possibility of having relevant information and resources can be easily selected by a metasearch engine. We evaluate our algorithm by comparing the directory built by the proposed algorithm with another one that was built by human-judgments. In addition, we present a metasearch engine prototype, which demonstrates that such a specialty search engine directory can be beneficial iri locating essential but hidden Web resources.", "authors": ["Jacky K. H. Shiu", "Stephen Chi-fai Chan", "Korris Fu-Lai Chung"], "n_citation": 0, "title": "Accessing hidden Web documents by metasearching a directory of specialty search engines", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f514421f-1df6-4a97-8a29-1a31734cd135"}
{"abstract": "Large displays are becoming commonplace for home and office computers. Although researchers have quantified the benefits of working on large displays, there has been little investigation of how large displays physiologically and emotionally impact the user. Using subjective and physiological measures, we compared the user's experience when game playing on a large display versus a small display. We found that the large display caused greater physiological responses and higher subjective ratings of excitement. These physiological results were mirrored in the participants' subjective reports. The study contributes to understanding of interaction with large displays and refining the requirements for what constitutes effective and desirable human-computer interaction (HCI).", "authors": ["Tao Lin", "Wanhua Hu", "Atsumi Imamiya", "Masaki Omata"], "n_citation": 0, "title": "Large display size enhances user experience in 3D games", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f5bee8ef-41c5-44e7-9960-763361c0e27f"}
{"abstract": "Semantic resources of predicate-argument structure have high potential to enable increased quality in language understanding. Several alternative frame collections exist, but they cover different sets of predicates and use different role sets. We integrate semantic frame information given a predicate verb using three available collections: FrameNet, PropBank, and the LCS database. For each word sense in WordNet, we automatically assign the corresponding FrameNet frame and align frame roles between FrameNet and PropBank frames and between FrameNet and LCS frames, and verify the results manually. The results are avilable as part of ISI's Omega ontology.", "authors": ["Namhee Kwon", "Eduard H. Hovy"], "n_citation": 0, "title": "Integrating semantic frames from multiple sources", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f5ddaf4b-98f9-47cc-a797-44b20be47724"}
{"abstract": "This paper presents an Al architecture for multiple robots working collaboratively in a future smart office. This architecture integrates the control, communication, planning, and learning necessary to agentify office robots. Such integration is based on our multiagent robot language (MRL), which is all extension of concurrent logic programming languages (CCL). While the behavior of an agent is specified within guarded Horn clause logic, the communication and concurrency controls are amenable to the operational semantics of CCL. Our planning module provides well-balanced coordination between single agent planning and supervisor-level planning, yielding the minimum interaction for multiagent communication and control. Furthermore, inductive learning is incorporated into the planning module and is applied to produce empirical rules for action selection, providing the utility of multiagent problem solving. These features allow a unified view of both low- and high-level computation, which enables intelligent collaboration between robotic agents and provides a powerful framework for distributed AI and agent-oriented programming in the real world.", "authors": ["Hiroyuki Nishiyama", "Hayato Ohwada", "Fumio Mizoguchi"], "n_citation": 0, "title": "Towards agent-oriented smart office based on concurrent logic languages", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "f7a565d1-88d0-428f-b772-a6e9d23f19c0"}
{"abstract": "During the past ten years, requirements on functionality of business information systems have been slowly changing. This shift consists of moving from traditional command based applications to the applications of workflow and groupware type. Such applications are aimed at controlling business processes. Designing an appropriate control system presumes that the nature of the process that we want to control is fully understood and modeled. The objective of the tutorial is to understand the problems and solutions of designing a new generation of business information systems. It is intended for Software Engineers involved in business applications development, Business Analysts, and Researchers interested in business process modeling.", "authors": ["Ilia Bider", "Paul Johannesson"], "n_citation": 0, "title": "Modeling dynamics of business processes: Key for building next generation of business information systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fa24638f-b640-435d-9e8d-e6462fc552eb"}
{"abstract": "Among the most important expected benefits of a global service oriented architecture leveraging web service standards is an increased level of automation in the discovery, composition, verification, monitoring and recovery of services for the realization of complex processes. Most existing works addressing this issue are based on the Ontology Web Language for Services (OWL-S) and founded on description logic. Because the discovery and composition tasks are designed to be fully automatic, the solutions are limited to the realization of rather simple processes. To overcome this deficiency, this paper proposes an approach in which service capability descriptions are based on full first order predicate logic and enable an interactive discovery and composition of services for the realization of complex processes. The proposed approach is well suited when automatic service discovery does not constitute an absolute requirement and the discovery can be done interactively (semi-automatically) with human expert intervention. Such applications are, for instance, often met in e-science. The proposed approach is an extension and adaptation of the compositional information systems development (CISD) method based on the SYNTHESIS language and previously proposed by some of the authors. The resulting method offers a canonical extensible object model with its formal automatic semantic interpretation in the Abstract Machine Notation (AMN) as well as reasoning capabilities applying AMN interactively to the discovery and composition of web services.", "authors": ["Sergey A. Stupnikov", "Leonid A. Kalinichenko", "St\u00e9phane Bressan"], "n_citation": 50, "title": "Interactive discovery and composition of complex web services", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fb473d80-3100-4c71-866f-57a24936100c"}
{"abstract": "The web site for this conference states that: The challenge now is to achieve general acceptance of formal methods as a part of industrial development of high quality systems, particularly trusted systems. We are all going to be discussing How to achieve this, but before that we should maybe ask the other questions: What are the real benefits of formal methods and Why should we care about them? When and Where should we expect to use them, and Who should be involved? I will suggest some answers to those questions and then describe some ways that the benefits are being realised in practice, and what I think needs to happen for them to become more widespread.", "authors": ["Anthony Hall"], "n_citation": 0, "title": "Realising the benefits of formal methods", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fc0e90f1-affa-4ea7-ba85-5f3082c38582"}
{"abstract": "Recurrent Neural Networks (RNNs) possess an implicit internal memory and are well adapted for time series forecasting. Unfortunately, the gradient descent algorithms which are commonly used for their training have two main weaknesses: the slowness and the difficulty of dealing with long-term dependencies in time series. Adding well chosen connections with time delays to the RNNs often reduces learning times and allows gradient descent algorithms to find better solutions. In this article, we demonstrate that the principle of time delay learning by gradient descent, although efficient for feed-forward neural networks and theoretically adaptable to RNNs, shown itself to be difficult to use in this latter case.", "authors": ["Romuald Bon\u00e9", "Hubert Cardot"], "n_citation": 0, "title": "Time delay learning by gradient descent in recurrent neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fedaa02c-63f5-4999-a541-040f3a719d48"}
{"abstract": "In this paper, a learning approach coupling Support Vector Machines (SVMs) and a Genetic Algorithm (GA) is presented for knowledge-assisted semantic image analysis in specific domains. Explicitly defined domain knowledge under the proposed approach includes objects of the domain of interest and their spatial relations. SVMs are employed using low-level features to extract implicit information for each object of interest via training in order to provide an initial annotation of the image regions based solely on visual features. To account for the inherent visual information ambiguity spatial context is subsequently exploited. Specifically, fuzzy spatial relations along with the previously computed initial annotations are supplied to a genetic algorithm, which uses them to decide on the globally most plausible annotation. In this work, two different fitness functions for the GA are tested and evaluated. Experiments with outdoor photographs demonstrate the performance of the proposed approaches.", "authors": ["G. Th. Papadopoulos", "Vasileios Mezaris", "Stamatia Dasiopoulou", "Ioannis Kompatsiaris"], "n_citation": 50, "title": "Semantic image analysis using a learning approach and spatial context", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fedb4659-81f2-4549-a894-2c6e2e597297"}
{"abstract": "Point pattern matching problems are of fundamental importance in various areas including computer vision and structural bioinformatics. In this paper, we study one of the more general problems, known as LCP (largest common point set problem): Let P and Q be two point sets in R 3 , and let e > 0 be a tolerance parameter, the problem is to find a rigid motion \u03bc that maximizes the cardinality of subset I of Q, such that the Hausdorff distance dist(P, \u03bc(I))   0 and the minimum interpoint distance is greater than c. A \u03b2-distance-approximation algorithm for tolerant-LCP finds a subset I \u2286 Q such that |I| > LCP(P, Q) and dist(P, \u03bc(I))   1. This paper has three main contributions. (1) We introduce a new algorithm, called T-hashing, which gives the fastest known deterministic 4-distance-approximation algorithm for tolerant-LCP. (2) For the exact-LCP, when the matched set is required to be large, we give a simple sampling strategy that improves the running times of all known deterministic algorithms, yielding the fastest known deterministic algorithm for this problem. (3) We use expander graphs to speed-up the T-hashing algorithm for tolerant-LCP when the size of the matched set is required to be large, at the expense of approximation in the matched set size. Our algorithms also work when the transformation \u03bc is allowed to be scaling transformation.", "authors": ["Vicky Choi", "Navin Goyal"], "n_citation": 50, "title": "An efficient approximation algorithm for point pattern matching under noise", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "02453d34-2c31-46f7-a98e-0b08e6f22f48"}
{"abstract": "It is typical for the EA community that theory follows experiments. Most theoretical approaches use some model of the considered evolutionary algorithm (EA) but there is also some progress where the expected optimization time of EAs is analyzed rigorously. There are only three well-known problems of combinatorial optimization where such an approach has been performed for general input instances, namely minimum spanning trees, shortest paths, and maximum matchings. The theoretical results are asymptotic ones and several questions for realistic dimensions of the search space are open. We supplement the theoretical results by experimental ones. Many hypotheses are confirmed by rigorous statistical tests.", "authors": ["Patrick Briest", "Dimo Brockhoff", "Bastian Degener", "Matthias Englert", "Christian Gunia", "Oliver Heering", "Thomas Jansen", "Michael Leifhelm", "Kai Plociennik", "Heiko R\u00f6glin", "Andrea Schweer", "Dirk Sudholt", "Stefan Tannenbaum", "Ingo Wegener"], "n_citation": 0, "title": "Experimental supplements to the theoretical analysis of EAs on problems from combinatorial optimization", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0297e5e4-8bc6-4c38-8033-4716c9577ef1"}
{"abstract": "Virtual anatomy system is helpful in medical education. We designed and implemented virtual anatomy training system for medical students. Anatomical images of Visible Korean Human (VKH) data were produced with serial sectioning of the Korean cadaver's whole body at 0.2 mm intervals. 3D volume data was made by stacking the color and segmented images, and the volume data was reconstructed with 1 mm voxel size since entire volume data is so large. We present the virtual anatomy training system using volume rendering, on which the 3D anatomical volume and 3D segmented volume can be sectioned at arbitrary angles, and the 3D rendered images of the several anatomical structures can be displayed and rotated at any angles.", "authors": ["Koojoo Kwon", "Byeong-Seok Shin", "Min Suk Chung"], "n_citation": 0, "title": "Computer-aided training system of educational virtual dissection using visible Korean human", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "03f6e5cf-d596-4281-8326-2bd4e2324501"}
{"abstract": "Temporal databases provide a complete history of all changes to a database and include the times when changes occurred. This permits users to query the current status of the database as well as the past states, and even future states that are planned to occur. Traditional temporal data models concentrated on describing temporal data based on versioning of objects, tuples or attributes. However, this approach does not effectively manage time-series data that is frequently found in real-world applications, such as sales, economic, and scientific data. In this paper, we first review and formalize a conceptual model that supports time-series objects as well as the traditional version-based objects. The proposed model, called integrated temporal data model (ITDM), is based on EER. It includes in it the concept of time and provides necessary constructs for modeling all different types of objects. We then propose a temporal query language for ITDM, that treats both version-based and time-series data in a uniform manner.", "authors": ["Jae Young Lee", "Ramez Elmasri"], "n_citation": 0, "title": "An EER-based conceptual model and query language for time-series data", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "08380962-4e10-45d1-a525-d2ff8d9fc91b"}
{"abstract": "This paper studies the global convergence properties of Cohen-Grossberg neural networks with discrete time delays. Without assuming the symmetry of interconnection weight coefficients, and the monotonicity and differentiability of activation functions, and by employing Lyapunov functionals, we derive new delay independent sufficient conditions under which a delayed Cohen-Grossberg neural network converges to a globally asymptotically stable equilibrium point. Some examples are given to illustrate the advantages of the results over the previously reported results in the literature.", "authors": ["Zeynep Orman", "Sabri Arik"], "n_citation": 50, "title": "New Results for Global Stability of Cohen-Grossberg Neural Networks with Discrete Time Delays", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0948bab9-e96e-4458-9504-7c60b32b639f"}
{"abstract": "Bayes' rule specifies how to obtain a posterior from a class of hypotheses endowed with a prior and the observed data. There are three principle ways to use this posterior for predicting the future: marginalization (integration over the hypotheses w.r.t. the posterior), MAP (taking the a posteriori most probable hypothesis), and stochastic model selection (selecting a hypothesis at random according to the posterior distribution). If the hypothesis class is countable and contains the data generating distribution, strong consistency theorems are known for the former two methods, asserting almost sure convergence of the predictions to the truth as well as loss bounds. We prove the first corresponding results for stochastic model selection. As a main technical tool, we will use the concept of a potential: this quantity, which is always positive, measures the total possible amount of future prediction errors. Precisely, in each time step, the expected potential decrease upper bounds the expected error. We introduce the entropy potential of a hypothesis class as its worst-case entropy with regard to the true distribution. We formulate our results in the online classification framework, but they are equally applicable to the prediction of non-i.i.d. sequences.", "authors": ["Jan Poland"], "n_citation": 50, "title": "The missing consistency theorem for bayesian learning : Stochastic model selection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "094900ed-72e5-40f9-86ef-f7198ed7e917"}
{"abstract": "The application of kernel method to petrochemical industry is explored in this paper. A nonlinear soft analyzer for the flashpoint measurement of Dearomatization process is developed by using kernel principal component regression (KPCR) method. To trace the time varying dynamics and reject disturbances, a novel online eigenspace decomposing algorithm is proposed to update that of the Kernel Matrix, which is much faster than direct decomposition and meanwhile has stable numerical performance. Simulation results indicate the developed soft analyzer has satisfying prediction precision under both nominal and faulty operating conditions.", "authors": ["Haiqing Wang", "Daoying Pi", "Ning Jiang", "Steven X. Ding"], "n_citation": 0, "title": "Soft Analyzer Modeling for Dearomatization Unit Using KPCR with Online Eigenspace Decomposition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0cd23c82-5703-40b0-b5b4-b8962956dffb"}
{"abstract": "The primary mechanism for developing current data-centric and net-centric applications is through software frameworks that extend mainstream languages with runtime libraries. While library-based approaches can be pragmatic and expedient, we assert that programming language extensions are necessary in the long run to obtain application software that is robust, maintainable, and efficient. We discuss, through case studies, how programming language extensions can increase programmer productivity over library-based approaches for data-centric and net-centric applications.", "authors": ["Mukund Raghavachari", "Vivek Sarkar"], "n_citation": 0, "title": "The role of programming languages in future data-centric and net-centric applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0f076a56-8ab4-4a0c-a2a8-ca91209c41bc"}
{"abstract": "We consider scheduling heuristics for batching machines from semiconductor manufacturing. A batch is a collection of jobs that are processed at the same time on the same machine. The processing time of a batch is given by the identical processing time of the jobs within one incompatible family. We are interested in minimizing total weighted tardiness and makespan at the same time. In order to solve this problem, i.e. generate a Pareto-front, we suggest a multiobjective genetic algorithm. We present results from computational experiments on stochastically generated test instances that show the good solution quality of the suggested approach.", "authors": ["Dirk Reichelt", "Lars M\u00f6nch"], "n_citation": 50, "title": "Multiobjective scheduling of jobs with incompatible families on parallel batch machines", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "108ce190-0f96-4dca-8928-efa1a8e93eba"}
{"abstract": "In this paper we extend the area of applications of the Abstract Harmonic Analysis to the field of Boolean function complexity. In particular, we extend the class of functions to which a spectral technique developed in a series of works of the first author can be applied. This extension allows us to prove that testing square-free numbers by unbounded fan-in circuits of bounded depth requires a superpolynomial size. This implies the same estimate for the integer factorization problem.", "authors": ["Anna Bernasconi", "Igor E. Shparlinski"], "n_citation": 50, "title": "Circuit complexity of testing square-free numbers", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "12d790be-d446-435b-8286-b28a75662019"}
{"abstract": "Nowadays, the vast volume of collected digital data obliges us to employ processing methods like pattern recognition and data mining in order to reduce the complexity of data management. In this paper, we present the architecture and the logical foundations for the management of the produced knowledge artifacts, which we call patterns. To this end, we first introduce the concept of Pattern-Base Management System; then, we provide the logical foundations of a general framework based on the notions of pattern types and pattern classes, which stand for the intensional and extensional description of pattern instances, respectively. The framework is general and extensible enough to cover a broad range of real-world patterns, each of which is characterized by its structure, the related underlying data, an expression that carries the semantics of the pattern, and measurements of how successful the representation of raw data is. Finally, some remarkable types of relationships between patterns are discussed.", "authors": ["Stefano Rizzi", "Elisa Bertino", "Barbara Catania", "Matteo Golfarelli", "Maria Halkidi", "Manolis Terrovitis", "Panos Vassiliadis", "Michalis Vazirgiannis", "Euripides Vrachnos"], "n_citation": 0, "title": "Towards a logical model for patterns", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1a56efda-294a-4b0f-972c-b69ca5f74f50"}
{"abstract": "This paper describes a designed and implemented system for efficient storage, indexing and search in collections of spoken documents that takes advantage of automatic speech recognition. As the quality of current speech recognizers is not sufficient for a great deal of applications, it is necessary to index the ambiguous output of the recognition, i. e. the acyclic graphs of word hypotheses - recognition lattices. Then, it is not possible to directly apply the standard methods known from text-based systems. The paper discusses an optimized indexing system for efficient search in the complex and large data structure that has been developed by our group. The search engine works as a server. The meeting browser JFerret, developed withing the European AMI project, is used as a client to browse search results.", "authors": ["Michal Fapso", "Pavel Smrz", "Petr Schwarz", "Igor Sz\u00f6ke", "Milan Schwarz", "Jan Cernocky", "Martin Karafiat", "Lukas Burget"], "n_citation": 0, "title": "Information retrieval from spoken documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "203e4107-1b04-4c8e-9a0b-d7310ad2d110"}
{"abstract": "IETF Mobile IP version 6 and its fast handover protocol are proposed to handle routing of packets to a mobile node when it has moved away from its home network. To do this, each time a mobile node moves to a new location, it configures and confirms its temporal IP address. In this paper, we study the impact of the address configuration and confirmation procedure on the IP handover latency. We first argue that the current strategies for the address configuration and confirmation are unnecessarily slow, so they prevent current protocols (e.g. MIPv6) from being used for real-time traffic. We present a new scheme that can completely eliminate the latency taken by the address configuration and confirmation from the handover latency, so it can be a substitute of the current strategies. A mathematical analysis is developed to show the benefits of our scheme. In the analysis, we compare our scheme with the existing proposals in terms of the handover latency.", "authors": ["Seung-Hee Hwang", "Youn-Hee Han", "Sung-Gi Min", "Chong-Sun Hwang"], "n_citation": 13, "title": "An address configuration and confirmation scheme for seamless mobility support in IPv6 network", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "20b897b9-d50d-44d6-9c7b-dd1083d049fd"}
{"abstract": "Resource management system is the core component of a Grid system. It has two important functions: resource publication and discovery. This paper presents an ontology-based model for Grid Resource Publication and Discovery(GRPD). We adopt multiple domain-specific registries to manage corresponding resources of a Virtual Organization(VO) in order to obtain high GRPD efficiency. Resource descriptions and resource requests are all based on domain-specific ontology. The ontology-based matchmaker of the domain-specific registry plays the important role in resources selection. The Index node of a VO hosts the general registry. Other domain-specific registries are distributed in the VO. This is a two-level registry mechanism. A large-scale Grid system may contain many VOs. Index nodes from various VOs connect to each other in the peer-to-peer mode instead of the hierarchical mode.", "authors": ["Lei Cao", "Minglu Li", "Henry Rong", "Joshua Huang"], "n_citation": 0, "title": "An ontology-based model for Grid resource Publication and discovery", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "20c41466-4fa5-4365-9dde-5b8a2c15edb0"}
{"abstract": "In recent years, Wireless Local Area Networks (WLANs) have become extremely popular. IEEE 802.11 protocol is the dominating standards for WLANs employing the Distributed Coordination Function (DCF) as its Medium Access Control (MAC) mechanism. In the literature, several papers have stuided performance of IEEE 802.11 protocol and assume that a station always have a packet available for transmission. This paper 1  presents a Markov chain model to compute IEEE 802.11 DCF performance taking into account the packet arrival rates. Throughput and packet delay analysis are carried out in order to study the performance of IEEE 802.11 DCF under various traffic conditions. We present the analytical results of throughput and packet delay through network size and arrival rates. Results indicate that throughput and packet delay depends on the number of stations and packet arrival rates.", "authors": ["Sinam Woo", "Woojin Park", "Younghwan Jung", "Sunshin An", "Dongho Kim"], "n_citation": 0, "title": "Throughput and delay analysis considering packet arrival in IEEE 802.11", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "22669664-32dd-41d7-8fc3-7bfb97ac7428"}
{"authors": ["Markus Grassl"], "n_citation": 0, "title": "Computing Equiangular Lines in Complex Space", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "283c074a-3687-41f0-8a82-f90f105c5b7e"}
{"abstract": "Identity management has arisen as a major and urgent challenge for intemet-based communications and information services. Internet services involve complex networks of relationships among users and providers - human and automated - acting in many different capacities under interconnected and dynamic contexts. There is a pressing need for frameworks and models to support the analysis and design of complex social relationships and identities in order to ensure the effective use of existing protection technologies and control mechanisms. Systematic methods are needed to guide the design, operation, administration, and maintenance of internet services, in order to address complex issues of security, privacy, trust and risk, as well as interactions in functionality. All of these rely on sophisticated concepts for identity and techniques for identity management. We propose using a requirements modeling framework GRL to facilitate identity management for Internet Services. Using this modeling approach, we are able to represent different types of identities, social dependencies between identity users and owners, service users and providers, and third party mediators. We may also analyze the strategic rationales of business players/stakeholders in the context of identity management. This modeling approach will help identity management technology vendors to provide customizable solutions, user organizations to form integrated identity management solution, system operators and administrators to accommodate changes, and policy auditors to enforce information protection principles, e.g., Fair Information Practice Principles.", "authors": ["Lin Liu", "Eric Yu"], "n_citation": 0, "title": "Intentional modeling to support identity management", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2b9a63b7-1bfa-412c-97f1-e40ca09f42a5"}
{"abstract": "In this paper we propose a probabilistic analysis of the fully asynchronous behavior (i.e., two cells are never simultaneously updated, as in a continuous time process) of elementary finite cellular automata (i.e., {0,1} states, radius 1 and unidimensional) for which both states are quiescent (i.e., (0,0,0) \u2192 0 and (1,1,1) \u2192 1). It has been experimentally shown in previous works that introducing asynchronism in the global function of a cellular automaton may perturb its behavior, but as far as we know, only few theoretical work exist on the subject. The cellular automata we consider live on a ring of size n and asynchronism is introduced as follows: at each time step one cell is selected uniformly at random and the transition rule is applied to this cell while the others remain unchanged. Among the sixty-four cellular automata belonging to the class we consider, we show that fifty-five other converge almost surely to a random fixed point while nine of them diverge on all non-trivial configurations. We show that the convergence time of these fifty-five automata can only take the following values: either 0, \u0398(n In n), \u0398(n 2 ), \u0398(n 3 ), or \u0398(n2 n ). Furthermore, the global behavior of each of these cellular automata can be guessed by simply reading its code.", "authors": ["Nazim Fat\u00e8s", "Damien Regnault", "Nicolas Schabanel", "Eric Thierry"], "n_citation": 0, "title": "Fully asynchronous behavior of double-quiescent elementary cellular automata", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2f0a8870-2b53-4ff7-bc9e-d34ddde3dbf0"}
{"abstract": "In today's software engineering, the ability to adapt to requirement changes on-the-fly has become a major asset to computer systems. Traditionally, systems were developed following a process consisting of: requirements study, design and implementation phases. Nowadays, requirements are very dynamic - changing constantly. For the engineering of distributed complex software systems, the frameworks used in the implementation phase need to be able to cope with environment changes - bandwidth fluctuations, network topology, host down time, etc. This paper looks at the ability of Linda-based systems to adapt to the predictable and unpredictable changes in distributed environments. Several adaptiveness aspects are discussed and a few of Linda-based systems analyzed.", "authors": ["Ronaldo Menezes", "Robert Tolksdorf"], "n_citation": 0, "title": "Adaptiveness in Linda-based coordination models", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2f15887e-bc9b-4845-b23b-900c27badee3"}
{"abstract": "The problem of the efficient computation of the relative entropy of two distributions represented by deterministic weighted automata arises in several machine learning problems. We show that this problem can be naturally formulated as a shortest-distance problem over an intersection automaton defined on an appropriate semiring. We describe simple and efficient novel algorithms for its computation and report the results of experiments demonstrating the practicality of our algorithms for very large weighted automata. Our algorithms apply to unambiguous weighted automata, a class of weighted automata that strictly includes deterministic weighted automata. These are also the first algorithms extending the computation of entropy or of relative entropy beyond the class of deterministic weighted automata.", "authors": ["Corinna Cortes", "Mehryar Mohri", "Ashish Rastogi", "Michael Riley"], "n_citation": 0, "title": "Efficient computation of the relative entropy of probabilistic automata", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2f24ca91-8e1b-4b91-bfe8-17a0e377e748"}
{"abstract": "Modeling video data poses a great challenge since they do not have as clear an underlying structure as traditional databases do. We propose a graphical object-based model, called VideoGraph, in this paper. This scheme has the following advantages: (1) In addition to semantics of video individual events, we capture their temporal relationships as well. (2) The inter-event relationships allow us to deduce implicit video information. (3) Uncertainty can also be handled by associating the video event with a temporal Boolean-like expression. This also allows us to exploit incomplete information. The above features make VideoGraph very flexible in representing various metadata types extracted from diverse information sources. To facilitate video retrieval, we also introduce a formalism for the query language based on path expressions. Query processing involves only simple traversal of the video graphs.", "authors": ["Duc A. Tran", "Kien A. Hua", "Khanh Vu"], "n_citation": 0, "title": "VideoGraph: A graphical object-based model for representing and querying video data", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "3057ebfa-7823-4593-8cb1-0ad69d14f76d"}
{"abstract": "This paper presents an architecture for managing database evolution when all the components of the database (conceptual schema, logical schema and extension) are available. The strategy of evolution in which our architecture is based is that of 'forward database maintenance', that is, changes are applied to the conceptual schema and propagated automatically down to the logical schema and to the extension. In order to put into practice this strategy, each component of a database is seen under this architecture as the information base of an information system. Furthermore, a translation information system is considered in order to manage the translation of conceptual elements into logical schema elements. A current Oracle implementation of this architecture is also presented.", "authors": ["Eladio Dom\u00ednguez", "Jorge L\u00f3pez Lloret", "Mar\u00eda Antonia Zapata"], "n_citation": 0, "title": "An architecture for managing database evolution", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "33008a31-b85a-477e-a3d6-cbac454841bf"}
{"abstract": "UML is widely accepted and extensively used in software modeling. However, using different diagrams to model different aspects of a system brings the risk of inconsistency among diagrams. In this paper, we investigate an approach to check the consistency between the sequence diagrams and statechart diagrams using the SPIN model checker. To deal with the hierarchy structure of statechart diagrams, we propose a formalism called Split Automata, a variant of automata, which is helpful to bridge the statechart diagrams to SPIN efficiently. Compared with the existing work on model checking UML which do not have formal verification for their translation from UML to the model checker, we formally define the semantics and prove that the automatically translated model (i.e. Split Automata) does simulate the UML model. In this way, we can guarantee that the translated model does represent the original model.", "authors": ["Xiangpeng Zhao", "Quan Long", "Zongyan Qiu"], "n_citation": 0, "title": "Model Checking Dynamic UML Consistency", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "33f2df7a-e9f2-4ee5-aa2d-38717b9c1590"}
{"abstract": "Since evolutionary algorithms make heavy use of randomness it is typically the case that they succeed only with some probability. In cases of failure often the algorithm is restarted. Of course, it is desirable that the point of time when the current run is considered to be a failure and therefore the algorithm is stopped and restarted is determined by the algorithm itself rather than by the user. Here, very simple restart strategies that are non-adaptive are compared on a number of examples with different properties. Circumstances under which specific types of dynamic restart strategies should be applied are described and the potential loss by choosing an inadequate restart strategy is estimated.", "authors": ["Thomas Jansen"], "n_citation": 0, "title": "On the analysis of dynamic restart strategies for evolutionary algorithms", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "34f072e6-479c-44a8-a9b0-3cb87696d943"}
{"abstract": "We a give an intrinsic characterization of the class of functions which are computable in NC1 that is by a uniform, logarithmic depth and polynomial size family circuit. Recall that the class of functions in ALogTime, that is in logarithmic time on an Alternating Turing Machine, is NC 1 . Our characterization is in terms of first order functional programming languages. We define measure-tools called Supinterpretations, which allow to give space and time bounds and allow also to capture a lot of program schemas. This study is part of a research on static analysis in order to predict program resources. It is related to the notion of Quasi-interpretations and belongs to the implicit computational complexity line of research.", "authors": ["Guillaume Bonfante", "Jean-Yves Marion", "Romain P\u00e9choux"], "n_citation": 0, "title": "A characterization of alternating log time by first order functional programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "35be52d4-2bc6-4d9a-8518-ea0ae686a98e"}
{"abstract": "This paper describes a natural language system that extracts entity relationship diagram components from natural language database design documents. The system is a fully integrated composite of existing, publicly available components including a parser, WordNet and Google web corpus search facilities, and a novel rule-based tuple-extraction process. The system differs from previous approaches in being fully automatic (as opposed to approaches requiring human disambiguation or other interaction) and in providing a higher level of performance than previously reported results.", "authors": ["Siqing Du", "Douglas P. Metzler"], "n_citation": 0, "title": "An automated multi-component approach to extracting entity relationships from database requirement specification documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "35f52441-a4ec-497a-8a26-0732fb32094e"}
{"abstract": "Borrowing the title of Nobel-prize laureate physicist Erwin Schrodinger's work What is Life?[1], we ask What are Living Systems? This paper attempts to answer the question by looking at a manufacturing information system and an organization model as examples. The fundamental concept is harmonized autonomous decentralized systems. We will introduce examples that show how this concept is reflected on manufacturing information systems, where humans and machines coexist and work together in the production line, and identify the characteristics of the concept. We will show that, with the current level of technology, human-machine coexistence systems are needed to meet the requirements for manufacturing systems, especially in order to respond quickly to environmental changes. We will also evaluate psychological aspects such as challenges and satisfaction of people involved in operating the systems. Finally, we will introduce issues remaining yet to be addressed.", "authors": ["Noriaki Kurosu", "Shunji Yamada"], "n_citation": 50, "title": "Living manufacturing systems with living organizations", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "36c56996-5fbc-4bb4-842a-6a5ef930a5ca"}
{"abstract": "This paper presents an evaluation framework for requirements engineering tools (RETs). We provide a list of qualitative requirements to guide the customer in evaluating the appropriateness and features functionality of RET. Verification and validation (V&V) activities should be an on-going process throughout life cycle of system development. The paper discusses the framework for evaluating the requirements engineering tools capability for V&V. We tested our proposed evaluation framework on eight different commercial requirements engineering tools. Proposed framework guides the participants (developers and end-users) in evaluating the RET features for assessing the accuracy of RE process.", "authors": ["Raimundas Matulevi\u010dius", "Darijus Strasunskas"], "n_citation": 0, "title": "Evaluation framework of requirements engineering tools for verification and validation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "378075a1-086e-4a79-82a1-ea93ddd4d379"}
{"authors": ["Jan Treur", "Vu1010095", "Faculteit der Exacte Wetenschappen"], "n_citation": 4, "title": "A Computational Agent Model for Hebbian Learning of Social Interaction", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "3f76e9db-6be7-4f52-b68e-e42752f3423b"}
{"authors": ["Dengxin Dai", "Mukta Prasad", "Gerhard N. Schmitt", "Luc Van Gool"], "n_citation": 50, "title": "Learning domain knowledge for fa\u00e7ade labelling", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "3fd6e3af-5dee-40fe-932b-f1ef2e4e9c9c"}
{"abstract": "Grid travelers are special mobile processes responsible for coordinating resources that are distributed across multiple virtual organizations (VOs). We propose a security infrastructure called G-PASS to provide security support for grid travelers during their trip and credential mapping when crossing VO boundaries. We demonstrate the power and feasibility of G-PASS with a bio-informatics application running on multiple VOs. We report and analyze the overheads incurred in migration decisions and the actual process migrations. G-PASS can be installed with GSI as the base, thus making it compatible with existing grid middleware.", "authors": ["Tianchi Ma", "Lin Chen", "Cho-Li Wang", "Francis C. M. Lau"], "n_citation": 0, "title": "G-PASS: Security infrastructure for grid travelers", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4384dacb-5195-425f-9ef9-e555fb3f0a5e"}
{"abstract": "Using the variational approaches to estimate optical flow between two frames, the flow discontinuities between different motion fields are usually not distinguished even when an anisotropic diffusion operator is applied. In this paper, we propose a multi-cue driven adaptive bilateral filter to regularize the flow computation, which is able to achieve the smoothly varied optical flow field with highly desirable motion discontinuities. First, we separate the traditional one-step variational updating model into a two-step filtering-based updating model. Then, employing our occlusion detector, we reformulate the energy functional of optical flow estimation by explicitly introducing an occlusion term to balance the energy loss due to the occlusion or mismatches. Furthermore, based on the two-step updating framework, a novel multi-cue driven bilateral filter is proposed to substitute the original anisotropic diffusion process, and it is able to adaptively control the diffusion process according to the occlusion detection, image intensity dissimilarity, and motion dissimilarity. After applying our approach on various video sources (movie and TV) in the presence of occlusion, motion blurring, non-rigid deformation, and weak textureness, we generate a spatial-coherent flow field between each pair of input frames and detect more accurate flow discontinuities along the motion boundaries.", "authors": ["Jiangjian Xiao", "Hui Cheng", "Harpreet S. Sawhney", "Cen Rao", "Michael A. Isnardi"], "n_citation": 0, "title": "Bilateral Filtering-Based Optical Flow Estimation with Occlusion Detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "439023a5-11b3-4cff-9e0e-d1401a815d38"}
{"abstract": "A new method for constructing minimum-redundancy prefix codes is described. This method does not build a Huffman tree; instead it uses a property of optimal codes to find the codeword length of each weight. The running time of the algorithm is shown to be O(nk), where n is the number of weights and k is the number of different codeword lengths. When the given sequence of weights is already sorted, it is shown that the codes can be constructed using O(log 2k-1  n) comparisons, which is sub-linear if the value of k is small.", "authors": ["Ahmed A. Belal", "Amr Elmasry"], "n_citation": 50, "title": "Distribution-sensitive construction of minimum-redundancy prefix codes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4400fb19-f6ac-4afa-b8df-6ac29915268b"}
{"abstract": "This paper presents mechanisms for managing the content of several Universal Description, Discovery, and Integration (UDDI) registries. These mechanisms are deployed in a wireless environment of Web services. By content, it is meant the announcements of Web services that providers submit to an UDDI registry. Unlike other initiatives in Web services field that consider a single UDDI registry and a wired communication infrastructure, this paper is concerned with the following aspects: several UDDI registries are deployed, there is no wired communication infrastructure between the UDDI registries, and absence of a centralized component that coordinates the UDDI registries. The solution presented integrates users and software agents into what we call messenger. Initially, software agents reside in users' mobile devices and cache a description of the Web services that satisfy their users' needs. Each time a user is in the vicinity of an UDDI registry, her software agent interacts with that registry so the details stored on Web services are submitted.", "authors": ["Zakaria Maamar", "Hamdi Yahyaoui", "Qusay H. Mahmoud", "Fahim Akhter"], "n_citation": 0, "title": "Dynamic management of UDDI registries in a wireless environment of Web services", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "44b0dabe-d580-473e-b9a4-5c3938a18c62"}
{"abstract": "This paper presents the integration between two types of genetic algorithm: a multi-objective genetic algorithm (MOGA) and a co-operative co-evolutionary genetic algorithm (CCGA). The resulting algorithm is referred to as a multi-objective co-operative co-evolutionary genetic algorithm or MOCCGA. The integration between the two algorithms is carried out in order to improve the performance of the MOGA by adding the co-operative co-evolutionary effect to the search mechanisms employed by the MOGA. The MOCCGA is benchmarked against the MOGA in six different test cases. The test problems cover six different characteristics that can be found within multi-objective optimisation problems: convex Pareto front, non-convex Pareto front, discrete Pareto front, multi-modality, deceptive Pareto front and non-uniformity in the solution distribution. The simulation results indicate that overall the MOCCGA is superior to the MOGA in terms of the variety in solutions generated and the closeness of solutions to the true Pareto-optimal solutions. A simple parallel implementation of MOCCGA is described. With an 8-node cluster, the speed up of 2.69 to 4.8 can be achieved for the test problems.", "authors": ["Nattavut Keerativuttitumrong", "Nachol Chaiyaratana", "Vara Varavithya"], "n_citation": 0, "title": "Multi-objective co-operative co-evolutionary genetic algorithm", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "464e7515-0709-4bf5-822d-37f59866a900"}
{"abstract": "We consider an algorithmic problem that arises in manufacturing applications. The input is a sequence of objects of various types. The scheduler is fed the objects in the sequence one by one, and is equipped with a finite buffer. The goal of the scheduler/sorter is to maximally reduce the number of type transitions. We give the first polynomial-time constant approximation algorithm for this problem. We prove several lemmas about the combinatorial structure of optimal solutions that may be useful in future research, and we show that the unified algorithm based on the local ratio lemma performs well for a slightly larger class of problems than was apparently previously known.", "authors": ["Jens S. Kohrt", "Kirk Pruhs"], "n_citation": 0, "title": "A constant approximation algorithm for sorting buffers", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "46550763-6d94-4a99-919d-2a2c731341fd"}
{"abstract": "Orc is a new language for task orchestration, a form of concurrent programming with applications in workflow, business process management, and web service orchestration. Orc provides constructs to orchestrate the concurrent invocation of services - while managing timeouts, priorities, and failure of services or communication. In this paper, we show a trace-based semantic model for Orc, which induces a congruence on Orc programs and facilitates reasoning about them. Despite the simplicity of the language and its semantic model, Orc is able to express a variety of useful orchestration tasks.", "authors": ["David Kitchin", "William R. Cook", "Jayadev Misra"], "n_citation": 0, "title": "A language for task orchestration and its semantic properties", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "48a0a9d4-d459-4d32-8ad7-f81fa4430646"}
{"authors": ["Mohammad Adibuzzaman", "Ferdaus Ahmed Kawsar", "Munirul M. Haque", "Sheikh Iqbal Ahamed", "Chowdhury Sharif Hasan"], "n_citation": 0, "title": "PryGuard: A Secure Distributed Authentication Protocol for Pervasive Computing Environment", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "4a368163-7a61-45fa-9433-bb26dbb1e9d7"}
{"abstract": "We consider the problem of PAC-learning distributions over strings, represented by probabilistic deterministic finite automata (PDFAs). PDFAs are a probabilistic model for the generation of strings of symbols, that have been used in the context of speech and handwriting recognition, and bioinformatics. Recent work on learning PDFAs from random examples has used the KL-divergence as the error measure; here we use the variation distance. We build on recent work by Clark and Thollard, and show that the use of the variation distance allows simplifications to be made to the algorithms, and also a strengthening of the results; in particular that using the variation distance, we obtain polynomial sample size bounds that are independent of the expected length of strings.", "authors": ["Nick Palmer", "Paul W. Goldberg"], "n_citation": 0, "title": "PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4a70e56f-1b8a-4224-8d7e-3a2d7e128d1c"}
{"abstract": "This paper considers the problem of efficient incremental maintenance of memo tables in a tabled logic programming system. Most existing techniques for this problem consider insertion and deletion of facts as primitive changes, and treat update as deletion of the old version followed by insertion of the new version. They handle insertion and deletion using independent algorithms, consequently performing many redundant computations when processing updates. In this paper, we present a local algorithm for handling updates to facts. The key idea is to interleave the propagation of deletion and insertion operations generated by the updates through a dynamic (and potentially cyclic) dependency graph. The dependency graph used in our algorithm is more general than that used in algorithms previously proposed for incremental evaluation of attribute grammars and functional programs. Nevertheless, our algorithm's complexity matches that of the most efficient algorithms built for these specialized cases. We demonstrate the effectiveness of our algorithm using data-flow analysis and parsing examples.", "authors": ["Diptikalyan Saha", "C. R. Ramakrishnan"], "n_citation": 0, "title": "A local algorithm for incremental evaluation of tabled logic programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4d1e23c3-d4e8-422a-9b27-61bd323b77db"}
{"abstract": "We develop new mathematical results based on the spherical harmonic convolution framework for reflection from a curved surface. We derive novel identities, which are the angular frequency domain analogs to common spatial domain invariants such as reflectance ratios. They apply in a number of canonical cases, including single and multiple images of objects under the same and different lighting conditions. One important case we consider is two different glossy objects in two different lighting environments. Denote the spherical harmonic coefficients by B light,material lm, where the subscripts refer to the spherical harmonic indices, and the superscripts to the lighting (1 or 2) and object or material (again 1 or 2). We derive a basic identity, B 1,1  lm B 2,2  lm  = B 1,2  lm B 2,1  lm , independent of the specific lighting configurations or BRDFs. While this paper is primarily theoretical, it has the potential to lay the mathematical foundations for two important practical applications. First, we can develop more general algorithms for inverse rendering problems, which can directly relight and change material properties by transferring the BRDF or lighting from another object or illumination. Second, we can check the consistency of an image, to detect tampering or image splicing.", "authors": ["Dk Mahajan", "Ravi Ramamoorthi", "Brian Curless"], "n_citation": 0, "title": "A Theory of Spherical Harmonic Identities for BRDF/Lighting Transfer and Image Consistency", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4d3abf93-4d7f-4f8b-a955-9cd0be53994c"}
{"abstract": "Several state-of-the-art Generic Visual Categorization (GVC) systems are built around a vocabulary of visual terms and characterize images with one histogram of visual word counts. We propose a novel and practical approach to GVC based on a universal vocabulary, which describes the content of all the considered classes of images, and class vocabularies obtained through the adaptation of the universal vocabulary using class-specific data. An image is characterized by a set of histograms - one per class - where each histogram describes whether the image content is best modeled by the universal vocabulary or the corresponding class vocabulary. It is shown experimentally on three very different databases that this novel representation outperforms those approaches which characterize an image with a single histogram.", "authors": ["Florent Perronnin", "Christopher R. Dance", "Gabriela Csurka", "Marco Bressan"], "n_citation": 0, "title": "Adapted Vocabularies for Generic Visual Categorization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "52059c0c-89fd-45a6-b51b-a5a624382db3"}
{"abstract": "Adaptation in open, multi-agent information gathering systems is important for several reasons. These reasons include the inability to accurately predict future problem-solving workloads, future changes in existing information requests, future failures and additions of agents and data supply resources, and other future task environment characteristic changes that require system reorganization. We are developing a multi-agent financial portfolio management system that must deal with all of these problems. This paper will briefly describe our approaches and solutions at several different levels within the agents: adaptation at the organizational, planning, scheduling, and execution levels. We discuss our solution for execution-level adaptation ( cloning ) in detail, and present empirical evidence backing up the theory behind this execution-level solution.", "authors": ["Keith Decker", "Katia P. Sycara", "Malcolm Williamson"], "n_citation": 0, "title": "Cloning for intelligent adaptive information agents", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "52b5a27c-a102-4191-9d5a-e0aa9a82b28c"}
{"abstract": "As the Internet infrastructure has been developed, many diverse and effective applications attempt to gain the potential of that infrastructure. Peer-to-Peer, one of the most representative systems for sharing information on a distributed environment, is a system can helps peer users to share their files with other peer users easily. But Peer-to-Peer network includes not only uncountable files but also plenty of duplicates, which is, bring about increase of network traffic. To solve this problem, we suggest an effective information sharing system supporting collaboration among distributed users with similar interests, or who are part of the same workgroup. In this paper, we exploit the techniques of association rules in deriving peer user profiles represented as a prefix tree structure called PTP-tree (Personalized Term Pattern tree). In addition, we employ content-based filtering approach to search documents that are similar to personalized term patterns. For the performance evaluation, we formed a simple Peer-to-Peer network to make experiments on real data and 10 users. Experimental results show that the proposed system helps users to reduce time for gathering documents relevant to users' needs. In addition, PTP-tree structure of a user profile saves the memory usage.", "authors": ["Heung-Nam Kiml", "Hyun-Jun Kim", "Geun-Sik Jo"], "n_citation": 0, "title": "Content-based document recommendation in collaborative Peer-to-Peer network", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "537f75bd-c28a-4473-9ca7-911a38a1c31e"}
{"abstract": "This paper reopens the long dormant topic of natural language updates to databases. A protocol to handle database updates of the IDM (Insert-Delete-Modify) class is proposed and implemented. This protocol exploits modern relational update facilities and constraints and structures update dialogues using DAMSL dialogue acts. The protocol may be used with any natural language parser that maps to relational queries.", "authors": ["Michael Minock"], "n_citation": 50, "title": "Natural language updates to databases through dialogue", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5558ba0d-51df-421c-a145-41070f9ee016"}
{"abstract": "This paper addresses a novel steganography method for images. Most statistical steganalysis algorithms are strong to defeat previous steganography algorithms. RS steganalysis and pixel difference histogram analysis are two well-known statistical steganalysis algorithms which detect non-random changes caused by embedding a secret message into cover image. In this paper, we first explain how two steganalysis algorithms exploit the effect of the non-random changes and then propose a new steganography method that avoids the non-random changes to evade statistical analysis methods. For this purpose, we adjust the embedding process to be more adaptive to cover image by considering embedding in Gray code bit planes, not natural binary bit planes, of cover images, and two parameters: (1) similarity threshold for selecting non-flat area in lower bit planes, and (2) size of flat blocks n x n in embedding bit planes. Experimental results show that the secret messages embedded by our method are undetectable under RS steganalysis and pixel difference histogram analysis.", "authors": ["Bui Cong Nguyen", "Sang Moon Yoon", "Heung-Kyu Lee"], "n_citation": 0, "title": "Multi Bit Plane Image Steganography", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "569695d0-7966-4416-af4e-75e78975fad0"}
{"abstract": "The conventional side-view and rear-view mirrors are not enough for driver's safety in an automobile. A driver may not be able to recognize the vehicle in a blind spot. In this paper, we propose an automotive detector algorithm using biologically motivated selective attention model for a blind spot monitor. This method decides a region of interest (ROI) which includes the blind spot from the successive image frames obtained by side-view cameras. It can detect the dangerous situations in the ROI using novelty points from the biologically motivated selective attention model, and alerts the driver whether there is dangerous object for changing the lane in driving. The proposed algorithm is based on deciding the ROI using difference from intensity histogram of a Gaussian smoothed image and finding the novelty points from the biologically motivated selective attention model. From variations of those novelty points, we determine whether a vehicle is approaching or not.", "authors": ["Jaekyoung Moon", "Jiyoung Yeo", "Sungmoon Jeong", "Paljoo Yoon", "Minho Lee"], "n_citation": 50, "title": "An Automotive Detector Using Biologically Motivated Selective Attention Model for a Blind Spot Monitor", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "57c88ced-46ba-4781-9de7-8a80d880dc78"}
{"abstract": "Learning with adaptivity is a key issue in many nowadays applications. The most important aspect of such an issue is incremental learning (IL). This latter seeks to equip learning algorithms with the ability to deal with data arriving over long periods of time. Once used during the learning process, old data is never used in subsequent learning stages. This paper suggests a new IL algorithm which generates categories. Each is associated with one class. To show the efficiency of the algorithm, several experiments are carried out.", "authors": ["Abdelhamid Bouchachia"], "n_citation": 0, "title": "Learning with Incrementality", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "591cddbd-4b38-4fdf-880a-7503657502fa"}
{"abstract": "The concept of Oblivious Routing for general undirected networks was introduced by Racke [12] when he showed that there exists an oblivious routing algorithm with polylogarithmic competitive ratio (w.r.t. edge congestion) for any undirected graph. In a following result, Racke and Rosen [13] presented admission control algorithms achieving a polylogarithmic fraction (in the size of the network) of the optimal number of accepted messages. Both these results assume that the network incurs a cost only after it is accepted and the message is routed. Admission control and routing algorithms for sensor networks under energy constraints, however, need to account for the energy spent in checking for feasible routes prior to the acceptance of a message and hence, it is unclear if these algorithms achieve polylogarithmic bounds under this condition. In this paper, we address this problem and prove that such algorithms do not exist when messages are generated by an adversary. Furthermore, we show that an oblivious routing algorithm cannot have a polylogarithmic competitive ratio without a packet-admission protocol. We present a deterministic O(p log n)-competitive algorithm for tree networks where the capacity of any node is in [k, \u03c1k]. For grids, we present an O(log n)-competitive algorithm when nodes have capacities in \u0398(log n) under the assumption that each message is drawn uniformly at random from all pairs of nodes in the grid.", "authors": ["Mohamed Aly", "John Augustine"], "n_citation": 0, "title": "Online Packet Admission and Oblivious Routing in Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5bbb4eb8-23fa-409f-b617-578646d17ca4"}
{"abstract": "This paper proposes an evolutionary multiobjective optimization (EMO) approach to knowledge extraction from numerical data for high-dimensional pattern classification problems with many continuous attributes. The proposed approach is a three-stage rule extraction method. First each continuous attribute is discretized into several intervals using a class entropy measure. In this stage, multiple partitions with different granularity are specified. Next a prespecified number of candidate rules are generated from numerical data using a heuristic rule evaluation measure in a similar manner to data mining. Then a small number of candidate rules are selected by an EMO algorithm. The EMO algorithm tries to maximize the accuracy of selected rules. At the same time, it tries to minimize their complexity. Our rule selection problem has three objectives: to maximize the number of correctly classified training patterns, to minimize the number of selected rules and to minimize their total rule length. The length of each rule is defined by the number of its antecedent conditions. The main characteristic feature of the proposed EMO approach is that many rule sets with different accuracy and different complexity are simultaneously obtained from its single run. They are tradeoff solutions (i.e., non-dominated rule sets) with respect to the accuracy and the complexity. Through computational experiments, we demonstrate the applicability of the proposed EMO approach to high-dimensional pattern classification problems with many continuous attributes. We also demonstrate some advantages of the proposed EMO approach over single-objective ones.", "authors": ["Hisao Ishibuchi", "Satoshi Namba"], "n_citation": 0, "title": "Evolutionary multiobjective knowledge extraction for high-dimensional pattern classification problems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5c3238e0-b2e9-4611-8a65-597c4afa6866"}
{"abstract": "The problem of finding anomaly has received much attention recently. However, most of the anomaly detection algorithms depend on an explicit definition of anomaly, which may be impossible to elicit from a domain expert. Using discords as anomaly detectors is useful since less parameter setting is required. Keogh et al proposed an efficient method for solving this problem. However, their algorithm requires users to choose the word size for the compression of subsequences. In this paper, we propose an algorithm which can dynamically determine the word size for compression. Our method is based on some properties of the Haar wavelet transformation. Our experiments show that this method is highly effective.", "authors": ["Ada Wai-Chee Fu", "Oscar Tat-Wing Leung", "Eamonn J. Keogh", "Jessica Lin"], "n_citation": 0, "title": "Finding Time Series Discords Based on Haar Transform", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5c3aa5a5-7fcc-4fce-ba0e-4744e3d9587b"}
{"abstract": "This work focuses on interface design for supporting users of instrumented environments to interact with virtual information embedded in the real world. A gesture-based interaction paradigm is presented and the prototype of an interface providing affordances for gesture-based direct manipulation of digital information is described.", "authors": ["Lucia Terrenghi"], "n_citation": 0, "title": "Design of affordances for direct manipulation of digital information in ubiquitous computing scenarios", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5c84e2a0-4a21-4eaf-b5b2-cb7956097283"}
{"abstract": "Considerable research has gone into developing ontologies and applying them to a variety of applications. The extraction of domain knowledge for developing these ontologies is often performed on a manual basis. The World Wide Web contains a wealth of knowledge about an application domain; however it is embedded within web pages. This research presents a methodology for semi-automatically extracting knowledge from the World Wide Web and organizing it into domain ontologies. Initial semantics of a target domain are provided by a set of keywords. From these, web pages are identified that contain relevant information for the subject domain using search engines. Web data extraction techniques are employed to extract information from these web pages and infer how the information is related. Extracted knowledge is then organized into a domain ontology. Testing of the methodology on various application domains illustrates the feasibility of the approach.", "authors": ["Veda C. Storey", "Roger H. L. Chiang", "G. Lily Chen"], "n_citation": 50, "title": "Ontology creation : Extraction of domain knowledge from web documents", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5fe47f36-4ce2-415c-846a-6cc8d601a804"}
{"abstract": "Security critical processes in the context of this paper are considered information technology processes that are implemented to achieve a high level of integrity, authenticity and confidentiality of data processing. It's a matter of fact that high-level security may only be achieved through adequate physical protection. The classical approach is to close away such processes in computing centers protected by physical barriers with high drag factors - well known to cause substantial investment and operational costs. This paper describes a new, recently patented method to protect such a security critical process by different means - not to keep the attacker out but to take the valuables out when the attacker comes in. Practical application of this strategy is detailed giving an example of protecting a Trust Center of a Public Key Infrastructure by such means.", "authors": ["Ferdinand J. Dafelmair"], "n_citation": 0, "title": "Survivability strategy for a security critical process", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6280a20c-dcab-4e4d-8b16-e3736e8a40aa"}
{"abstract": "Conceptual modeling has become a key task in the early phases of object oriented (OO) software life cycle. In the development of OO software class diagrams represent the conceptual schema that reflects not only the objects of the application domain but also the behaviour of them. Indeed, class diagrams constitute the backbone of OO software so, their quality has a great impact on the quality of the product which is finally implemented. To assess class diagram quality, it is useful to have quantitative and objective measurement instruments. After having thoroughly reviewed existing OO measures applicable to class diagrams at a high-level design stage, we defined a set of metrics for UML class diagram structural complexity (and internal quality attribute), with the idea that it is related to maintainability of such diagrams. In order to gather empirical evidence that the proposed metrics could be early indicators of class diagrams maintainability, we carried out a controlled experiment. The main goal of this paper is to show each of the steps of the experimental process, and how we have built a prediction model for class diagram maintainability based upon the data collected in the experiment using a novel process, the Fuzzy Prototypical Knowledge Discovery process.", "authors": ["Marcela Genero", "Josh Olivas", "Mario Piattini", "Francisco P. Romero"], "n_citation": 0, "title": "Assessing object-oriented conceptual models maintainability", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "655d06b3-f612-43ee-aa2b-40f4b22bdc4a"}
{"authors": ["Lars R. Knudsen", "Florian Mendel", "Christian Rechberger", "S\u00f8ren S. Thomsen"], "n_citation": 0, "title": "Cryptanalysis of MDC-2", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "67c28d22-3b3c-4e3a-a342-f56c99ca8fca"}
{"abstract": "We define the Subclass of Unrollable List Formulas in ACL2 (SULFA). SULFA is a subclass of ACL2 formulas based on list structures that is sufficiently expressive to include invariants of finite state machines (FSMs). We have extended the ACL2 theorem prover to include a new proof mechanism, which can recognize SULFA formulas and automatically verify them with a SAT-based decision procedure. When this decision procedure is successful, a theorem is added to the ACL2 system database as a lemma for use in future proof attempts. When unsuccessful, a counter-example to the SULFA property is presented. We are using SULFA and its SAT-based decision procedure as part of a larger system to verify components of the TRIPS processor. Our verification system translates Verilog designs automatically into ACL2 models. These models are written such that their invariants are SULFA properties, which can be verified by our SAT-based decision procedure, traditional theorem proving, or a mixture of the two.", "authors": ["Erik Reeber", "Warren A. Hunt"], "n_citation": 0, "title": "A SAT-based decision procedure for the subclass of unrollable list formulas in ACL2 (SULFA)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6980f8f2-62d7-48b6-8352-a6b59aa0085f"}
{"abstract": "This paper provides a solution to the curse of dimensionality problem in the pairwise scoring techniques that are commonly used in bioinformatics and biometrics applications. It has been recently discovered that stacking the pairwise comparison scores between an unknown patterns and a set of known patterns can result in feature vectors with nice discriminative properties for classification. However, such technique can lead to curse of dimensionality because the vectors size is equal to the training set size. To overcome this problem, this paper shows that the pairwise score matrices possess a symmetric and diagonally dominant property that allows us to select the most relevant features independently by an FDA-like technique. Then, the paper demonstrates the capability of the technique via a protein sequence classification problem. It was found that 10-fold reduction in the number of feature dimensions and recognition time can be achieved with just 4% reduction in recognition accuracy.", "authors": ["Man-Wai Mak", "Sun-Yuan Kung"], "n_citation": 0, "title": "A Solution to the Curse of Dimensionality Problem in Pairwise Scoring Techniques", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6abbafe1-4e29-471d-9f1f-25c7e38d7878"}
{"abstract": "This paper deals with the problem of handling semantic heterogeneity during schema integration. Semantics refer to the meaning of data in contrast to syntax, which solely defines the structure of schema elements. We focus on the part of semantics related to the meanings of terms used to name schema elements. Our approach does not rely on the names of the schema elements or the structure of the schema. Instead, we present an approach based on formal ontologies presented in a logical language for integrating schemas to generate a global schema. Semantic similarity relations between definitions in formal ontologies are defined and used for merging ontologies. We show how similarity relations are discovered by a reasoning system using a higher-level ontology. The result of the merging process is used for schema integration. Schema integration is used to obtain the global schema of a tightly-coupled federated database system. Afterwards, we illustrate how the produced global schema can help for the mapping of data elements.", "authors": ["Farshad Hakimpour", "Andreas Geppert"], "n_citation": 86, "title": "Global schema generation using formal ontologies", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6b80ed7d-cc6d-455e-945f-f8ae939480c1"}
{"abstract": "Many recent results are concerned with interpreting proofs of security done in symbolic models in the more detailed models of computational cryptography. In the case of symmetric encryption, these results stringently demand that no key cycle (e.g. {k} k ) can be produced during the execution of protocols. While security properties like secrecy or authentication have been proved decidable for many interesting classes of protocols, the automatic detection of key cycles has not been studied so far. In this paper, we prove that deciding the existence of key-cycles is NP-complete for a bounded number of sessions. Next, we observe that the techniques that we use are of more general interest and apply them to reprove the decidability of a significant existing fragment of protocols with timestamps.", "authors": ["V\u00e9ronique Cortier", "Eugen Zalinescu"], "n_citation": 0, "title": "Deciding key cycles for security protocols", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6d4b5ffc-67b7-4180-88b5-497c87b2e446"}
{"abstract": "We study algorithms for computing stable models of propositional logic programs and derive estimates on their worst-case performance that are asymptotically better than the trivial bound of O(m2 n ), where m is the size of an input program and n is the number of its atoms. For instance, for programs, whose clauses consist of at most two literals (counting the head) we design an algorithm to compute stable models that works in time O(m x 1.44225 n ). We present similar results for several broader classes of programs, as well.", "authors": ["Zbigniew Lonc", "Miroslaw Truszczynski"], "n_citation": 50, "title": "Computing stable models: Worst-case performance estimates", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6db1fe28-d70d-4033-91bc-25d6d46a8081"}
{"abstract": "We study auctioning multiple units of the same good to potential buyers with single unit demand (i.e. every buyer wants only one unit of the good). Depending on the objective of the seller, different selling mechanisms are desirable. The Vickrey auction with a truthful reserve price is optimal when the objective is efficiency - allocating the units to the parties who values them the most. The Myerson auction is optimal when the objective is the seller's expected utility. These two objectives are generally in conflict, and cannot be maximized with one mechanism. In many real-world settings-such as privatization and competing electronic marketplaces-it is not clear that the objective should be either efficiency or seller's expected utility. Typically, one of these objectives should weigh more than the other, but both are important. We account for both objectives by designing a new deterministic dominant strategy auction mechanism that maximizes expected social welfare subject to a minimum constraint on the seller's expected utility. This way the seller can maximize social welfare subject to doing well enough for himself.", "authors": ["Anton Likhodedov", "Tuomas Sandholm"], "n_citation": 0, "title": "Mechanism for optimally trading off revenue and efficiency in multi-unit auctions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6eb3b52f-9310-42ba-94c3-e2eac8474d78"}
{"abstract": "Due to the progress in computer graphics hardware high resolution 3d models may be explored at interactive frame rates, and facilities to explore them are a part of modern radiological workstations and therapy planning systems. Despite their advantages, 3d visualizations are only employed by a minority of potential users and even these employ 3d visualizations for a few selected tasks only. We hypothesize that this results from a lack of intuitive interaction techniques for 3d rotation. In this paper, we compare existing techniques with respect to design principles derived by clinical applications and present results of an empirical study. These results are relevant beyond clinical applications and strongly suggest that the presented design principles are crucial for comfortable and predictable interaction techniques for 3d rotation.", "authors": ["Ragnar Bade", "Felix Ritter", "Bernhard Preim"], "n_citation": 0, "title": "Usability comparison of mouse-based interaction techniques for predictable 3d rotation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7018da88-c0be-4183-bfb1-e3978b04c005"}
{"abstract": "Region-based memory management is an attractive alternative to garbage collection. It relies on a compile-time analysis to annotate the program with explicit allocation and deallocation instructions, where lifetimes of memory objects are grouped together in regions. This paper investigates how to adapt the runtime part of region-based memory management to the WAM setting. We present additions to the memory architecture and instruction set of the WAM that are necessary to implement regions. We extend an optimized WAM-based Prolog implementation with a region-based memory manager which supports backtracking with instant reclamation, and cuts. The performance of region-based execution is compared with that of the baseline garbage-collected implementation on several benchmark programs. A region-enabled WAM performs competitively and often results in time and/or space improvements.", "authors": ["Henning Makholm", "Konstantinos F. Sagonas"], "n_citation": 0, "title": "On enabling the WAM with region support", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "73134cd8-236a-493f-a039-858241db0c46"}
{"authors": ["Florian Mendel", "Vincent Rijmen", "Deniz Toz", "Kerem Varici"], "n_citation": 50, "title": "Collisions for the WIDEA-8 Compression Function", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "740876ef-8b4e-4eb0-8d02-d9dcb67f8139"}
{"abstract": "In this paper, we present a new middle-ware, streaming engine that can implement existing OpenGL-based 3D network games onto heterogeneous platforms. The engine consists of capturing OpenGL command stream, scene graph reconstruction, data simplification, and compression and transmission. Without modifying the original source code, our system can extend 3D network games onto various platforms, using hierarchical geometry hashing, a client-server scene graph, and a simple NPR(Non-Photorealistic Rendering) technique to reduce the amount of transmission.", "authors": ["Gi Sook Jung", "Soon Ki Jung"], "n_citation": 50, "title": "A streaming engine for PC-based 3D network games onto heterogeneous mobile platforms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "746f47e1-cfbb-41f8-8512-2991163fecb9"}
{"abstract": "The large computational price of formal verification of general w-regular properties has led to the study of restricted classes of properties, and to the development of verification methodologies for them. Examples that have been widely accepted by the industry include the verification of safety properties, and bounded model checking. We introduce and study another restricted class of properties - the class of locally checkable properties. For an integer k > 1, a language L C \u03a3 \u03c9  is k-checkable if there is a language R \u2282 \u03a3 \u03ba  (of allowed subwords) such that a word w belongs to L iff all the subwords of w of length k belong to R. A property is locally checkable if its language is \u03ba-checkable for some k. Locally checkable properties, which are a special case of safety properties, are common in the specification of systems. In particular, one can often bound an eventuality constraint in a property by a fixed time frame. The practical importance of locally checkable properties lies in the low memory demand for their run-time verification. A monitor for a \u03ba-checkable property needs only a record of the last \u03ba computation cycles. Furthermore, even if a large number of \u03ba-checkable properties are monitored, the monitors can share their memory, resulting in memory demand that do not depend on the number of properties monitored. This advantage of locally checkable properties makes them particularly suitable for run-time verification. In the paper, we define locally checkable languages, study their relation to other restricted classes of properties, study the question of deciding whether a property is locally checkable, and study the relation between the size of the property (specified by an LTL formula or an automaton) and the smallest k for which the property is \u03ba-checkable.", "authors": ["Orna Kupferman", "Yoad Lustig", "Moshe Y. Vardi"], "n_citation": 0, "title": "On locally checkable properties", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "74d3e2b1-41a6-40c9-8d08-0ca0e041b741"}
{"authors": ["Joan Daemen", "Ren\u00e9 Govaerts", "Joos Vandewalle"], "n_citation": 50, "title": "A framework for the design of one-way hash functions including cryptanalysis of Damg\u00e5rd's one-way function based on a cellular automaton", "venue": "Lecture Notes in Computer Science", "year": 1993, "id": "7502e459-d328-4866-a625-6a70daa4f78c"}
{"abstract": "Most researchers focused on particular result ignore intermediate stages of learning process of neural networks. The unstable and transitory phenomena, discovered in neural networks during the learning process, long time after the initial stage of learning, when the network knows nothing because of random values of all weights, and long time before final stage of learning process, when the network knows (almost) everything - can be very interesting, especially when we can associate with them some psychological interpretations. Some immature neurons exhibit behavior that can be interpreted as source of artificial dreams. Article presents examples of simple neural networks with capabilities which might explain the origins of dreams and myths.", "authors": ["Ryszard Tadeusiewicz", "Andrzej Izworski"], "n_citation": 0, "title": "Learning in Neural Network : Unusual Effects of Artificial Dreams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7529682c-fee6-4658-802a-9acafae733c2"}
{"abstract": "Genetic Algorithms (GAs) are well-known heuristic algorithms and have been widely applied to solve combinatorial problems. Edge recombination is one of the famous crossovers designed for GAs to solve combinatorial problems. The essence of edge recombination is to achieve maximal inheritance from parental edges. This paper presents two strategies to improve edge recombination. First, we encourage alternation of parents in edge inheritance. Second, a greedy method is used to handle the failures occurred in edge recombination. A modified edge recombination, called edge recombination with tabu (Edge-T), is proposed according to these two strategies. The traveling salesman problem is used as a benchmark. to demonstrate the effectiveness of the proposed method. Experimental results indicate that Edge-T can achieve better performance than the conventional edge recombination Edge-3 in terms of both solution quality and convergence speed.", "authors": ["Chuan-Kang Ting"], "n_citation": 0, "title": "Improving edge recombination through alternate inheritance and greedy manner", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "778c6762-7136-445a-91f8-d912242c3802"}
{"abstract": "Currently, many researches have been pursued for cardiovascular disease diagnosis using ECG so far. In this paper we extract multi-parametric features by HRV analysis from ECG, data preprocessing and heart disease pattern classification method. This study analyzes the clinical information as well as the time and the frequency domains of HRV, and then discovers cardiovascular disease patterns of patient groups. In each group, its patterns are a large frequency in one class, patients with coronary artery disease but are never found in the control or normal group. These patterns are called emerging patterns. We also use efficient algorithms to derive the patterns using the cohesion measure. Our studies show that the discovered patterns from 670 participants are used to classify new instances with higher accuracy than other reported methods.", "authors": ["Heon Gyu Lee", "Kiyong Noh", "Burn Ju Lee", "Ho-Sun Shon", "Keun Ho Ryul"], "n_citation": 0, "title": "Cardiovascular Disease Diagnosis Method by Emerging Patterns", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "77bd5d20-5c7f-4ace-a048-8ea054bcbb54"}
{"abstract": "A new approach to software design based on an agent-oriented architecture is presented. Unlike current research, we consider software to be designed and implemented with this methodology in mind. In this approach agents are considered adaptively communicating concurrent modules which are divided into a white box module responsible for the communications and learning, and a black box which is the independent specialized processes of the agent. A distributed Learning policy is also introduced for adaptability.", "authors": ["Babak Hodjat", "Christopher J. Savoie", "Makoto Amamiya"], "n_citation": 0, "title": "An adaptive agent oriented software architecture", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "783d39da-fc7a-4b94-82f6-3f167de38bf1"}
{"abstract": "We study an evolutionary algorithm used for optimizing in a chaotically changing dynamic environment. The corresponding chaotic non-stationary fitness landscape can be characterized by quantifiers of the underlying dynamics-generating system. We give experimental results about how these quantifiers, namely the Lyapunov exponents, together with the environmental change period of the landscape influence performance measures of the evolutionary algorithm used for tracking optima.", "authors": ["Hendrik Richter"], "n_citation": 0, "title": "Behavior of evolutionary algorithms in chaotically changing fitness landscapes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7b3f5341-6e77-4487-bf4e-eca7acf228a0"}
{"abstract": "Integrating information from diverse sources is of great importance in the database area. The main difficulty in information integration is reconciling data semantics. Common approaches to semantic reconciliation are based on first identifying similar entity types in various sources, and then reconciling entity type properties (attributes and relationships). Such approaches assume all instances to be reconciled belong to well-defined types. We suggest an alternative approach based on two fundamental principles. First, reconciliation does not require that instances be assigned to specific types. Instead, sources can be reconciled by analyzing similarities of properties. Second, properties that appear different may be manifestations of a higher-level property that has the same meaning across sources. We present the fundamental ideas underlying our approach, analyze its potential advantages, suggest how the approach can be formalized, demonstrate with examples the feasibility of using it for semantic reconciliation, and suggest directions for further research.", "authors": ["Jeffrey Parsons", "Yair Wand"], "n_citation": 50, "title": "Property-based semantic reconciliation of heterogeneous information sources", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7d422b94-2202-47d8-b894-8c1a75562bf9"}
{"abstract": "Mismatches of units and of scales of values in physical calculations are disastrous, but rather common, in the development of embedded control systems. They can be as plain as mixing feet and metres, or as hidden as a wrong exponent in a complex calculation formula. These errors can be found by a checking algorithm, following some simple rules, if information on the units of the used variables is provided. This paper describes a developer friendly approach of providing this checking functionality in SCADE, a model-based graphical development tool for safety-critical embedded applications.", "authors": ["Rupert Schlick", "Wolfgang Herzner", "Thierry Le Sergent"], "n_citation": 0, "title": "Checking SCADE Models for Correct Usage of Physical Units", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7d83a928-f74d-4430-8d94-8a4dc45bb4d6"}
{"abstract": "Let S be a set of n elements, and let H be a set-system on S, which satisfies that the size of any element of H is divisible by m, but the intersection of any two elements of H is not divisible by m. If m is a prime or prime-power, then the famous Frankl- Wilson theorem [3] implies that |H| = O(n m-1 ), i.e. for fixed m, its size is at most polynomial in n. This theorem has numerous applications in combinatorics and also in geometry, (c.f. the disproof of Borsuk's conjecture by Kahn and Kalai in 1992 ([4]), or explicit constructions of Ramsey graphs, or other geometric applications related to the Hadwiger-problem.) Frankl and Wilson asked in [3] whether an analogous upper bound existed for non-prime power, composite moduli. Here we show a surprising construction of a superpolynomial-sized uniform set-system H satisfying the intersection-property, for every non-prime-power, composite m, negatively settling a related conjecture of Babai and Frankl [1]. The proof uses a low-degree polynomial-construction of Barrington, Beigel and Rudich [2], and a new method (Lemma 8), for constructing set-systems from multivariate polynomials.", "authors": ["Vince Grolmusz"], "n_citation": 50, "title": "On set systems with restricted intersections modulo a composite number", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "7e9945c6-9519-4842-adbc-0b4dd780f6be"}
{"abstract": "In this paper, a new improvement approach of the perfect recall rate of a block splitting type morphological associative memory (BMAM) is presented. The BMAM is one of MAMs without the kernel image, which is realized in more compact size as keeping the perfect recall rate as same as a normal MAM (without the kernel image). However, the MAM without kernel image has a problem that the perfect recall rate is inferior to a standard MAM (with the kernel image). Therefore, we try to improve the problem by a majority logic scheme and confirm the effectiveness of the proposed approach through autoassociation experiments of alphabet patterns compared to the traditional approaches in terms of the noise tolerance.", "authors": ["Takashi Saeki", "Tsutomu Miki"], "n_citation": 0, "title": "Improvement of the Perfect Recall Rate of Block Splitting Type Morphological Associative Memory Using a Majority Logic Approach", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ff60202-2ce3-4bb0-b857-2c9ecbbfb05e"}
{"abstract": "This article presents a rare case of a deterministic second preimage attack on a cryptographic hash function. Using the notion of controllable output differences, we show how to construct second preimages for the SMASH hash functions. If the given preimage contains at least n + 1 blocks, where n is the output length of the hash function in bits, then the attack is deterministic and requires only to solve a set of n linear equations. For shorter preimages, the attack is probabilistic.", "authors": ["Mario Lamberger", "Norbert Pramstaller", "Christian Rechberger", "Vincent Rijmen"], "n_citation": 0, "title": "Second preimages for SMASH", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "80233550-b15b-47ff-9b63-6d05ce13d5b3"}
{"abstract": "The eXtensible Markup Language (XML) is increasingly finding acceptance as a standard for storing and exchanging structured and semi-structured information. With its expressive power, XML enables a great variety of applications relying on such structures - notably product catalogs, digital libraries, and electronic data interchange (EDI). As the data schema, an XML Document Type Definition (DTD) is a means by which documents and objects can be structured. Currently, there is no suitable way to model DTDs conceptually. Our approach is to model DTDs and thus classes of documents on the basis of UML (Unified Modeling Language). We consider UML to be the connecting link between software engineering and document design, i.e., it is possible to design object-oriented software together with the necessary XML structures. For this reason, we describe how to transform the static part of UML, i.e. class diagrams, into XML DTDs. The major challenge for the transformation is to define a suitable mapping reflecting the semantics of a UML specification in a DTD correctly. Because of XML's specific properties, we slightly extend the UML language in a UML-compliant way. Our approach provides the stepping stone to bridge the gap between object-oriented software design and the development of XML data schemata.", "authors": ["Rainer Conrad", "Dieter Scheffner", "J. Christoph Freytag"], "n_citation": 0, "title": "XML conceptual modeling using UML", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "82cd28e4-d55b-48c0-9e73-6bc2a4832dc7"}
{"abstract": "This paper discusses FearNot, a virtual learning environment populated by synthetic characters aimed at the 8-12 year old age group for the exploration of bullying and coping strategies. Currently, FearNot is being redesigned from a lab-based prototype into a classroom tool. In this paper we focus on informing the design of the characters and of the virtual learning environment through our interpretation of qualitative data gathered about interaction with FearNot by 345 children. The paper focuses on qualitative data collected using the Classroom Discussion Forum technique and discusses its implications for the redesign of the media used for FearNot. The interpretation of the data identifies that the use of fairly naive synthetic characters for achieving empathic engagement appears to be an appropriate approach. Results do indicate a focus for redesign, with a clear need for improved transitions for animations; identification and repair of inconsistent graphical elements; and for a greater cast of characters and range of sets to achieve optimal engagement levels.", "authors": ["Lynne E. Hall", "Marco Vala", "Marc Hall", "Marc Webster", "Sarah Woods", "Adrian Gordon", "Ruth Aylett"], "n_citation": 0, "title": "Fearnot's appearance : Reflecting children's expectations and perspectives", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "83acf327-e3e2-4c93-a21f-01c285d4733f"}
{"abstract": "Classical description logics are limited to dealing with crisp concepts and crisp roles. However, Web applications based on description logics should allow the treatment of the inherent imprecision. Therefore, it is necessary to add fuzzy features to description logics. A family of extended fuzzy description logics, which is a fuzzy extension of description logics by introducing cut set to describe fuzzy feature, is proposed to enable representation and reasoning for complex fuzzy information. This paper discusses the reasoning technique for reasoning tasks of a given extended fuzzy description logic extended fuzzy ALCQ by adopting classical description logic ALCQ to discretely simulate extended fuzzy ALCQ in polynomial time and reusing the existing result to prove the complexity of extended fuzzy ALCQ reasoning tasks.", "authors": ["Yanhui Li", "Baowen Xu", "Jianjiang Lu", "Dazhou Kang"], "n_citation": 0, "title": "Reasoning Technique for Extended Fuzzy ALCQ", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "841795bd-2a70-4010-936d-cafea215c28a"}
{"abstract": "Performance evaluation shows that Hierarchical Mobile IPv6 (HMIPv6) cannot outperform standard Mobile IPv6 (MIPv6) in all scenarios. Thus, adaptive protocol selection under certain circumstances is desired. Moreover, the registration cost of HMIPv6 needs to be reduced further. However, the performance of existing mobility based schemes is limited because they all neglected the traffic parameter of the mobile node (MN). In this paper, we propose an efficient three-level hierarchical architecture for HMIPv6 networks, in which an MN may register with either a higher/lower Mobility Anchor Point (MAP) or its home agent. By analyzing the inter-domain registration cost function, we obtain a novel parameter for MAP selection, which takes both the mobility parameter and the traffic parameter of an MN into account to minimize the overall registration cost. Simulation results show that normally the novel MAP selection algorithm achieves 25% percentage reduction of registration cost over the traditional mobility based algorithm. In addition, this algorithm obtains better performance when the mobility parameter and the traffic parameter of the MN vary.", "authors": ["Zheng Wan", "Zhengyou Wang", "Zhijun Fang", "Weiming Zeng", "Shiqian Wu"], "n_citation": 0, "title": "An Efficient Mobility Management Scheme for Hierarchical Mobile IPv6 Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "84662dcf-0d02-47d1-806f-f6ffa97944e6"}
{"abstract": "Many real-life scheduling, routing and location problems can be formulated as combinatorial optimization problems whose goal is to find a linear layout of an input graph in such a way that the number of edge crossings is minimized. In this paper, we study a restricted version of the linear layout problem where the order of vertices on the line is fixed, the so-called fixed linear crossing number problem (FLCNP). We show that this NP-hard problem can be reduced to the well-known maximum cut problem. The latter problem was intensively studied in the literature; efficient exact algorithms based on the branch-and-cut technique have been developed. By an experimental evaluation on a variety of graphs, we show that using this reduction for solving FLCNP compares favorably to earlier branch-and-bound algorithms.", "authors": ["Christoph Buchheim", "Lanbo Zheng"], "n_citation": 0, "title": "Fixed Linear Crossing Minimization by Reduction to the Maximum Cut Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "852e9189-f301-4a5f-9591-97b693b2fdf6"}
{"abstract": "We propose a formalism for higher-order rewriting in de Bruijn notation. This notation not only is used for terms (as usually done in the literature) but also for metaterms, which are the syntactical objects used to express general higher-order rewrite systems. We give formal translations from higher-order rewriting with names to higher-order rewriting with de Bruijn indices, and vice-versa. These translations can be viewed as an interface in programming languages based on higher-order rewrite systems, and they are also used to show some properties, namely, that both formalisms are operationally equivalent, and that confluence is preserved when translating one formalism into the other.", "authors": ["Eduardo Bonelli", "Delia Kesner", "Alejandro R\u00edos"], "n_citation": 0, "title": "A de Bruijn notation for higher-order rewriting", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "88549bf0-44a5-45fe-aebb-c268cb7b30c0"}
{"abstract": "Nowadays the number of mobile device users is continuously increasing. However the available data services for those users are rare and usually provide an inefficient performance. More particularly, a growing interest is arising around location-based services but the processing of location-dependent queries is still a subject of research in the new mobile computing environment. Special difficulties arise when considering the need of keeping the answer to these queries up-to-date, due to the mobility of involved objects. In this paper we introduce a new approach for processing location-dependent queries that presents the following features: 1) it deals with scenarios where users issuing queries as well as objects involved in such queries can change their location, 2) it deals with continuous queries and so answers are updated with a certain frequency, 3) it provides a completely decentralised solution and 4) it optimises wireless communication costs by using mobile agents. We focus on the way in which data presented to the user must be refreshed in order to show an up-to-date answer but optimising communications effort.", "authors": ["Sergio Ilarri", "Eduardo Mena", "Arantza Illarramendi"], "n_citation": 0, "title": "Monitoring continuous location queries using mobile agents", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8ab60326-c633-4257-8260-7b8877c8fd4b"}
{"abstract": "The result of introducing Fuzzy Logic into Logic Programming has been the development of several Fuzzy Prolog systems. These systems replace the inference mechanism of Prolog with a fuzzy variant which is able to handle partial truth as a real value or as an interval on [0, 1]. Most of these systems consider only one operator to propagate the truth value through the fuzzy rules. We aim at defining a Fuzzy Prolog Language in a general way and to provide an implementation of a Fuzzy Prolog System for our general approach that is extraordinary simple thanks to the use of constraints. Our approach is general in two aspects: (i) Truth value will be a countable union of sub-intervals on [0,1], representation also called Borel Algebra over this interval, B([0, 1]). Former representations of truth value are particular cases of this definition and many real fuzzy problems only can be modeled using this representation. (ii) The concept of aggregation generalizes the computable operators. It subsumes conjunctive operators (triangular norms as min, prod, etc), disjunctive operators (triangular co-norms as max, sum, etc), average operators (arithmetic average, cuasi-linear average, etc) and hybrid operators (combinations of previous operators). We define and use aggregation operator for our language instead of limiting ourselves to a particular one. Therefore, we have implemented several aggregation operators and others can be added to the system with little effort. We have incorporated uncertainty into a Prolog system in a simple way. This extension to Prolog is realized by interpreting fuzzy reasoning (truth values and the result of aggregations) as a set of constraints then translating fuzzy rules into CLP(R) clauses. The implementation is based on a syntactic expansion of the source code at compilation-time. The novelty of the Fuzzy Prolog presented is that it is implemented over Prolog, using its resolution system, instead of implementing a new resolution system such as other approaches. The current implementation is a syntactic extension that uses the CLP(R) system of Ciao Prolog. Lastest distributions includes our Fuzzy Prolog implementation and can be downloaded from http://www.clip.dia.fi.upm.es/Software. Our approach can be easily implemented on other CLP(R) system.", "authors": ["Claudio A. Vaucheret", "Sergio Guadarrama", "Susana Mu\u00f1oz"], "n_citation": 50, "title": "Fuzzy Prolog: A simple general implementation using CLP(R)", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8b082e44-7166-4cc1-aaf0-0c4dd513ae68"}
{"authors": ["Patrick P. Cousot", "Pierre Ganty", "Jean-Fran\u00e7ois Raskin"], "n_citation": 23, "title": "Fixpoint-guided abstraction refinements", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "8d1534ff-4e47-40ae-81ab-79944930c742"}
{"authors": ["Yannis Tzitzikas", "Raimo Launonen", "Mika Hakkarainen", "Pekka Korhonen", "Tero Lepp\u00e4nen", "Esko Simpanen", "Hannu T\u00f6rnroos", "Pekka Uusitalo", "Pentti V\u00e4nsk\u00e4"], "n_citation": 0, "title": "FASTAXON: A system for FAST (and Faceted) TAXOnomy design", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8d1f1f29-2647-4c83-b1a2-60b7b0e34d4c"}
{"abstract": "We present an algorithm to compute a greatest common divisor of two integers in a quadratic number ring that is a unique factorization domain. The algorithm uses O(n log 2  n  log log n + \u0394 1 2+e ) bit operations in a ring of discriminant A. This appears to be the first gcd algorithm of complexity o(n 2 ) for any fixed non-Euclidean number ring. The main idea behind the algorithm is a well known relationship between quadratic forms and ideals in quadratic rings. We also give a simpler version of the algorithm that has complexity O(n 2 ) in a fixed ring. It uses a new binary algorithm for reducing quadratic forms that may be of independent interest.", "authors": ["Saurabh Agarwal", "Gudmund Skovbjerg Frandsen"], "n_citation": 0, "title": "A new GCD algorithm for quadratic number rings with unique factorization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9124b43d-34c7-4aa9-a769-e2a3ef57ffb6"}
{"abstract": "Equality Logic with uninterpreted functions is used for proving the equivalense or refinement between systems (hardware verification, compiler translation, etc). Current approaches for deciding this type of formulas use a transformation of an equality formula to the propositional one of larger size, and then any standard SAT checker can be applied. We give an approach for deciding satisfiability of equality logic formulas (E-SAT) in conjunctive normal form. Central in our approach is a single proof rule called ER. For this single rule we prove soundness and completeness. Based on this rule we propose a complete procedure for E-SAT and prove its correctness. Applying our procedure on a variation of the pigeon hole formula yields a polynomial complexity contrary to earlier approaches to E-SAT.", "authors": ["Olga Tveretina", "Hans Zantema"], "n_citation": 0, "title": "A proof system and a decision procedure for equality Logic", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "913ab143-65dd-4274-a3cc-b275b270c410"}
{"abstract": "We consider the problem of defining an approximation measure for functional dependencies (FDs). An approximation measure for X \u2192 Y is a function mapping relation instances, r, to non-negative real numbers. The number to which r is mapped, intuitively, describes the degree to which the dependency X \u2192 Y holds in r. We develop a set of axioms for measures based on the following intuition. The degree to which X \u2192 Y is approximate in r is the degree to which r determines a function from \u03a0 X (r) to \u03a0 Y (r). The axioms apply to measures that depend only on frequencies (i.e. the frequency of x E \u03a0 X (r) is the number of tuples containing x divided by the total number of tuples). We prove that a unique measure satisfies these axioms (up to a constant multiple), namely, the information dependency measure of [5]. We do not argue that this result implies that the only reasonable, frequency-based, measure is the information dependency measure. However, if an application designer decides to use another measure, then the designer must accept that the measure used violates one of the axioms.", "authors": ["Chris Giannella"], "n_citation": 4, "title": "An axiomatic approach to defining approximation measures for functional dependencies", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9173ad8b-6620-4f8e-8c4d-0e5c5cae0170"}
{"abstract": "Reinforcement learning agents explore their environment in order to collect reward that allows them to learn what actions are good or bad in what situations. The exploration is performed using a policy that has to keep a balance between getting more information about the environment and exploiting what is already known about it. This paper presents a method for guiding exploration by pre-existing knowledge expressed as heuristic rules. A dual memory model is used where the value function is stored in long-term memory while the heuristic rules for guiding exploration act on the weights in a short-term memory. Experimental results from a grid task illustrate that exploration is significantly improved when appropriate heuristic rules are available.", "authors": ["Kary Fr\u00e4mling"], "n_citation": 0, "title": "Dual memory model for using pre-existing knowledge in reinforcement learning tasks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "91a24f7f-543a-4b48-a977-d86787b5f7a2"}
{"abstract": "Self-modifying codes (SMC) refer to programs that intentionally modify themselves at runtime, causing the runtime code to differ from the static binary representation of the code before execution. Hence SMC is an effective method to obstruct software disassembling. This paper presents a method which circumvents the SMC protection, thus improving the performance of disassembling. By disabling the write privilege to the code section, an access violation exception occurs when an SMC attempts to execute. Intercepting this exception allows the attacker to determine and thus compromise the SMC and generate equivalent static code. Our experiments demonstrate that it is viable and efficient.", "authors": ["Yongdong Wu", "Zhigang Zhao", "Tian Wei Chui"], "n_citation": 50, "title": "An attack on SMC-based software protection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9358f449-dae8-47ca-a78f-7c15a790dab5"}
{"abstract": "We show exact values for the price of anarchy of weighted and unweighted congestion games with polynomial latency functions. The given values also hold for weighted and unweighted network congestion games.", "authors": ["Sebastian Aland", "Dominic Dumrauf", "Martin Gairing", "Burkhard Monien", "Florian Schoppmann"], "n_citation": 0, "title": "Exact price of anarchy for polynomial congestion games", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "962c94a4-14d5-4974-8469-8ca64ea12e4a"}
{"abstract": "This paper introduces a new social metaheuristic for the Max-Cut problem applied to a weighted undirected graph. This problem consists in finding a partition of the nodes into two subsets, such that the sum of the weights of the edges having endpoints in different subsets is maximized. This NP-hard problem for non planar graphs has several application areas such as VLSI and ASICs CAD. The hierarchical social (HS) metaheuristic here proposed for solving the referred problem is tested and compared with other two metaheuristics: a greedy randomized adaptive search procedure (GRASP) and an hybrid memetic heuristic that combines a genetic algorithm (GA) and a local search. The computational results on a set of standard test problems show the suitability of the approach.", "authors": ["Abraham Duarte", "Felipe Fern\u00e1ndez", "\u00c1ngel S\u00e1nchez", "Antonio P\u00e9rez Sanz"], "n_citation": 0, "title": "A hierarchical social metaheuristic for the Max-Cut problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "97f7dceb-348e-4607-a1dd-186189718ac9"}
{"abstract": "We provide sharp estimates for the probabilistic behaviour of the main parameters of the Euclid algorithm, and we study in particular the distribution of the bit-complexity which involves two main parameters: digit-costs and length of continuants. We perform a dynamical analysis which heavily uses the dynamical system underlying the Euclidean algorithm. Baladi and Vallee [2] have recently designed a general framework for distributional dynamical analysis, where they have exhibited asymptotic gaussian laws for a large class of digit-costs. However, this family contains neither the bit-complexity cost nor the length of continuants. We first show here that an asymptotic gaussian law also holds for the length of continuants at a fraction of the execution. There exist two gcd algorithms, the standard one which only computes the gcd, and the extended one which also computes the Bezout pair, and is widely used for computing modular inverses. The extended algorithm is more regular than the standard one, and this explains that our results are more precise for the extended algorithm. We prove that the bit-complexity of the extended Euclid algorithm asymptotically follows a gaussian law, and we exhibit the speed of convergence towards the normal law. We describe also conjectures [quite plausible], under which we can obtain an asymptotic gaussian law for the plain bit-complexity, or a sharper estimate of the speed of convergence towards the gaussian law.", "authors": ["Lo\u00efck Lhote", "Brigitte Vall\u00e9e"], "n_citation": 0, "title": "Sharp estimates for the main parameters of the euclid algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "997aa584-15ce-4a99-b4c2-5b8274f12793"}
{"abstract": ", We provide a general method to generate randomized roundings that satisfy cardinality constraints. Our approach is different from the one taken by Srinivasan (FOCS 2001) and Gandhi et al. (FOCS 2002) for one global constraint and the bipartite edge weight rounding problem. Also for these special cases, our approach is the first that can be derandomized. For the bipartite edge weight rounding problem, in addition, we gain an O(|V|) factor run-time improvement for generating the randomized solution. We also improve the current best result on the general problem of derandomizing randomized roundings. Here we obtain a simple O(mn log n) time algorithm that works in the RAM model for arbitrary matrices with entries in Q \u22650 . This improves over the O(m n  2  log(mn)) time solution of Srivastav and Stangier.", "authors": ["Benjamin Doerr"], "n_citation": 0, "title": "Generating randomized roundings with cardinality constraints and derandomizations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "99a675db-4929-4693-a68c-0d31b975e627"}
{"abstract": "This work obtains truthful mechanisms that aim at maximizing both the revenue and the economic efficiency (social welfare) of unit-demand auctions. In a unit-demand auction a set of k items is auctioned to a set of n consumers, and although each consumer bids on all items, no consumer can purchase more than one item. We present a framework for devising polynomial-time randomized truthful mechanisms that are based on a new variant of the Vickrey-Clarke-Groves (VCG) mechanism. Instead of using reserve prices, this variant of VCG uses the number of objects that we wish to sell as a parameter. Our mechanisms differ in their selection of the number of items to be sold, and allow an interesting trade-off between revenue and economic efficiency, while improving upon the state-of-the-art results for the Unit-Demand Auctions problem (Guruswami et. al.[SODA 2005]). Our probabilistic results depend on what we call the competitiveness of the auction, i.e., the minimum number of items that need to be sold in order to obtain a certain fraction of the maximum efficiency. We denote by T the optimal efficiency achieved by the VCG mechanism. Our efficiency-oriented mechanism achieves \u03a9(T) efficiency and i?(T/ ln(min{k, n}) revenue with probability that grows with the competitiveness of the auction. We also show that no truthful mechanism can obtain an \u03c9(T/ln(min{k, n}) expected revenue on every set of bids. In fact, the revenue-oriented mechanism we present achieves \u03a9(T/ ln(min{k, n}) efficiency and \u03a9(T/ln(min{k,n}) revenue, but the revenue can actually be much higher, even as large as \u03a9(T) for some bid distributions.", "authors": ["Claudson F. Bornstein", "Eduardo Sany Laber", "Marcelo Mas"], "n_citation": 0, "title": "On behalf of the seller and society : Bicriteria mechanisms for unit-demand auctions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "99c48a05-a313-47a8-9908-dd96cbf347d5"}
{"abstract": "We describe a Prolog implementation of the sequent calculus for the type theory Nabla that can make syntactical and semantical analyses of a fragment of natural language using combinators.", "authors": ["Juan Fern\u00e1ndez Ortiz", "J\u00f8rgen Villadsen"], "n_citation": 0, "title": "Natural language processing using lexical and logical combinators", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9b3a80f2-bcc1-410d-8726-2766202d0f8e"}
{"abstract": "Typically, the design of an object-oriented database schema starts with an analysis of the application and ends with the implementation of the application. We advocate a design process that employs an intermediate phase where the designer can choose between different abstract object-oriented database schemas. This choice influences the space and time costs that arise when the schema is implemented. We present a cost model for abstract object-oriented database schemas that allows the designer to estimate these costs. At the core of the cost model is an object-oriented database machine. Access structures that are used by this abstract database machine are given by an internal schema. With this we can estimate the space costs. Queries and updates are expressed as programs of the abstract database machine. By providing cost functions that characterise cost relevant aspects of the operations of the abstract database machine we can estimate the time costs of the machine programs. Our cost model is parameterised. So, for example, it can be adopted to reflect different implementation database systems. We provide an example to show how the preferred choice of an abstract database schema changes when the parameters of the cost model vary.", "authors": ["Joachim Biskup", "Ralf Menzel"], "n_citation": 0, "title": "A flexible cost model for abstract object-oriented database schemas", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9b5ce67e-6551-4eab-8319-cd919c1f512f"}
{"abstract": "Static camera planning is a fundamental problem for both interactive and non-interactive computer graphics. We examine the nature or the problem, contrasting procedural and declarative specifications of camera position. In establishing the desirability of the declarative approach, we identify the a number of components: an image description language; an object ontology; and a framework for performing constrained optimization.", "authors": ["Jonathan Pickering", "Patrick Olivier"], "n_citation": 0, "title": "Declarative camera planning: Roles and requirements", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9ecd29cc-e090-443a-bde1-16b396e89fee"}
{"abstract": "How to filter emails is a problem for Internet users. Support vector machine (SVM) is a valid filtering emails method. As it is well known, there exists uncertainty in deciding the legitimate email by Internet users. To formalize the uncertainty, the legitimate email is understood as fuzzy concept on a set of email samples in this paper, its membership function is obtained by aggregating opinions of Internet users, and aggregation operator is ordered weighted averaging (OWA) operator. Due to email training samples with membership degrees of the legitimate email, fuzzy support vector machine (FSVM) is adopted to classify emails, and penalty factor of FSVM is decided by content-specific misclassification costs. The advantages of our method are: 1) uncertainty of the legitimate email, i.e., membership degree, is considered in classifying emails, and a method to obtain membership degree is given; 2) content-specific misclassification costs is used to decide penalty factor of FSVM. Simulative experiments are shown to the effectiveness and human consistent of our method.", "authors": ["Jilin Yang", "Hong Peng", "Zheng Pei"], "n_citation": 1, "title": "Filtering E-Mail Based on Fuzzy Support Vector Machines and Aggregation Operator", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a16e1ce0-ccf2-4180-b879-fde93cfb03b1"}
{"abstract": "Collections of electronic music are mostly organized according to playlists based on artist names and song titles. Music genres are inherently ambiguous and, to make matters worse, assigned manually by a diverse user community. People tend to organize music based on similarity to other music and based on the music's emotional qualities. Taking this into account, we have designed a music player which derives a set of criteria from the actual music data and then provides a coherent visual metaphor for a similarity-based navigation of the music collection.", "authors": ["Otmar Hilliges", "Phillipp Holzer", "Rene Kl\u00fcber", "Andreas Butz"], "n_citation": 0, "title": "AudioRadar : A metaphorical visualization for the navigation of large music collections", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a1b5af65-6205-4c26-a575-e08f5409a209"}
{"abstract": "The goal of this paper is to present concepts and techniques to build views and organize the information search space of heterogeneous semistructured data sources with respect to expected queries. We formalize the notion of object pattern and of semantic correspondence between object patterns. Object pattern analysis is ontology-driven and leads to the construction of reconciled views of the sources, called global object patterns. Global object patterns mediate between heterogeneous terminology and structure of data in different sources and are used for query formulation and data extraction.", "authors": ["Silvana Castano", "V. De Antonellis"], "n_citation": 50, "title": "Building views over semistructured data sources", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "a218bda4-171c-42c7-a59b-81d4c2305674"}
{"abstract": "We consider maintaining information about the rank of a matrix under changes of the entries. For n x n matrices, we show an upper bound of 0(n 1.575 ) arithmetic operations and a lower bound of \u03a9(n) arithmetic operations per change. The upper bound is valid when changing up to O(n 0.575 ) entries in a single column of the matrix. Both bounds appear to be the first non-trivial bounds for the problem. The upper bound is valid for arbitrary fields, whereas the lower bound is valid for algebraically closed fields. The upper bound uses fast rectangular matrix multiplication, and the lower bound involves further development of an earlier technique for proving lower bounds for dynamic computation of rational functions.", "authors": ["Gudmund Skovbjerg Frandsen", "Peter Frandsen"], "n_citation": 0, "title": "Dynamic Matrix Rank", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a37e510e-3acb-4135-82a1-0b6474eda7a1"}
{"abstract": "In this paper, we study the solvability of fair exchange in the context of Byzantine failures. In doing so, we first present a generic model with trusted and untrusted processes, and propose a specification of the fair exchange problem that clearly separates safety and liveness, via fine-grained properties. We then show that the solvability of fair exchange depends on a necessary and sufficient topological condition, which we name the reachable majority condition. The first part of this result, i.e., the condition is necessary, was shown in a companion paper and is briefly recalled here. The second part, i.e., the condition is sufficient, is the focal point of this paper. The correctness proof of this second part consists in proposing a solution to fair exchange in the aforementioned model.", "authors": ["Beno\u00eet Garbinato", "Ian Rickebusch"], "n_citation": 0, "title": "A topological condition for solving fair exchange in byzantine environments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a41e44b1-d58d-4d3a-84fe-c8618cef87c4"}
{"abstract": "We propose an approach to combining logic programming and knowledge representation paradigms. This approach is based on the conception of description terms. LP and KR are integrated in such a way that their underlying logics are carefully separated. A core idea here is to push the KR techniques on the functional level. On the LP level the knowledge base is considered as a constraint store, in which special propagation methods are ruling. A constraint logic programming language based on this idea is outlined.", "authors": ["Andrei Mantsivoda", "Vladimir Lipovchenko", "A. A. Malykh"], "n_citation": 0, "title": "Logic programming in knowledge domains", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a62e401b-69bc-4e89-b137-561e2798492c"}
{"abstract": "We define a language whose type system, incorporating session types, allows complex protocols to be specified by types and verified by static typechecking. A session type, associated with a communication channel, specifies the state transitions of a protocol and also the data types of messages associated with transitions; thus typechecking can verify both correctness of individual messages and correctness of sequences of transitions. Previously session types have mainly been studied in the context of the \u03c0-calculus; instead, our formulation is based on a multi-threaded functional language with side-effecting input/output operations. Our typing judgements statically describe dynamic changes in the types of channels, our channel types statically track aliasing, and our function types not only specify argument and result types but also describe changes in channels. We formalize the syntax, semantics and typing rules of our language, and prove subject reduction and runtime type safety theorems.", "authors": ["Vasco Thudichum Vasconcelos", "Ant\u00f3nio Ravara", "Simon J. Gay"], "n_citation": 0, "title": "Session types for functional multithreading", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a8481be0-85db-48b5-a416-4fd8de14cd98"}
{"abstract": "We outline a procedure called communicative and material functions analysis that can be used to derive business modeling concepts. It is rooted in the language-action perspective on organizations and has its point of departure in Business Action Theory, an empirically grounded framework for modeling business processes from an action perspective. We apply this procedure to enhance an existing method, the Situation-adaptable work and Information systems Modeling Method. This extended method is then used to analyze a business situation in order to follow up the commitments that are made in the course of a business process with the ultimate aim of detecting flaws in that process.", "authors": ["Peter Rittgen"], "n_citation": 50, "title": "Deriving Concepts for Modeling Business Actions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aafc44a2-deaa-4f34-8c0b-5cc62b0505d6"}
{"abstract": "In this study, the microarray data under diauxic shift condition of Saccharomyces Cerevisiae was considered. The objective of this study is to propose another strategy of cluster analysis for gene expression levels under time-series conditions. The continuous hidden markov model was newly proposed to select genes which significantly expressed. Then, new approach of hidden markov model clustering was proposed to include Bayesian information criterion technique which helped to determine the size of model. The result of this technique provided a good quality of clustering from gene expression patterns.", "authors": ["Phasit Charoenkwan", "Aompilai Manorat", "Jeerayut Chaijaruwanich", "Sukon Prasitwattanaseree", "Sakarindr Bhumiratana"], "n_citation": 0, "title": "DNA Microarray Data Clustering by Hidden Markov Models and Bayesian Information Criterion", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "adb2f82a-eb88-494d-9b15-c9b6c16a0abe"}
{"authors": ["Bert den Boer", "Bart Van Rompay", "Bart Preneel", "Joos Vandewalle"], "n_citation": 50, "title": "New (Two-Track-)MAC Based on the Two Trails of RIPEMD", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "b05fe701-1636-47f1-89b5-b9461649a677"}
{"abstract": "Let G = (V,A) be an Eulerian directed graph with an arc-labeling. In this work we study the problem of finding an Eulerian circuit of lexicographically minimal label among all Eulerian circuits of the graph. We prove that this problem is NP-hard by showing a reduction from the DIRECTED-HAMILTONIAN-CIRCUIT problem. If the labeling of the arcs is such that arcs going out from the same vertex have different labels, the problem can be solved in polynomial time. We present an algorithm to construct the unique Eulerian circuit of lexicographically minimal label starting at a fixed vertex. Our algorithm is a recursive greedy algorithm which runs in O(|A|) steps. We also show an application of this algorithm to construct the minimal De Bruijn sequence of a language.", "authors": ["Eduardo Moreno", "Mart\u00edn Matamala"], "n_citation": 0, "title": "Minimal eulerian circuit in a labeled digraph", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b0be6ecc-70e4-4450-abde-8d089fb571cd"}
{"abstract": "We consider the polynomial time learnability of ordered tree patterns with internal structured variables, in the query learning model of Angluin (1988). An ordered tree pattern with internal structured variables, called a term tree, is a representation of a tree structured pattern in semistructured or tree structured data such as HTML/XML files. Standard variables in term trees can be substituted by an arbitrary tree of arbitrary height. In this paper, we introduce a new type of variables, which are called height-bounded variables. An i-height-bounded variable can be replaced with any tree of height at most i. By this type of variables, we can define tree structured patterns with rich structural features. We assume that there are at least two edge labels. We give a polynomial time algorithm for term trees with height-bounded variables using membership queries and one positive example. We also give hardness results which indicate that one positive example is necessary to learn term trees with height-bounded variables.", "authors": ["Satoshi Matsumoto", "Takayoshi Shoudai"], "n_citation": 0, "title": "Learning of ordered tree languages with height-bounded variables using queries", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b3c5788f-1a57-4bcf-923c-52fe4a9dc8cc"}
{"abstract": "We show that membership is decidable for languages defined by iterated template-guided recombination systems when the set of templates is regular and the initial language is context-free. Using this result we show that when the set of templates is regular and the initial language is context-free (respectively, regular) we can effectively construct a push-down automaton (respectively, finite automaton) for the corresponding iterated template-guided recombination language.", "authors": ["Ian McQuillan", "Kai Salomaa", "Mark Daley"], "n_citation": 0, "title": "Iterated TGR Languages : Membership Problem and Effective Closure Properties", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b404c059-1fcc-4fee-8bf9-d20be753c335"}
{"abstract": "We present an architecture for multi-agent negotiation for implementing a distributed meeting scheduler. In the scheduling system, an agent is assigned to an user who plans private schedules and events. An agent negotiates with other agents about making an public schedule by referring user's private schedules and preferences. The multi-agent negotiation we proposed here facilitates reaching an agreement among agents effectively. A characteristic function based on a game theory is used for reflecting users' preferences in the negotiation process. We have implemented a distributed meeting scheduler to see how effectively the multi-agent negotiation can be used. The result shows that the multi-agent negotiation based on private preferences is an effective method for a distributed meeting scheduler.", "authors": ["Takahiro Shintani", "T. Ito"], "n_citation": 0, "title": "An architecture for multi-agent negotiation using private preferences in a meeting scheduler", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "b53b2c36-ec5c-477c-91f7-baaf2388f4f1"}
{"abstract": "Multivalued dependencies are considered to be difficult to teach, to handle and to model. This observation is true if multivalued dependencies are treated in the classical approach. We introduce another treatment of multivalued dependencies based on ER modeling techniques and show that multivalued dependenices can be handled in a more natural and intuitive way within our framework. Based on the concept of competing multivalued dependencies we can prove in which case a unique ER schema representation exists. If multivalued dependencies are competing then either one of the competing schemata is chosen or an approximation which combines the competing schemata can be used.", "authors": ["Bernhard Thalheim"], "n_citation": 0, "title": "Conceptual treatment of multivalued dependencies", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b6504e4e-d331-4413-b601-b3045ab53257"}
{"abstract": "In this work we experiment with the application of island models to Estimation of Distribution Algorithms (EDAs) in the field of combinatorial optimization. This study is motivated by the success obtained by these models when applied to other meta-heuristics (such as genetic algorithms, simulated annealing or VNS) and by the use of a compact representation of the population that make EDAs through probability distributions. This fact can be exploited during information interchange among islands. In this work we experiment with two types of island-based EDAs: (1) migration of individuals, and (2) migration of probability models. Also, two alternatives are studied for the phase of model combinations: assigning constant weights to inner and incoming models or assigning adaptive weights based-on their fitness. The proposed algorithms are tested over a suite of four combinatorial optimization problems.", "authors": ["Luis delaOssa", "Jos\u00e9 A. G\u00e1mez", "Jos\u00e9 Miguel Puerta"], "n_citation": 0, "title": "Migration of probability models instead of individuals: An alternative when applying the island model to EDAs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b8170dcf-2e46-457e-aa2a-e84d02eed456"}
{"abstract": "With the increasing importance of the World Wide Web as an information repository, how to locate documents of interest becomes more and more significant. The current practice is to send keywords to search engines. However, these search engines lack the capability to take the structure of the Web into consideration. We thus present a novel query language, NetQL and its implementation, for accessing the World Wide Web. Rather than working on global text-full search, NetQL is designed for local structure-based queries. It not only exploits the topology of web pages given by hyperlinks, but also supports queries involving information inside pages. A novel approach to extract information from web pages is presented. In addition, the methods to control the complexity of query processing are also addressed in this paper.", "authors": ["Tao Guan", "Miao Liu", "Lawrence V. Saxton"], "n_citation": 14, "title": "Structure-based queries over the World Wide Web", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "b849561b-83dc-49a1-b7b3-998c44fc2275"}
{"authors": ["Jaldert O. Rombouts", "A. van Ooyen", "P.R. Roelfsema", "Sander M. Bohte"], "n_citation": 4, "title": "Biologically plausible multi-dimensional reinforcement learning in neural networks.", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "ba128a5f-3899-41ae-a3a6-f1766661419f"}
{"abstract": "This paper proposes a prototype of an optical tracking system based on a single camera, and presents a design of the active infrared optical tracker to improve the stability of the tracking. Moreover, a method based on Virtual Visual Servoing (VVS) is utilized to compute the pose of the device to be tracked. With the proposed method, the pose computation can be considered as the dual problem of visual servoing, thus an accurate and robust result can be obtained. The proposed system can be regarded as a 6-DOF controller and can be widely applied in the fields of entertainment, education and industry, etc.", "authors": ["Dongdong Weng", "Yue Liu", "Yongtian Wang"], "n_citation": 0, "title": "Single camera based optical tracking system for personal entertainment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ba1c9bba-4ce3-4f18-a4ec-d3506bf6f0dd"}
{"abstract": "Multiple classifier systems (MCS) have become popular during the last decade. Self-generating neural tree (SGNT) is one of the suitable base-classifiers for MCS because of the simple setting and fast learning. However, the computation cost of the MCS increases in proportion to the number of SGNT. In an earlier paper, we proposed a pruning method for the structure of the SGNT in the MCS to reduce the computation cost. In this paper, we propose a novel pruning method for more effective processing and we call this model as self-organizing neural grove (SONG). The pruning method is constructed from an online pruning method and an off-line pruning method. Experiments have been conducted to compare the SONG with an unpruned MCS based on SGNT, an MCS based on C4.5, and k-nearest neighbor method. The results show that the SONG can improve its classification accuracy as well as reducing the computation cost.", "authors": ["Hirotaka Inoue", "Hiroyuki Narihisa"], "n_citation": 0, "title": "Self-organizing neural grove: Efficient multiple classifier system using pruned self-generating neural trees", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "bdae484b-352f-450e-b9a0-b0e6fbe039f0"}
{"abstract": "We present an optimal strategy for searching for a goal in a street which achieves the competitive factor of \u221a2, thus matching the best lower bound known before. This finally settles an interesting open problem in the area of competitive path planning many authors have been working on.", "authors": ["Christian Icking", "Rolf Klein", "Elmar Langetepe"], "n_citation": 50, "title": "An optimal competitive strategy for walking in streets", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "be227f06-c36a-43bd-a31b-a9c75c1f3d16"}
{"abstract": "We report on how implementing a Model Based Automotive SW Engineering Process in an industrial setting can ensure the correctness of automotive applications when a process based on formal models is used. We show how formal methods, in particular model checking, can be used to ensure consistency of the models and can prove that the models satisfy selected functional and safety requirements. The technique can also be used to automatically generate test vectors from the model. Hence we show how in many ways formal verification techniques can add value to the models used for different purposes in developing automotive applications.", "authors": ["Eckard B\u00f6de", "Werner Damm", "Jarl H\u00f8yem", "Bernhard Josko", "J\u00fcrgen Niehaus", "Marc Segelken"], "n_citation": 0, "title": "Adding Value to Automotive Models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bede6c41-f9f8-417d-8248-774847a0474f"}
{"abstract": "Several studies in literature have investigated the performance of the proposed IEEE 802.11E standard for Qos differentiation in WLAN, but most of them are limited both with respect to the range of the parameter settings and the considered traffic scenarios. The aim of the present study is to systematically investigate (by simulations) the impact of each of the QoS differentiation parameters, under more realistic traffic conditions. In particular, we investigate flow-level performance characteristics (e.g., file transfer times) in the situation that the number of active stations varies dynamically in time.", "authors": ["Frank Roijers", "Hans van den Berg", "Xiang Fan", "Maria Fleuren"], "n_citation": 0, "title": "Evaluation of QoS provisioning capabilities of IEEE 802.11E wireless LANs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bf7456fa-2cc6-4af8-8e32-061fc54a1531"}
{"abstract": "Existing Separation Logic (a.ka Difference Logic, DL) solvers can be broadly classified as eager or lazy, each with its own merits and de-merits. We propose a novel Separation Logic Solver SDSAT that combines the strengths of both these approaches and provides a robust performance over a wide set of benchmarks. The solver SDSAT works in two phases: allocation and solve. In the allocation phase, it allocates non-uniform adequate ranges for variables appearing in separation predicates. This phase is similar to previous small domain encoding approaches, but uses a novel algorithm Nu-SMOD with 1-2 orders of magnitude improvement in performance and smaller ranges for variables. Furthermore, the Separation Logic formula is not transformed into an equi-satisfiable Boolean formula in one step, but rather done lazily in the following phase. In the solve phase, SDSAT uses a lazy refinement approach to search for a satisfying model within the allocated ranges. Thus, any partially DL-theory consistent model can be discarded if it can not be satisfied within the allocated ranges. Note the crucial difference: in eager approaches, such a partially consistent model is not allowed in the first place, while in lazy approaches such a model is never discarded. Moreover, we dynamically refine the allocated ranges and search for a feasible solution within the updated ranges. This combined approach benefits from both the smaller search space (as in eager approaches) and also from the theory-specific graph-based algorithms (characteristic of lazy approaches). Experimental results show that our method is robust and always better than or comparable to state-of-the art solvers.", "authors": ["Malay K. Ganai", "Muralidhar Talupur", "Aarti Gupta"], "n_citation": 0, "title": "SDSAT : Tight integration of Small Domain Encoding and Lazy approaches in a separation logic solver", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c22c2771-b282-4a41-907f-72760200317e"}
{"abstract": "Predicate detection is an important problem in testing and debugging distributed programs. Cooper and Marzullo introduced two modalities possibly and definitely as a solution to this problem. Given a predicate p, a computation satisfies possibly: p if p is true for some global state in the computation. A computation satisfies definitely: p if all paths from the initial to the final global state go through some global state that satisfies p. In general, definitely modality is used to detect good conditions such as a leader is eventually chosen by all processes, or a commit point is reached by every process, whereas possibly modality is used to detect bad conditions such as violation of mutual exclusion. There are several efficient algorithms for possibly modality in the literature [10, 14, 1, 2, 30]. However, this is not the case for definitely modality. Cooper and Marzullo's definitely: p algorithm for arbitrary p has a worst-case space and time complexity exponential in the number of processes. This is due to the state explosion problem. In this paper we present efficient algorithms for detecting definitely: p. In particular, we give a simple algorithm that uses polynomial space. Then, we present an algorithm that can significantly reduce the global state-space. We determine necessary conditions and sufficient conditions under which detecting definitely: p may be efficiently solved. We apply our algorithms to example protocols, achieving a speedup of over 100, compared to partial order reduction based technique of SPIN [13].", "authors": ["Alper Sen", "Vijay K. Garg"], "n_citation": 0, "title": "On checking whether a predicate definitely holds", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c2e2358f-dec8-41be-86e3-c459f465b616"}
{"abstract": "This paper presents a variation of OTIS-k-ary n-cube networks (OTIS-Q k  n ) which is called enhanced OTIS-Q k  n  or E-OTIS-Q k  n . E-OTIS-Q k  n  is defined only for even values of k and is obtained from the normal OTIS-k-ary n cube by adding some extra links without increasing the maximum degree of 2n + 1. We have established an upper bound of [2nk+5/3] on the diameter of E-OTIS-Q k  n . We have also found the actual diameter using breadth first search for specific values of k and n. It was observed that this upper bound is quite tight, in the sense that it is either equal to the actual diameter or exceeds the diameter by one. We have also defined a classification of the nodes in E-OTIS-Q k  n  based on some properties and shown that the nodes in the same class have the same eccentricity. Finally, we have developed an algorithm for point-to-point routing in E-OTIS-Q k  n . It is proved that the algorithm always routes by the shortest path.", "authors": ["Rajib K. Das"], "n_citation": 0, "title": "Enhanced OTIS k-ary n-cube networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c853766b-f9dd-492b-8c0d-207d246451f5"}
{"abstract": "We present the main algorithmic challenges that large Web search engines face today. These challenges are present in all the modules of a Web retrieval system, ranging from the gathering of the data to be indexed (crawling) to the selection and ordering of the answers to a query (searching and ranking). Most of the challenges are ultimately related to the quality of the answer or the efficiency in obtaining it, although some are relevant even to the existence of current search engines: context based advertising.#R##N##R##N#As the Web grows and changes at a fast pace, the algorithms behind these challenges must rely in large scale experimentation, both in data volume and computation time, to understand the main issues that affect them. We show examples of our own research and of the state of the art. The full version of this paper appears in [1] .", "authors": ["Ricardo A. Baeza-Yates"], "n_citation": 50, "references": ["084f441a-0b31-4275-80f5-ecdabca3486e", "253b35dd-73be-4cd8-8e81-eda98564603f", "3ab7b0db-bc4b-46ca-b200-5e7c5ce64918", "4aad3fe3-6ce2-4fa7-9feb-3241006df534", "77414b09-1aa5-401b-a549-a1d0b55a6348", "7b139a2f-d0e7-4631-8d9b-72a29b25f24a", "8f9e92cf-f266-4e51-807f-c098a260a0dc", "98a20932-00a7-4e29-9b7f-de12632c55aa", "a730e30d-caf2-4f90-9719-7a72e65b49b9", "a863ec87-1ff4-46bc-81f6-46d6aa255917", "b4b250c0-83ec-4eb8-89a7-135194c45397", "e8de75ed-778a-4086-9cd7-3f7aa306114b", "f20ab06c-82e9-479c-8fae-8de295e016df"], "title": "Algorithmic challenges in web search engines", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c90297ec-f201-4f6d-918e-d0ef92cf10a6"}
{"abstract": "As pointed out in the pioneering work of [WSW99,EW01], an upper level ontology allows to evaluate the ontological correctness of a conceptual model and to develop guidelines how the constructs of a conceptual modeling language should be used. In this paper we adopt the General Ontological Language (GOL), proposed in [DHHS01], for this purpose. We discuss a number of issues that arise when applying the concepts of GOL to UML class diagrams as a conceptual modeling language. We also compare our ontological analysis of some parts of the UML with the one proposed in [EW01].", "authors": ["Giancarlo Guizzardi", "Heinrich Herre", "Gerd Wagner"], "n_citation": 0, "title": "On the General Ontological foundations of conceptual modeling", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ca345b15-8b16-4571-8a24-a6d92c20125d"}
{"abstract": "We present a solution to the problem of regular expression searching on compressed text. The format we choose is the Ziv-Lempel family, specifically the LZ78 and LZW variants. Given a text of length u compressed into length n, and a pattern of length m, we report all the R occurrences of the pattern in the text in O(2 m  + mn + Rm log m) worst case time. On average this drops to O(m 2  + (n + R) log m) or O(m 2  + n + Ru/n) for most regular expressions. This is the first nontrivial result for this problem. The experimental results show that our compressed search algorithm needs half the time necessary for decompression plus searching, which is currently the only alternative.", "authors": ["Gonzalo Navarro"], "n_citation": 50, "title": "Regular expression searching over Ziv-Lempel compressed text", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "cc0a5696-312b-4723-a00d-54357a3eaf96"}
{"abstract": "Scenario-based specifications such as message sequence charts (MSC) offer an intuitive and visual way of describing design requirements. Such specifications focus on message exchanges among communicating entities in distributed software systems. Structured specifications such as MSC-graphs and Hierarchical MSC-graphs (HMSC) allow convenient expression of multiple scenarios, and can be viewed as an early model of the system. In this paper, we present a comprehensive study of the problem of verifying whether this model satisfies a temporal requirement given by an automaton, by developing algorithms for the different cases along with matching lower bounds. When the model is given as an MSC, model checking can be done by constructing a suitable automaton for the linearizations of the partial order specified by the MSC, and the problem is coNP-complete. When the model is given by an MSC-graph, we consider two possible semantics depending on the synchronous or asynchronous interpretation of concatenating two MSCs. For synchronous model checking of MSC-graphs and HMSCs we present algorithms whose time complexity is proportional to the product of the size of the description and the cost of processing MSCs at individual vertices. Under the asynchronous interpretation, we prove undecidability of the model checking problem. We, then, identify a natural requirement of boundedness, give algorithms to check boundedness, and establish asynchronous model checking to be PSPACE-complete for bounded MSC-graphs and EXPSPACE-complete for bounded HMSCs.", "authors": ["Rajeev Alur", "Mihalis Yannakakis"], "n_citation": 370, "title": "Model checking of message sequence charts", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "cf8c928e-6fef-4f06-bee7-5554704e6df7"}
{"abstract": "This paper proposes a mobile conferencing system which allows the mixed usage of Wi-Fi and Bluetooth and the performance analysis of it. The performance analysis includes the analysis of interference due to the mixed usage of Wi-Fi and Bluetooth. The objective of this study is to propose a method for formulating the best networking structure of a mobile conferencing system in a given situation. We developed a mobile conferencing system which works for both types of mobile networking methods; they are Wi-Fi for configuring infrastructure networking and Bluetooth for configuring ad hoc networking. We performed some experiments for comparing three mobile networking methods; Wi-Fi only, Bluetooth only, and the mixed usage of Wi-Fi and Bluetooth. These experiments lead us to learn that the mixed usage of Wi-Fi and Bluetooth for a mobile conferencing system can be a good solution for formulating an efficient mobile conferencing application. Our mobile conferencing system can benefit greatly the advantages of both types of networking methods (Wi-Fi for infrastructure networking and Bluetooth for ad hoc networking). Through our experiments, we experienced that the interference of Wi-Fi and Bluetooth degrades the system performance to some degree, however, it occurs randomly and it is tolerable if the application is carefully implemented to avoid the effect of interference.", "authors": ["Mee Young Sung", "Jong Hyuk Lee", "Yong Il Lee", "Yoon Sik Hong"], "n_citation": 0, "title": "Mixed usage of Wi-Fi and Bluetooth for formulating an efficient mobile conferencing application", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cfa388f9-4502-47ba-8b27-70adc4614ce5"}
{"authors": ["Bing Sun", "Meicheng Liu", "Jian Guo", "Ruilin Li", "Vincent Rijmen"], "n_citation": 50, "title": "Provable Security Evaluation of Structures against Impossible Differential and Zero Correlation Linear Cryptanalysis", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "cfbcae18-4507-409b-bd9a-3117598f59fe"}
{"abstract": "The PARTIAL DIGEST problem asks for the coordinates of m points on a line such that the pairwise distances of the points form a given multiset of ( m 2) distances. PARTIAL DIGEST is a well-studied problem with important applications in physical mapping of DNA molecules. Its computational complexity status is open. Input data for PARTIAL DIGEST from real-life experiments are always prone to error, which suggests to study variations of PARTIAL DIGEST that take this fact into account. In this paper, we study the computational complexity of the variation of PARTIAL DIGEST in which each distance is known only up to some error, due to experimental inaccuracies. The error can be specified either by some additive offset or by a multiplicative factor. We show that both types of error make the PARTIAL DIGEST problem strongly NP-complete, by giving reductions from 3-PARTITION. In the case of relative errors, we show that the problem is hard to solve even for constant relative error.", "authors": ["Mark Cieliebak", "Stephan Eidenbenz"], "n_citation": 0, "title": "Measurement errors make the partial digest problem NP-hard", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d0644d6a-9bb0-40c7-9223-eb18bae696ee"}
{"abstract": "We developed an importer from both HOL 4 and HOL-light into Isabelle/HOL. The importer works by replaying proofs within Isabelle/HOL that have been recorded in HOL 4 or HOL-light and is therefore completely safe. Concepts in the source HOL system, that is types and constants, can be mapped to concepts in Isabelle/HOL; this facilitates a true integration of imported theorems and theorems that are already available in Isabelle/HOL. The importer is part of the standard Isabelle distribution.", "authors": ["Steven Obua", "Sebastian Skalberg"], "n_citation": 62, "title": "Importing HOL into Isabelle/HOL", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d0bf1fae-ca31-4c71-a55a-f7c9908c5e61"}
{"abstract": "This paper studies approximate distributed routing schemes on dynamic communication networks. The paper focuses on dynamic weighted general graphs where the vertices of the graph are fixed but the weights of the edges may change. Our main contribution concerns bounding the cost of adapting to dynamic changes. The update efficiency of a routing scheme is measured by the number of messages that need to be sent, following a weight change, in order to update the scheme. Our results indicate that the graph theoretic parameter governing the amortized message complexity of these updates is the local density D of the underlying graph, and specifically, this complexity is \u2296(D). The paper also establishes upper and lower bounds on the size of the databases required by the scheme at. each site.", "authors": ["Amos Korman", "David Peleg"], "n_citation": 0, "title": "Dynamic Routing Schemes for General Graphs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d3bb0c36-ccde-45c9-b5db-5f7a6900ebda"}
{"abstract": "We present an algorithm for inferring a timed-automaton model of a system from information obtained by observing its external behavior. Since timed automata can not in general be determinized, we restrict our attention to systems that can be described by deterministic event-recording automata. In previous work we have presented algorithms for event-recording automata that satisfy the restriction that there is at most one transition per alphabet symbol from each state. This restriction was lifted in subsequent work by an algorithm based on the region graph. In this paper, we extend previous work by considering the full class of event-recording automata, while still avoiding to base it on the (usually prohibitively large) region graph. Our construction deviates from previous work on inference of automata in that it first constructs a so called timed decision tree from observations of system behavior, which is thereafter folded into an automaton.", "authors": ["Olga Grinchtein", "Bengt Jonsson", "Paul Pettersson"], "n_citation": 0, "title": "Inference of event-recording automata using timed decision trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d3e93a37-ace4-4cad-a3eb-c09db216bb49"}
{"abstract": "In its simplest form, checkpointing is the act of saving a program's computation state in a form external to the running program, e.g. the computation state is saved to a filesystem. The checkpoint files can then be used to resume computation upon failure of the original process(s), hopefully with minimal loss of computing work. A checkpoint can be taken using a variety of techniques in every level of the system, from utilizing special hardware/architectural checkpointing features through modification of the user's source code. This survey will discuss the various techniques used in application-level checkpointing, with special attention being paid to techniques for checkpointing parallel and distributed applications.", "authors": ["John Paul Walters", "Vipin Chaudhary"], "n_citation": 50, "title": "Application-level checkpointing techniques for parallel programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d62d1394-520b-4c11-8ef6-c53a2830fc0d"}
{"abstract": "Nowadays web services technology is widely used to integrate heterogeneous systems and develop new applications. In this paper, an application of integration of hotel management systems by web services technology is presented. The Group Hotel Integration Reservation System (GHIRS) integrates lots of systems of hotel industry such as Front Office system, Property Management system, Enterprise Information System (EIS), Enterprise Information Portal system (EIP), Customer Relationship Management system (CRM) and Supply Chain Management system (SCM) together. This integration solution can add or expand hotel software system in any size of hotel chains environment.", "authors": ["Yang Xiang", "Wanlei Zhou", "Morshed U. Chowdhury"], "n_citation": 0, "title": "GHIRS: integration of hotel management systems by web services", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d677c099-b475-4ead-8fa2-3d13b1d68743"}
{"abstract": "In the context of learning paradigms of identification in the limit, we address the question: why is uncertainty sometimes desirable? We use mind change bounds on the output hypotheses as a measure of uncertainty, and interpret 'desirable' as reduction in data memorization, also defined in terms of mind change bounds. The resulting model is closely related to iterative learning with bounded mind change complexity, but the dual use of mind change bounds - for hypotheses and for data - is a key distinctive feature of our approach. We show that situations exists where the more mind changes the learner is willing to accept, the lesser the amount of data it needs to remember in order to converge to the correct hypothesis. We also investigate relationships between our model and learning from good examples, set-driven, monotonic and strong-monotonic learners, as well as class-comprising versus class-preserving learnability.", "authors": ["Eric Martin", "Arun Sharma", "Frank Stephan"], "n_citation": 0, "title": "On the data consumption benefits of accepting increased uncertainty", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d763587c-aeda-4787-86f8-1f0e8132f349"}
{"abstract": "We study the Edge Disjoint Paths (EDP) problem in undirected graphs: Given a graph G with n nodes and a set T of pairs of terminals, connect as many terminal pairs as possible using paths that are mutually edge disjoint. This leads to a variety of classic NP-complete problems, for which approximability is not well understood. We show a polylogarithmic approximation algorithm for the undirected EDP problem in general graphs with a moderate restriction on graph connectivity; we require the global minimum cut of G to be \u03a9(log 5  n). Previously, constant or polylogarithmic approximation algorithms were known for trees with parallel edges, expanders, grids and grid-like graphs, and most recently, even-degree planar graphs. These graphs either have special structure (e.g., they exclude minors) or there are large numbers of short disjoint paths. Our algorithm extends previous techniques in that it applies to graphs with high diameters and asymptotically large minors.", "authors": ["Satish Rao", "Shuheng Zhou"], "n_citation": 0, "title": "Edge Disjoint Paths in Moderately Connected Graphs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d794c1c1-3e86-4870-9ea6-f01225003e3c"}
{"abstract": "The growing complexity of new smart card platforms, including multi-subscription or multi-application functionalities, led up to more and more difficulty in testing such systems. In previous work, we have introduced a new method for automated test generation from state-based formal specifications (B abstract machines, UML/OCL models, Z specifications). This method uses cause-effect analysis and boundary computation to produce test cases as sequences of operation invocations. This method is embedded in a model-based test generator which has been exercised on several applications in the domain of smart card software (GSM 11-11 application, electronic purse system and Java Card transaction mechanism). In all these applications, a B abstract machine was built specifically for automatic test generation by an independent validation team. Writing a specific formal model for testing has been shown to be cost-effective, and has the advantages that it can be tailored towards the desired test objectives. This paper focuses on showing the application of this test generation process from formal models in the context of Smart Card applications. We describe how the test generation can be controlled by using several model coverage criteria. These criteria are of three kinds: multiple condition coverage, boundary-value coverage and behavior coverage. This makes it possible to generate a systematic minimal test suite achieving strong coverage results. The test engineer chooses the criteria depending on the application test objectives and then fully controls the test generation process.", "authors": ["Fabrice Bouquet", "Bruno Legeard", "Fabien Peureux", "Eric Torreborre"], "n_citation": 0, "title": "Mastering test generation from smart card software formal models", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d8d1040a-553f-49d1-83a2-4606eefb6bd3"}
{"abstract": "A wide range of algorithms have been developed for various types of automated negotiation. In developing such algorithms the main focus has been on their efficiency and their effectiveness. However, this is only part of the picture. Agents typically negotiate on behalf of their owner and for this to be effective, the agent must be able to adequately represent their owners' preferences. However, the process by which such knowledge is acquired is typically left unspecified. To remove this shortcoming, we present a case study indicating how the knowledge for a particular negotiation algorithm can be acquired. Specifically, we devise new knowledge acquisition techniques for obtaining information about a user's tradeoffs between various negotiation issues and develop knowledge acquisition tools to support this endeavour.", "authors": ["Xudong Luo", "Nicholas R. Jennings", "Nigel Shadbolt"], "n_citation": 0, "title": "Acquiring tradeoff preferences for automated negotiations: A case study", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ded42e94-5e43-4b40-a96f-4a3bd727a86d"}
{"abstract": "Web services provide a language-neutral, loosely-coupled, and platform-independent way for linking applications within organizations or enterprises across the Internet. In such a scenario, quantitative characteristics such as service execution throughput should be evaluated to measure the system performance. Usually, the first step is to define an abstract workflow model, for example Stochastic Petri Net Models. However, large system always raises the problem of state explosion. In this paper, we discuss a set of simplification rules for five basic structures of web service flow: sequential, parallel, conditional, loop and mutex. Our approach can effectively reduce the state space and is applied to the performance analysis of a web service flow management system.", "authors": ["Zhangxi Tan", "Chuang Lin", "Hao Yin", "Ye Hong", "Guangxi Zhu"], "n_citation": 0, "title": "Approximate performance analysis of Web services flow using Stochastic Petri Net", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "df226454-cc2e-49d9-af29-87d5bdb0dd5f"}
{"abstract": "We present a taxonomy-based conceptual modeling approach for building P2P systems that support semantic-based retrieval services. We adopt this simple conceptual modeling approach due to its advantages in terms of ease of use, uniformity, scalability and efficiency. As each peer uses its own taxonomy for describing the contents of its objects and for formulating queries to the other peers, peers are equipped with articulations, i.e. inter-taxonomy mappings, in order to carry out the required translation tasks. We describe various kinds of articulations and we give the semantics for each case. Then we discuss the differences between query evaluation in mediators and query evaluation in P2P systems, and finally we identify issues for further research.", "authors": ["Yannis Tzitzikas", "Carlo Meghini", "Nicolas Spyratos"], "n_citation": 0, "title": "Taxonomy-based conceptual modeling for peer-to-peer networks", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "dfe80545-07af-48c1-b6e3-cd5f7d87bdee"}
{"abstract": "We consider an extension of first-order logic by modular quantifiers of a fixed modulus q. Drawing on collapse results from finite model theory and techniques of finite semigroup theory, we show that if the only available numerical predicate is addition, then sentences in this logic cannot define the set of bit strings in which the number of l's is divisible by a prime p that does not divide q. More generally, we completely characterize the regular languages definable in this logic. The corresponding statement, with addition replaced by arbitrary numerical predicates, is equivalent to the conjectured separation of the circuit complexity class ACC from NC1. Thus our theorem can be viewed as proving a highly uniform version of the conjecture.", "authors": ["Amitabha Roy", "Howard Straubing"], "n_citation": 0, "title": "Definability of languages by generalized first-order formulas over (N,+)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e16565cd-3ee5-4a86-81d4-818b6c851b58"}
{"abstract": "This paper is the first part of a sequence of two papers that present algebraic constructions of quasi-cyclic LDPC codes for AWGN, binary random and burst erasure channels. In this paper, a class of quasi-cyclic LDPC codes for both AWGN and binary random erasure channels is constructed based on finite fields and special vector representations of finite field elements.", "authors": ["Lan Lan", "Lingqi Zeng", "Ying Yu Tai", "Lei Chen", "Shu Lin", "Khaled A. S. Abdel-Ghaffar"], "n_citation": 50, "title": "Algebraic Constructions of Quasi-cyclic LDPC Codes -Part I For AWGN and Binary Random Erasure Channels", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e1a7d290-b6ff-4680-9447-97af65f1b07b"}
{"abstract": "In this paper, an efficient decentralized Grid service discovery approach is proposed, which utilizes service taxonomy to address both scalability and efficiency. Grid information nodes are organized into different community overlays according to the types of the services registered, and a DHT (Distributed Hash Table) P2P based upper layer network is constructed to facilitate the organization of community overlays and provide efficient navigation between different communities. As a result, service discovery will be limited into related communities, which improves efficiency. A simple and lightweight greedy search based service location (GSBSL) method is also introduced to identify service providers with high QoS efficiently within the same community. Simulation results show that, the efficiency is improved compared with existing de-centralized solutions, and the overhead is acceptable and controllable.", "authors": ["Cheng Zhu", "Zhong Liu", "Weiming Zhang", "Zhenning Xu", "Dongsheng Yang"], "n_citation": 0, "title": "Using service taxonomy to facilitate efficient decentralized Grid service discovery", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e32756d6-4721-47cd-a94c-c5db0b659d74"}
{"abstract": "In Component-Based Development(CBD), by building new software with independently developed components, we can gain the benefits promised by the software reuse such as quality improvement and rapid development. Hence, methods to design the components in component-based system are very important and the component architecture should consist of highly cohesive components. Accordingly, to design highly cohesive components in component development phase, we propose new types of dynamic dependency relationship between classes and a component-based cohesion metric applying the properties of linear increment by dynamic dependency relationships between classes. In addition, we prove the theoretical soundness of the proposed metric by the axioms of briand et al. and suggest the accuracy and practicality of the proposed metric through a comparison with the conventional metrics.", "authors": ["Misook Choi", "Jong-Suk Lee", "Ha Jong-Sung"], "n_citation": 50, "title": "A Component Cohesion Metric Applying the Properties of Linear Increment by Dynamic Dependency Relationships Between Classes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e4f4e489-43b1-4d87-bc43-c63f4808266f"}
{"abstract": "Counterexample-guided abstraction refinement (CEGAR) has proven to be a powerful method for software model-checking. In this paper, we investigate this concept in the context of sequential (possibly recursive) programs whose statements are given as BDDs. We examine how Craig interpolants can be computed efficiently in this case and propose a new, special type of interpolants. Moreover, we show how to treat multiple counterexamples in one refinement cycle. We have implemented this approach within the model-checker Moped and report on experiments.", "authors": ["Javier Esparza", "Stefan Kiefer", "Stefan Schwoon"], "n_citation": 0, "title": "Abstraction refinement with craig interpolation and symbolic pushdown systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e5cc8df6-9e59-4147-ad25-e02f7fdafa9b"}
{"abstract": "Artificial neural networks have been an interesting alternative to use instead of classic statistical techniques, however, artificial neural networks have some disadvantages, as for example: the training process is long, the choice of topology and input variables (attributes) are difficult. This work uses three models of binomial regression (each model has a different link function) for selecting statistical significant variables for being used as input nodes on each neural network. Hybrid models were constructed, in this paper, in two steps.", "authors": ["Gecynalda S. da S. Gomes", "Teresa Bernarda Ludermir"], "n_citation": 0, "title": "Feature Selection for Neural Networks Through Binomial Regression", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e98774dd-69f4-4c6c-aa9f-f13eee97c1ba"}
{"authors": ["Guorong Wu", "Sebastiano Stramaglia", "Daniele Marinazzo"], "n_citation": 0, "title": "Decomposition of the transfer entropy: partial conditioning and informative clustering", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "ea834fec-4801-4feb-9ad0-03918af2cfa1"}
{"abstract": "In this paper, several methodologies for designing synthesis strategies in distributed expert systems are investigated. They are analytic methods, inductive methods, and analogical methods. Firstly, synthesis problems are formally described. Secondly, the measurements for synthesis strategies are formally defined. After that, all methodologies are analyzed thoroughly and corresponding examples are introduced. Furthermore, all methodologies are compared and we conclude that they compensate each other.", "authors": ["M. Zhang", "Zhang C"], "n_citation": 0, "title": "Methodologies of solution synthesis in distributed expert systems", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "eacdc164-8859-4e75-a7d7-2d21906172e8"}
{"abstract": "We investigate the satisfiability problem of word equations where each variable occurs at most twice (quadratic systems). We obtain various new results: The satisfiability problem is NP-hard (even for a single equation). The main result says that once we have fixed the lengths of a possible solution, then we can decide in linear time whether there is a corresponding solution. If the lengths of a minimal solution were at most exponential, then the satisfiability problem of quadratic systems would be NP-complete. (The inclusion in NP follows also from [21]) In the second part we address the problem with regular constraints: The uniform version is PSPACE-complete. Fixing the lengths of a possible solution doesn't make the problem much easier. The non-uniform version remains NP-hard (in contrast to the linear time result above). The uniform version remains PSPACE-complete.", "authors": ["John Michael Robson", "Volker Diekert"], "n_citation": 0, "title": "On quadratic word equations", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "eb750f96-bd55-4bd6-8716-14e6e1f456c1"}
{"abstract": "In this paper, we propose an image denoising method that incorporates anisotropic diffusion and independent component analysis (ICA) techniques. An image is decomposed into independent component coefficients, and anisotropic diffusion is applied to filtering the IC coefficients. The proposed method achieved much better noise suppression with minimum edge blurring compared with other denoising methods, such as original anisotropic diffusion filter and wavelet shrinkage. The effectiveness of the proposed method is demonstrated by simulation experiments on medical image denoising.", "authors": ["Yinghong Luo", "Caixia Tao", "Xiaohu Qiang", "Xiangyan Zeng"], "n_citation": 0, "title": "Denoising by Anisotropic Diffusion of Independent Component Coefficients", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ebf75c49-6b19-4220-887a-c198815a93b6"}
{"abstract": "New requirements to the overload control of Parlay gateway are analyzed. And the maximum revenue based overload control model of Parlay gateway is proposed. Furthermore, the network-based overload control architecture based on the agent linear programming theory is proposed. And network overload control architecture based on linear programming is given out. Simulation results show that using agent linear method, it is quick to find out maximum revenue based overload control algorithm; also it is suitable for large-scale softswitch network.", "authors": ["Yun-Yong Zhang", "Zhi-Jiang Zhang", "Fan Zhang", "Yun-Jie Li"], "n_citation": 0, "title": "A new overload control algorithm of NGN service gateway", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ec3f8e46-a4d1-448e-a528-0e1bca46aea5"}
{"abstract": "This paper describes a novel and efficient visualization technique intended for hierarchical-temporal data using a tree-ring like layout. Temporal hierarchies appear in numerous fields such as genealogy, evolution taxonomies or time lines. In many cases, state-of-the-art static diagrams are produced in these fields. By using several information visualization strategies, such as focus + context, the tree-ring approach has the ability to visualize and navigate these, potentially complex, hierarchies trough time. Thus, a deeper insight into the problem at hand can be gained.", "authors": ["Roberto Ther\u00f3n"], "n_citation": 0, "title": "Hierarchical-temporal data visualization using a tree-ring metaphor", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ec508034-7794-4aa7-ae6a-673bec10654b"}
{"abstract": "First-order logic with k-ary deterministic transitive closure has the same power as two-way k-head deterministic automata that use a finite set of nested pebbles. This result is valid for strings, ranked trees, and in general for families of graphs having a fixed automaton that can be used to traverse the nodes of each of the graphs in the family. Other examples of such families are grids, toruses, and rectangular mazes.", "authors": ["Joost Engelfriet", "Hendrik Jan Hoogeboom"], "n_citation": 0, "title": "Nested pebbles and transitive closure", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ee7f96e1-9915-47b7-b5f7-94540222deed"}
{"abstract": "Phylogenetic networks are the generalization of the tree models used to represent evolutionary relationship between the species. Tree models of evolutionary process are not adequate to represent the evolutionary events such as, hybridization, lateral/ horizontal gene transfer and genetic recombination. A well-formulated problem in phylogenetic networks, due to recombination, is to derive a set of input sequences on a network with minimum number of recombinations. No efficient algorithm exists for this problem as it is known to be NP-hard. Efficient solutions exist for the constrained recombination networks, where the nodes on each recombination cycles are disjoint. These solutions are based on the assumption that the ancestral sequence is known in advance. On the other hand, the more biologically realistic case is that where the ancestor sequence is not known in advance. In this paper we propose an efficient classification based method for deriving a phylogenetic network under constrained recombination without knowing the ancestral sequence.", "authors": ["M. A. H. Zahid", "Ankush Mittal", "R. C. Joshi"], "n_citation": 0, "title": "A classification based approach for root unknown phylogenetic networks under constrained recombination", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f149d3bf-ee6d-446a-a7c9-9ee00c5f1aa3"}
{"abstract": "We address the problem of recognition of natural motions such as water, smoke and wind-blown vegetation. Such dynamic scenes exhibit characteristic stochastic motions, and we ask whether the scene contents can be recognized using motion information alone. Previous work on this problem has considered only the case where the texture samples have sufficient overlap to allow registration, so that the visual content of the scene is very similar between examples. In this paper we investigate the recognition of entirely non-overlapping views of the same underlying motion, specifically excluding appearance-based cues. We describe the scenes with time-series models-specifically multivariate autoregressive (AR) models -so the recognition problem becomes one of measuring distances between AR models. We show that existing techniques, when applied to non-overlapping sequences, have significantly lower performance than on static-camera data. We propose several new schemes, and show that some outperform the existing methods.", "authors": ["Franco Woolfe", "Andrew W. Fitzgibbon"], "n_citation": 63, "title": "Shift-Invariant Dynamic Texture Recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f1afad91-8775-47e4-8bc8-7d7f42d9c610"}
{"abstract": "This paper introduces a dependability assessment tool (WSsDAT) for Web Services monitoring and testing. It allows users to evaluate dependability of Web Services from the point of view of their clients by collecting metadata representing a number of dependability metrics. This Java-based tool can be deployed in diverse geographical locations to monitor and test the behavior of a selected set of Web Services within preset time intervals. The graphical user interface of the tool provides real-time statistical data describing dependability of the Web Services under monitoring. The tool makes it possible for the users to identify typical patterns of dependability-specific behavior depending on the time of the day, days of the week or the client locations. In addition, WSsDAT can collect and analyze service dependability measurements during long periods of time, as well as obtaining dependability-related information about current service behavior. The paper reports on a successful series of experiments using this tool for investigating the dependability of two BLAST Web Services which are widely used in the bioinformatics domain. The paper shows how the tool can be employed by e-scientists in making informed choices of the distributed services being used in an in silico experiment and thereby improving their overall dependability.", "authors": ["Peter Li", "Yuhui Chen", "Alexander Romanovsky"], "n_citation": 50, "title": "Measuring the Dependability of Web Services for Use in e-Science Experiments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f363945e-cc25-49b6-848d-91d8a624323d"}
{"abstract": "Coverage is a central issue in the design of wireless sensor networks. There are many measures for coverage, based on what aspect of surveillance quality we wish to address. Designing a network that achieves desired standards in terms of the measure chosen is a non-trivial problem. In this paper we take the Maximal Breach Path measure and formulate the sensor-network design problem as a geometric optimisation problem. We present improved polynomial time algorithms for computing the aforesaid measure for a given sensor network. Also, as a first step toward solving the optimisation problem posed in this paper, we present a geometric transformation on a given configuration of sensors that brings the maximal breach to a local optimal - in the sense that the resulting breach is the best we can get keeping the topology of the starting configuration intact.", "authors": ["Anirvan Duttagupta", "Arijit Bishnu", "Indranil Sengupta"], "n_citation": 0, "title": "Optimisation problems based on the maximal breach path measure for wireless sensor network coverage", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f392a775-a78e-4978-af8e-712e27737a2f"}
{"abstract": "World wide web has gained a lot of prominence with respect to information retrieval and data delivery. With such a prolific growth, a user interested in a specific change has to continuously retrieve/pull information from the web and analyze it. This results in wastage of resources and more importantly the burden is on the user. Pull-based retrieval needs to be replaced with a push-based paradigm for efficiency and notification of relevant information in a timely manner. WebVigiL is an efficient profile-based system to monitor, retrieve, detect and notify specific changes to HTML and XML pages on the web. In this paper, we describe the expressive profile specification language along with its semantics. We also present an efficient implementation of these profiles. Finally, we present the overall architecture of the WebVigiL system and its implementation status.", "authors": ["Ajay Eppili", "Jyoti Jacob", "Alpa Sachde", "Sharma Chakravarthy"], "n_citation": 0, "title": "Expressive profile specification and its semantics for a Web monitoring system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f3d2be43-0c53-4c45-b63f-9a4e483f2e47"}
{"abstract": "We consider the control problem for timed automata against specifications given as MTL formulas. The logic MTL is a linear-time timed temporal logic which extends LTL with timing constraints on modalities, and recently, its model-checking has been proved decidable in several cases. We investigate these decidable fragments of MTL (full MTL when interpreted over finite timed words, and Safety-MTL when interpreted over infinite timed words), and prove two kinds of results, (1) We first prove that, contrary to model-checking, the control problem is undecidable. Roughly, the computation of a lossy channel system could be encoded as a model-checking problem, and we prove here that a perfect channel system can be encoded as a control problem. (2) We then prove that if we fix the resources of the controller (by resources we mean clocks and constants that the controller can use), the control problem becomes decidable. This decidability result relies on properties of well (and better) quasi-orderings.", "authors": ["Patricia Bouyer", "Laura Bozzelli", "Fabrice Chevalier"], "n_citation": 50, "title": "Controller synthesis for MTL specifications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f587e950-fbce-4395-9554-98e150b314de"}
{"abstract": "This paper introduces a novel algebra for reasoning about step reactions in synchronous languages, such as macro steps in Harel, Pnueli and Shalev's Statecharts and instantaneous reactions in Berry's Esterel. The algebra describes step reactions in terms of configurations which can both be read in a standard operational as well as in a model-theoretic fashion. The latter arises by viewing configurations as propositional formulas, interpreted intuitionistically over finite linear Kripke structures. Previous work by the authors showed the adequacy of this approach by establishing compositionality and full-abstraction results for Statecharts and Esterel. The present paper generalizes this work in an algebraic setting and, as its main result, provides a sound and complete equational axiomatization of step reactions. This yields, for the first time in the literature, a complete axiomatization of Statecharts macro steps, which can also be applied, modulo encoding, to Esterel reactions.", "authors": ["Gerald L\u00fcttgen", "Michael Mendler"], "n_citation": 0, "title": "Axiomatizing an algebra of step reactions for synchronous languages", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f86e0b1e-c00c-4e99-ab95-68ba7aa863d5"}
{"abstract": "This paper presents an approach to structured integration of different application subsystems on the same embedded hardware, as currently developed in DECOS (Dependable Embedded Components and Systems), an integrated project within the Sixth Framework Programme of the European Commission. Those application subsystems can have different criticality levels and vendors. Furthermore, reliable communication among application subsystems is a major concern. Focusing on the Encapsulated Execution Environment (EEE), which separates application subsystems in the space AND the time domain, this approach outlines the concepts and principles of an exokernel operating system, of partitioning, and of virtualization. The Core Operating System (COS) is described as a case study, including the hardware used, the current feature set, and benchmark values of central COS operations. This paper also presents a model for a platform-independent application interface layer. Parts of this interface layer are generated from task specification to provide tasks with tailored communication services.", "authors": ["Martin Schlager", "Wolfgang Herzner", "Andreas Wolf", "Oliver Gr\u00fcndonner", "Maximilian Rosenblattl", "Erwin Erkinger"], "n_citation": 50, "title": "Encapsulating Application Subsystems Using the DECOS Core OS", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f97bad83-79db-409d-92f6-1d9d39a20644"}
{"abstract": "The decomposition method is currently one of the major methods for solving the convex quadratic optimization problems being associated with Support Vector Machines (SVM-optimization). A key issue in this approach is the policy for working set selection. We would like to find policies that realize (as good as possible) three goals simultaneously: (fast) convergence to an optimal solution, efficient procedures for working set selection, and high degree of generality (including typical variants of SVM-optimization as special cases). In this paper, we study a general policy for working set selection that has been proposed quite recently. It is known that it leads to convergence for any convex quadratic optimization problem. Here, we investigate its computational complexity when it is used for SVM-optimization. We show that it poses an NP-complete working set selection problem, but a slight variation of it (sharing the convergence properties with the original policy) can be solved in polynomial time. We show furthermore that so-called rate certifying pairs (introduced by Hush and Scovel) can be found in linear time, which leads to a quite efficient decomposition method with a polynomial convergence rate for SVM-optimization.", "authors": ["Hans Ulrich Simon"], "n_citation": 50, "title": "On the complexity of working set selection", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fb6acee9-6dfe-47ef-a8c3-20bbbbb3ff8a"}
{"abstract": "Standard land price is an economical indicator for measuring land value. In this paper, we propose to use the tension spline interpolation function to mine standard land price. First, we extend the definition of standard land price, which is based on land region composed of several neighboring land parcels with the same or similar features. Second, the regional factors that affect the standard land price are classified into the geometric features of point, line and area according to the quantitative rules. Third, a tension spline interpolation function is proposed to mine standard land price, which is determined by the influential factors. Finally, as a case study, the proposed method is applied to mine land prices for Nanning City in China. The case study shows that the proposed method is a practical and satisfactory one.", "authors": ["Hanning Yuan", "Wenzhong Shi", "Jiabing Sun"], "n_citation": 0, "title": "Mining standard land price with tension spline function", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fd680b27-d3ec-4f1c-860a-ba08595f7555"}
{"abstract": "Recovering the shape of a class of objects requires establishing correct correspondences between manually or automatically annotated landmark points. In this study, we utilise a novel approach to automatically recover the shape of hand outlines from a series of 2D training images. Automated landmark extraction is accomplished through the use of the self-organising model the growing neural gas (GNG) network which is able to learn and preserve the topological relations of a given set of input patterns without requiring a priori knowledge of the structure of the input space. To measure the quality of the mapping throughout the adaptation process we use the topographic product. Results are given for the training set of hand outlines.", "authors": ["Anastassia Angelopoulou", "Jos\u00e9 Garc\u00eda Rodr\u00edguez", "Alexandra Psarrou"], "n_citation": 15, "title": "Learning 2D Hand Shapes Using the Topology Preservation Model GNG", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ffa15fdb-cbb9-404b-a9c3-470ee0c13ecb"}
{"abstract": "In schema integration, schematic discrepancies occur when data in one database correspond to metadata in another. We define this kind of semantic heterogeneity in general using the paradigm of context that is the meta information relating to the source, classification, property etc of entities, relationships or attribute values in entity-relationship (ER) schemas. We present algorithms to resolve schematic discrepancies by transforming metadata into entities, keeping the information and constraints of original schemas. Although focusing on the resolution of schematic discrepancies, our technique works seamlessly with existing techniques resolving other semantic heterogeneities in schema integration.", "authors": ["Qi He", "Tok Wang Ling"], "n_citation": 0, "title": "Resolving schematic discrepancy in the integration of entity-relationship schemas", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "01c72a57-ab7b-4b29-86e2-8b95e6625e2f"}
{"abstract": "We propose a new method to segment 3D structures with competitive level sets driven by a shape model and fuzzy control. To this end, several contours evolve simultaneously toward previously defined targets. The main contribution of this paper is the original introduction of prior information provided by a shape model, which is used as an anatomical atlas, into a fuzzy decision system. The shape information is combined with the intensity distribution of the image and the relative position of the contours. This combination automatically, determines the directional term of the evolution equation of each level set. This leads to a local expansion or contraction of the contours, in order to match the borders of their respective targets. The shape model is produced with a principal component analysis, and the resulting mean shape and variations are used to estimate the target location and the fuzzy states corresponding to the distance between the current contour and the target. By combining shape analysis and fuzzy control, we take advantage of both approaches to improve the level set segmentation process with prior information. Experiments are shown for the 3D segmentation of deep brain structures from MRI and a quantitative evaluation is performed on a 18 volumes dataset.", "authors": ["Cyb\u00e8le Ciofolo", "Christian Barillot"], "n_citation": 0, "title": "Shape Analysis and Fuzzy Control for 3D Competitive Segmentation of Brain Structures with Level Sets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0a104aae-1459-4ef1-8c42-dd53de2a4581"}
{"authors": ["Madhu Sudan"], "n_citation": 0, "title": "Modelling errors and recovery for communication", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0d5bb832-75a8-41d9-8449-747954b0846b"}
{"abstract": "MPML3D is our first candidate of the next generation of authoring languages aimed at supporting digital content creators in providing highly appealing and highly interactive content with little effort. The language is based on our previously developed family of Multimodal Presentation Markup Languages (MPML) that broadly followed the sequential and parallel tagging structure scheme for generating pre-synchronized presentations featuring life-like characters and interactions with the user. The new markup language MPML3D deviates from this design framework and proposes a reactive model instead, which is apt to handle interaction-rich scenarios with highly realistic 3D characters. Interaction in previous versions of MPML could be handled only at the cost of considerable scripting effort due to branching. By contrast, MPML3D advocates a reactive model that allows perceptions of other characters or the user interfere with the presentation flow at any time, and thus facilitates natural and unrestricted interaction. MPML3D is designed as a powerful and flexible language that is easy-to-use by non-experts, but it is also extensible as it allows content creators to add functionality such as a narrative model by using popular scripting languages.", "authors": ["Michael Nischt", "Helmut Prendinger", "Elisabeth Andr\u00e9", "Mitsuru Ishizuka"], "n_citation": 0, "title": "MPML3D : A reactive framework for the multimodal presentation markup language", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "12d74de9-e9e3-4845-aaa7-12486d65fa6c"}
{"abstract": "In this paper, an optimal algorithm to solve the continuous/discrete weighted 2-center problem is proposed. The method generalizes the trimming technique of Megiddo [5] in a nontrivial way. This result allows an improved O(nlogn) time algorithm for the weighted 3-center and 4-center problems.", "authors": ["Boaz Ben-Moshe", "Binay K. Bhattacharya", "Qiaosheng Shi"], "n_citation": 0, "title": "An optimal algorithm for the continuous/discrete weighted 2-center problem in trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "25475a58-e733-4220-bf37-cf69454a03c3"}
{"abstract": "We introduce a hypergraph-based process calculus with a generic type system. That is, a type system checking an invariant property of processes can be generated by instantiating the original type system. We demonstrate the key ideas behind the type system, namely that there exists a hypergraph morphism from each process graph into its type, and show how it can be used for the analysis of processes. Our examples are input/output-capabilities, secrecy conditions and avoiding vicious circles occurring in deadlocks. In order to specify the syntax and semantics of the process calculus and the type system, we introduce a method of hypergraph construction using concepts from category theory.", "authors": ["Barbara K\u00f6nig"], "n_citation": 0, "title": "Generating type systems for process graphs", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "2a00e656-8799-4f05-b533-7ae54720df94"}
{"abstract": "Contemporary software systems are exposed to demanding, dynamic, and unpredictable environments where the traditional adaptability mechanisms may not be sufficient. To imitate and fully benefit from life-like adaptability in software systems that might come closer to the complexity levels of biological organisms, we seek a formal mathematical model of certain fundamental concepts such as: life, organism, evolvability and adaptation. In this work we concentrate on the concept of software evolvability. Our work proposes an evolutionary computation model, based on the theory of hypercycles and autopoiesis. The intrinsic properties of hypercycles allow them to evolve into higher levels of complexity, analogous to multi-level, or hierarchical evolutionary processes. We aim to obtain structures of self-maintaining ensembles, that are hierarchically organised, and our primary focus is on such open-ended hierarchically organised evolution.", "authors": ["Mariusz Nowostawski", "Martin K. Purvis", "Stephen Cranefield"], "n_citation": 0, "title": "An architecture for self-organising evolvable virtual machines", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2a732852-72aa-411f-b219-ffb35427e9bc"}
{"abstract": "This paper describes a new approach towards a component architecture for hard real time control applications as found, for example, in the automotive domain. Based on the paradigm of Logical Execution Time (LET) as introduced by Giotto [1 we adapt the high-level language construct module which allows us to organize and parallelize real time code in the large. Our module construct serves multiple purposes: (1) it introduces a namespace for program entities and supports information hiding, (2) it represents a partitioning of the set of actuators and control logic available in a system, (3) it acts as a static specification of components and dependencies, (4) it may serve as the unit of dynamic loading of system extensions and (5) it may serve as the unit of distribution of functionality over a network of electronic control units. We describe the individual usage cases of modules, introduce the syntax required to specify our needs and discuss various implementation aspects.", "authors": ["Wolfgang Pree", "Josef Templ"], "n_citation": 0, "title": "Towards a Component Architecture for Hard Real Time Control Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d95ebf3-f5aa-4167-9eca-2e27ea09adf5"}
{"abstract": "We describe an approach to modelling biological networks by action languages via answer set programming. To this end, we propose an action language for modelling biological networks, building on previous work by Baral et al. We introduce its syntax and semantics along with a translation into answer set programming. Finally, we describe one of its applications, namely, the sulfur starvation response-pathway of the model plant Arabidopsis thaliana and sketch the functionality of our system and its usage.", "authors": ["Susanne Grell", "Torsten Schaub", "Joachim Selbig"], "n_citation": 50, "title": "Modelling biological networks by action languages via answer set programming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3732d021-c013-4b37-b420-1bd9919b3773"}
{"abstract": "We consider the issues involved in taking educational role-play into a virtual environment with intelligent graphical characters, who implement a cognitive appraisal system and autonomous action selection. Issues in organizing emergent narratives are discussed with respect to a Story Facilitator as well as the impact on the authoring process.", "authors": ["Ruth Aylett", "Rui Figueiredo", "Sandy Louchart", "Jo\u00e3o Dias", "Ana Paiva"], "n_citation": 0, "title": "Making it up as you go along : Improvising stories for pedagogical purposes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c5e8416-f361-42b0-b22d-e890c3087bed"}
{"abstract": "Alternating transition systems are a general model for composite systems which allow the study of collaborative as well as adversarial relationships between individual system components. Unlike in labeled transition systems, where each transition corresponds to a possible step of the system (which may involve some or all components), in alternating transition systems, each transition corresponds to a possible move in a game between the components. In this paper, we study refinement relations between alternating transition systems, such as Does the implementation refine the set A of specification components without constraining the components not in A? In particular, we generalize the definitions of the simulation and trace containment preorders from labeled transition systems to alternating transition systems. The generalizations are called alternating simulation and alternating trace containment. Unlike existing refinement relations, they allow the refinement of individual components within the context of a composite system description. We show that, like ordinary simulation, alternating simulation can be checked in polynomial time using a fixpoint computation algorithm. While ordinary trace containment is PSPACE-complete, we establish altemating trace containment to be EXPTIME-complete. Finally, we present logical characterizations for the two preorders in terms of ATL, a temporal logic capable of referring to games between system components.", "authors": ["Rajeev Alur", "Thomas A. Henzinger", "Orna Kupferman", "Moshe Y. Vardi"], "n_citation": 0, "title": "Alternating refinement relations", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "5a5071a4-6c64-4331-830d-426ee835b7b7"}
{"abstract": "We demonstrate how current knowledge about the neurobiology and structure of human personality can be used as the basis for a computational model of personality in intelligent agents (PAC-personality, affect, and culture). The model integrates what is known about the neurobiology of human motivation and personality with knowledge about the psychometric structure of trait language and personality tests. Thus, the current model provides a principled theoretical account that is based on what is currently known about the structure and neurobiology of human personality and tightly integrates it into a computational architecture. The result is a motive-based computational model of personality that provides a psychologically principled basis for intelligent virtual agents with realistic and engaging personality.", "authors": ["Stephen J. Read", "Lynn C. Miller", "Brian M. Monroe", "Aaron L. Brownstein", "Wayne Zachary", "Jean-Christophe LeMentec", "Vassil Iordanov"], "n_citation": 50, "title": "A neurobiologically inspired model of personality in an intelligent agent", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5c5923ac-c557-4ce2-bce0-9a8fffde2b7f"}
{"abstract": "In the fields of data mining and knowledge discovery, many semistructured data such as HTML/XML files are represented by rooted trees t such that all children of each internal vertex of t are ordered and t has edge labels. In order to represent structural features common to such semistructured data, we propose a regular term tree which is a rooted tree pattern consisting of ordered tree structures and internal structured variables. For a regular ordered term tree t, the term tree language of t, denoted by L(t), is the set of all trees which are obtained from t by substituting arbitrary trees for all variables in t. In this paper, we consider a polynomial time learnability of the class OTTL = {L(t) |t \u2208 OTT} from positive data, where OTT denotes the set of all regular ordered term trees. First of all, we present a polynomial time algorithm for solving the minimal language problem for OTT which is, given a set of labeled trees S, to find a term tree t in OTT such that L(t) is minimal among all term tree languages which contain all trees in S. Moreover, by using this algorithm and the polynomial time algorithm for solving the membership problem for OTT in our previous work [15], we show that OTTL is polynomial time inductively inferable from positive data. This result is an extension of our previous results in [14].", "authors": ["Yusuke Suzuki", "Takayoshi Shoudai", "Tomoyuki Uchida", "Tetsuhiro Miyahara"], "n_citation": 50, "title": "Ordered term tree languages which are polynomial time inductively inferable from positive data", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5f9ef1d5-50fb-4e7a-87de-a9e4fd8a3ec7"}
{"abstract": "In many stream applications, a kind of query that join the streams with the data stored in disk is often used, which is termed SDJoin query. To process SDJoin query, two novel evaluating algorithms based on buffer are proposed in this paper, namely BNLJ and BHJ. Since the existed cost models are not suitable for SDJoin evaluating algorithms, a one-run-basis cost model is also presented to analyze the expected performance of proposed algorithms. Theoretical analysis and experimental results show that BHJ are more efficient.", "authors": ["Weiping Wang", "Jianzhong Li", "Xu Wang", "Dongdong Zhang", "Longjiang Guo"], "n_citation": 1, "title": "Evaluating stream and disk join in continuous queries", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "61ceef33-dc7e-4e82-b8bf-dc7fb4c6b640"}
{"abstract": "Most efforts to combine formal methods and software testing go in the direction of exploiting formal methods to solve testing problems, most commonly test case generation. Here we take the reverse viewpoint and show how the technique of partition testing can be used to improve a formal proof technique (induction for correctness of loops). We first compute a partition of the domain of the induction variable, based on the branch predicates in the program code of the loop we wish to prove. Based on this partition we derive a #R##N#partitioned induction rule, which is (hopefully) easier to use than the standard induction rule. In particular, with an induction rule that is tailored to the program to be verified, less user interaction can be expected to be required in the proof. We demonstrate with a number of examples the practical efficiency of our method.", "authors": ["Angela Wallenburg", "Reiner H\u00e4hnle"], "n_citation": 0, "title": "Using a Software Testing Technique to Improve Theorem Proving", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6b848947-ccf8-427f-a800-7ab477e11d7d"}
{"abstract": "An N-superconcentrator is a directed graph with N input vertices and N output vertices and some intermediate vertices, such that for k = 1,2,..., N, between any set of k input vertices and any set of k output vertices, there are k vertex disjoint paths. In a depth-two N-superconcentrator each edge either connects an input vertex to an intermediate vertex or an intermediate vertex to an output vertex. We consider tradeoffs between the number of edges incident on the input vertices and the number of edges incident on the output vertices in a depth-two N-superconcentrator. For an N-superconcentrator G, let a(G) be the average degree of the input vertices and b(G) be the average degree of the output vertices. Assume that b(G) > a(G). We show that there is a constant k 1  > 0 such that a( G ) log (2b(G) a(G)) log b(G) > k 1 . log 2  N. We further show a complementary sufficient condition: there is a constant k 2  > 0, such that if some a and b (a   0). Before this work, it was only known [3] that one of these operations requires \u03a9(log 2 N log log N) time.", "authors": ["Chinmoy Dutta", "Jaikumar Radhakrishnan"], "n_citation": 0, "title": "Tradeoffs in depth-two superconcentrators", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7708cd2b-dcc1-4c2f-8fc6-f5d2e45a2581"}
{"abstract": "Up to 70 electronic control units (ECU'S) serve for safety and comfort functions in a car. Communicating over different bus systems most ECU's perform close loop control functions and reactive functions and have to fulfill hard real time constraints. Some ECU's controlling on board entertainment/office systems are software intensive, incorporating millions of lines of code. The challenge for the design of those distributed and networked control units is to define all requirements and constraints, understand and analyze those manifold interactions between the control units, the car and the environment (driver, road, weather) in normal as well as stress situations (crash). To improve the design of safety critical ECU's we propose an enhanced development process (double-V-model). The use of different modeling descriptions for closed loop control, reactive systems and software intensive systems requires a CASE-tool integration platform. We have developed Generalstore as a platform to support model driven design with heterogeneous models in a design process which is concurrent and distributed between the automotive manufacturer and several suppliers.", "authors": ["Klaus D. Mueller-Glaser", "Clemens Reichmann", "Markus Kuehl", "Stefan Benz"], "n_citation": 0, "title": "Quality Assurance and Certification of Software Modules in Safety Critical Automotive Electronic Control Units Using a CASE-Tool Integration Platform", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ddbd43d-f812-4629-b965-fea1265f86ec"}
{"abstract": "Loops are the most frequent cause of non-termination in string rewriting. In the general case, non-terminating, non-looping string rewriting systems exist, and the uniform termination problem is undecidable. For rewriting with only one string rewriting rule, it is unknown whether non-terminating, non-looping systems exist and whether uniform termination is decidable. If in the one-rule case, non-termination is equivalent to the existence of loops, as McNaughton conjectures, then a decision procedure for the existence of loops also solves the uniform termination problem. As the existence of loops of bounded lengths is decidable, the question is raised how long shortest loops may be. We show that string rewriting rules exist whose shortest loops have superexponential lengths in the size of the rule.", "authors": ["Alfons Geser"], "n_citation": 0, "title": "Loops of superexponential lengths in one-rule string rewriting", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "83c4f3e0-56fc-4bd7-acb6-f59d599ccc06"}
{"abstract": "Implementations of cryptographic algorithms are vulnerable to side-channel attacks. Masking techniques are employed to counter side-channel attacks that are based on multiple measurements of the same operation on different data. Most currently known techniques require new random values after every nonlinear operation and they are not effective in the presence of glitches. We present a new method to protect implementations. Our method has a higher computational complexity, but requires random values only at the start, and stays effective in the presence of glitches.", "authors": ["Svetla Nikova", "Christian Rechberger", "Vincent Rijmen"], "n_citation": 50, "title": "Threshold implementations against side-channel attacks and glitches", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8682f8eb-8162-4261-b804-857e4faa4e1e"}
{"authors": ["Nicole Lazzeri", "Daniele Mazzei", "Abolfazl Zaraki", "D. De Rossi"], "n_citation": 50, "title": "Towards a Believable Social Robot", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "88cc4f46-07d1-49d6-ad55-d681e6717e86"}
{"abstract": "Ad hoc networking research suffers from the lack of meaningful and realistic models to describe the route acquisition process of ad hoc routing protocols. There is a strong need for such models to be able to perform realistic calculations supporting important yet difficult tasks, such as performance estimation and protocol scalability analysis. Based on existing work for ideal source routing we formulate and validate an analytical model to match the route acquisition process executed by the Ad Hoc On-Demand Distance Vector (AODV) protocol. This allows us to predict the probability density function of estimated route lengths, a powerful metric for characterization of the network behavior. We further extend our study to include multiple refinements to the basic AODV protocol. The instantiation and validation of the model is completed by means of an experimental analysis.", "authors": ["Matthias Hollick", "Jens B. Schmitt", "Christian Seipl", "Ralf Steinmetz"], "n_citation": 0, "title": "The Ad Hoc on-demand distance Vector protocol: An analytical model of the route acquisition process", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8c7bf90f-4e9f-4ff2-9a2a-edd5a1819738"}
{"abstract": "A run (or a maximal repetition) in a string is an inclusion-maximal periodic segment in a string. Let p(n) be the maximal number of runs in a string of length n. It has been shown in [8] that p(n) = O(n), the proof was very complicated and the constant coefficient in O(n) has not been given explicitly. We propose a new approach to the analysis of runs based on the properties of subperiods: the periods of periodic parts of the runs. We show that p(n) < 5 n. Our proof is inspired by the results of [4], where the role of new periodicity lemmas has been emphasized.", "authors": ["Wojciech Rytter"], "n_citation": 86, "title": "The Number of Runs in a String: Improved Analysis of the Linear Upper Bound", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8f95630d-cce9-4929-b4e9-34ba47b2b6bf"}
{"abstract": "This paper studies \u03b1-CoAgnostic learnability of classes of boolean formulas. To \u03b1-CoAgnostic learn C from H, the learner seeks a hypothesis h \u2208 H that agrees (rather than disagrees as in Agnostic learning) within a factor a of the best agreement of any f E C. Although 1-CoAgnostic learning is equivalent to Agnostic learning, this is not true for \u03b1-CoAgnostic learning for   < \u03b1 < 1. It is known that \u03b1-CoAgnostic learning algorithms are equivalent to \u03b1-approximation algorithms for maximum agreement problems. Many studies have been done on maximum agreement problems, for classes such as monomials, monotone monomials, antimonotone monomials, halfspaces and balls. We study these problems further and some extensions of them. For the above classes we improve the best previously known factors a for the hardness of \u03b1-CoAgnostic learning. We also find the first constant lower bounds for decision lists, exclusive-or, halfspaces (over the boolean domain), 2-term DNF and 2-term multivariate polynomials.", "authors": ["Nader H. Bshouty", "Lynn Burroughs"], "n_citation": 0, "title": "Maximizing agreements and CoAgnostic learning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "97a991a7-a8c7-4f18-a0cf-e0a829432695"}
{"abstract": "Motion can be an effective tool to focus user's attention and to support the parsing of complex information in graphical user interfaces. Despite the ubiquitous use of motion in animated displays, its effectiveness has been marginal at best. The ineffectiveness of many animated displays may be due to a mismatch between the attributes of motion and the nature of the task at hand. To test this hypothesis, we examined different modes of route presentation that are commonly used today (e.g. internet maps, GPS maps, etc.) and their effects on the subsequent route memory. Participants learned a route from a map of a fictitious town. The route was presented to them either as a solid line (static) or as a moving dot (dynamic). In a subsequent memory task, participants recalled fewer pertinent landmarks (i.e. landmarks at the turns) in the dynamic condition, likely due to the moving dot that focused equally on critical and less important parts of the route. A second study included a combined (i.e. both static and dynamic) presentation mode, which potentially had a better recall than either presentation mode alone. Additionally, verbalization data confirmed that the static presentation mode allocated the attention to the task relevant information better than the dynamic mode. These findings support the hypothesis that animated tasks are conceived of as sequences of discrete steps, and that the motion in animated displays inhibits the discretization process. The results also suggest that a combined presentation mode can unite the benefits of both static and dynamic modes.", "authors": ["Paul U. Lee", "Alexander Klippel", "Heike Tappe"], "n_citation": 0, "title": "The effect of motion in graphical user interfaces", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9ae45e26-7625-43e6-9dc0-d9eebea34ac2"}
{"abstract": "PSSMs (Position-Specific Score Matrices) have been applied to various problems in Bioinformatics. We study the following problem: given positive examples (sequences) and negative examples (sequences), find a PSSM which correctly discriminates between positive and negative examples. We prove that this problem is solved in polynomial time if the size of a PSSM is bounded by a constant. On the other hand, we prove that this problem is NP-hard if the size is not bounded. We also prove similar results on deriving a mixture of PSSMs.", "authors": ["Tatsuya Akutsu", "Hideo Bannai", "Satoru Miyano", "Sascha Ott"], "n_citation": 0, "title": "On the complexity of deriving position specific Score matrices from examples", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a0d1b22a-6fa2-4c7e-b883-42d3eb748ed2"}
{"authors": ["Stefan Schiffner", "Sebastian Clau\u00df", "Sandra Steinbrecher"], "n_citation": 50, "title": "Privacy and Liveliness for Reputation Systems", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "afa22f1f-7bf7-4f05-a3b4-71690f1620ee"}
{"abstract": "Informally, a public-key steganography protocol allows two parties, who have never met or exchanged a secret, to send hidden messages over a public channel so that an adversary cannot even detect that these hidden messages are being sent. Unlike previous settings in which provable security has been applied to steganography, public-key steganography is information-theoretically impossible. In this work we introduce computational security conditions for public-key steganography similar to those introduced by Hopper, Langford and von Ahn [7] for the private-key setting. We also give the first protocols for public-key steganography and steganographic key exchange that are provably secure under standard cryptographic assumptions. Additionally, in the random oracle model, we present a protocol that is secure against adversaries that have access to a decoding oracle (a steganographic analogue of Rackoff and Simon's attacker-specific adaptive chosen-ciphertext adversaries from CRYPTO 91 [10]).", "authors": ["Luis von Ahn", "Nicholas Hopper"], "n_citation": 0, "title": "Public-key steganography", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b22d2829-d7dd-4b46-bfea-8b674fac4163"}
{"abstract": "The envisioned Semantic Web aims to provide richly annotated and explicitly structured Web pages in XML, RDF, or description logics, based upon underlying ontologies and thesauri. Ideally, this should enable a wealth of query processing and semantic reasoning capabilities using XQuery and logical inference engines. However, we believe that the diversity and uncertainty of terminologies and schema-like annotations will make precise querying on a Web scale extremely elusive if not hopeless, and the same argument holds for large-scale dynamic federations of Deep Web sources. Therefore, ontology-based reasoning and querying needs to be enhanced by statistical means, leading to relevance-ranked lists as query results. This paper presents steps towards such a statistically semantic Web and outlines technical challenges. We discuss how statistically quantified ontological relations can be exploited in XML retrieval, how statistics can help in making Web-scale search efficient, and how statistical information extracted from users' query logs and click streams can be leveraged for better search result ranking. We believe these are decisive issues for improving the quality of next-generation search engines for intranets, digital libraries, and the Web, and they are crucial also for peer-to-peer collaborative Web search.", "authors": ["Gerhard Weikum", "Jens Graupmann", "Ralf Schenkel", "Martin Theobald"], "n_citation": 20, "title": "Towards a statistically Semantic Web", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b5ab78d4-3735-4371-a1d0-901d3fe2960a"}
{"abstract": "In this paper we study the learning complexity of a vast class of quantifed formulas called Relatively Quantified Generalized Formulas. This class of formulas is parameterized by a set of predicates, called a bar sis. We give a complete classification theorem, showing that every basis gives rise to quantified formulas that are either polynomially learnable with equivalence queries, or not polynomially predictable with membership queries under some cryptographic assumption. We also provide a simple criteria distinguishing the learnable cases from the non-learnable cases.", "authors": ["Andrei A. Bulatov", "Hubie Chen", "V\u00edctor Dalmau"], "n_citation": 0, "title": "Learnability of Relatively Quantified Generalized Formulas", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b67cd011-aa1e-4b52-b93e-c430890b1327"}
{"abstract": "There are many new studies on the contents for mobile devices that are all connected to networks in a seemingly ubiquitous environment. The purpose of this paper is to propose a framework for obtaining Internet content intended for computers on mobile devices. In other words, the primary concern of this paper is to select the best images for optimal performance and convert them into images that can be effectively reproduced on mobile devices. For this, the performance of the server is to be optimized through selecting useful images among the many available images and converting them while considering the display pixel rates (DPR) and the image distribution areas.", "authors": ["Daehyuck Park", "Maria Hong", "Euisun Kang", "Seongjin Ahn", "Youngsong Mun", "Young-Hwan Lim"], "n_citation": 0, "title": "Scalable Mobile Internet Servers : Selecting Useful Images from the Web for Mobile Services", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bb3ce5e8-b460-42d4-bf6e-46f9856a56c4"}
{"abstract": "In E-learning, structure analysis of lecture video is the first step for effective and efficient indexing, browsing and retrieval. A hierarchical model of narrative structure for lecture video is introduced. The root is lecture video; the next is layer of narrative elements conveying meaningful information in semantics; then is narrative features layer closely to both visual and auditory physical features. A framework is proposed to analyze narrative structure. Extraction of narrative features is described as well. Hierarchical hidden Markov model is introduced to determine the parameters and detect narrative elements automatically.", "authors": ["Yu-Chi Liu", "Xi-Dao Luan", "Yuxiang Xie", "Duan-Hui Dai", "Ling-Da Wu"], "n_citation": 0, "title": "Narrative structure analysis of lecture video with hierarchical hidden markov model for E-learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bd96b26f-b88f-4523-b3d4-c2ef7238b91f"}
{"abstract": "It is an important research topic to improve detection rate and reduce false positive rate of detection model in the field of intrusion detection. This paper adopts an improved boosting method to enhance generalization performance of intrusion detection model based on rule learning algorithm, and presents a boosting intrusion detection rule learning algorithm (BIDRLA). The experiment results on the standard intrusion detection dataset validate the effectiveness of BIDRLA.", "authors": ["Wu Yang", "Xiaochun Yun", "Yongtian Yang"], "n_citation": 0, "title": "Using boosting learning method for intrusion detection", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "be50773b-2727-42d7-815c-9afb45c65a98"}
{"authors": ["Nicolas Luyckx", "Gauthier Van Damme", "Karel Wouters"], "n_citation": 50, "title": "A PKI-based Mobile Banking Demonstrator", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "cc92518a-36c1-44cc-9770-171e4a092856"}
{"authors": ["Bart De Decker", "Mohamed Loyouni", "Hans Vangheluwe", "Kristof Verslype"], "n_citation": 50, "title": "A privacy-preserving ehealth protocol compliant with the Belgian healthcare system", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "ce13e1d2-c559-400d-9ca4-023e3440d41c"}
{"abstract": "Coevolutionary algorithms offer great promise as adaptive problem solvers but suffer from several known pathologies. Historically, spatially embedded coevolutionary algorithms seem to have succeeded where other coevolutionary approaches fail; however, explanations for this have been largely unexplored. We examine this idea more closely by looking at spatial models in the context of a particular coevolutionary pathology: loss of gradient. We believe that loss of gradient in cooperative coevolution is caused by asymmetries in the problem or initial conditions between populations, driving one population to convergence before another. Spatial models seem to lock populations together in terms of evolutionary change, helping establish a type of dynamic balance to thwart loss of gradient. We construct a tunably asymmetric function optimization problem domain and conduct an empirical study to justify this assertion. We find that spatial restrictions for collaboration and selection can help keep population changes balanced when presented with severe asymmetries in the problem.", "authors": ["R. Paul Wiegand", "Jayshree Sarma"], "n_citation": 0, "title": "Spatial embedding and loss of gradient in cooperative coevolutionary algorithms", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d50de66b-4788-419c-9105-680fcade23ed"}
{"abstract": "Transliteration of Arabic numerals is not easily resolved. Arabic numerals occur frequently in scientific and informative texts and deliver significant meanings. Since readings of Arabic numerals depend largely on their context, generating accurate pronunciation of Arabic numerals is one of the critical criteria in evaluating TTS systems. In this paper, (1) contextual, pattern, and arithmetic features are extracted from a transliterated corpus; (2) ambiguities of homographic classifiers are resolved based on the semantic relations in KorLex1.0 (Korean Lexico-Semantic Network); (3) a classification model for accurate and efficient transliteration of Arabic numerals is proposed in order to improve Korean TTS systems. The proposed model yields 97.3% accuracy, which is 9.5% higher than that of a customized Korean TTS system.", "authors": ["Youngim Jung", "Aesun Yoon", "Hyuk-Chul Kwon"], "n_citation": 0, "title": "Disambiguation based on wordnet for transliteration of arabic numerals for korean TTS", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "da6dc278-4f41-496f-9225-eb650ad96413"}
{"abstract": "The class of finitary normal logic programs-identified recently, in [1]-makes it possible to reason effectively with function symbols, recursion, and infinite stable models. These features may lead to a full integration of the standard logic programming paradigm with the answer set programming paradigm. For all finitary programs, ground goals are decidable, while nonground goals are semidecidable. Moreover, the existing engines (that currently accept only much more restricted programs [11,7]) can be extended to handle finitary programs by replacing their front-ends and keeping their core inference mechanism unchanged. In this paper, the theory of finitary normal programs is extended to disjunctive programs. More precisely, we introduce a suitable generalization of the notion of finitary program and extend all the results of [1] to this class. For this purpose, a consistency result by Fages is extended from normal programs to disjunctive programs. We also correct an error occurring in [1].", "authors": ["Piero A. Bonatti"], "n_citation": 50, "title": "Reasoning with infinite stable models II: Disjunctive programs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e27b158d-018a-4753-955a-41921a159224"}
{"abstract": "In this paper we tackle the main problem presented by the majority of Information Visualization techniques, that is, the limited number of data items that can be visualized simultaneously. Our approach proposes an innovative and interactive systematization that can augment the potential for data presentation by utilizing multiple views. These multiple presentation views are kept linked according to the analytical decisions took by the user and are tracked in a tree-like structure. Our emphasis is on developing an intuitive yet powerful system that helps the user to browse the information and to make decisions based both on overview and on detailed perspectives of the data under analysis. The visualization tree keeps track of the interactive actions taken by the user without losing context.", "authors": ["Jos\u00e9 Fernando Rodrigues", "Agma J. M. Traina", "Caetano Traina"], "n_citation": 0, "title": "Visualization tree, multiple linked analytical decisions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e464d6d1-30de-4d8b-a161-85d08f6a62b6"}
{"abstract": "Evolution Strategies, a class of Evolutionary Algorithms based on Gaussian mutation and deterministic selection, are today considered the best choice as far as parameter optimization is concerned. However, there are multiple ways to tune the covariance matrix of the Gaussian mutation. After reviewing the state of the art in covariance matrix adaptation, a new approach is proposed, in which the update of the covariance matrix is based on a quadratic approximation of the target function, obtained by some Least-Square minimization. A dynamic criterion is designed to detect situations where the approximation is not accurate enough, and original Covariance Matrix Adaptation (CMA) should rather be directly used. The resulting algorithm is experimentally validated on benchmark functions, outperforming CMA-ES on a large class of problems.", "authors": ["Anne Auger", "Marc Schoenauer", "Nicolas Vanhaecke"], "n_citation": 0, "title": "LS-CMA-ES: A second-order algorithm for Covariance Matrix Adaptation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e5585453-f8a6-4864-bf37-3b1e5afc84d5"}
{"abstract": "We propose an evolutionary approach for studying strategic agents that interact in electronic marketplaces. We describe how this approach can be used when agents' strategies are based on different methodologies, employing incompatible rules for collecting information and for reproduction. We present experimental results from a simulated market, where multiple service providers compete for customers using different deployment and pricing schemes. The results show that heterogeneous strategies evolve in the same market and provide useful research data.", "authors": ["Alexander Babanov", "Wolfgang Ketter", "Maria L. Gini"], "n_citation": 0, "title": "An evolutionary approach for studying heterogeneous strategies in electronic markets", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e5e36e4d-a5e0-4e53-895c-c0f772254b82"}
{"abstract": "We prove a general finite convergence theorem for upward-guarded fixpoint expressions over a well-quasi-ordered set. This has immediate applications in regular model checking of well-structured systems, where a main issue is the eventual convergence of fixpoint computations. In particular, we are able to directly obtain several new decidability results on lossy channel systems.", "authors": ["Christel Baier", "Nathalie Bertrand", "Philippe Schnoebelen"], "n_citation": 50, "title": "On computing fixpoints in well-structured regular model checking, with applications to lossy channel systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ee328a8c-8b34-4143-b29f-b73173a9146e"}
{"abstract": "SoC plays a key role in building software-intensive hardware systems such as embedded systems. It is commonly known that the complexity of SoC and the demand for shorter time-to-market are sharply increasing. However, current SoC methodologies do not address the trends and demands effectively. To provide SoC methodologies that satisfy these issues, the current SoC methodologies should be technically assessed so that the full requirement for future SoC methodologies can be formulated. In this paper, we first give a foundation on SoC, and summaries representative methodologies compared. For the comparison, we identified six comparison criteria and present in-depth assessments using logical reasoning in text, tables, and figures. Using the assessment, we define and elaborate three key requirements for future SoC methodologies; Balanced Process Framework, Reuse-focused Activity, and Traceability.", "authors": ["Du Wan Cheun", "Tae Kwon Yu", "Soo Ho Chang", "Soo Dong Kim"], "n_citation": 0, "title": "A Technical Assessment of SoC Methodologies and Requirements for a Full-Blown Methodology", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f2cada9c-54fb-4365-be5e-a1cc59d30419"}
{"abstract": "In modern database applications the similarity or dissimilarity of complex objects is examined by performing distance-based queries (DBQs) on data of high dimensionality. The R-tree and its variations are commonly cited multidimensional access methods that can be used for answering such queries. Although, the related algorithms work well for low-dimensional data spaces, their performance degrades as the number of dimensions increases (dimensionality curse). In order to obtain acceptable response time in high-dimensional data spaces, algorithms that obtain approximate solutions can be used. Three approximation techniques (a-allowance, N-consider and M-consider) and the respective recursive branch-and-bound algorithms for DBQs are presented and studied in this paper. We investigate the performance of these algorithms for the most representative DBQs (the K-nearest neighbors query and the K-closest pairs query) in high-dimensional data spaces, where the point data sets are indexed by tree-like structures belonging to the R-tree family: R*-trees and X-trees. The searching strategy is tuned according to several parameters, in order to examine the trade-off between cost (I/O activity and response time) and accuracy of the result. The outcome of the experimental evaluation is the derivation of the outperforming DBQ approximate algorithm for large high-dimensional point data sets.", "authors": ["Antonio Corral", "Joaqu\u00edn Ca\u00f1adas", "Michael Vassilakopoulos"], "n_citation": 0, "title": "Approximate algorithms for distance-based queries in high-dimensional data spaces using R-trees", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f53e0622-4523-48e2-90b1-d9a286d2be1c"}
{"authors": ["Reinhard Muskens", "Mehrnoosh Sadrzadeh", "Maxime Amblard", "Philippe de Groote", "Sylvain Pogodalla", "Christian Retor\u00e9"], "n_citation": 0, "title": "Context Update for Lambdas and Vectors", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "fbd7d8b5-e2c0-452c-9c6f-e6d0e880afd6"}
{"abstract": "Windowing methods are useful techniques to reduce the computational cost of Pittsburgh-style genetic-based machine learning techniques. If used properly, they additionally can be used to improve the classification accuracy of the system. In this paper we develop a theoretical framework for a windowing scheme called ILAS, developed previously by the authors. The framework allows us to approximate the degree of windowing we can apply to a given dataset as well as the gain in run-time. The framework sets the first stage for the development of a larger methodology with several types of learning strategies in which we can apply ILAS, such as maximizing the learning performance of the system, or achieving the maximum run-time reduction without significant accuracy loss.", "authors": ["Jaume Bacardit", "David E. Goldberg", "Martin V. Butz", "Xavier Llor\u00e0", "Josep Maria Garrell"], "n_citation": 0, "title": "Speeding-up Pittsburgh learning classifier systems: Modeling time and accuracy", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fff5b006-2f39-4841-91d1-d75fb77d3dfc"}
{"authors": ["Wenchao Cai", "Albert Chi Shing Chung"], "n_citation": 50, "title": "Multi-resolution vessel segmentation using normalized cuts in retinal images", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "546ad501-c139-46c8-ad3d-f3662327049c"}
{"abstract": "In a previous study, we found that the accuracy of human haptic perception of force direction is not very high. We also found an effect of physical force direction on the error subjects made, resulting in \u2018error patterns\u2019. In the current study, we assessed the between- and within-subject variation of these patterns. The within-subject variation was assessed by measuring the error patterns repeatedly over time for the same set of subjects. Many of these patterns were correlated, which indi- cates that they are fairly stable over time and thus subject-specific. The between-subject analysis, conversely, yielded hardly any significant correlations. We also measured general subject parameters that might explain this between-subject variation, but these parameters did not correlate with the error patterns. Concluding, we found that the error patterns of haptic perception of force direction are subject-specific and probably governed by an internal subject parameter that we did not yet discover.", "authors": ["van F.E. Beek", "W.M. Bergmann Tiest", "Frank L. Gabrielse", "Bart W. J. Lagerberg", "Thomas K. Verhoogt", "Bart G. A. Wolfs", "Astrid M. L. Kappers"], "n_citation": 0, "title": "Subject-specific distortions in haptic perception of force direction", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "992c54e1-3739-4fa2-8aab-43fa2539763e"}
{"authors": ["Joerg Endrullis", "Clemens Grabmayer", "R.D.A. Hendriks"], "n_citation": 0, "title": "Complexity of Fractran and Productivity", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "ee06dc11-393c-4a6c-b59a-5aa41a492156"}
{"authors": ["Serkan Kiranyazi", "Turker Ince", "Moncef Gabbouj"], "n_citation": 0, "title": "Dynamic Data Clustering Using Stochastic Approximation Driven Multi-Dimensional Particle Swarm Optimization", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "4e59dc22-359e-40c8-9eae-a7f9a33fade5"}
{"abstract": "Fisher Linear Discriminant Analysis (LDA) has recently been successfully used as a data discriminantion technique. However, LDA-based face recognition algorithms suffer from a small sample size (S3) problem. It results in the singularity of the within-class scatter matrix S w . To overcome this limitation, this paper has developed a novel subspace approach in determining the optimal projection. This algorithm effectively solves the small sample size problem and eliminates the possibility of losing discriminative information.", "authors": ["Haitao Zhao", "Pong Chi Yuen", "Jingyu Yang"], "n_citation": 7, "references": ["04159164-700d-4aa5-91f5-f8a17bebfc1a", "13f0ac10-8298-4644-8deb-73534aad3884", "2b46141b-41bf-46b8-be0c-577ac840b450", "56f4b72a-ec39-47ac-8220-899296e7fb18", "caf7b89c-4983-4187-84dd-f6b0ea626774", "f309e865-8db8-46d3-8e5c-3db8fb2c84fc"], "title": "An optimal subspace analysis for face recognition", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6d27631e-7afc-4151-b086-f38304ad902e"}
{"authors": ["Markus Esser", "Michael Gras", "Mark H. M. Winands", "Maarten P. D. Schadd", "Marc Lanctot"], "n_citation": 0, "title": "Improving Best-Reply Search", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "7b4c48c4-6b31-4c2d-b7e0-d03627d3af92"}
{"abstract": "Many web sites incorporate dynamic web pages to deliver customized contents to their users. However, dynamic pages result in increased user response times due to their construction overheads. In this paper, we consider mechanisms for reducing these overheads by utilizing the excess capacity with which web servers are typically provisioned. Specifically, we present a caching technique that integrates fragment caching with anticipatory page pre-generation in order to deliver dynamic pages faster during normal operating situations. A feedback mechanism is used to tune the page pre-generation process to match the current system load. The experimental results from a detailed simulation study of our technique indicate that, given a fixed cache budget, page construction speedups of more than fifty percent can be consistently achieved as compared to a pure fragment caching approach.", "authors": ["Suresha", "Jayant R. Haritsa"], "n_citation": 0, "title": "On reducing dynamic web page construction times", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "214192c3-6a51-4bc7-b08b-c475e4e9ad3f"}
{"authors": ["Kalle Rutanen", "Germ\u00e1n G\u00f3mez-Herrero", "Sirkka-Liisa Eriksson", "Karen O. Egiazarian"], "n_citation": 0, "title": "Least-squares transformations between point-sets.", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "26a745b9-6c17-4176-abed-13f8ff12d6c8"}
{"abstract": "Adverse reactions to drugs are a leading cause of hospitalisation and death worldwide. Most post-marketing Adverse Drug Reaction (ADR) detection techniques analyse spontaneous ADR reports which underestimate ADRs significantly. This paper aims to signal ADRs from administrative health databases in which data are collected routinely and are readily available. We introduce a new knowledge representation, Unexpected Temporal Association Rules (UTARs), to describe patterns characteristic of ADRs. Due to their unexpectedness and infrequency, existing techniques cannot perform effectively. To handle this unexpectedness we introduce a new interestingness measure, unexpected-leverage, and give a user-based exclusion technique for its calculation. Combining it with an event-oriented data preparation technique to handle infrequency, we develop a new algorithm, MUTARA, for mining simple UTARs. MUTARA effectively short-lists some known ADRs such as the disease esophagitis unexpectedly associated with the drug alendronate. Similarly, MUTARA signals atorvastatin followed by nizatidine or dicloxacillin which may be prescribed to treat its side effects stomach ulcer or urinary tract infection, respectively. Compared with association mining techniques, MUTARA signals potential ADRs more effectively.", "authors": ["Huidong Jin", "Jie Chen", "Chris Kelman", "Hongxing He", "Damien McAullay", "Christine M. O'Keefe"], "n_citation": 0, "title": "Mining unexpected associations for signalling potential adverse drug reactions from administrative health databases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3888772f-4a9a-40b3-8b94-51758acca990"}
{"abstract": "Much research has been done on discovering interesting and frequent user access patterns from web logs. Recently, a novel data structure, known as Web Access Pattern Tree (or WAP-tree), was developed. The associated WAP-mine algorithm is obviously faster than traditional sequential pattern mining techniques. However, WAP-mine requires re-constructing large numbers of intermediate conditional WAP-trees during mining, which is also very costly. In this paper, we propose an efficient WAP-tree mining algorithm, known as CS-mine (Conditional Sequence mining algorithm), which is based directly on the initial conditional sequence base of each frequent event and eliminates the need for re-constructing intermediate conditional WAP-trees. This can improve significantly on efficiency comparing with WAP-mine, especially when the support threshold becomes smaller and the size of database gets larger.", "authors": ["Baoyao Zhou", "Siu Cheung Hui", "A.C.M. Fong"], "n_citation": 0, "title": "CS-mine: An efficient WAP-tree mining for Web Access Patterns", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "544696cb-36cf-405c-9188-06ac73335ff4"}
{"abstract": "So far, query routing strategies of unstructured P2P system are described qualitatively or conducted expensively. In this paper, we propose an adaptive query routing method by using quantitative information in the form of probabilistic knowledge for the purpose of (1) maximizing the likelihood of locating desired resource, and (2) using feedback from previous user queries to update the probabilistic information for guiding future ones. To achieve the goal, two kinds of probabilistic information are considered: information about overlap between topics and coverage and completeness of each peer. A declarative formalism for specifying the two kinds -of probabilistic information is described, and then the algorithms for-using and maintaining such information are presented. Finally, a preliminary experiment is conducted to evaluate the efficiency and effectiveness of our proposed approach.", "authors": ["Linhao Xu", "Chenyun Dai", "Wenyuan Cai", "Shuigeng Zhou", "Aoying Zhou"], "n_citation": 0, "title": "Towards adaptive probabilistic search in unstructured P2P systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6745bf08-4330-45ae-9fa2-bcbc4682efaa"}
{"abstract": "XML is becoming the de-facto standard for exchanging information in distributed applications and services. This has resulted in the development of a large number of XML documents with their associated schemas, such as DTD and XML Schema. A major challenge in using XML Schema is the difficulty in reading and understanding even a relatively small XML Schema because of its textual nature and its XML syntax. In this paper, we present transformations from textual XML Schema to graphical UML to facilitate understanding of XML Schema. Our transformation approach is unique in that we focus on all thirteen building blocks of XML Schema and is based on existing UML notation without introducing new stereotypes that would require additional user training.", "authors": ["Flora Dilys Salim", "Rosanne Price", "Maria Indrawan", "Shonali Krishnaswamy"], "n_citation": 0, "title": "Graphical representation of XML Schema", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "904664d7-4c3e-4357-beaf-b8d0a856f996"}
{"abstract": "Mining frequent traversal patterns is to discover the consecutive reference paths traversed by a sufficient number of users from web logs. The previous approaches for mining frequent traversal patterns need to repeatedly scan the traversal paths and take a large amount of computation time to find frequent traversal patterns. However, the discovered frequent traversal patterns may become invalid or inappropriate when the databases are updated. In this paper, we propose an incremental updating technique to maintain the discovered frequent traversal patterns when the user sequences are inserted into or deleted from the database. Our approach partitions the database into some segments and scans the database segment by segment. For each segment scan, the candidate traversal sequences that cannot be frequent traversal sequences can be pruned and the frequent traversal sequences can be found out earlier. Besides, the number of database scans can be significantly reduced because some information can be computed by our approach. The experimental results show that our algorithms are more efficient than other algorithms for the maintenance of mining frequent traversal patterns.", "authors": ["Show-Jane Yen", "Yue-Shi Lee"], "n_citation": 0, "title": "An incremental updating technique for discovering frequent traversal patterns", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b57abc26-56f2-4976-ba07-1cdcc8367a53"}
{"abstract": "In this paper, we define the notion of social classes of data nodes and present a framework of query evaluation in which both positions and social classes of data nodes are used with structural joins to further improve query performance. A social class of a data node is defined as an equivalence class induced by tags of other nodes that are associated with the given node in a given structural relation. In our framework, social classes of data nodes are obtained during data loading. Then during query compilation, queries are analyzed to determine required structural relations among query nodes and to derive required social classes for each individual query node. The positions of data nodes, the social classes of data nodes, and the required social classes of query nodes are used during query evaluation to provide an effective mechanism for filtering and indexing XML data. We present a number of algorithms that implement this framework and report on results from our experiments. Our results show that this new method could substantially improve performance of XML queries that require multiple structural joins.", "authors": ["Weining Zhang", "Douglas Pollok"], "n_citation": 0, "title": "Improving XML query performance using social classes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b829e2f3-1e51-4fee-8379-f1b0103dd83f"}
{"abstract": "An arrangement of n lines chosen at random from R 2  has a vertex set whose convex hull has constant (expected) size.", "authors": ["Mordecai J. Golin", "Stefan Langerman", "William W. Steiger"], "n_citation": 0, "title": "The convex hull for random lines in the plane", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c9c5c0b6-b7fc-4945-a65a-a662f520f7a8"}
{"abstract": "This paper proposes a simple Hybrid Robust Hash Routing Web Caching System(2HRCS) that clients can directly perform object allocation and load balancing without an additional DNS for load balancing. The proposed system reduces the cost of setup and operation by removing DNS that needs to balance the load in the existing system and does the original role of the DNS server for the shared proxies. The proposed system has clients with a consistent hashing method, so it extends its environment to other distributed web caching system that has caches of different capacity. A distributed web caching system is composed and tested to evaluate the performance. As a result, it shows superior performance to consistent hashing systems. Since this system can maintain performance of the existing system and reduce costs, it also has the advantage of constructing medium or small size CDN (Contents Delivery Network).", "authors": ["Jong Ho Park", "Kil To Chong"], "n_citation": 0, "title": "A simple client-based Hybrid Robust Hash Routing Web Caching System (2HRCS)", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "db001a84-629f-411e-b487-8a10b0fe4b21"}
{"abstract": "XQuery has been recently proposed by W3C as a standardized query language for XML. Because of its complexity, optimizing XQuery is difficult and requires more complicated optimization schemes than those for traditional query languages. This paper first proposes an XML data model and query algebra to provide a semantics and a means of optimization for the XQuery language. Following the language algebra, algebraic transformation (with optimization rules) is presented. Finally, physical algebraic operators are also defined to specify queries in this algebra into physical query plans.", "authors": ["Damien K. Fisher", "Franky Lam", "Raymond K. Wong"], "n_citation": 0, "title": "Algebraic transformation and optimization for XQuery", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e045baf1-36a6-49ee-a82d-a093ea567a77"}
{"abstract": "We propose a Semantic, Hierarchical, Online Clustering (SHOC) approach to automatically organizing Web search results into groups. SHOC combines the power of two novel techniques, key phrase discovery and orthogonal clustering; to generate clusters which are both reasonable and readable. Moreover; SHOC can work for multiple languages: not only English but also oriental languages like Chinese. The main contribution of this paper includes the following. (1) The benefits of using key phrases as Web document features are discussed. A key phrase discovery algorithm based on suffix array is presented. This algorithm is highly effective and efficient no matter how large the language's alphabet is. (2) The concept of orthogonal clustering is proposed for general clustering problems. The reason why matrix Singular Value Decomposition (SVD) can provide solution to orthogonal clustering is strictly proved. The orthogonal clustering has a solid mathematics foundation and many advantages over traditional heuristic clustering algorithms.", "authors": ["Dell Zhang", "Yisheng Dong"], "n_citation": 0, "title": "Semantic, Hierarchical, Online Clustering of Web search results", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fc445e57-2aa9-4e10-abdb-a16e67f5a27d"}
{"abstract": "SVMs (support vector machines) have met with a significant success in information retrieval field, especially handling text classification tasks. Although various performance estimators for SVMs have been proposed, they only focus on the accuracy based on the LOO (leave-one-out) cross validation procedure. The information-retrieval-related performance measures are always neglected in kernel learning methodology. In this paper, we have proposed a set of information-retrieval-oriented performance estimators for SVMs, which are based on the span bound of the LOO procedure. Experiments have proved that our proposed estimators are both effective and stable.", "authors": ["Shui Yu", "Hui Song", "Fanyuan Ma"], "n_citation": 1, "title": "Novel SVM performance estimators for information retrieval systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "04248689-4e80-4071-a059-45db99a5fd61"}
{"abstract": "In this paper, we propose an efficient query routing approach for XML documents retrieval in unstructured Peer-to-Peer networks. In our approach, when a node forwards a query specified by XPath expression(XPE), some information about matched document that this node provides is attached to the query message. Then the nearby nodes receiving the messages create an index structure based on this relevant information, termed Routing Guide(RG). With RGs, a node forwards queries only to a subset of. its neighbors which are more likely to have matched documents rather than by selecting neighbors randomly or by selecting all. Simulative experiments show that our approach outperforms most search techniques currently in use(e.g. BFS and random walk) especially when the number of the queries is very large.", "authors": ["Deqing Yang", "Linhao Xu", "Wenyuan Cai", "Shuigeng Zhou", "Aoying Zhou"], "n_citation": 0, "title": "Efficient query routing for XML documents retrieval in unstructured Peer-to-Peer networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1036dd9c-a2a0-477e-a9c9-6b637409aef8"}
{"abstract": "Text Categorization is the process of automatically assigning predefined categories to free text documents. Feature weight, which calculates feature (term) values in documents, is one of important preprocessing techniques in text categorization. This paper is a comparative study of feature weight methods in statistical learning of text categorization. Four methods were evaluated, including tf*idf, tf*CRF, tf*OddsRatio, and tf*CHI. We have evaluated these methods on benchmark collection Reuters-21578 with Support Vector Machines (SVMs) classifiers. We found that tf*CHI is most effective in our experiments. Using tf*CHI with a SVMs classifier yielded a very high classification accuracy (87.5% for micro-average F 1  and 87.8% for micro-average break-even point). tf*idf, which is widely used in text categorization, compares favorably with tf*CRF but is not as effective as tf*CHI and tf*OddsRatio.", "authors": ["Zhi-Hong Deng", "Shiwei Tang", "Dongqing Yang", "Ming Zhang", "Li-Yu Li", "Kunqing Xie"], "n_citation": 0, "title": "A comparative study on feature weight in text categorization", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "15b0d060-08a2-4b12-b5cf-034d146d92bd"}
{"authors": ["Ua Buy", "Robert N. Moll"], "n_citation": 0, "title": "A PROOF LATTICE-BASED TECHNIQUE FOR ANALYZING LIVENESS OF RESOURCE CONTROLLERS", "venue": "Lecture Notes in Computer Science", "year": 1991, "id": "3d024e96-a41e-44e0-a970-68483df6c2d3"}
{"abstract": "Constraints are an important aspect of role-based access control (RBAC). Constraints have to be satisfied when an administrator wants to assign (revoke) a role to a user or a permission to a role. The importance of constraints associated with user-role assignments and permission-role assignments in RBAC has been recognized but the modelling of these constraints has not been received much attention. In this paper we use a de facto constraints specification language in software engineering to analyze the constraints in user-role assignments and permission-role assignments. We show how to represent role-based access constraints with object constraint language (OCL) and discuss the future work. Finally, comparisons with other related work are presented.", "authors": ["Hua Wang", "Yanchun Zhang", "Jinli Cao", "Jian Yang"], "n_citation": 0, "title": "Specifying role-based access constraints with object constraint language", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "62132693-8d10-434b-8d36-d66719bac944"}
{"abstract": "RoboCup Simulation Server provides a wonderful challenge for all the participants. This paper explains key technology implemented by Tsinghuaeolus RoboCup team played in RoboCup environment, including basic adversarial skills, which is developed by using Dynamic Programming in combination with heuristic search algorithm, and reactive strategy architecture. Tsinghuaeolus was the winner of RoboCup2001.", "authors": ["Jinyi Yao", "Jiang Chen", "Yunpeng Cai", "Shi Li"], "n_citation": 0, "title": "Architecture of TsinghuAeolus", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "795d3012-1235-4a0c-aeb7-8f9f5d5562af"}
{"abstract": "Duplication of Web pages greatly hurts the perceived relevance of a search engine. Existing methods for detecting duplicated Web pages can be classified into two categories, i.e. offline and online methods. The offline methods target to detect all duplicates in a large set of Web pages, but none of the reported methods is capable of processing more than 30 million Web pages, which is about 1% of the pages indexed by today's commercial search engines. On the contrary, the online methods focus on removing duplicated pages in the search results at run time. Although the number of pages to be processed is smaller, these methods could heavily increase the response time of search engines. Our experiments on real query logs show that there is a significant difference between popular and unpopular queries in terms of query number and duplicate distributions. Then, we propose a hybrid query-dependent duplicate detection method which combines both advantage of offline and online methods. This hybrid method provides not only an effective but also scalable solution for duplicate detection.", "authors": ["Shaozhi Ye", "Ruihua Song", "Ji-Rong Wen", "Wei-Ying Ma"], "n_citation": 0, "title": "A query-dependent duplicate detection approach for large scale search engines", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "82d68749-f431-4269-b336-e0e72fbdc1a2"}
{"abstract": "We propose a specification of moving objects query language over GML for LBS extended by XQuery. Conventional query languages over GML have been focused on the development of interoperable Web GIS, the enrichment of existing XML language with spatial operators, and etc. However, these studies are not suitable for direct query of GML repositories by the lack of alignment with XQuery. Also, there is no consideration on the query language over GML to support moving objects query for LBS. Therefore, we show the data model, the algebra, the set of operators and representative query examples underlying the proposed moving objects query language to specify moving objects features. Finally, we apply the proposed query language to tornado monitoring system to show that this language comes into use in various LBS applications effectively.", "authors": ["Warnill Chung", "Hae-Young Bae"], "n_citation": 0, "title": "A specification of a moving objects query language over GML for location-based services", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "86d3d5e1-04c9-4b68-b58a-c01ebb96f23a"}
{"authors": ["Yuanyuan Zhang", "Yasushi Inoguchi"], "n_citation": 1, "title": "Influence of performance prediction inaccuracy on task scheduling in grid environment", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9c893c24-6240-42d8-af47-69e8e478324b"}
{"authors": ["Jesper Nederlof", "E. van Leeuwen", "G.J. van der Zwaan", "Raji\u0107 Branislav", "Vladimiro Sassone", "P. Widmayer"], "n_citation": 50, "title": "Reducing a target interval to a few exact queries", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "a1e8f9a6-ca3b-40f0-a2f8-a00c95139f1d"}
{"abstract": "We present an original concept for patch generation: we propose to do it directly in production. Our idea is to generate patches on-the-fly based on automated analysis of the failure context. By doing this in production, the repair process has complete access to the system state at the point of failure. We propose to perform live regression testing of the generated patches directly on the production traffic, by feeding a sandboxed version of the application with a copy of the production traffic, the \"shadow traffic\". Our concept widens the applicability of program repair, because it removes the requirements of having a failing test case.", "authors": ["Thomas Durieux", "Youssef Hamadi", "Martin Monperrus"], "n_citation": 0, "title": "Production-Driven Patch Generation", "venue": "international conference on software engineering", "year": 2017, "id": "b2809942-5605-4b8b-b9e0-a8eb3e1e574d"}
{"abstract": "This paper describes multiple schemes to improve TCP congestion control method under wireless and mobile situations. According to wireless situation, the paper discusses multiple control methods in several categories by analyzing and comparing their characteristics. Based on the simulation analysis on multiple TCP congestion control methods, the paper proposes TCPW+SACK method to improve TCP congestion control. According to mobile situation, the paper simulated five typical TCP congestion control mechanisms, which are Reno; New Reno, SACK, TCPW, and TCPW+SACK, on condition of Cellular handoff and Mobile IP handoff. The analysis and comparison of these methods points out that TCPW+SACK can achieve better performance on Cellular handoff but no prominent improvement on Mobile IP. The paper also points out that two phenomena exist in mobile handoff situation, which are Quiet Idle and Inefficient Retransmission.", "authors": ["ShiNing Li", "Jiping Fang", "Fan Yu"], "n_citation": 0, "title": "On analysis and comparison performance of TCP in wireless network", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c1251c57-5b0f-40f9-9938-ee998f6f8d0d"}
{"abstract": "Query Refinement is an effective information retrieval technique that interactively provides users with new keywords related to a particular query. Chen, et al. ([7]) proposed a concept called coverage to solve the problem that hundreds of thousands of keywords are presented as candidates due to their presence in the relevant documents. By this concept, all keywords are divided into two parts, that is, prime keywords and non-prime keywords. Refinement candidates are chosen only from the prime keywords which compose a very small subset of all the keywords. In this paper, we proposed an algorithm of representing non-prime keywords effectively and efficiently, which is remained as an open problem in the previous work. A Web-based prototype system is implemented to show the feasibility of the refinement system. Except for the refinement, our system behaves exactly as most of today's keyword-based search engines on the Internet. Experiments we conducted with a kinds of datasets confirm the effectiveness and efficiency in candidate reduction.", "authors": ["Chaoyuan Cui", "Hanxiong Chen", "Kazutaka Furuse", "Nobuo Ohbo"], "n_citation": 0, "title": "Web query refinement without information loss", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d166a316-1451-44af-9c26-5bc1f1e11c25"}
{"authors": ["Jonida Milaj", "Jeanne Mifsud Bonnici", "Stefan Schiffner", "Jetzabel Serna", "Demosthenes Ikonomou", "Kai Rannenberg"], "n_citation": 0, "title": "Smart meters as non-purpose built surveillance tools", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "d723eac4-5589-4ba0-8d8c-4d8f68cd18de"}
{"abstract": "With the growing importance of XML in data exchange, much research tends to provide a compact labeling scheme and a flexible query facility to extract data from dynamic XML trees. In this paper, we first propose GRP, namely GRoup based Prefix labeling scheme. Compared to the previous labeling schemes, the total size of labels in GRP is much shorter. Experiment results with synthetic and real life data show that the size of labels with GRP is about 2%-10% of that with the previous labeling scheme. Based on GRP, we further propose GRJ (GRoup based structural Join), a structural join algorithm. GRJ is similar to the hash join algorithm in RDBMS and needs to scan the join data only twice. Furthermore, unlike other structural join algorithms, GRJ can perform efficiently without dependency on the join data sorted or indexed, for the data in the dynamic XML trees are usually unsorted. Finally our experiments show that GRJ is efficient in supporting structural joins on the context of dynamic XML trees.", "authors": ["Jiaheng Lu", "Tok Wang Ling"], "n_citation": 0, "title": "Labeling and querying dynamic XML trees", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d86a93fc-66cc-42e8-974a-ba2d836d4a74"}
{"authors": ["Jussi Hakanen", "Joshua D. Knowles"], "n_citation": 0, "references": ["27611301-687a-4225-9bb8-fb38aa369b48", "298701cf-cf9e-4cdf-a918-1d28885469b1", "3c2dd6af-0684-4fe6-b3a3-e06a245fbbe2", "4178199a-8cbc-4cc4-acf3-30f90719060a", "5c6951bc-8dc5-4ffa-8cb9-f7434ab468a2", "809a3639-ed02-49f0-b289-28d7f8fe3102", "b01c8a88-d46e-40f2-9867-c5eff42866b7", "bde534aa-984c-48f3-bef1-7be447885134", "f5a0cf6b-a5f4-4dbe-a4eb-a398e7229bdd"], "title": "On Using Decision Maker Preferences with ParEGO", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "eff5f8a1-5964-413e-a3d2-52765ba6a46d"}
{"abstract": "One key idea of semantic web is that the content of the web is usable to machines (i.e. software agents). On the semantic web, data interoperability and ontology heterogeneity between agents are becoming ever more important issues. This paper presents a multi-strategy learning approach to resolve these problems. In this paper we describe the SIMON (Semantic Interoperation by Matching between ONtologies) system, which applies multiple classification methods to learn the matching between ontologies. We use the general statistic classification method to discover category features in data instances and use the first-order learning algorithm FOIL to exploit the semantic relations among data instances. On the prediction results of individual methods, the system combines their outcomes using our matching committee rule called the Best Outstanding Champion. The experiments show that SIMON system achieves high accuracy on real-world domain.", "authors": ["Leyun Pan", "Shui Yu", "Fanyuan Ma"], "n_citation": 0, "title": "SIMON: A multi-strategy classification approach resolving ontology heterogeneity on the semantic Web", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f19a793c-b105-46f3-8df9-35a19e6fc03d"}
{"authors": ["An Lu", "Wilfred Ng"], "n_citation": 0, "title": "Mining hesitation information by vague association rules", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "05bc207e-be3e-4886-be2c-a39f6456691e"}
{"authors": ["Shu Liao", "Albert Chi Shing Chung"], "n_citation": 111, "title": "Face recognition by using elongated local binary patterns with average maximum distance gradient magnitude", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "0b77eb6d-7e34-416c-b764-eaed8c666706"}
{"abstract": "This paper proposes a new method of reconstructing high-resolution facial image from a low-resolution facial image using a recursive error back-projection of example-based learning. A face is represented by a linear combination of prototypes of shape and texture. With the shape and texture information about the pixels in a given low-resolution facial image, we can estimate optimal coefficients for a linear combination of prototypes of shape and those of texture by solving least square minimization. Then high-resolution facial image can be reconstructed by using the optimal coefficients for linear combination of the high-resolution prototypes. Moreover recursive error back-projection is applied to improve the accuracy of high-resolution reconstruction. An error back-projection is composed of estimation, simulation, and error compensation. The encouraging results of the proposed method show that our method can be used to improve the performance of the face recognition by applying our method to enhance the low-resolution facial images captured at visual surveillance systems.", "authors": ["Jeong-Seon Park", "Seong-Whan Lee"], "n_citation": 0, "references": ["52dd3c93-336c-46c7-ab7f-d5a01fc7ac2f"], "title": "Reconstruction of High-Resolution Facial Image Using Recursive Error Back-Projection.", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1d63cc33-e77f-4986-afc5-38577d4e7aec"}
{"authors": ["Qinyi Wang", "Jieying She", "Tianshu Song", "Yongxin Tong", "Lei Chen", "Ke Xu"], "n_citation": 0, "title": "Adjustable Time-Window-Based event detection on twitter", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "36276644-519f-4526-9b5a-888852abeb4c"}
{"abstract": "With the development of e-commerce and information access, recommender systems have become a popular technique to prune large information spaces so that users are directed toward those items that best meet their needs and preferences. In this paper 1 , clustering technique is applied in the collaborative recommender framework to consider semantic contents available from the user profiles. We also suggest methods to construct user profiles from rating information and attributes of items to accommodate user preferences. Further, we show that the correct application of the semantic content information obtained from user profiles does enhance the effectiveness of collaborative recommendation.", "authors": ["Qing Li", "Byeong Man Kim"], "n_citation": 0, "title": "Constructing user profiles for collaborative recommender system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4a821c9a-045f-4fb3-9bc2-9cc9c47b5703"}
{"abstract": "Keywords provide rich semantic information for documents. It benefits many applications such as topic retrieval, document clustering, etc. However, there still exist a large amount documents without keywords. Manually assigning keywords to existing documents is very laborious. Therefore it is highly desirable to automate the process. Traditional methods are mainly based on a predefined controlled-vocabulary, which is limited by unknown words. This paper presents a new approach based on Bayesian decision theory. The approach casts keyword distillation to a problem of loss minimization. To determine which word can be assigned as keywords becomes a problem to estimate the loss. Feature selection is one of the most important issues in machine learning. Several plausible attributes are always be assigned as the learning features, but they are all based on the assumption of words' independence. Machine learning based on them dose not produce satisfactory results. In this paper, taking the words' context and linkages between words into account, we extend the work of feature selection. Experiments show that our approach significantly improves the quality of extracted keywords.", "authors": ["Jie Tang", "Li Juanzi", "Kehong Wang", "Yue-Ru Cai"], "n_citation": 0, "title": "Loss minimization based keyword distillation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "533fd524-e90d-41ac-8bdd-44a5f34e3196"}
{"abstract": "A lot of wired and wireless home networking technologies have been proposed in the literature. However, an effective middleware is needed to adopt all kinds of multiple home networking technologies at home. All the home appliances can be integrated and controlled using the architecture that leverages TCPIIP and Web technologies, which is like Universal Plug and Play (UPNP) home networking technology. In this paper, UPnP technology and IEEE1394 technology are introduced, and UPnP over 1394 is proposed. The results from the implementation show that it is possible to control IEEE 1394 device through the IP-based UPnP technology instead of just using a direct AV/C command set. For simulation, a model of UPnP-enabled 1394 device is developed, as the UPnP-enabled 1394 devices are not available in the market. The simulation results show that a promising UPnP home networking technology can be applicable to a home gateway in the future.", "authors": ["Seung-Hak Rhee", "Soo-Kyung Yang", "Seung-Jin Park", "Jonghun Chun", "Jong-An Park"], "n_citation": 0, "title": "UPnP home networking-based IEEE1394 digital home appliances control", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5abaa9b8-08bd-44ee-9681-0560745fa4d6"}
{"abstract": "In recent years, more and more information appeared on the web. Extracting information from the web and converting them into regular format become significantly important work. After observing a number of web sites, we found that most of useful information is contained in the web sources, which have a large number of similarly structured web documents. So in this paper we present an approach for discovering the useful information sources from the web and extracting information from them. A useful web information source discovering method and a novel information extraction method are proposed. We also develop a prototype system WIEAS (Web Information Extraction, Analysis And Services) to implement our idea, and use the information extracted by WIEAS to provide plentiful services.", "authors": ["Liyu Li", "Shiwei Tang", "Dongqing Yang", "Tengjiao Wang", "Zhi-Hong Deng", "Zhihua Su"], "n_citation": 0, "title": "WIEAS: Helping to discover web information sources and extract data from them", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6228a336-cd1e-49d1-ab28-7edc9220fbf2"}
{"authors": ["Jiri Kosinka", "Bert J\u00fcttler", "Ralph Robert Martin", "Malcolm A. Sabin", "Joab R. Winkler"], "n_citation": 0, "title": "MOS surfaces: Medial surface transforms with rational domain boundaries", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "69a2d6bb-9910-43c9-a54a-a778f45be6a9"}
{"abstract": "Content delivery grid is a new kind of service-oriented grid different from computational grid. We propose a novel service model of content delivery grid in this paper, and then describe the main components, key services, relationships and parameters of the modeled content delivery environment. These items make content delivery grid really stand out from other traditional content delivery models. Furthermore, this paper describes the delivery process of this model. As part of this study, this paper demonstrates a prototype system design based on this novel service model, and analyses the contractive experiment results obtained for the prototype system. At last, we analyze the prospective research direction and challenges.", "authors": ["Zhihui Lv", "Shiyong Zhang", "Yiping Zhong"], "n_citation": 0, "title": "Research on service model of content delivery grid", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "716caa2a-46bb-475a-bb6c-f15983d1b392"}
{"abstract": "Recently, semantic caching for the database-backed Web applications has received much attention. Since emergence of XML as a standard for data exchange on the Web, today's Web applications are to retrieve information from the remote XML source across the network. Cache-answerability for XML queries is thus crucial for efficient support of XML database-backed Web applications. In this paper, we address two issues involved in realizing such functionality: incremental refresh of the XML cache against the updates at XML source, and integration of the partial query results obtained from the cache and from the source..", "authors": ["Jung Kee Park", "Hyunchul Kang"], "n_citation": 0, "title": "Issues in cache-answerability for XML queries on the Web", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "72812540-622a-47b6-822c-a0c855a55e37"}
{"authors": ["Stefan Eppe", "Yves De Smet"], "n_citation": 0, "references": ["4029712a-b285-4549-87e7-f9f79a44e413", "481d49c8-d8b7-4ca1-b966-5e12b32e485f", "987a291c-4604-4af9-80f3-5e47894d3110", "b1b0d345-2a3a-427f-8851-68f7ef04d1b6", "e6d93ed2-1843-4dd0-8a1e-bd4030777089"], "title": "On the Influence of Altering the Action Set on PROMETHEE II\u2019s Relative Ranks", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "89e55ad7-caf9-4444-99d8-8cdda07d3375"}
{"abstract": "In this paper, Community management and some optimization strategies of Web Services are presented based on the Peer-Serv framework, a decentralized infrastructure to apply Web Services in Peer-to-Peer (P2P) environments. With respect to the essential issues of Community management of services, our work focuses on the improvements and corresponding optimizations, as well as some novel strategies. Further, for the first time some efficient strategies of Community Reorganization, i.e. Simple, Threshold, and Demand-Driven Reorganization, are proposed, as well as the Community Saturation based method. What's more, a criterion called DoSic is also proposed to measure the satisfaction between providing and demanding of services in a Community. Preliminary experiments show our methods are effective, while they are suitable for other cases of the management of Web Services in P2P environments.", "authors": ["Zhirning Pan", "Yizhong Wu", "Kun Yue", "Xiaoling Wang", "Aoying Zhou"], "n_citation": 0, "title": "Efficient Community management and optimization strategies of Web Services in Peer-to-Peer environments", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "966d9ceb-c430-4dab-958c-435165527ecb"}
{"abstract": "Experience suggests that fully automated schema matching is infeasible, especially for n-to-m matches involving semantic functions. It is therefore advisable for a matching algorithm not only to do as much as possible automatically, but also to accurately identify the critical points where user input is maximally useful. Our matching algorithm combines several existing approaches, with a new emphasis on using the context provided by the way elements are embedded in paths. A prototype tested on biological data (gene sequence, DNA, RNA, etc.) and on bibliographic data, shows significant performance improvements from user feedback and context checking. In non-interactive mode on the purchase order schemas, it compares favorably with COMA, the most mature schema matching system in literature, and also correctly identifies critical points for user input.", "authors": ["Guilian Wang", "Joseph A. Goguen", "Young-Kwang Nam", "Kai Lin"], "n_citation": 0, "title": "Critical points for interactive schema matching", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a40d7ace-ca9a-4402-9eaa-07ebf0ef5bcb"}
{"abstract": "The rapid growth of on-line information including multimedia contents during the last decade caused a major problem for Web users - there is too much information available, most of it poorly organized and hard to find. To help a user to find proper information, web news search functions are devised and developed. Although those search engines provide some solutions, users still suffer from reading huge amounts of hyperlinks. Also, users of new media now have great expectations of what they can see on the Web. To provide better user satisfaction, we proposed a story model (story structures) that can be dynamically instantiated for different user requests from various multi-modal elements. The proposed story model defines four domain-independent story types. We compared traditional web news search functions and our story model by using usability test. The result shows that our multimedia presentation methodology is significantly better than the current search functions.", "authors": ["Hyun Woong Shin", "Dennis McLeod", "Larry Pryor"], "n_citation": 0, "title": "A new methodology for information presentations on the web", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "caf73e90-e43e-40b7-94ca-e13cb946c7ff"}
{"authors": ["Fabrizio Frati"], "n_citation": 0, "title": "A Lower Bound on the Area Requirements of Series-Parallel Graphs", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "cd7e89d9-0ec3-4dba-a7c2-365962c84694"}
{"abstract": "Web applications such as personalization and recommendation have raised the concerns of people because they are crucial to improve customer services, particularly for E-commerce Websites. Understanding customer preferences and requirements in time is a premise to optimize these Web services. In this paper, a new data model for Web data is introduced to analyze user behavior. The merit of the cube model is that it not only aggregates user access information but also takes the Web structure information into account. Based on the model, we propose some solutions to intelligently discover interesting user access patterns for Website optimization, Web personalization and recommendation. We used the Web usage data from a sports Website in China to evaluate the effectiveness of the model. The results show that this integrated data model is effective and efficient to apply into practical Web applications.", "authors": ["Edmond HaoCun Wu", "Michael K. Ng", "Joshua Huang"], "n_citation": 0, "title": "An efficient multidimensional data model for Web usage mining", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e90f4d64-a937-4f23-a7cf-82ba17695808"}
{"authors": ["van Ja Jeroen Oijen", "Han La Poutr\u00e9", "F. Dignum", "J\u00f6rg P. M\u00fcller", "Massimo Cossentino"], "n_citation": 50, "title": "Agent Perception within CIGA: Performance Optimizations and Analysis", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "f817878f-c086-4228-ada8-ebd0b416e66d"}
{"authors": ["W. Ng"], "n_citation": 50, "title": "Prioritized preferences and choice constraints", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "159286a1-23a3-4608-8ee0-6fe5d071306a"}
{"abstract": "Support Vector Machines (SVMs) have become a popular tool for learning with large amounts of high dimensional data. But it may sometimes be preferable to learn incrementally from previous SVM results, as SVMs which involve the solution of a quadratic programming problem suffer from the problem of large memory requirement and CPU time when trained in batch mode on large data sets. And the SVMs may be used in online learning setting. In this paper an approach for incremental learning with Support Vector Machines is presented. We define the normal solution of the incremental learning for SVMs which is defined as the solution minimizing a given positive-definite quadratic form in the coordinates of the difference vector between the normal vectors at the (k-1)-th and k-th incremental step and discuss the relation to standard SVM. It was shown that concept learned at last step will not change if new data satisfy separable condition and empirical evidence is given to prove that this approach can effectively deal with changes in the target concept that are results of the incremental learning setting according to three evaluation criteria: stability, improvement and recoverability.", "authors": ["Yangguang Liu", "Chen Q", "Yongchuan Tang", "Qinming He"], "n_citation": 0, "title": "An incremental updating method for Support Vector Machines", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1e51e4d5-1bca-4501-ae50-e4b898b678e7"}
{"abstract": "To gain higher performance under many constraints, effective scheduling is a key concern in data-intensive grid computing. Based on a Dual-Component and Dual-Queue Distributed Schedule Model (DCDQDSM), we present task and data co-scheduling algorithms, by which the waiting time to access datasets for the scheduled task will reduce. Firstly data replication and elimination schedule are processed by an independent approach. Secondly, if a task is divisible, the task and its dataset are divided into subtasks and their necessary data subsets. Task scheduling adopts a general approach. Finally, when a scheduled task/subtask doesn't hit its dataset, associated data transferring is bound to this task. On the basis of relation between task execution and data access, data replication and computing may proceed concurrently in one scheduled task with divisible dataset or between scheduled tasks. Corresponding theoretic analysis and experimental results suggest that the scheduling algorithms improve execution performance and resource utilization.", "authors": ["Changqin Huang", "Deren Chen", "Yao Zheng", "Hualiang Hu"], "n_citation": 0, "title": "Performance-driven task and data co-scheduling algorithms for data-intensive applications in grid computing", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6260f221-d70f-4a48-9668-8f486ac6ec09"}
{"abstract": "Electronic transaction protocols have been found with subtle flaws. Recently, model checking has been used to verify electronic transaction protocols for the limitations of low efficiency and error prone in the traditional approaches. This paper proposes an extendable verification model to especially validate electronic transaction protocols. In particular, the verification model is able to deal with the inconsistency in transmitted messages. Thus, we can measure the incoherence in secure messages coming from different sources and at different moments and ensure the validity of verification result. We analyze two instances by using this model. The analyses uncover some subtle flaws in the protocols.", "authors": ["Qingfeng Chen", "Chengqi Zhang", "Shichao Zhang"], "n_citation": 0, "title": "A verification model for electronic transaction protocols", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8754ed2e-4bd6-4248-9c85-e2b5d28ced82"}
{"abstract": "Most existing approaches for similarity search did not consider applying calendar concept hierarchy to search for similar patterns from time series data. In this paper, we present two techniques that capture scale oriented features of time series and provide an analyzing method for the multi-resolution view along the time dimension. Especially, we propose that a similarity search which makes the most of calendar concept hierarchy involves three stages which consist of data cube count based on time concept hierarchy, sequence division by time level and feature vector extraction. Because these feature vectors are inserted into multi-dimensional index, pre-processing step executes only one time at the beginning of the search process without adding considerable computing cost. Finally, we show that the proposed techniques find useful knowledge with low computational complexity and discovered rules can be applied to industrial fields.", "authors": ["Sungbo Seo", "Long Jin", "Jun Wook Lee", "Keun Ho Ryu"], "n_citation": 0, "title": "Similarity pattern discovery using calendar concept hierarchy in time series data", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "93d3235e-e7c4-48ae-96e4-1b43ff2f6a96"}
{"authors": ["Jianbo Jia", "Jian Sun", "Chi Keung Tang", "Heung Yeung Shum"], "n_citation": 0, "title": "Bayesian correction of image intensity with spatial consideration", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ac52348a-9808-4de2-88a6-7fe1782d2fd7"}
{"authors": ["Jack B. Dennis", "Earl C. Van Horn"], "n_citation": 0, "title": "Programming Semantics for Multiprogrammed Computations (Reprint).", "venue": "Communications of The ACM", "year": 1983, "id": "afc1797e-f215-40bc-a05b-7273bd46e1db"}
{"abstract": "Abstract. Over the past few years, Monte-Carlo Tree Search (MCTS) has become a popular search technique for playingmulti-player games. In this paper we propose a technique called Playout Search. This enhance- ment allows the use of small searches in the playout phase of MCTS in order to improve the reliability of the playouts. We investigate maxn, Paranoid, and BRS for Playout Search and analyze their performance in two deterministic perfect-information multi-player games: Focus and Chinese Checkers. The experimental results show that Playout Search significantly increases the quality of the playouts in both games. How- ever, it slows down the speed of the playouts, which outweighs the benefit of better playouts if the thinking time for the players is small. When the players are given a sufficient amount of thinking time, Playout Search employing Paranoid search is a significant improvement in the 4-player variant of Focus and the 3-player variant of Chinese Checkers.", "authors": ["J. Nijssen", "Mark H. M. Winands"], "n_citation": 0, "title": "Playout search for Monte-Carlo tree search in multi-player games", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "b30c4733-3b15-45c5-98c8-3bea06b7e884"}
{"abstract": "We propose a new similarity measure based on keyword positioning to search images collected from the Web, for the improvement of precision and recall performance. Our image representation model fully exploits the image semantics from the text associated with the images in the Web pages. A prototype search engine Image Search is implemented with these techniques.", "authors": ["Lakshman Jayaratne", "Athula Ginige", "Zhuhan Jiang"], "n_citation": 0, "title": "Effective indexing of web images with keyword positioning", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e7123436-4ef6-49db-8fd5-f8bc8d182110"}
{"abstract": "We present an approach to build highly adaptable extractor for collecting data from diverse Web sites. This approach uses Graph Model to represent content and structures as well as their various types of features. The generated graph is accompanied by a script in a special language called GQML containing the extraction rules. The running of the script transforms the graph into a specified format such as XML file that stores data from various Web sites in a uniform format. The experimental results show the presented approach is both effective and efficient.", "authors": ["Qi Guo", "Lizhu Zhou", "Zhiqiang Zhang", "Jianhua Feng"], "n_citation": 0, "title": "A highly adaptable Web information extractor using Graph data Model", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f11248ee-bf01-4325-aaa9-5867c815d3fe"}
{"authors": ["Leonardo C. T. Bezerra", "Manuel L\u00f3pez-Ib\u00e1\u00f1ez", "Thomas St\u00fctzle"], "n_citation": 0, "references": ["0b1363e0-b6a4-4c99-8a2a-5f82850aee67", "1892f85e-25bc-4f42-b028-569f299e3aca", "195318c6-c9a5-471d-81f1-6906ef348699", "1e46ff1e-3010-4ef4-84a3-d61cd129868d", "23dc6e53-9579-4198-bb00-dedfd3e6071b", "28e1d0bc-0406-4a85-a2a6-72031835512c", "63d2ea00-e29e-46b5-8b36-b63e9c494513", "6a5b18f9-61fa-4e8c-8b1d-28ea4458d29c", "6a67c32e-d3e5-49ea-98c5-fabb2de715ad", "72545d7f-fb07-4eb5-902d-bff4a0e42980", "7e703be5-1f30-43a8-8983-6209b8bf4add", "a1d1ce88-9cca-478e-b158-dde99adc0476"], "title": "An Empirical Assessment of the Properties of Inverted Generational Distance on Multi- and Many-Objective Optimization", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "fd905e4c-da3c-4988-bc3a-5ab937215552"}
{"abstract": "We consider graphs that can be embedded on a surface of bounded genus such that each edge has a bounded number of crossings. We prove that many optimization problems, including maximum independent set, minimum vertex cover, minimum dominating set and many others, admit polynomial time approximation schemes when restricted to such graphs. This extends previous results by Baker [1] and Eppstein [7] to a much broader class of graphs.", "authors": ["Alexander Grigoriev", "Hans L. Bodlaender"], "n_citation": 0, "title": "Algorithms for graphs embeddable with few crossings per edge", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "0de284bb-4826-4992-9141-033616307d80"}
{"abstract": "The paper deals with quality attributes for mobile web applications. It describes typical challenges when developing mobile web applications and relates the challenges to the ISO 9126 quality attributes. The quality attributes are summarized to an ISO model that is focusing on the most important quality attributes for the quality assurance of mobile web applications. Finally, the paper proposes that applying formal quality assurance methods during the development of mobile web applications may solve some of the challenges in mobile web application development.", "authors": ["Axel Spriestersbach", "Thomas Springer"], "n_citation": 38, "title": "Quality attributes in mobile Web application development", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2a179a1c-70b9-44a3-a412-6eac0fd5b6ac"}
{"abstract": "With the development of Web services, more and more applications based on Web services are emerging on the web. Given the highly dynamic and distributed nature of Web services, implementing Web services in Peer-to-Peer (P2P) network is recently becoming increasingly popular. Our goals are to balance the loads among all the nodes through the management of services and resources in the P2P Web services system so as to ensure the efficiency of service execution. In this paper, we describe the solutions to the load balancing problem. First, we propose an architecture to organize the nodes in such systems. Then, we formally define the problems of load balancing and develop algorithms to them. Finally, we perform a series of experiments to verify our approaches. The experimental results show that our algorithms are effective to achieve the load balancing goals.", "authors": ["Yang Yuan", "Zhimao Guo", "Xiaoling Wang", "Aoying Zhou"], "n_citation": 0, "title": "Towards load balancing in Peer-to-Peer environment of Web services", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "30dc0cb0-188a-4093-b2fa-6cdb3d4386a7"}
{"abstract": "Web search engines help users find useful information on the WWW. However, when the same query is submitted by different users, typical search engines return the same result regardless of who submitted the query. Generally, each user has different information needs for his/her query. Therefore, the search result should be adapted to users with different information needs. In this paper, we first propose several approaches to adapting search results according to each user's need for relevant information without any user effort. Experimental results show that search systems that adapt to a user's preferences can be achieved by constructing user profiles based on modified collaborative filtering.", "authors": ["Kazunari Sugiyama", "Kenji Hatano", "Masatoshi Yoshikawa", "Shunsuke Uemura"], "n_citation": 0, "title": "User-oriented adaptive Web information retrieval based on implicit observations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3b71b659-61e1-4406-86b1-538cfa184977"}
{"abstract": "Nowadays there are some popular full-text index models, such as the Inverted-File model, Signature File model and Pat array model. But all these models have their own disadvantages. In this paper a new full-text index model-Inter-relevant Successive Trees model is put forward. Experiments show that it outperforms Inverted-File Model.", "authors": ["Yongdan Liu", "Zhan Shen", "Jianhui Wang", "Yunfa Hu"], "n_citation": 0, "title": "Inter-relevant Successive Trees model and its implementation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3d04e4ba-f3b8-4a6d-9ac3-ed007c96fc7c"}
{"abstract": "XSLT is an increasingly popular language for processing XML data. It is widely supported by application platform software. However, little optimization effort has been made inside the current XSLT processing engines. Evaluating a very simple XSLT program on a large XML document with a simple schema may result in extensive usage of memory. In this paper, we present a novel notion of Streaming Processing Model (SPM) to evaluate a subset of XSLT programs on XML documents, especially large ones. With SPM, an XSLT processor can transform an XML source document to other formats without extra memory buffers required. Therefore, our approach can not only tackle large source documents, but also produce large results. We demonstrate with a performance study the advantages of the SPM approach. Experimental results clearly confirm that SPM improves XSLT evaluation typically 2 to 10 times better than the existing approaches. Moreover, the SPM approach also features high scalability.", "authors": ["Zhimao Guo", "Min Li", "Xiaoling Wang", "Aoying Zhou"], "n_citation": 0, "title": "Scalable XSLT evaluation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3d24c0b9-9632-40ec-b32b-6229a4185ec8"}
{"abstract": "We propose a method that identifies from Web pages pairs of keywords in which one word describes the other and uses these relations to modify the query. It takes into account the positions of the words in the page structures when counting their occurrences and applies statistical tests to examine the differences between word co-occurrence rates. It finds related keywords more robustly regardless of the word type than the conventional methods, which do not consider page structures. It can also identify subject and description keywords in the user's input and find additional keywords for detailing the query. By considering the document structures, our method can construct queries that are more focused on the user's topic of interest.", "authors": ["Satoshi Oyama", "Katsumi Tanaka"], "n_citation": 0, "title": "Query modification by discovering topics from Web page structures", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "46d585aa-9eef-46f7-8ec7-189abb67d45c"}
{"abstract": "It becomes possible to acquire our interesting information from diverse information source and media to improve the quality and detailedness of information. For instance, with the spreading of digital broadcasting and broadband internet connection services, the infrastructure for integration of TV-programs and the Internet is prepared and we can find the additional information of a TV-program from the Web. In this paper, we propose a novel information retrieval mechanism for information augmentation based on the notion of topic structure. One of the notable features of our retrieval mechanism is that it can be used to find complementary information of a given web page or video. That is to say, the retrieved information is not just similar to the given web page or video. It can provide some additional information to detail the given one or describe it from different perspective. In this paper, we also show some evaluation results of the complementary information retrieval mechanism.", "authors": ["Qiang Ma", "Katsumi Tanaka"], "n_citation": 0, "title": "Topic-structure based complementary information retrieval for information augmentation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "56512d57-b049-437d-a808-8a2d48070900"}
{"abstract": "Increasing the robustness and flexibility of the tool path finding method may broaden the application fields of pocket machining. We aim to develop a path finding method for various applications. Through an integration of the entire tool path finding process, the method becomes not merely optimal to generate an efficient path but also robust enough to be applied to any system with any configuration. The flexibility of the devised method enables us to broaden the application fields of pocketing to fields such as prototype printed circuit board manufacturing. The devised method is applied to generate a clearing path for prototype printed circuit boards. The results verify that the method is concise and simple, but robust and flexible enough to achieve the optimal path in any configuration.", "authors": ["Jae-Sung Song", "Manseung Seo", "Masahiko Onosato"], "n_citation": 0, "title": "Path Finding Method for Various Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5c11ee45-97c7-4ec1-943b-da4551b8af1e"}
{"abstract": "This paper proposes a moving objects data model, query language, main memory database system for location-based service. Location-based services require GIS databases system to represent moving objects and to support querying on the motion properties of objects. For example, fleet management applications may require storage of information about moving vehicles. Also, advanced CRM applications may require to store and query the trajectories of mobile phone users. But, modeling consistent information about the location of continuously moving objects and processing motion-specific queries is challenging problem. We formally define a data model and data types for moving objects that includes complex evolving spatial structure and indeterminacy, and then describe the system implementation.", "authors": ["Kwang Woo Nam", "Jai Ho Lee", "Seong Ho Lee", "Jun Wook Lee", "Jong Hyun Park"], "n_citation": 0, "title": "Developing a main memory moving objects DBMS for high-performance location-based services", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5dd4a15e-daba-47cc-bfaa-cd7a319a99f0"}
{"abstract": "Wikis play a leading role among the web publishing environments, being collaborative tools used for fast and easy writing and sharing of content. Although powerful and widely used, wikis do not support users in the aided generation of content specific for a given domain but they still require manual, time-consuming and error-prone interventions. On the other hand, semantic portals support users in browsing, searching and managing content related to a given domain, by exploiting ontologies. In this paper we propose a specific application of web ontologies, applied to the wikis: exploiting an ontological description of a domain in order to deploy a customized wiki for that specific domain. We describe the design of an ontology-based framework, named WikiFactory, that aids users to automatically generate a complex and complete wiki website related to a specific area of interest with few efforts. In order to show the applicability of our framework, we present a specific case study that describes the main WikiFactory capabilities in constructing the wiki website for a Computer Science Department in a University.", "authors": ["Angelo Di Iorio", "Valentina Presutti", "Fabio Vitali"], "n_citation": 50, "title": "WikiFactory : An Ontology-Based Application for Creating Domain-Oriented Wikis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6536d17e-6418-4355-98ff-d2291a45117a"}
{"abstract": "Modelling a process using techniques such as Role Activity Diagrams (RADs) [13] can illustrate a large amount of useful information about the process under study. What they cannot show as easily however, are the informal practices during that process. In this paper, we analyse the prototyping process as part of an IS development strategy across five companies. Interview text from project managers, prototypers and other development staff across the five companies was analysed. Interestingly, results point to several key recurring issues amongst staff. These include non-adherence to any prototyping guidelines or standards, sketchy change request procedures, concern over time and cost deadlines and the importance attached to developer experience during the overall process. The notion of prototyping as a simple and easily managed development strategy does not hold. Our analysis provides complementary qualitative data about the opinions of prototyping to inform business process re-engineering of those formal RADs.", "authors": ["Steve Counsell", "Keith Phalp", "Emilia Mendes", "Stella Geddes"], "n_citation": 0, "title": "What formal models cannot show us : People issues during the prototyping process", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6c972c75-df23-41b7-8f76-a12a50cb6624"}
{"abstract": "This paper presents a focused Web crawling system iSurfer for information retrieval from the Web. Different from other focused crawlers, iSurfer uses an incremental method to learn a page classification model and a link prediction model. It employs an online sample detector to incrementally distill new samples from crawled Web pages for online updating of the model learned. Other focused crawling systems use classifiers that are built from initial positive and negative samples and can not learn incrementally. The performances of these classifiers depend on the topical coverage of the initial positive and negative samples. However, the initial samples, particularly the negative ones, with a good coverage of target topics are difficult to find. Therefore, the iSurfer's incremental learning strategy has an advantage. It starts from a few positive samples and gains more integrated knowledge about the target topics over time. Our experiments on various topics have demonstrated that the- incremental learning method can improve the harvest rate with a few initial samples.", "authors": ["Yunming Ye", "Fanyuan Ma", "Yiming Lu", "Matthew Chiu", "Joshua Huang"], "n_citation": 0, "title": "iSurfer: A focused Web crawler based on incremental learning from positive samples", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6eec201d-18b2-4ce0-a254-4655ca4d5e8b"}
{"abstract": "Since software development projects often fall into runaway situations, detecting signs of runaway status in early stage of development has become important. In this paper, we propose a new scheme for the prediction of runaway projects based on an empirical questionnaire. We first design a questionnaire from five viewpoints within the projects: requirements, estimations, planning, team organization, and project management activities. Each of these viewpoints consists of questions in which experience and knowledge of software risks are included. Secondly, we classify projects into runaway and success using resultant metrics data. We then analyze the relationship between responses to the questionnaire and the runaway status of projects by the Bayesian classification. The experimental result using actual project data shows that 33 out of 40 projects were predicted correctly. As a result, we confirm that the prediction of runaway projects is successful.", "authors": ["Osamu Mizuno", "T. Hamasaki", "Yasunari Takagi", "Tohru Kikuno"], "n_citation": 0, "title": "An empirical evaluation of predicting runaway software projects using Bayesian classification", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "716f2330-0261-4808-8fe2-a8a9612140ae"}
{"abstract": "We present translational lemmas for alternating Turing machines (ATMs) and parallel random access machines (PRAMs), and apply them to obtain tight hierarchy results on ATM- and PRAM-based complexity classes. It is shown that, for any small rational constant e, there is a language which can be accepted by a c(9 + e) log r  n-time d(4 + e) log n-space ATM with l worktapes but not by any c log r  n-time d log n-space ATM with the same l worktapes if the number of tape symbols is fixed. Here, c,d > 0 and r > 1 are arbitrary rational constants, and l > 2 is an arbitrary integer. It is also shown that, for any small rational constant e, there is a language which can be accepted by a c(1 + \u2208) log''1 n-time PRAM with n r 2 processors but not by any c log r 1 n-time PRAM with n r 2 (1+ \u2208 )  processors, where c > 0, r 1  > 1, and r 2  > 1 are arbitrary rational constants.", "authors": ["Chuzo Iwamoto", "Yoshiaki Nakashiba", "Kenichi Morita", "Katsunobu Imai"], "n_citation": 50, "title": "Translational lemmas for alternating TMs and PRAMs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "72ae46bc-3c05-4de3-9934-aab4febc7108"}
{"abstract": "An underlying formation of a Virtual Enterprise (VE) is efficient partner selection mechanisms that enable all the partners in the VE being assembled for a short period. In this paper, we present an ontology-oriented approach that use description logics formally to represent concepts and roles (relations) of partner's view of domain knowledge. We further address the use of such a shared knowledge in a VE under the knowledge model defined in this paper to help partner selection by proposing assessment criteria of potential partners. We suggest that value ranges as thresholds to filter unable biddings before evaluation should be a better way. Filtering process works well for its fitting with the dynamic criteria of the principal agent during the process of selection partners in a VE. We illustrate the approach through an e-shop case study.", "authors": ["Li Li", "Baolin Wu", "Yun Yang"], "n_citation": 0, "title": "An ontology-oriented approach for virtual enterprises", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "730db57c-a793-4ea3-b865-613505f3aede"}
{"abstract": "IT enforced access control policies in medical information systems have to be fine-grained and dynamic. We justify this observation on the basis of legislation and on the basis of the evolution within the healthcare domain. Consequently, a reconfigurable or at least adaptable implementation of access control facilities has become extremely important. For this purpose, current technology provides insufficient support. We highlight a basic solution to address shortcomings by using interception techniques. In addition, we identify further research that is required to address the challenges of dynamic and fine-grained access control in the long run.", "authors": ["Tine Verhanneman", "Liesbeth Jaco", "Bart De Win", "Frank Piessens", "Wouter Joosen"], "n_citation": 16, "title": "Adaptable Access Control Policies for Medical Information Systems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7aea0473-3b80-45ed-8108-0e104abbf8f7"}
{"abstract": "The monotone curvature condition for rational quadratic B-spline curves is studied in this paper. At first, we present the necessary and sufficient conditions of monotone curvature for the uniform rational quadratic B-spline segment and we compare it to the curvature condition of rational quadratic Bezier curve. Then, we give the sufficient condition of monotone curvature for the nonuniform rational quadratic B-spline segment. At last, we obtain the condition of monotone curvature for general rational quadratic B-spline curves with any number of control points.", "authors": ["Zhong Li", "Lizhuang Ma", "Dereck S. Meek", "Wuzheng Tan", "Zhihong Mao", "Mingxi Zhao"], "n_citation": 50, "title": "Curvature Monotony Condition for Rational Quadratic B-spline Curves", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "894268e2-3e08-4e96-8783-8a08e3413b14"}
{"abstract": "With the explosion of information on the Web, information that is made available from websites is generally overwhelming to users surfing the sites. The majority of the users who are facing this information overloading problem are the ordinary home users who do not have much technical knowledge. It is thus important to allow these users to easily create personalized views of websites such that they only see what they want in the way they prefer. In this paper, we propose the concept of a personalized Web view to cater to this requirement. Underlying this concept is a data model that represents websites from the logical point of view and a declarative langauge that transforms logical views into personalized Web views. To empower ordinary users with the ability to build their own personalized Web views, we have designed and implemented a software system, known as WICCAP. This system includes a wizard to help users create data models that map physical websites into logical views. It also has. an information extraction agent that allows users to instantiate their personalized Web views of the target websites by transforming from logical views previously defined. In order to increase the fun and flexibility of using this software, a flexible presentation toolkit has been designed to present the information in a manner that is programmable by the users.", "authors": ["Zehua Liu", "Wee Keong Ng", "Ee-Peng Lim", "Yangfeng Huang", "Feifei Li"], "n_citation": 0, "title": "Unloading unwanted information: From physical websites to personalized Web views", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "899021ff-9706-47b1-95e8-7afd6001ecd5"}
{"authors": ["Yves De Smet", "Jean-Philippe Hubinont", "Jean Rosenfeld"], "n_citation": 0, "references": ["18b0770a-d2e3-43c8-9f9c-ed0440a7c5cf", "5fc534cc-dfde-4eac-9c9c-4e561e032eb6", "ff68534d-8334-4bd2-9000-0063bbfa332d"], "title": "A Note on the Detection of Outliers in a Binary Outranking Relation", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "97921302-c549-4975-802e-47947bce9a08"}
{"abstract": "Association rule mining is an active data mining research area.. Recent years have witnessed many efforts on discovering fuzzy associations. The key strength of fuzzy association rule mining is its completeness. This strength, however, comes with a major drawback. It often produces a huge number of fuzzy associations. This is particularly true for datasets whose attributes are highly correlated. The huge number of fuzzy associations makes it very difficult for a human user to analyze them. Existing research has shown that most of the discovered rules are actually redundant or insignificant. In this paper, we propose a novel technique to overcome this problem.The approach is effective because experiment results show that the set of produced rules is typically very small. Our solution also reduces the size of average transactions and dataset. Our performance study shows that this solution has a superior performance over the other algorithms.", "authors": ["Zahra Farzanyar", "Mohammadreza Kangavari", "Sattar Hashemi"], "n_citation": 50, "title": "Effect of Similar Behaving Attributes in Mining of Fuzzy Association Rules in the Large Databases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a0ffa2e7-428e-4cf9-932e-66cb9ff6c37c"}
{"abstract": "ROLAP is used to answer queries for analysis support on the data stored in the data warehouse. To fulfill this purpose rapidly and correctly, ROLAP always precompute some views in the data warehouse. However, selecting views to materialize is an NP-hard problem. Some previous works, such as the Greedy Algorithm, BPUS and PBS, have focused on it with high time complexity, which is exponential with the number of dimensions. It leads poor performance even for mid-size datasets. This paper introduces a framework ANNE in which any above algorithm can be applied to solve the problem several orders of magnitude faster than the algorithm itself, while maintaining almost the same effect.", "authors": ["Yong Yan", "Peng Wang", "Chen Wang", "Haofeng Zhou", "Wei Wang", "Baile Shi"], "n_citation": 0, "title": "ANNE: An efficient framework on view selection problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ad9271c5-f829-488d-b1f7-3cf760e41486"}
{"authors": ["Qiang Yang"], "n_citation": 0, "title": "Transfer learning beyond text classification", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "b4512433-2c44-4ce3-aae2-db413fab7191"}
{"authors": ["Zhenni Feng", "Yanmin Zhu", "Lionel M. Ni"], "n_citation": 0, "title": "IMac: Strategy-proof incentive mechanism for mobile crowdsourcing", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "c2b3886d-2a45-4091-98f0-1b3589a3a70c"}
{"abstract": "The institutionalization of a Quality System improves the levels of technical and managerial efficiency of Enterprises. Moreover, the market itself solicits the acquisition of a Quality Certification for getting a steady confirmation of Enterprise's capabilities. The introduction of a Quality System in Small and Medium Enterprises can entail prohibitive costs for them and affect their agility and flexibility. The paper proposes a lightweight approach as a solution to either avoid or reduce such drawbacks; it consists of a method for redesigning processes and a software system to control and monitoring processes' execution. Consequently, a research question arises: is the approach suitable for establishing a Quality System effectively in a Small Medium Enterprise? In order to have a preliminary evaluation of the proposed approach, a case study has been carried out in an Italian Enterprise, aiming at owning VISION 2000 Certification.", "authors": ["Lerina Aversano", "Gerardo Canfora", "Giovanni Capasso", "Giuseppe A. Di Lucca", "Corrado Aaron Visaggio"], "n_citation": 0, "title": "Introducing Quality System in small and medium enterprises: An experience report", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c8019a04-352d-4fcf-82aa-b78b81e9d017"}
{"abstract": "Although software measurements have been used in industry for several decades, it is still difficult for companies to make the most of them. An efficient use of measurements is a fundamental issue - without it, measurement is waste of time and effort. In this paper we present an approach to supporting and boosting the utilisation of measurements at both project and organisational levels. The underlying idea behind the approach is to create a practical measurement utilisation plan. We present how the utilisation plan can be developed and how this approach can motivate software developers and managers to collect and utilise metrics. We also describe the experiences and feedback gained from a case study. We consider our approach as a vital enhancement to existing measurement methods, one to be adopted when aiming at a comprehensive utilisation of measurements. Moreover, our approach prepares the ground for packaging the measurement results in an organisation.", "authors": ["Outi Salo", "Maarit Tihinen", "Matias Vierimaa"], "n_citation": 0, "title": "Enabling comprehensive use of metrics", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d052c516-fa0f-4040-8f37-3930850a893b"}
{"authors": ["Alexander Grigoriev", "J. van Loon", "Marc Uetz", "J. van Leeuwen"], "n_citation": 0, "title": "On the Complexity of the Highway Pricing Problem.", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "db43e31a-c1bd-4167-a835-d1088dd61409"}
{"abstract": "For most XP techniques only a few experimental results on their effects are available. In October 2004 we started a medium-term experiment to investigate the impact of test-first compared to a classical-testing approach. We carefully designed a controlled experiment and conducted it with 18 graduated students randomly assigned to 9 pairs. Hypotheses dealt with development speed, number of test-cases and the test-coverage when applying the testing approaches. Results show differences however not significant ones. This paper also addresses other observations we made during the experimental run. Two major problems strongly affect the results of the experiment: the low number of data points and the non-trivial question, whether students really applied test-first all the time. Although we cannot provide any new results on testing to the research community, this paper contains valuable information about further experimental studies on this topic.", "authors": ["Thomas Flohr", "Thorsten Schneider"], "n_citation": 0, "title": "Lessons learned from an XP experiment with students : Test-first needs more teachings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dc1d3ccb-e2c8-4bc8-97ec-399ca4cb5a39"}
{"abstract": "In the pattern matching problem, there can be a quadratic number of matching substrings in the size of a given text. The linearizing restriction finds, at most, a linear number of matching substrings. We first explore two well-known linearizing restriction rules, the longest-match rule and the shortest-match substring search rule, and show that both rules give the same result when a pattern is an infix-free set even though they have different semantics. Then, we introduce a new linearizing restriction, the leftmost non-overlapping match rule that is suitable for find-and-replace operations in text searching, and propose an efficient algorithm when the pattern is a regular language according to the new match rule.", "authors": ["Yo-Sub Han", "Derick Wood"], "n_citation": 0, "title": "A new linearizing restriction in the pattern matching problem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ef353212-f067-4720-8cf3-f8ddca41b1fe"}
{"abstract": "Standard GP crossover is widely accepted as being a largely destructive operator, creating many poor offspring in the search for better ones. One of the major reasons for its destructiveness is its disrespect for the context of swapped subtrees in their respective parent trees when creating offspring. At times, this hampers GP's performance considerably, and results in populations with low average fitness values. Many attempts have been made to make it a more constructive crossover, mostly by preserving the context of the selected subtree in the offspring. Although successful at preserving context, none of these methods provide the opportunity to discover new and better contexts for exchanged subtrees. We introduce a context-aware crossover operator which operates by identifying all possible contexts for a subtree, and evaluating each of them. The context that produces the highest fitness is used to create a child which is then passed into the next generation. We have tested its performance on many benchmark problems. It has shown better results than the standard GP crossover operator, using either the same number or fewer individual evaluations. Furthermore, the average fitness of populations using this scheme improves considerably, and programs produced in this way are much smaller than those produced using standard crossover.", "authors": ["Hammad Majeed", "Conor Ryan"], "n_citation": 0, "title": "A less destructive, context-aware crossover operator for GP", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "00759bd9-8ff3-4632-a651-638904700b01"}
{"abstract": "We show that the problem of finding optimal strategies for both players in a simple stochastic game reduces to the generalized linear complementarity problem (GLCP) with a P-matrix, a well-studied problem whose hardness would imply NP = co-NP. This makes the rich GLCP theory and numerous existing algorithms available for simple stochastic games. As a special case, we get a reduction from binary simple stochastic games to the P-matrix linear complementarity problem (LCP).", "authors": ["Bernd G\u00e4rtner", "Leo R\u00fcst"], "n_citation": 0, "title": "Simple stochastic games and P-matrix generalized linear complementarity problems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "08c024b5-515c-425f-9504-5be21e2218ef"}
{"abstract": "We refine the genericity concept of [1], by assigning a real number in [0,1] to every generic set, called its generic density. We construct sets of generic density any E-computable real in [0,1]. We also introduce strong generic density, and show that it is related to packing dimension [2]. We show that all four notions are different. We show that whereas dimension notions depend on the underlying probability measure, generic density does not, which implies that every dimension result proved by generic density arguments, simultaneously holds under any (biased coin based) probability measure. We prove such a result: we improve the small span theorem of Juedes and Lutz [3], to the packing dimension [2] setting, for k-bounded-truth-table reductions, under any (biased coin) probability measure.", "authors": ["Philippe Moser"], "n_citation": 50, "title": "Generic density and small span theorem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "097c354d-73c7-4678-9ab1-d4529c72e971"}
{"abstract": "Free viewpoint television (FTV) based on ray-space representation was adopted in the MPEG draft because the ray-space can generate an arbitrary viewpoint view without complicated analysis and rendering process. Ray-Space predictive coding is one of main techniques in ray-space based FTV systems, in which block matching and compensation plays a very important role to improve coding performance. However, the computational burden of block matching for ray-space coding is the problem to be solved. Ray-space representation of FTV is introduced and inter-slice correlations of ray-space data are analyzed. Then, a fast adaptive block matching algorithm for ray-space predictive coding is proposed to reduce the encoding complexity. Experimental results show that the proposed block matching method reduce the computational burden and speed up the coding process greatly.", "authors": ["Gangyi Jiang", "Feng Shao", "Mei Yu", "Ken Chen", "Tae-Young Choi"], "n_citation": 0, "title": "Efficient Block Matching for Ray-Space Predictive Coding in Free-Viewpoint Television Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0cd8e7a6-86b2-40a5-ba33-1448c2351747"}
{"abstract": "The focus of this paper is to outline the experience of a European based software organization utilizing the IDEAL model, while implementing a tailored Capability Maturity Model (CMM) software process improvement program. The goal was to achieve process improvement rather than a specific CMM maturity level. In doing this, the IDEAL model was extensively researched and employed. The benefits and limitations of the IDEAL model are presented as experienced. Further details on this research are available in [1]. Research was carried out on a number of software process improvement paradigms prior to the selection of the CMM. A key element of this approach was to see the requirements of the organization as paramount and immediate. It was deemed important to achieve process improvement in specific Key Process Areas regardless of their position in the CMM. This provided the flexibility for future investment in SPI to capitalize on the current work.", "authors": ["Valentine Casey", "Ita Richardson"], "n_citation": 0, "title": "A practical application of the IDEAL Model", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0fe88884-0dbf-4f86-bcc3-9a9b3f2beb82"}
{"abstract": "Solving complex real-world problems using evolutionary computation is a CPU time-consuming task that requires a large amount of computational resources. Peer-to-Peer (P2P) computing has recently revealed as a powerful way to harness these resources and efficiently deal with such problems. In this paper, we present a P2P implementation of Genetic Programming based on the JXTA technology. To run genetic programs we use a distributed environment based on a hybrid multi-island model that combines the island model with the cellular model. Each island adopts a cellular genetic programming model and the migration occurs among neighboring peers. The implementation is based on a virtual ring topology. Three different termination criteria (effort, time and max-gen} have been implemented. Experiments on some popular benchmarks show that the approach presents a accuracy at least comparable with classical distributed models, retaining the obvious advantages in terms of decentralization, fault tolerance and scalability of P2P systems.", "authors": ["Gianluigi Folino", "Giandomenico Spezzano"], "n_citation": 0, "title": "P-CAGE : An environment for evolutionary computation in peer-to-peer systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "11d66952-6d29-4836-ac24-de4e67b2ed23"}
{"abstract": "This paper proposes an algorithm to detect human faces under various environments. In the first step, information on three color spaces of various features is used to determine histogram of color in the first frame of an image. The histogram obtained by interpolation after combining three color of the image is used as an input of LMCUH network. In the second step, the neural network of Levenberg - Marquadt training algorithm minimizes the error. Next, we find the face in test image by using the trained sets. This method is especially suited for various scales, rotations, lighting levels, or occlusions of the target image. Experimental results show that two - dimensional images of a face can be effectively implemented by using artificial neural network training under various environments. Thus, we can detect the face effectively and this can inevitably lead to the Ubiquitous Computing Environment.", "authors": ["Jin Ok Kim", "Jun Yeong Jang", "Chin Hyun Chung"], "n_citation": 0, "title": "On a Feature Extraction by LMCUH Algorithm for a Ubiquitous Computing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "19b72cdd-4d85-4eee-93fd-f062d3d784e0"}
{"abstract": "Organizations that develop software have recognized that software process models are particularly useful for maintaining a high standard of quality. In the last decade, simulations of software processes were used in several settings and environments. This paper gives a short overview of the benefits of software process simulation and describes the development of a discrete-event model, a technique rarely used before in that field. The model introduced in this paper captures the behavior of a detailed code inspection process. It aims at reducing the risks inherent in implementing inspection processes and techniques in the overall development process. The determination of the underlying cause-effect relations using data mining techniques and empirical data is explained. Finally, the paper gives an outlook on our future work.", "authors": ["Holger Neu", "Thomas Hanne", "J\u00fcrgen M\u00fcnch", "Stefan Nickel", "Andreas Wirsen"], "n_citation": 0, "title": "Simulation-based risk reduction for planning inspections", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "19e3eeef-6af9-40b2-81ef-dee949108578"}
{"abstract": "In order to check if the Fractal theory could be a useful tool for some quantitative descriptions of the fracture parameters, the present work studied different theoretical models (e.g. the Bazant's Size Effect Law (SEL) [1], the Modified Size Effect Law [2,3] and the Carpinteri's MultiFractal Scaling Law (MFSL) [4] of the fracture parameters of concrete specimen, and the compatibility of some of the above studied theoretical models relative to the experimental data, using certain recent procedures to study the global and local compatibility. The fracture parameters can be considered as main quantities for computational procedures for modeling the fracture of a certain ensemble (a suddenly emerging phenomena). In the next phase, the thermoelastic generation of ultrasonic perturbations in semitransparent solids was analyzed (using computer simulation) so as to find similarities with material properties as fractal dimensions, when the heat source is a laser radiation. The algorithm, the numerical analysis has taken into account three main physical phenomena: the absorption of electromagnetic energy in substance with heat generation; thermal diffusion with electromagnetic energy based heat source and elastodynamic wave generation by thermoelastic expansion.", "authors": ["D. Iordache", "Stefan Pusca", "Ghiocel Toma", "Viorel Paun", "Andreea Sterian", "Cristian Morarescu"], "n_citation": 0, "title": "Analysis of Compatibility with Experimental Data of Fractal Descriptions of the Fracture Parameters", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1bcdeba8-bc4b-4f78-9792-3ff84c1d9a3b"}
{"abstract": "Fuel cell stack systems are under intensive development for mobile and stationary power applications. In particular, Proton Exchange Membrane (PEM) Fuel Cells (also known as Polymer Electrolyte Membrane Fuel Cells) are currently in a more mature stage for ground vehicle applications. This paper proposes a theoretical innovative approach to the analysis of the electrochemical transient behavior (anode-cathode). The transient beliavior due to the electrochemical dynamic may impact the behavior of the resulting load current. Boundary conditions influence the resulting electric field, the boundary condition are strongly depending of H 2  and O 2  physical parameters. Maxwell's equations are used in order to describe the model. Solutions through dyadic harmonic wavelets at different levels of resolution are presented. Wavelets approach, through their different space-time levels of resolution, can favorable describe the segmented space structure of the stack. In the meantime, transient dynamic inside of the stack can be adaptively studied. An outlook closes the paper.", "authors": ["Carlo Cattani", "Paolo Mercorelli", "Francesco Villecco", "Klaus Harbusch"], "n_citation": 0, "title": "A Theoretical Multiscale Analysis of Electrical Field for Fuel Cells Stack Structures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1f30e264-eabd-4caf-8c37-1d594ba281c8"}
{"abstract": "During the past few years mobile phones have become an ubiquitous companion. In parallel the semantic web provides enabling technologies to annotate and match information with a user's interests. This paper presents a thorough definition of annotations and profiles. The optimizations of annotations and profiles make the mobile phone a first class participant of a semantic environment rather than a mere displaying client of services running elsewhere. The implementation of the system - including a first order model generating theorem prover and a description logic interface - renders the idea viable in the real world. The choosen solution even enables explanations within the matchmaking process. The profile does not have to leave the personal device and ensures privacy by doing so. An additionally benefit is the independence from communication with any reasoning backends.", "authors": ["Thomas Kleemann", "Alex Sinner"], "n_citation": 0, "title": "User profiles and matchmaking on mobile phones", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "21398b6e-b350-4e39-a2d9-d105a09f54a8"}
{"abstract": "In this paper, we investigate the problem of finding acyclic subhypergraphs in a hypergraph. First we show that the problem of determining whether or not a hypergraph has a spanning connected acyclic subhypergraph is NP-complete. Also we show that, for a given K > 0, the problem of determining whether or not a hypergraph has an acyclic subhypergraph containing at least K hyperedges is NP-complete. Next, we introduce a maximal acyclic subhypergraph, which is an acyclic subhypergraph that is cyclic if we add any hyperedge of the original hypergraph to it. Then, we design the linear-time algorithm mas to find it, which is based on the acyclicity test algorithm designed by Tarjan and Yannakakis (1984).", "authors": ["Kouichi Hirata", "Megumi Kuwabara", "Masateru Harao"], "n_citation": 50, "title": "On finding acyclic subhypergraphs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "21407d7c-c3fa-4c70-b8a1-cab5445a6e5c"}
{"abstract": "The core element of the PROFES improvement methodology is the concept of product-process dependency (PPD) models. The purpose of PPD models is to help focus process improvement activities to those development technologies and processes that are most effective with regards to achieving specific customer-defined product quality goals. This paper describes how system dynamics simulation models can be used to check the plausibility of achieving positive effects on software product quality when implementing improvement actions derived from PPD models. Basically, this is done through extending an existing generic software project simulation model with structures that represent expected local cause-effect mechanisms of the PPD models. By running simulations with the extended software project simulation model, the potential effects of the PPD models on product quality can be investigated at low cost before conducting pilot applications in real projects.", "authors": ["Dietmar Pfahl", "Andreas Birk"], "n_citation": 0, "title": "Using simulation to visualise and analyse product-process dependencies in software development projects", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "22ddbb5e-eb2d-450a-ac66-f191e157fab5"}
{"abstract": "This paper presents a survey about use of quantitative management indicators in a Japanese software development organization. This survey is conducted in order to investigate possible criteria for selecting and customizing organizational standard indicators according to the context of each project. Based on results of the survey, we propose a process tailoring support system that is mainly focusing to quantitative management planning. The system EPDG+ (Electronic Process Data Guidebook Plus) helps project planners select / customize indicators to be employed in process control. Derived software project plans including measurement and analysis activities can be browsed in detail with this system.", "authors": ["Kazumasa Hikichi", "Kyohei Fushida", "Hajimu Iida", "Ken-ichi Matsumoto"], "n_citation": 5, "title": "A software process tailoring system focusing to quantitative management plans", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2399bdc9-c5f1-4c2e-ab74-2a91b94178a7"}
{"abstract": "In this paper we describe a framework which combines several models for organizational change. The framework enables an organization to decide which strategies will be most successful when implementing a specific change in its particular setting. The conditions for change is assessed in relation to each of the strategies for organizational change and a list-of-fit is produced, which reveals the degree to which each of the strategies fits the specific setting. The framework was developed and evaluated within a field study involving four companies in the financial sector. The IT organizations in two of these collaborated with the researchers in providing promising evaluations of the framework.", "authors": ["Jan Pries-Heje", "Otto Vinter"], "n_citation": 50, "title": "A framework for selecting change strategies in IT organizations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "26c8a03d-d97b-4925-8876-07e3ff55031b"}
{"abstract": "Knowledge Management spans on three abstraction layers: the first one is concerned with organisations, the second one regards engineering practices, and the third one is about tools. The organisation level encloses concepts strongly tied with strategies and resources management; the engineering level regards processes, methods and heuristics, tested or empirically validated, that support effective processes design, management and enactment; the third level comprehends software tools for storing and operating with knowledge. Currently, a major concern is a gap between the first and the second level in order to properly exploit theory and to cope with the nowadays turmoil of the marketplace. In this paper we propose a map linking a significant set of selected theories in knowledge management with a set of appropriate engineering practices able to realise them; furthermore we wish to determine the components from the third layer that are effective in making the selected engineering practices working in real contexts.", "authors": ["Gerardo Canfora", "Aniello Cimitile", "Corrado Aaron Visaggio"], "n_citation": 0, "title": "From knowledge Management concepts toward software engineering practices", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "270c6b15-9c85-45ab-bf61-338fa75ce6a7"}
{"abstract": "A set of points shown on the map usually represents special sites like cities or towns in a country. If the map in the interactive geographical information system (GIS) is browsed by users on the computer screen or on the web, the points and their labels can be viewed in a query window at different resolutions by zooming in or out according to the users' requirements. How can we make use of the information obtained from different resolutions to avoid doing the whole labeling from scratch every time the zooming factor changes? We investigate this important issue in the interactive GIS system. In this paper, we build low-height hierarchies for one and two dimensions so that optimal and approximating solutions for adaptive zooming queries can be answered efficiently. To the best of our knowledge, no previous results have been known on this issue with theoretical guarantees.", "authors": ["Sheung-Hung Poon", "Chan-Su Shin"], "n_citation": 0, "title": "Adaptive zooming in point set labeling", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2b8f475d-094b-4cd8-9a31-6706875e8402"}
{"abstract": "When designing a software system it is beneficial to study and use architectural styles from literature, to ensure certain quality attributes. However, as the interpretation of literature may differ depending on the background and area of expertise of the person reading the literature, we suggest that structured discussions about different architecture candidates provides more valuable insight not only in the architectures themselves, but in peoples' opinions of the architectures' benefits and liabilities. In this paper, we propose a method to elicit the views of individuals concerning architecture candidates for a software system and pinpoint where discussions are needed to come to a consensus view of the architectures.", "authors": ["Mikael Svahnberg", "Claes Wohlin"], "n_citation": 14, "title": "Consensus building when comparing software architectures", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2fba761b-e96e-48b5-8652-f58bd828d854"}
{"abstract": "The requirements of specific applications and services are often over estimated when ontologies are reused or built. This sometimes results in many ontologies being too large for their intended purposes. It is not uncommon that when applications and services are deployed over an ontology, only a few parts of the ontology are queried and used. Identifying which parts of an ontology are being used could be helpful to winnow the ontology, i.e., simplify or shrink the ontology to smaller, more fit for purpose size. Some approaches to handle this problem have already been suggested in the literature. However, none of that work showed how ontology-based applications can be used in the ontology-resizing process, or how they might be affected by it. This paper presents a study on the use of the AKT Reference Ontology by a number of applications and services, and investigates the possibility of relying on this usage information to winnow that ontology.", "authors": ["Harith Alani", "Stephen Harris", "Ben O'neil"], "n_citation": 50, "title": "Winnowing Ontologies Based on Application Use", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "33bd4824-454e-40d1-b9e3-2364f524d8b8"}
{"abstract": "CAS-SCF and MRCI calculations are presented, in order to investigate the electronic states involved in the intramolecular charge-transfer process of a bistable spiro cation. The potential energy curves of the ground and the first three excited states have been calculated, and a double well potential has been obtained for the ground state. The effect of dynamical correlation was found to be crucial for a quantitative description of this system. Our results also indicate the usefulnes of a local-orbital description of bistable systems.", "authors": ["Wissam Helal", "Beno\u00eet Bories", "Stefano Evangelisti", "Thierry Leininger", "Daniel Maynau"], "n_citation": 0, "title": "Ab-Initio Multi-reference Study of a Bistable Spiro Molecule", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "36301fb4-b33d-4bbf-b813-c64e3c7e469f"}
{"abstract": "Development of a virtual-reality training system based on an exercise bicycle and a personal-computer for rehabilitation of motor functions and balance capability in elderly patients has been described. The system can take three inputs from the bicycle: the pedaling speed, the direction of the handles, and the location of the center of pressure on the supporting base of the bicycle. These inputs allow user interactions with a virtual environment that increase motivation of the user. A series of experiments were conducted with three different visual feedback conditions: without visual feedback of balance information; with visual feedback of weight shift; or with visual feedback of the center of pressure. The assessment of balance parameters obtained from the experiments showed the improvement of balance capability on the bicycle. Especially, the visual feedback of weight shift most increased the effectiveness of the training. The findings suggest that the system might be used as a balance and motor function rehabilitation system with further objective measurements of balance ability of the patients in longer terms.", "authors": ["Nam-Gyun Kim", "Yong-Yook Kim", "Tae-Kyu Kwon"], "n_citation": 0, "title": "Development of a Virtual Reality Bicycle Simulator for Rehabilitation Training of Postural Balance", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3d91b5c2-625e-4fd7-988a-ca10dca50034"}
{"abstract": "The development of Web Services has transformed the World Wide Web into a more application-aware information portal. The various standards ensure that Web Services are interpretable and extensible, opening up possibilities for simple services to be combined to build complex ones. The Semantic Web presents a new mechanism for users and software agents to discover, describe, invoke, compose and monitor Web services. For these purposes the Semantic Web Services (OWL-S) ontologies have been developed to provide vocabularies to describe Web Services in a precise and machine-understandable way. It is necessary to ensure the ontological descriptions of the services capture the intended meaning as erroneous description may cause invocation of wrong services, with wrong parameters, resulting in undesired outcome. In this paper, we propose to apply software engineering method and tools to visualize, simulate and verify OWL-S process models. Namely, Live Sequence Charts (LSCs) is used to model services, capturing the inner workings of services, and its tool support Play-Engine is used to perform automated visualization, simulation and checking.", "authors": ["Jun Sun", "Yuan Fang Li", "Hai Wang", "Jing Sun"], "n_citation": 5, "title": "Visualizing and simulating semantic web services ontologies", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4e20b00a-fa74-4585-9351-755bee2dc764"}
{"abstract": "Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.", "authors": ["Christian Gagn\u00e9", "Marc Schoenauer", "Marc Parizeau", "Marco Tomassini"], "n_citation": 0, "title": "Genetic programming, validation sets, and parsimony pressure", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4e5d014f-4b91-4454-9ae3-a66ac51645b6"}
{"abstract": "Driven by the industrial challenge of labeling maps for GIS applications, we investigate the problem of computing the largest convex partitioning of the map P such that the rectangular axis-parallel label L can be placed in it. The map region to be labeled is in general non-convex and may contain holes. Our main result is the new polygonal area removal (PAR) algorithm to identify the area within P where the center of the label L can be placed. We then derive a new and faster algorithm based on the sweep technique that determines the complete set of maximum inscribed rectangles (MIR) in P in the most common case when rectangle sides have an axis-parallel orientation. The set of all maximum inscribed rectangles is then post-processed to produce the best size/orientation combination of the final label placement depending on the specific requirements from the end users.", "authors": ["Marina L. Gavrilova"], "n_citation": 0, "title": "Two Map Labeling Algorithms for GIS Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f7f2170-f184-4d4d-a5ac-ba1ef0812bdb"}
{"abstract": "Improving the performance and effectiveness of usability engineering in software and product development in companies is perceived as a true challenge by many usability professionals. Findings from interviews and observations in eleven assessments of usability engineering processes indicate that usability engineering include typically problems such as poor impact of usability activities in product designs; limited skills and knowledge on usability among the designers and management; unawareness on various activities of usability engineering life-cycle; inappropriately used usability methods; even political games around usability. On the other hand, issues such as project and configuration management, and process performance measures are not the key problems of usability. It is concluded other kinds of methods but standard process assessment should be considered for revealing the problems of usability engineering. The problems identified in the assessment should be clearly communicated to the management, but for developers an assessment should aim for a constructive training occasion on usability.", "authors": ["Timo Jokela"], "n_citation": 0, "title": "Performance rather than capability problems. Insights from assessments of usability engineering processes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5157d702-64a5-4e17-8369-9bc5b8911d8c"}
{"abstract": "In various fields of computer science, rapidly growing hardware power, such as high-speed network, high-performance CPU, huge disk capacity, and large memory space, has been fruitfully harnessed. Examples of such usage are large scale data and web mining, grid computing, and multimedia environments. We propose that such rich hardware can also catapult software engineering to the next level. Huge amounts of software engineering data can be systematically collected and organized from tens of thousands of projects inside organizations, or from outside an organization through the Internet. The collected data can be analyzed extensively to extract and correlate multi-project knowledge for improving organization-wide productivity and quality. We call such an approach for software engineering Mega Software Engineering. In this paper, we propose the concept of Mega Software Engineering, and demonstrate some novel data analysis characteristic of Mega Software Engineering. We describe a framework for enabling Mega Software Engineering.", "authors": ["Katsuro Inoue", "Pankaj K. Garg", "Hajimu Iida", "Ken-ichi Matsumoto", "Koji Torii"], "n_citation": 0, "title": "Mega software engineering", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "545016c9-2127-4a92-8348-d82be60bda06"}
{"abstract": "Rate-constrained coding is one of the many coding-efficiency oriented tools of H.264/AVC, however, the mode decision process of Rate Distortion Optimization (RDO), is characterized by high computational complexity. Many fast mode decision algorithms have been proposed to reduce the computational complexity of mode decision. In this paper, two algorithms are proposed for reduction of mode decision in H.264/AVC, fast reference frame selection and selective intra prediction mode decision. Fast reference frame selection is efficient for inter predication, and selective intra prediction mode decision can effectively reduce excessive calculation load of intra prediction mode decision. The simulation results demonstrated that the proposed two methods applied together could reduce the encoding time of the overall sequences by an average of 44.63%, without any noticeable degradation in coding efficiency.", "authors": ["Woongho Lee", "Jungho Lee", "Ik-Hwan Cho", "Dong-Seok Jeong"], "n_citation": 0, "title": "Reduction of Mode Decision Complexity in H.264/AVC Using Adaptive Selection of Reference Frame and Intra Prediction Mode", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "55f0b70c-2fcc-4a38-a20d-445e50298d2d"}
{"abstract": "Environmental information systems (EIS) have been in professional use for quite a long time. Applications of this domain often include features of quite common location based services (LBS). This means utilization of space-, time- and content-oriented data for positioning and routing services with various maps. Firstly, new integrated technology and lower prices of related technology have made it possible for users to benefit from new mobile services with reasonable cost. Secondly, new kinds of ambient aware applications are active research and development topics. We define mobile environmental information systems (MEIS) as Integrated mobile information systems used to study, monitor and exploit the nature as well as to guide users like tourists and biologists in the nature. In this paper we present our research in progress, where we have built the first MEIS prototype to be used as a basis for MEIS services that exploit LBS and ambient awareness. The main purpose of this experiment has been to explore the technologies and methods for efficient building of easily extendable mobile platforms that would support main MEIS functionalities. According to our literature survey no similar technical solution as a mobile phone service did not exist.", "authors": ["Ari Keronen", "Mauri Myllyaho", "Pasi Alatalo", "Markku Oivo", "Harri Antikainen", "Jarmo Rusanen"], "n_citation": 0, "title": "Experimental development of a prototype for mobile environmental information systems (MEIS)", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "562b17b5-0bad-4978-9050-240b77df6a11"}
{"authors": ["T. G. Semenova"], "n_citation": 0, "title": "Episode-based Conceptual Mining of Large Health Collections", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5b00d265-8d30-4ad0-bb19-2a872bd514ac"}
{"abstract": "In the last several years, software industry has undergone a significant transition to the use of existing component products in building systems. Nowadays, more and more solutions are built by integrating Commercial-Off-The-Shelf (COTS) products rather than building from scratch. This new approach for software development has specific features that add new factors that need to be taken into account to successfully face software development. In this paper, we present the first results of developing a dynamic simulation model to model and simulate the COTS-based software development process with the aim of helping to understand the specific features of this kind of software development, and design and evaluate software process improvements. An example of how to use these dynamic simulation models to study how the system integration starting point affects the main project variables is shown.", "authors": ["Mercedes Ruiz", "Isabel Ramos", "Miguel Toro"], "n_citation": 0, "title": "Using dynamic modeling and simulation to improve the COTS software process", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5d1cdaa9-c9d3-48bd-8aec-ca588e56ade3"}
{"abstract": "The passage of an H +  ion along a carbon nanotube is studied using a time-dependent wavepacket method. The initial state of the problem can be completely specified in terms of the mean energy of the ion along the nanotube, its radial energy (which is necessarily quantised given the wall boundary condition) and its angular momentum along an axis parallel to the nanotube. Its time-dependent flux across two boundaries on the two ends of the nanotube is monitored and examined for various initial conditions. Such calculations can serve to model more complicated systems, such as the migration of ions along cellular membranes.", "authors": ["Dimitris Skouteris", "Antonio Lagan\u00e0"], "n_citation": 0, "title": "Study of the Passage of an H+ Ion Along a Carbon Nanotube Using Quantum Wavepacket Dynamics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5d4c42b0-3576-426e-8637-92c0ecf0bda0"}
{"abstract": "Augmented Reality systems have recently become widely used. This is due to the new open source libraries that have emerged for fast application development. In this paper we address one of the most relevant problems in this type of systems, oscillation in the camera pose estimates. We study the oscillation of a system developed using the ARToolkit library. We apply both average and Kalman filters to stabilize the estimates. Using filter substantially reduces oscillation, thus improving the system's usability.", "authors": ["Monica Rubio", "Arturo Quintana", "Hebert P\u00e9rez-Ros\u00e9s", "Ricardo Quir\u00f3s", "Emilio Camahort"], "n_citation": 0, "title": "Jittering Reduction in Marker-Based Augmented Reality Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6330d93f-b29d-4578-9d5c-0d0042b723f7"}
{"abstract": "The motivation for this research comes from a need to improve software inspection practices in software organizations. Even if inspections are well defined and regularly used in an organization, there may be some problems which can greatly reduce inspection effectiveness. The paper presents a list of inspection related problems which are known in the literature. It also relates some experiences from two case organizations. In addition, this paper provides an approach which helps identifying problems of this kind and directing limited improvement resources effectively.", "authors": ["Sami Kollanus"], "n_citation": 8, "title": "Issues in software inspection practices", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6ac2c015-e0be-4cd6-abc7-7534ab924991"}
{"abstract": "In feed-forward neural networks, all inputs contribute to a greater or lesser extent when calculating the outputs. Therefore, inputs may be ordered from the greatest contributor to the least. Input ranking is non-trivial - cursory examination of the weight and bias matrices fails to reveal ranking. Solving the ranking issue allows the elimination of inputs with little influence on output. This paper presents a new method of determining the input sensitivity of three-layer feed-forward neural networks. Specifically, sensitivity of an input is independent of the magnitudes of the remaining inputs, providing an unambiguous ranking of input importance. Small changes to influential inputs will result in great changes to output. This concept motivated the theoretical approach to input ranking. Examination of theoretical results will demonstrate the correctness of this approach.", "authors": ["Sanggil Kang", "Steve Morphet"], "n_citation": 0, "title": "Estimation of Input Ranking Using Input Sensitivity Approach", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6ae36c20-8d84-4592-93b2-c2141b10b7e2"}
{"abstract": "Towards providing a suitable tool for building the Rule Layer of the Semantic Web, HEx-programs have been introduced as a special kind of logic programs featuring capabilities for higher-order reasoning, interfacing with external sources of computation, and default negation. Their semantics is based on the notion of answer sets, providing a transparent interoperability with the Ontology Layer of the Semantic Web and full declarativity. In this paper, we identify classes of Hex-programs feasible for implementation yet keeping the desirable advantages of the full language. A general method for combining and evaluating sub-programs belonging to arbitrary classes is introduced, thus enlarging the variety of programs whose execution is practicable. Implementation activity on the current prototype is also reported.", "authors": ["Thomas Eiter", "Giovambattista Ianni", "Roman Schindlauer", "Hans Tompits"], "n_citation": 0, "title": "Effective Integration of Declarative Rules with External Evaluations for Semantic-Web Reasoning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6b626a35-e8e6-4a19-b610-363a400f683e"}
{"abstract": "Surface reconstruction is a common problem in computer graphics. Given a set of points sampled from some surface, a triangle mesh interpolating or approximating the points is to be obtained. One of very often used techniques for solving this problem is the selection of surface triangles from the set of Delaunay tetrahedronization faces. In the case of large data, it is difficult to obtain the tetrahedronization due to its huge memory requirements. One of possible solutions is to use distributed computing. Here, we describe the newly developed VSM (Virtual Shared Memory) distributed toolkit and its utilization for the task of surface reconstruction. By our approach, we were able to process dataset having 1.4M points on 4xP4 interconnected via 100Mb Ethernet in 6 hours. About 5GB of memory was consumed during the reconstruction.", "authors": ["Josef Kohout", "Michal Varnuska", "Ivana Kolingerov\u00e1"], "n_citation": 0, "title": "Surface Reconstruction from Large Point Clouds Using Virtual Shared Memory Manager", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6daea09c-2261-4c8a-9fc3-35a50e8928c8"}
{"abstract": "This paper introduces Incentive Method to handle both hard and soft constraints in an evolutionary algorithm for solving some multi-constraint optimization problems. The Incentive Method uses hard and soft constraints to help allocating heuristic search effort more effectively. The main idea is to modify the objective fitness function by awarding differential incentives according to the defined qualitative preferences, to solution sets which are divided by their satisfaction to constraints. It does not exclude the right to access search spaces that violate some or even all constraints. We test this technique through its application on generating solutions for a classic infinite-horizon extensive-form game. It is solved by an Evolutionary Algorithm incorporated by Incentive method. Experimental results are compared with results from a penalty method and from a non-constraint setting. Statistic analysis suggests that Incentive Method is more effective than the other two techniques for this specific problem.", "authors": ["Edward P. K. Tsang", "Nanlin Jin"], "n_citation": 0, "title": "Incentive method to handle constraints in evolutionary algorithms with a case study", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "789f5237-74ee-4ccc-aea1-49582ee0d57c"}
{"abstract": "In this paper we firstly describe the appraisal method that was developed by the Centre for Software Process Technologies (CSPT) to assess software processes within small to medium sized (SMEs) organisations that have little or no experience of software process improvement programmes. We then discuss our experience of developing and using our appraisal method within six SMEs organisations within Northern Ireland. Next we compare our assessment method with existing lightweight assessment methods that have also been used to assess software processes within SMEs software development organisations. We then describe new features that we are currently introducing to improve our software process appraisal method.", "authors": ["Fergal McCaffery", "Donald McFall", "F.G. Wilkie"], "n_citation": 0, "title": "Improving the express process appraisal method", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7934bf5b-de4e-44ba-8356-3824a714e089"}
{"abstract": "A web-based tool developed to automatically correct conceptual database schema is presented. This tool has been integrated into a more general e-learning platform and is used to reinforce teaching and learning on introductory database courses. This platform assigns to each student a set of database problems selected from a common repository. The student has to design a entity-relationship schema and enter it into the system through a user friendly interface specifically designed for it. The correction tool corrects the design and shows detected errors giving advice of how to solve them. The student has the chance to send a new solution. These steps can be repeated as many times as required until a correct solution is obtained.", "authors": ["Ferran Prados", "Imma Boada", "Josep Soler", "Jordi Poch"], "n_citation": 0, "title": "A Web-Based Tool for Entity-Relationship Modeling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7a120531-4d72-48bf-971c-627fa06e594b"}
{"abstract": "The minimum traveling salesman problem with distances one and two is the following problem: Given a complete undirected graph G = (V, E) with a cost function w: E \u2192 {1, 2}, find a Hamiltonian tour of minimum cost. In this paper, we provide an approximation algorithm for this problem achieving a performance guarantee of 315 271. This algorithm can be further improved obtaining a performance guarantee of 65 56. This is better than the one achieved by Papadimitriou and Yannakakis [8], with a ratio 7 6, more than a decade ago. We enhance their algorithm by an involved procedure and find an improved lower bound for the cost of an optimal Hamiltonian tour.", "authors": ["Markus Bl\u00e4ser", "L. Shankar Ram"], "n_citation": 50, "title": "An improved approximation algorithm for TSP with distances one and two", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7afe8d34-50f2-43b0-9473-db9e2da7a06f"}
{"abstract": "In this paper, we considered singularly perturbed self-adjoint boundary-value problems and proposed a computational technique based on spline scheme, which is also suitable for parallel computing. The whole domain is divided into three non-overlapping subdomains and corresponding subproblems are obtained by using zeroth order approximations of the solution at boundaries of these subproblems. The subproblems corresponding to boundary layer regions are solved using adaptive spline scheme. Numerical example is provided to show the efficiency and accuracy.", "authors": ["Rajesh K. Bawa"], "n_citation": 50, "title": "Parallelizable Computational Technique for Singularly Perturbed Boundary Value Problems Using Spline", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7b1548a7-349c-46fc-8cec-1e88061dc86d"}
{"abstract": "Software industries are pursuing the development of software intensive systems with a greater degree of re-use, reduction of costs, and shorter time to market. One of the successful approaches taken is based on the development of sets of similar systems where development efforts are shared. This approach is known as System Families. This article discusses an important issue in system family engineering activities: requirements modelling in system family context. The requirements must contain both the common and variable parts. Also, functional and non-functional aspects have to be considered in system family approach. Besides, an organization framework must be taken into account for requirements management. Some meta-models for these issues in system family are proposed and discussed. Based on the proposed model, a process for requirements management and development according to CMMI practices has been created.", "authors": ["Rodrigo Cer\u00f3n", "Juan C. Due\u00f1as", "Enrique Serrano", "Rafael Capilla"], "n_citation": 50, "title": "A meta-model for requirements engineering in system family context for software process improvement using CMMI", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7d7c5125-991e-4ae9-9aa7-ad4d7931611b"}
{"abstract": "Ontology development is mostly directed at the representation of domain knowledge and much less at the representation of textual or image-based symbols for this knowledge, i.e., the multilingual and multimedia lexicon. To allow for automatic multilingual and multimedia knowledge markup, a richer representation of text and image features is needed. At present, such information is mostly missing or represented only in a very impoverished way. In this paper we propose an RDF/S-based lexicon model, which in itself is an ontology that allows for the integrated representation of domain knowledge and corresponding multilingual and multimedia features.", "authors": ["Paul Buitelaar", "Michael Sintek", "Malte Kiesel"], "n_citation": 0, "title": "A Multilingual/Multimedia Lexicon Model for Ontologies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "85aab2b5-2599-42c5-8e3f-6ae75f5b5f07"}
{"abstract": "This paper reports on theoretical work aimed at providing a harmonious set of tools for tackling the thorny problem of resilience in complex systems. Specifically, key features of resilience are laid out, and the ramifications on necessary theoretical and implementational machinery are analysed. These ramifications constitute a problem definition that, to the authors' knowledge, no extant system is sufficiently sophisticated to meet. It is, however, possible to identify existing components that can be combined to provide the necessary expressivity. In particular, theoretical ecology has individual based modelling approaches that are consonant with artificial intelligence techniques in multi-agent systems, and in philosophical logic, channel theory provides a mechanism for modelling both system energy and system information flow. The paper demonstrates that it is possible to integrate these components into a coherent theoretical framework, laying a foundation for implementation and testing.", "authors": ["C. Hawes", "Chris Reed"], "n_citation": 50, "title": "Theoretical Steps Towards Modelling Resilience in Complex Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "919cb364-0d00-4e7a-a6eb-df4a245ddf5f"}
{"authors": ["Miros\u0142aw Kowaluk", "Gabriela Majewska"], "n_citation": 0, "title": "\u03b2-skeletons for a Set of Line Segments in R2", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "9797bf93-81f5-4185-a995-ea090e2498e4"}
{"abstract": "With the unprecedented growth of storage systems in all of today's society, threats on stored sensitive information have become the critical issues that need to be addressed. Further, compared to the transient risks of data inflight, the risks associated with data-at-rest are more enduring. While there have been many strategies and mechanisms to implement storage security on data-at-rest, these solutions implemented on application level or operating system level have several shortcomings, including weak security and heavy burden on sever load. In this paper, we propose two hardware security structures based on block level, namely, store-and-forward architecture and cut-through architecture. In our approach, we design and implement these architectures based on FPGA. Our experimental results show that our schemes achieve transparency and completeness in real time without decreasing performance of system.", "authors": ["Shichao Ma", "Jizhong Han", "Zhensong Wang"], "n_citation": 0, "title": "Block-Level Storage Security Architectures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "97c94e89-8ab3-4e82-ad20-4502da86f85c"}
{"abstract": "Subdivision surface intersections can be costly to compute. They require the intersection of high resolution meshes in order to obtain accurate results, which can lead to slow performance and high memory usage. In this paper we show how the strong convex hull property can lead to a method for efficiently computing intersections at high resolutions. Consequently, the method can be used with any subdivision scheme that has the strong convex hull property. In this method, a bipartite graph structure is used to track potentially intersecting faces.", "authors": ["Aaron Severn", "Faramarz F. Samavati"], "n_citation": 0, "title": "Fast Intersections for Subdivision Surfaces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "996f0d61-5742-41de-9038-34e25d2ea2be"}
{"abstract": "The use of formal models such as Role Activity Diagrams (RADs) for analysing a process often hide what really happens during that process. In this paper, we build on previous research on informal aspects of the prototyping process and look at the key concerns that prototypers had during the prototyping process. We contrasted those concerns with an analysis of whether documented practice during prototyping was likely to exacerbate or lessen those concerns. The basis of our analysis was a set of interviews with prototypers all of whom were part of a team actively producing evolvable prototypes in an industrial setting. Grounded Theory was used to extract the relevant data (concerns and mitigating practice) from the interview text. Interestingly, only a small number of the concerns of prototypers seemed to be supported by any supportive action, suggesting that there are factors that contribute to project success or failure beyond the control of the prototyping team. However, time and cost pressure seemed to figure largest in our analysis of prototyper concerns. The research highlights the problems that prototypers face and the benefits that an informal analysis can have on our understanding of the process. It also complements our understanding of the formal analysis of process using techniques such as RADs and the human factors therein.", "authors": ["Steve Counsell", "Keith Phalp", "Emilia Mendes", "Stella Geddes"], "n_citation": 0, "title": "The concerns of prototypers and their mitigating practices : An industrial case-study", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "99de88f9-59aa-4f8a-991c-ddf3a20d27df"}
{"abstract": "This paper describes an experiment we are conducting in an industrial setting concerning the introduction of use cases inside the software process with the goal to enhance the process in its very early phases. The process improvement action, named PIUC, is being conducted inside an Italian software development company named Thera S.p.A, and it is a best practice action the firm decided to undertake with the objective to better capture and formalize evolving customer expectations and requirements. In this paper we describe the work performed until now (the experiment will finish at the end of the year) and we present some quantitative data collected during the project and a preliminary qualitative evaluation of the project, highlighting the positive aspects we observed and some reflections about the impact of use cases.", "authors": ["Andrea Valerio", "Massimo Fenaroli"], "n_citation": 0, "title": "Software process improvement through use cases: Building quality from the very beginning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a1fa02e8-c217-481c-9b3e-9afb41bf8b87"}
{"abstract": "The purpose of this study is to introduce an instructional technology known as the learning object. After a review of the literature, the designing steps of a learning object (LO) are tried to explain. As a learning object standard a few details are given about SCORM-Content Aggregation Model and common metadata elements. At the end, a case study about designing of a learning object with the subject of Congestion Control in Computer Networks is tried to give. The content is prepared in English and it contains 13 pictures within 22.htm and 3.exe files. The basic principles of the network congestion and the designed congestion control algorithms are given in the htm pages. Simulators are used to show the working way of the related algorithms step by step. The LO design is made with an open source program RELOAD Editor, according to the ADL SCORM package. The designed LO has been tested on computer engineering students and positive feedbacks are received.", "authors": ["Birim Balci", "Mustafa Murat Inceoglu"], "n_citation": 0, "title": "Reusable Learning Objects (RLOs) for Computer Science Students", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a4273e5f-9463-4df7-b19a-5bc2253eb760"}
{"abstract": "One of the most prominent issues involved in incremental software development is to decide upon the most appropriate software release plans taking into account all explicit and implicit objectives and constraints. Such decisions have become even more complicated in the presence of large number of stakeholders such as different groups of users, managers, or developers. However, early involvement of customers and understanding of their real needs is one of the core success factors of software business [16]. This paper introduces a six step process model for release planning. It is inspired by the Quality Improvement Paradigm [2], as release planning is a learning and improvement process as well. Emphasis is on proposing the tool support implementing this process. The use of the intelligent decision support tool ReleasePlanner is presented by comparing a baseline scenario reflecting current state-of-the practice of release planning with a supposed improvement scenario obtained after usage of the tool. Initial experience from a real-world environment at iGrafx Corel Inc. is used to validate the improvement scenario.", "authors": ["Amandeep", "G\u00fcnther Ruhe", "Mark Stanford"], "n_citation": 0, "title": "Intelligent support for software release planning", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a8455780-19b7-4cbb-b04b-e13bbb52e8ba"}
{"authors": ["Krzysztof Benedyczak", "Aleksander Nowi\u0144ski", "Krzysztof Nowi\u0144ski", "Piotr Ba\u0142a"], "n_citation": 0, "title": "UniGrids Streaming Framework: Enabling Streaming for the New Generation of Grids", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "aa53b06b-7716-41bb-a770-2ae6ede2af12"}
{"abstract": "As a fast data acquisition technique, Light Detection and Ranging (LIDAR) can be widely used in many applications, such as visualization, GIS and mobile communication. Since manual surface reconstruction is very costly and time consuming, the development of automated algorithms is of great importance. In this paper a fully automated technique to extract urban building models from LIDAR data is presented. First, LIDAR points are re-sampled into regular grids with the optimal pixel size. After filling holes in the range image, Delaunay Triangulation is utilized to generate 3D triangle meshes. Building height mask is then applied to extract building roof points. Finally, a geometric primitive-based fitting approach is adopted to verify and refine the reconstructed models. The algorithm is tested on two buildings from a locally acquired LIDAR data set. The results indicate that our approach is suitable for automatically producing urban building models from LIDAR data.", "authors": ["Yuan Luo", "Marina L. Gavrilova"], "n_citation": 50, "title": "3D Building Reconstruction from LIDAR Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ad66e7bf-db89-4ab2-aacf-4c76255a0aa0"}
{"abstract": "The Myoglobin active site has been investigated using Molecular dynamics means in order to model its behaviour as gas molecules carrier. The simulations carried out using the Dl-poly package and the Dreiding force field were able to rationalize some of the observed properties of the system and well compared with DFT results.", "authors": ["Federico Filomia", "Noelia Faginas Lago"], "n_citation": 0, "title": "A Simplified Myoglobin Model for Molecular Dynamics Calculations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b0df7193-509a-4d61-969f-48583dcb3690"}
{"abstract": "The density of propane bulk system (in gas and liquid phase) have been estimated using molecular dynamics calculations. The effect of adopting two different force fields (OPLS/AMBER and Atom-Bond), varying the number of processors and increasing the numbers of molecules has been analysed.", "authors": ["Alessandro Costantini", "Antonio Lagan\u00e0", "Fernando Pirani"], "n_citation": 50, "title": "Parallel Calculation of Propane Bulk Properties", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b3d49793-17a5-4226-bfd9-ba112cbb39eb"}
{"abstract": "Blogging, as a subset of the web as a whole, can benefit greatly from the addition of semantic metadata. The result - which we will call Semantic Blogging - provides improved capabilities with respect to search, connectivity and browsing compared to current blogging technology. Moreover, Semantic Blogging will allow new ways of convenient data exchange between the actors within the blogosphere -blog authors and blog users alike. This paper identifies structural and content-related metadata as the kinds of semantic metadata which are relevant in the domain of blogging. We present in detail the nature of these two kinds of metadata, and discuss an implementation for creating such metadata in a convenient and unobtrusive way for the user, how to publish it on the web, and how to best make use of it from the point of view of a blog consumer.", "authors": ["Knud M\u00f6ller", "Uldis Bojars", "John G. Breslin"], "n_citation": 0, "title": "Using Semantics to Enhance the Blogging Experience", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b8083df1-0cd3-4dcd-abe6-0781098d59ed"}
{"abstract": "It is an important and intriguing issue to know the quantitative similarity of large software systems. In this paper, a similarity metric between two sets of source code files based on the correspondence of overall source code lines is proposed. A Software similarity MeAsurement Tool SMAT was developed and applied to various versions of an operating system(BSD UNIX). The resulting similarity valuations clearly revealed the evolutionary history characteristics of the BSD UNIX Operating System.", "authors": ["Tetsuo Yamamoto", "Makoto Matsushita", "Toshihiro Kamiya", "Katsuro Inoue"], "n_citation": 0, "title": "Measuring similarity of large software systems based on source code correspondence", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bcdd073b-bdb4-47c5-b4d3-8727c53eb6d7"}
{"abstract": "Among the many issues in high dimensional index structures using Minimum Bounding Rectangle(MBR), the reduction of fan-out and increase of overlapping area are the key factors in reduction of search speed. It is known that the usage of only minimum and maximum distance in MBR's pruning process lowers the accuracy of search. In this paper, we present an index structure using cell based MBR in which fan-out gets increased and overlapping is avoided, and a search algorithm which reflects the distribution status of data in MBR to the search. The proposed index structure produces MBR as Vector Approximation-file(VA-file)'s cell units and produces child-MBR by dividing cells. The search algorithm raises the search accuracy by executing pruning using centroid of values included in MBR other than the minimum and maximum distance of cell based MBR and query vector in the k-nn query concerned. Through experiment, we find that the proposed search algorithm has improved its search speed and its accuracy in comparison with existing algorithm.", "authors": ["Bohyun Wang", "Byung-Wook Lee"], "n_citation": 0, "title": "An Efficient Search Algorithm for High-Dimensional Indexing Using Cell Based MBR", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c6fc744f-9afe-4725-96a8-f7c4e5018427"}
{"abstract": "Software development organizations wanting to introduce usability practices into their defined software process have to undertake laborious efforts for that purpose, since, for the time being, there exists a lack of reference model or framework which indicates where and how in the software process usability needs to be considered. They also have to overcome the important differences between HCI (Human-Computer Interaction) and SE (Software Engineering) in terminology and approach to process definition. We offer developers who have the objective of integrating usability practices into their software process, a framework that characterizes 35 selected HCI techniques in relation to six relevant criteria from a SE viewpoint, and organizes them according to the kind of activities in the development process where they may be applied, and to the best moment of application in an iterative life cycle. The only requirement for the existing software process is to be based on an iterative approach.", "authors": ["Xavier Ferre", "Natalia Juristo", "Ana M. Moreno"], "n_citation": 0, "title": "Framework for integrating usability practices into the software process", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cb698a95-df4d-4bcc-b39c-e11017744b40"}
{"abstract": "Important tasks in requirement engineering are resolving requirements inconsistencies between regulators and developers of safety-critical computer systems, and the validation of regulatory requirements. This paper proposes a new approach to the regulatory process, including formulating requirements and elaborating methods for their assessment. We address the differences between prescriptive and nonprescriptive regulation, and suggest a middle approach. Also introduced is the notion of a normative package as the collection of documents to be used by a regulator and provided to a developer. It is argued that the normative package should include not only regulatory requirements but also methods of their assessment. We propose the use of formal regulatory requirements as a basis for development of software assessment methods. This approach is illustrated with examples of requirements for protecting computer control systems against unauthorized access, using the Z notation as the method of formalization.", "authors": ["Sergiy A. Vilkomir", "Aditya K. Ghose"], "n_citation": 0, "title": "Development of a normative package for safety-critical software using formal regulatory requirements", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cf615664-56f8-4a68-9f78-639c8d3d537b"}
{"abstract": "We prove completeness results for twenty-three problems in semilinear geometry. These results involve semilinear sets given by additive circuits as input data. If arbitrary real constants are allowed in the circuit, the completeness results are for the Blum-Shub-Smale additive model of computation. If, in contrast, the circuit is constant-free, then the completeness results are for the Turing model of computation. One such result, the P. NP[log] -completeness of deciding Zariski irreducibility, exhibits for the first time a problem with a geometric nature complete in this class.", "authors": ["Peter B\u00fcrgisser", "Felipe Cucker", "Paulin Jacob\u00e9 de Naurois"], "n_citation": 0, "title": "The complexity of semilinear problems in succinct representation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d80ae164-438a-46f2-b55f-9320294bb60a"}
{"abstract": "We describe a novel method for the generation of tangent-linear code by augmentation of LL-parsers generated by the software tool ANTLR. The main advantage of this approach to source code augmentation is the missing requirement for an internal representation of the original program. We consider this work as the basis for further investigations into how far this technique can be extended in the context of more sophisticated transformations, for example, the automatic generation of adjoint codes. Our prototype tool AD_C_ANTLR currently accepts a subset of the ANSI C standard. We discuss its theoretical basis, and we present a case study to underline the elegance of the parser-based approach to source augmentation.", "authors": ["Uwe Naumann", "Andre Vehreschild"], "n_citation": 0, "title": "Tangent-Linear Models by Augmented LL-Parsers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "db74b3f5-3d50-4a1a-921d-db361502ac62"}
{"authors": ["Milica Milutinovic", "Roel Peeters", "Bart De Decker"], "n_citation": 0, "title": "Secure Negotiation for Manual Authentication Protocols", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "e3aa1e8e-ea50-45c3-8f65-835b73a62f33"}
{"abstract": "We investigated the statistical characteristics of spike sequences of two types of Class II neurons, neurons with subcritical or supercritical Hopf bifurcations, with uncorrelated fluctuation inputs by two statistical coefficients; coefficient of variation and skewness coefficient. We used the Morris-Lecar model and the Hindmarsh-Rose model as neuron models. As a result. even if the models belong to the same class, the interspike interval statistics exhibit different characteristics. We also discovered that the origin of the differences comes from a precise bifurcation structure, and the differences also affect the relationship on variation of input and variation of output. The results indicate that we have to introduce at least three classes by its bifurcation types to classify the neurons.", "authors": ["Ryosuke Hosaka", "Yutaka Sakai", "Tohru Ikeguchi", "Shuji Yoshizawa"], "n_citation": 0, "title": "Different Responses of Two Types of Class II Neurons for Fluctuated Inputs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e44ebf24-774e-4c9c-a868-b1cfd31388fd"}
{"abstract": "In this article, we describe an approach to empirical software engineering based on a combined software factory and software laboratory. The software factory develops software required by an external customer while the software laboratory monitors and improves the processes and methods used in the factory. We have used this approach during a period of four years to define and evaluate a software process that combines practices from Extreme Programming with architectural design and documentation practices in order to find a balance between agility, maintainability and reliability.", "authors": ["Ralph-Johan Back", "Luka Milovanov", "Ivan Porres"], "n_citation": 50, "title": "Software development and experimentation in an academic environment : The gaudi experience", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e5534690-bd2f-45b6-bed8-16cbf7c7a4c7"}
{"abstract": "Every organisation has to acquire, create, store, distribute and use knowledge in order to operate effectively, or simply to be able to operate in the first place. Knowledge is needed during projects and tools can be used for supporting projects' knowledge management. However, it has proven to be difficult to find the best suitable tool for a specific project and to determine the requirements for a knowledge management tool that would enable supporting projects in an efficient way. This paper describes how knowledge needs for supporting the project lifecycle have been defined in an expert organisation and what the knowledge needs are in the different roles of the organisation. It was found that the main knowledge needs were the experiences and the main results from ongoing and closed projects, along with project specific information like used tools and methods.", "authors": ["Susanna Peltola", "Maarit Tihinen", "P\u00e4ivi Parviainen"], "n_citation": 0, "title": "What are the knowledge needs during the project lifecycle in an expert organisation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e8dc1f0f-69ec-4aee-a05f-779ec95a3826"}
{"abstract": "Software process improvement often lacks strong links to project management and control activities, which are concerned with identifying the need of process change and triggering improvement initiatives. Project management, on the other hand, often fails at selecting appropriate software engineering methods and technology that help to ensure project success. This paper proposes a model that guides project managers (1) to set up a project so that it can reach its specific goals and (2) to identify corrective actions (or changes) once a project is at risk of failing its goals. The model complements established improvement methods such as CMMI, GQM, and Experience Factory and links them to those project management activities that often are the starting point of improvement initiatives.", "authors": ["Andreas Birk", "Dietmar Pfahl"], "n_citation": 15, "title": "A systems perspective on software process improvement", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e99bd814-9a9e-44cc-a877-79f864f268e6"}
{"abstract": "Leftist grammars can be characterized in terms of rules of the form a \u2192 ba and cd \u2192 d, without distinction between terminals and nonterminals. They were introduced by Motwani et. al. [9], where the accessibility problem for some general protection system was related to the membership problem of these grammars. This protection system was originally proposed in [3,10] in the context of Java virtual worlds. We show that the set of languages defined by general leftist grammars is not included in CFL, answering in negative a question from [9]. Moreover, we relate some restricted but naturally defined variants of leftist grammars to the language classes of the Chomsky hierarchy.", "authors": ["Tomasz Jurdzinski", "Krzysztof Lorys"], "n_citation": 0, "title": "Leftist grammars and the chomsky hierarchy", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ed713365-04e9-432a-9526-f6ca14163e4f"}
{"abstract": "This paper proposes a Software Engineering Process (SEP) benchmarking methodology and a benchmark-gap analysis technique to assist industrial practitioners in quantitative software engineering. This work adopts the comprehensive SEP Assessment model (SEPRM), as a foundation to SEP benchmarking. In this approach, a number of conventional benchmarking challenges may be overcome. Case studies are presented to demonstrate the usage of the benchmarking technologies and supporting tools.", "authors": ["Vincent Chiew", "Yingxu Wang"], "n_citation": 4, "title": "Software Engineering Process benchmarking", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f19e9993-7548-4819-8c58-633b37777e5c"}
{"abstract": "Methods for network reliability and reliability polynomials computation are presented. Networks with absolutely reliable edges and unreliable nodes are considered. Tasks for reliability computation using such quality of service criteria as ability to transmit given data stream with time restrictions and without them are considered and corresponding algorithms are developed.", "authors": ["Mikhail Y. Murzin", "Alexey S. Rodionov"], "n_citation": 0, "title": "Branching Method for Computation of Probability of Given Data Stream Transmission", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "01f12a92-9536-44a3-b87f-b017ed44725a"}
{"abstract": "This paper proposes a model of successful maintenance that can represent how maintainability of each software module changes as it ages. We found that some of the metrics, measured from 20 years old system, fit our model; i.e., values of those metrics seem to be proportional to the degree of maintainability. We described each metric's critical line - a value that can be used for deciding whether a module should be re-engineered or not. We also described the interpretations on how each software metric relates to maintainability.", "authors": ["Akito Monden", "Shin-ichi Sato", "Ken Ichi Matsumoto", "Katsuro Inoue"], "n_citation": 0, "title": "Modeling and analysis of software aging process", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "0c2bc92f-b3c0-424c-a87e-0e7cc8a17136"}
{"abstract": "WebQuest is a model for constructivist inquiry-based learning in which the information used by learners is collected from the Web. A WebQuest exists in form of a Web site that contains a defined set of componential Web pages. In this paper, we specify the WebQuest Markup Language (WQML) for WebQuest construction. WQML enables WebQuests to be implemented as sharable courseware objects and thus to be interoperable with most learning management systems (LMS).", "authors": ["Sebastian Fleissner", "Yuen-Yan Chan", "Tsz Hon Yuen", "Victor Ng"], "n_citation": 0, "title": "WebQuest Markup Language (WQML) for Sharable Inquiry-Based Learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0e114215-ac3f-435e-8a00-98487715330d"}
{"abstract": "This paper discusses about the results of using OO (Object-Oriented) measures for the small-sized programs in C++. The metric values for the class level Object-Oriented properties: inheritance, coupling and cohesion are computed and compared with the existing study results for medium sized programs. Among the three properties, inheritance is used widely in the programs. The effective use of the three properties for the proper abstraction of the class design is investigated using six hypotheses proposed. The result of the investigation shows that inheritance and cohesion are used well for the design of attribute level abstraction in the classes.", "authors": ["S. Kanmani", "V. Rhymend Uthariaraj", "V. Sankaranarayanan", "P. Thwnbidurai"], "n_citation": 0, "title": "Measuring the Object-Oriented properties in small sized C++ programs: An empirical investigation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "10dc5c28-666b-4b96-b559-a99f0c3cb353"}
{"abstract": "Luminaire sampling plays an important role in global illumination calculation using Monte Carlo integration. A conventional approach generates samples on the surface of the luminaire, resulting in rendered images with high variance of noise. In this paper, we present an efficient solid angle sampling technique using tunable bounding volumes for global illumination calculation. In contrast to the conventional approach, our technique derives samples from the solid angle subtended by the luminaire. In the construction process, we build a convex, frustum-like polyhedron as a bounding volume for a light source. Front-facing polygons of the bounding volume are then projected onto the unit hemisphere around the shaded point. These projected polygons represent the approximated solid angle subtended by the luminaire. The third step samples the projected spherical polygons on which a number of stratified samples are generated. We employ various types of light sources including ellipse, elliptic cylinder, elliptic cone and elliptic paraboloid. We perform our technique for Monte Carlo Direct Lighting and Monte Carlo Path Tracing applications. Under similar sample numbers, our technique produces images with less variance of noise compared to the conventional method. In addition, our technique provides roughly equal image quality in less execution time. Our approach is simple, efficient, and applicable to many types of luminaries for global illumination calculation.", "authors": ["Yuan-Yu Tsai", "Chung-Ming Wang", "Chung-Hsien Chang", "Yu-Ming Cheng"], "n_citation": 50, "title": "Tunable Bounding Volumes for Monte Carlo Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "12c0f341-bb10-4829-bcba-c28a4cd3ef63"}
{"abstract": "Modern build systems help increase developer productivity by performing incremental building and testing. These build systems view a software project as a group of interdependent modules and perform regression test selection at the module level. However, many large software projects have imprecise dependency graphs that lead to wasteful test executions. If a test belongs to a module that has more dependencies than the actual dependencies of the test, then it is executed unnecessarily whenever a code change impacts those additional dependencies.#R##N##R##N#In this paper, we formulate the problem of wasteful test executions due to suboptimal placement of tests in modules. We propose a greedy algorithm to reduce the number of test executions by suggesting test movements while considering historical build information and actual dependencies of tests. We have implemented our technique, called TestOptimizer, on top of CloudBuild, the build system developed within Microsoft over the last few years. We have evaluated the technique on five large proprietary projects. Our results show that the suggested test movements can lead to a reduction of 21.66 million test executions (17.09%) across all our subject projects. We received encouraging feedback from the developers of these projects; they accepted and intend to implement a80% of our reported suggestions.", "authors": ["August Shi", "Suresh Thummalapenta", "Shuvendu K. Lahiri", "Nikolaj Bj\u00f8rner", "Jacek Czerwonka"], "n_citation": 0, "references": ["26e76d44-fedc-42de-a1bd-523c3a45acd5", "2b70ff71-d793-432c-acf8-629a0b4c9801", "35e51aa4-87f5-4732-88b7-5bbe9726330d", "37341ec2-1b55-4072-b931-075e1a7e5cc9", "44cc6070-b7e5-4fba-90a0-e0ed84e85775", "62e24d3f-a586-4ef3-b4aa-54f093bb7ec3", "6e8ea39b-5d8f-43e5-9663-84abb1b52da6", "7aee8844-f0de-43ba-946c-88d489f1ba13", "7fe3d188-1d72-4b4b-a932-8a557abc8198", "8034174e-33f3-4bf1-b75b-9b58d2ae6837", "86dcc01c-618c-44e8-a99a-786830b06861", "9198f950-e354-48ee-a05c-739fdc092071", "965c9dcf-f372-4ba9-aeb7-93a61cf1ec4e", "a3921949-ef96-4f72-9fa1-de54c33a5e9f", "aaaeeccc-1359-4036-a1dc-5d023223a3fc", "d01cb3f3-cd0e-4981-9c9b-880dc704c8f5", "df5ff672-d5a0-4eaa-8eea-046d0b892892"], "title": "Optimizing test placement for module-level regression testing", "venue": "international conference on software engineering", "year": 2017, "id": "15db751f-42be-43fd-bcc2-a444fc6115d3"}
{"abstract": "The main aim of this paper is to present how to refine software logical architectures by application of a recursive model-based transformation approach called 4SRS (four step rule set). It is essentially based on the mapping of UML use case diagrams into UML object diagrams. The technique is based on a sequence of steps that are inscribed in a tabular representation that is used to derive the software architecture for a focused part of the global system.", "authors": ["Ricardo J. Machado", "Jo\u00e3o M. Fernandes", "Paula Monteiro", "Helena Rodrigues"], "n_citation": 50, "title": "Refinement of software architectures by recursive model transformations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1682f778-ee82-4593-85d6-c1deab9a646b"}
{"abstract": "In this paper we present a MOF compliant metamodel and UML profile for the Semantic Web Rule Language (SWRL) that integrates with our previous work on a metamodel and UML profile for OWL DL. Based on this metamodel and profile, UML tools can be used for visual modeling of rule-extended ontologies.", "authors": ["Saartje Brockmans", "Peter Haase", "Pascal Hitzler", "Rudi Studer"], "n_citation": 0, "title": "A Metamodel and UML Profile for Rule-Extended OWL DL Ontologies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "16e1b3ce-6883-4738-9aee-de56e8f309eb"}
{"authors": ["Pedro Alonso\u2013Jord\u00e1", "Jes\u00fas Peinado Pinilla", "Isabel P\u00e9rez Arjona", "V\u00edctor Jos\u00e9 S\u00e1nchez Morcillo"], "n_citation": 50, "title": "Efficient Simulation of Spatio-temporal Dynamics in Ultrasonic Resonators", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "18cc8d02-8097-46f3-bdb9-bfe5be60a799"}
{"abstract": "A dextran monomer and a lOmer under constant pulling speed or constant force were studied using the atomistic simulations. Molecular dynamics (MD) with the Amber94 and Amber-Glycam04 forcefields were performed. The main result of the present Amber-based MD simulations is that the experimental plateau of the force-extension dependence for dextran can be explained by a transition of the glucopyranose rings in the dextran monomers from a chair ( 4 C 1 ) to a inverted chair ( 1 C 4 ) conformation whereas chair to boat transitions occur at higher forces. MD simulation of coarse-grained model of dextran consisting of two- or three-state monomers were performed to clarify the molecular mechanism of dextran extension.", "authors": ["Igor M. Neelov", "David B. Adolf", "Tom McLeish"], "n_citation": 0, "title": "New Molecular Mechanism of Dextran Extension in Single Molecule AFM", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "191a1c98-d5bf-4b80-a12d-f4ce5ab0fd02"}
{"abstract": "The activity of the software process improvement can be supported by reusing various kinds of knowledge on existing successful software processes in the form of process patterns. There are several catalogs of process patterns available on WWW; however, all of relations among patterns are closed in each pattern catalog. To acquire the cross-cutting relations over the different process pattern catalogs, we have applied the technique for the automatic relation analysis among the patterns. Our technique utilizes existing text processing techniques to extract patterns from documents and to calculate the strength of pattern relations. As a result of experimental evaluations, it is found that the system implementing our technique has extracted appropriate cross-cutting relations over the different process pattern catalogs without information on relations described in original pattern documents. These cross-cutting relations will be useful for dealing with larger problems than those dealt with by individual process patterns.", "authors": ["Hironori Washizaki", "Atsuto Kubo", "Atsuhiro Takasu", "Yoshiaki Fukazawa"], "n_citation": 50, "title": "Relation analysis among patterns on software development process", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1a976b1e-4ca4-4cec-8433-12a1397146bf"}
{"abstract": "We present a novel dynamic duty cycling scheme to maintain stochastic consistency for caches in sensor networks. To reduce transmissions, base stations often maintain caches for erratically changing sensor sources. Stochastic consistency guarantees the cache-source deviation is within a pre-specified bound with a certain confidence level. We model the erratic sources as Brownian motions, and adaptively predict the next cache update time based on the model. By piggybacking the next update time in each regular data packet, we can dynamically adjust the relaying nodes' duty cycles so that they are awake before the next update message arrives, and are sleeping otherwise. Through simulations, we show that our approach can achieve very high source-cache fidelity with low power consumption on many real-life sensor data. On average, our approach consumes 4-5 times less power than GAF [1] and achieves 50% longer network lifetime.", "authors": ["Shanzhong Zhu", "Wei Wang", "Chinya V. Ravishankar"], "n_citation": 0, "title": "Stochastically consistent caching and dynamic duty cycling for erratic sensor sources", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "20a9cc49-b53d-4702-9ccb-1cabaa9e8ddd"}
{"abstract": "This paper investigates the locality of the genotype-phenotype mapping (representation) used in grammatical evolution (GE). The results show that the representation used in GE has problems with locality as many neighboring genotypes do not correspond to neighboring phenotypes. Experiments with a simple local search strategy reveal that the GE representation leads to lower performance for mutation-based search approaches in comparison to standard GP representations. The results suggest that locality issues should be considered for further development of the representation used in GE.", "authors": ["Franz Rothlauf", "Marie Oetzel"], "n_citation": 0, "title": "On the locality of grammatical evolution", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2323a87f-52da-47d3-b7a0-929b9355feaf"}
{"abstract": "Software engineering measurement and metrics are key technologies toward quantitative software engineering. However, software measurement is so complicated that practitioners in software engineering might not be able to adopt and use a comprehensive measurement system. To address this problem, a software engineering measurement expert system tool (SEMEST) is developed based on the software engineering measurement system (SEMS) established by TESERC [8, 9]. SEMEST provides an expert environment for supporting and implementing software engineering measurement, metrical analysis, and benchmarking in the software industry.", "authors": ["Yingxu Wang", "Qing He", "Chris Kliewer", "Tony Khoo", "Vincent Chiew", "Wendy Nikoforuk", "Lian Chen"], "n_citation": 0, "title": "Product and process metrics: A software engineering measurement expert system", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "24042b6b-8cce-41c6-b29e-67a9ef2ccc72"}
{"abstract": "Semantic Web Services were developed with the goal of automating the integration of business processes on the Web. The main idea is to express the functionality of the services explicitly, using semantic annotations. Such annotations can, for example, be used for service discovery-the task of locating a service capable of fulfilling a business request. In this paper, we present a framework for annotating Web Services using description logics (DLs), a family of knowledge representation formalisms widely used in the Semantic Web. we show how to realise service discovery by matching semantic service descriptions, applying DL inferencing. Building on our previous work, we identify problems that occur in the matchmaking process due to the open-world assumption when handling incomplete service descriptions. We propose to use autoepistemic extensions to DLs (ADLs) to overcome these problems. ADLs allow for non-monotonic reasoning and for querying DL knowledge bases under local closed-world assumption. We investigate the use of epistemic operators of ADLs in service descriptions, and show how they affect DL inferences in the context of semantic matchmaking.", "authors": ["Stephan Grimm", "Boris Motik", "Chris Preist"], "n_citation": 0, "title": "Matching Semantic Service Descriptions with Local Closed-World Reasoning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "24d2642a-7e69-4b97-8014-a2aab8bf5f86"}
{"abstract": "We propose a generic communication primitive designed for sensor networks. Our primitive hides details of network communication while retaining sufficient programmer control over the communication behavior of an application; it is designed to ease the burden of writing application-specific communication protocols for efficient, long-lived, fault-tolerant, and scalable applications. While classical network communication methods expect high-reliability links, our primitive works well in highly unreliable environments without needing to detect and prune unreliable links. Our primitive resembles the chemical markers used by many biological systems to solve distributed problems (pheromones). We develop and analyze the performance of an implementation of this primitive called Virtual Pheromone (VP). We demonstrate that VP can attain performance comparable to classical methods for applications such as sleep scheduling, routing, flooding, and cluster formation.", "authors": ["Leo Szumel", "John D. Owens"], "n_citation": 0, "title": "The virtual pheromone communication primitive", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "285fa7f3-6293-463c-822b-488082e5f8dd"}
{"abstract": "We introduce a new version of the server problem: the delayed server problem. In this problem, once a server moves to serve a request, it must wait for one round to move again, but could serve a repeated request to the same point. We show that the delayed k-server problem is equivalent to the (k - 1)-server problem in the uniform case, but not in general.", "authors": ["Wolfgang W. Bein", "Kazuo Iwama", "Lawrence L. Larmore", "John Noga"], "n_citation": 0, "title": "The delayed k-server problem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2e620c52-3cd7-4a6e-844c-43c6ccc8464d"}
{"abstract": "Flash memory cannot be overwritten unless erased in advance. In order to avoid having to erase during every update, non-inplace-update schemes have been used. Since updates are not performed in place, obsolete data are later reclaimed by garbage collection. In this paper, we study a new garbage collection algorithm to reduce the cleaning cost such as the number of erase operations and the number of data copies. The proposed scheme automatically predicts the future I/O workload and intelligently selects the victims according to the predicted I/O workload. Experimental results show that the proposed scheme performs well especially when the degree of locality is high.", "authors": ["Long-zhe Han", "Yeonseung Ryu", "Tae-Sun Chung", "Myungho Lee", "Sukwon Hong"], "n_citation": 50, "title": "An Intelligent Garbage Collection Algorithm for Flash Memory Storages", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2ee1a7a4-33b4-4481-8913-83301e28cc84"}
{"abstract": "Many applications like picture processing, data compression or pattern recognition require a covering of a set of points most often located in the (discrete) plane by rectangles due to some cost constraints. In this paper we provide exact dynamic programming algorithms for covering point sets by regular rectangles, that have to obey certain boundary conditions. The objective function is to minimize sum of area, circumference and number of patches used. This objective function may be motivated by requirements of numerically solving PDE's by discretization over (adaptive multi-)grids.", "authors": ["Stefan Porschen"], "n_citation": 50, "title": "Algorithms for Rectangular Covering Problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2fe3b580-eea1-4ac9-8564-baf75f29a56a"}
{"abstract": "Current developments in Knowledge Management concern the sharing and usage of knowledge in dynamic environments. The need for systems that both react to and anticipate the needs and expectations of users calls for flexible and adaptable development and implementation frameworks. These are exactly the characteristics that identify software agents and agent societies, which make natural the application of the agent paradigm in KM. This paper attempts to identify both the advantages of agents for KM, and the aspects of KM that can benefit most from this paradigm. Furthermore, the paper describes several current KM projects that use agent technology and identifies open research areas.", "authors": ["Virginia Dignum"], "n_citation": 0, "title": "An overview of agents in knowledge management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "302145f7-6273-4925-8792-911c88a81aa9"}
{"abstract": "Data on a storage device are easier targets for malicious attackers. Storing data in an encrypted form is an effective way to improve data security. In an encrypted storage system, key management is one of the most challenging tasks. In this paper, we propose a new key management scheme for distributed encrypted storage that has various salient features. First, in the proposed scheme, encryption keys are not directly known to users. Due to this property, the security of the encrypted data is not deteriorated though some users that have shared the data lose the access right. Second, in the proposed scheme, even if some components of the system are attacked, the security of the system is still guaranteed. Third, the system provides high availability by exploiting the secret sharing scheme.", "authors": ["Myungjin Lee", "Hyokyung Bahn", "Kijoon Chae"], "n_citation": 0, "title": "A New Key Management Scheme for Distributed Encrypted Storage Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "340a5828-fcb4-48cb-8d4c-abe3f07ce3f3"}
{"abstract": "3D surface registration is commonly used in shape analysis, surface representation, and medical image aided surgery. This technique is extremely computationally expensive and sometimes will lead to bad result configured with unstructured mass data for its' iterative searching procedure and ill-suited distance function. In this paper, we propose a novel neural network strategy for surface registration. Before that, a typical preprocessing procedure-mesh PCA is used for coordinate direction normalization. The results and comparisons show such neural network method is a promising approach for 3D shape matching.", "authors": ["Heng Liu", "Jingqi Yan", "David Zhang"], "n_citation": 50, "title": "A Neural Network Strategy for 3D Surface Registration", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "34f8a31c-056b-4299-9520-98aee19bdff1"}
{"abstract": "This paper proposes a simple method for approximating the optimal moving paths of a huge robot reclaimer located in the outdoor material stock yard with emphasis on safety, energy consumption, and transfer time. The reclaimer is equipped with a 3D range finder to measure the shape of material piles in the yard, and the material yard is modeled into 3D space where 2D section of grid type is constructed in several layers. To define a safety function against moving between grids, a simplified Voronoi diagram that has a minimized extension error of vertex is used. In addition, the function of energy consumption and transfer time required when the control point of the reclaimer moves between 3D grids is defined. This is used as a cost evaluation factor of path optimization along with the safety function. The proposed method can be readily applied to low-performance industrial control devices.", "authors": ["Kwan-Hee Lee", "Hyo-Jung Bae", "Sung-Je Hong"], "n_citation": 50, "title": "Approximation of Optimal Moving Paths of Huge Robot Reclaimer with a 3D Range Finder", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3625cc78-d341-4de4-b768-76f2d8119c38"}
{"authors": ["David P. Anderson"], "n_citation": 0, "references": [], "title": "Nailing smoke", "venue": "Communications of The ACM", "year": 2016, "id": "36e94423-f25e-4c06-857f-e6e2b98059bd"}
{"abstract": "In the e-Business domain, workflows are central artifacts that are used to specify application systems. To realize reuse at a large scale for e-Business application systems, therefore, workflows need to be reused systematically. To this end workflows must be classified, documented, and stored in a way that enables their identification, evaluation, and adaptation in order to integrate them in an application. Software product line engineering is an established and approved software engineering approach that addresses these issues by handling a number of similar software systems together, enabling large scale reuse during the development and maintenance of the different systems covered by the product line. In this paper, we transfer the concepts of software product line engineering to the domain of e-Business systems by applying the product line techniques to workflows and present initial validation results.", "authors": ["Joachim Bayer", "Mathias Kose", "Alexis Ocampo"], "n_citation": 0, "title": "Improving the development of e-business systems by introducing process-based software product lines", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37a247f4-2b2c-4dc9-80bb-614cdf2b3b1c"}
{"abstract": "Component based development (CBD), product line engineering (PLE), and model driven architecture (MDA) are representative approaches for software reuse. CBD and PLE focus on reusable assets of components and core assets, MDA focuses on transforming reusable models into implementation. Although these approaches are orthogonal, they can be integrated into a comprehensive and extremely effective framework for software development. In this paper, we first present our strategies of integrating CBD, PLE and MDA, and propose an integrated process that adopts reuse engineering and automation paradigm. By applying the proposed approach, it becomes feasible to semiautomatically develop a number of applications in a domain.", "authors": ["Soo Dong Kim", "Hyun Gi Min", "Jin Sun Her", "Soo Ho Chang"], "n_citation": 0, "title": "An extreme approach to automating software development with CBD, PLE and MDA integrated", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3845b065-c052-4cca-9cd3-7f28b4eac7be"}
{"abstract": "Aiming to build a complete benchmark for better evaluation of existing ontology systems, we extend the well-known Lehigh University Benchmark in terms of inference and scalability testing. The extended benchmark, named University Ontology Benchmark (UOBM), includes both OWL Lite and OWL DL ontologies covering a complete set of OWL Lite and DL constructs, respectively. We also add necessary properties to construct effective instance links and improve instance generation methods to make the scalability testing more convincing. Several well-known ontology systems are evaluated on the extended benchmark and detailed discussions on both existing ontology systems and future benchmark development are presented.", "authors": ["Li Ma", "Yang Yang", "Zhaoming Qiu", "Guotong Xie", "Yue Pan", "Shengping Liu"], "n_citation": 269, "title": "Towards a Complete OWL Ontology Benchmark", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c93180f-7a9c-472e-84e2-738b0a6bf08e"}
{"abstract": "The development of software for wireless services on the Internet is a challenging task due to the extreme time-to-market pressure, the newness of the application domain, and the quick evolution of the technical infrastructure. Nevertheless, developing software of a predetermined quality in a predictable fashion can only be achieved with systematic development processes and the use of engineering principles. Thus, systematic development processes for this domain are needed urgently. This article presents a method for the design of an adaptable software development process based on existing practices from related domains, industrial piloting, and expert knowledge. First results of the application of the method for the wireless Internet services domain are described. The benefit for the reader is twofold: the article describes a validated method on how to gain process knowledge for an upcoming field fast and incrementally. Furthermore, first results of the process design for the wireless Internet services domain are given.", "authors": ["Ulrike Becker-Kornstaedt", "Daniela Boggio", "J\u00fcrgen M\u00fcnch", "Alexis Ocampo", "Gino Palladino"], "n_citation": 50, "title": "Empirically driven design of software development processes for wireless Internet services", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "41dd603a-49de-4b86-813a-f76951c11532"}
{"abstract": "Voronoi diagram for 3D balls can be applicable to various fields in science and engineering. The edge-tracing algorithm constructs the Voronoi diagram in O(mn) time in the worst-case where m and n are the numbers of edges and balls, respectively. The computation time of the algorithm is dominated by finding the end vertex of a given edge since all edges in the Voronoi diagram should be traced essentially. In this paper, we define the feasible region which a ball to define the end vertex of a given edge should intersect. Then, balls which do not intersect the feasible region are filtered out before finding an end vertex since they cannot define an end vertex. Therefore, we improve the runtime-performance of the edge-tracing algorithm via the feasible region.", "authors": ["Youngsong Cho", "Dong-Uk Kim", "Hyun Chan Lee", "Joon Young Park", "Deok-Soo Kim"], "n_citation": 50, "title": "Reduction of the Search Space in the Edge-Tracing Algorithm for the Voronoi Diagram of 3D Balls", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "473daca6-7b8b-44e1-93c7-b7aaafdc7f8b"}
{"abstract": "Knowledge representation formalisms used on the Semantic Web adhere to a strict open world assumption. Therefore, nonmonotonic reasoning techniques are often viewed with scepticism. Especially negation as failure, which intuitively adopts a closed world view, is often claimed to be unsuitable for the Web where knowledge is notoriously incomplete. Nonetheless, it was suggested in the ongoing discussions around rules extensions for languages like RDF(S) or OWL to allow at least restricted forms of negation as failure, as long as negation has an explicitly defined, finite scope. Yet clear definitions of such scoped negation as well as formal semantics thereof are missing. We propose logic programs with contexts and scoped negation and discuss two possible semantics with desirable properties. We also argue that this class of logic programs can be viewed as a rule extension to a subset of RDF(S).", "authors": ["Axel Polleres", "Cristina Feier", "Andreas Harth"], "n_citation": 0, "title": "Rules with Contextually Scoped Negation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "49ea1bc6-762b-4add-9c4b-dca19bcc1f01"}
{"abstract": "While the objectives of Six Sigma are to reduce variation and prevent defects, it is also a management philosophy that includes the need for fact-based decisions, customer focus, and teamwork. In attempting to build a COTS integrated system, selection of candidates typically pays attention to specify search criteria and goals to be met. Yet they often overlook some elements in the process such as fact-based decisions and teamwork, which might drive the process helping increase the probability of success. In this paper, we describe and illustrate a Six Sigma-based proposal for the process of selecting and integrating COTS components. Our approach also suggests some tools and measures to be applied during its specific phases.", "authors": ["Alejandra Cechich", "Mario Piattini"], "n_citation": 0, "title": "Managing COTS components using a six sigma-based process", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4a093c97-7142-412e-9fb9-b8d9bd5a363c"}
{"abstract": "This paper addresses the almost never-ending headache the role of documentation has given for software projects. Working software has been given recently a focus over comprehensive documentation, yet the required documents should be authored. This paper revisits the approach developed by Nokia improving the documentation work without scarifying the quantity or quality of documentation. The method is called RaPiD7. The cases presented are from Philips Digital Systems Laboratory. This paper elaborates the method by providing insights to applying RaPiD7 in practice, explains the encouraging results of the experiments and gives tips for practitioners of the method by explaining the lessons learned in Philips.", "authors": ["Ko Dooms", "Roope Kylm\u00e4koski"], "n_citation": 0, "title": "Comprehensive documentation made agile : Experiments with RaPiD7 in Philips", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "57b96b8a-0907-441c-8d77-aee47a69270a"}
{"abstract": "Background: Test-driven development (TDD) and pair programming are software development practices popularized by eXtreme Programming methodology. The aim of the practices is to improve software quality. Objective: Provide an empirical evidence of the impact of both practices on package dependencies playing a role of package level design quality indicators. Method: An experiment with a hundred and eighty eight MSc students from Wroclaw University of Technology, who developed finance-accounting system in different ways (CS -- classic solo, TS - TDD solo, CP - classic pairs, TP - TDD pairs). Results: It appeared that package level design quality indicators (namely package dependencies in an object-oriented design) were not significantly affected by development method. Limitations: Generalization of the results is limited due to the fact that MSc students participated in the study. Conclusions: Previous research revealed that using test-driven development instead of classic (test-last) testing approach had statistically significant positive impact on some class level software quality indicators (namely CBO and RFC metrics) in case of solo programmers as well as pairs. Combined results suggest that the positive impact of test-driven development on software quality may be limited to class level.", "authors": ["Lech Madeyski"], "n_citation": 0, "title": "The impact of pair programming and test-driven development on package dependencies in object-oriented design : An experiment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "57dbd766-184f-4d5a-8168-e85dfb1d5cbd"}
{"abstract": "This paper introduces logspace optimization problems as analogues of the well-studied polynomial-time optimization problems. Similarly to them, logspace optimization problems can have vastly different approximation properties, even though the underlying decision problems have the same computational complexity. Natural problems, including the shortest path problems for directed graphs, undirected graphs, tournaments, and forests, exhibit such a varying complexity. In order to study the approximability of logspace optimization problems in a systematic way, polynomial-time approximation classes are transferred to logarithmic space. Appropriate reductions are defined and optimization problems are presented that are complete for these classes. It is shown that under the assumption L \u00ac= NL some logspace optimization problems cannot be approximated with a constant ratio; some can be approximated with a constant ratio, but do not permit a logspace approximation scheme; and some have a logspace approximation scheme, but cannot be solved in logarithmic space. A new natural NL-complete problem is presented that has a logspace approximation scheme.", "authors": ["Till Tantau"], "n_citation": 0, "title": "Logspace optimization problems and their approximability properties", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "589c1437-d879-446c-8938-9ee501aeaef0"}
{"abstract": "We developed a collaborative software development community-ware whose goal is to provide practical tasks for the students in software engineering education in a university and to provide software for education purpose. We performed a pilot experimentation for the community-ware. The results show that software requesters made software change requests for not only functional aspects but also user interface. This paper proposes a user interface oriented requirement elicitation process. The process is that software requesters present functional requests and user interface design which met the requests. Then based on the requests and the user interface, developers and software requesters decide priority for each requirement in a collaborative manner.", "authors": ["Masatoshi Shimakage", "Atsuo Hazeyama"], "n_citation": 0, "title": "A requirement elicitation method in collaborative software development community", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5c31ed34-537e-459a-944c-ebe78fc20705"}
{"abstract": "Process Pattern is an emergent approach to reuse process knowledge. However, in practice this concept still remains difficult to be exploited due to the lack of formalization and supporting methodology. In this paper, we propose a way to formalize the process pattern concept by introducing it into a process meta-model. We provide a general definition to cover various kinds of process-related patterns in different domains. We define rigorously process concepts and their relations to allow representing processes based on process patterns and to facilitate the development of supporting tools. By distinguishing process patterns at different abstraction levels, we aim to develop a systematic approach to define and apply process patterns.", "authors": ["Hanh Nhi Tran", "Bernard Coulette", "Bich Thuy Dong"], "n_citation": 0, "title": "A UML-based process meta-model integrating a rigorous process patterns definition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "62b45553-97bf-4541-8a26-e422af48c748"}
{"abstract": "In order to survive in a strong competition software houses need to design high-quality software. To achieve this some companies try to certify their software development processes in accordance with well-known industrial standards. Through a case study we investigated what characterizes the use of a quality system among developers and project managers in a large software company that has successfully achieved an ISO 9001:2000 certification. We found that certification not always indicates that the company successfully uses the practices in accordance with quality standards. This caused serious problems, such as projects that follow outdated practices, project managers faking quality documentation before audits, resources wasted by producing documents no one needs, problems created for new employees since they cannot find descriptions of the processes people are working in accordance with, and an expensive system no one uses.", "authors": ["Darja \u0160mite", "Nils Brede Moe"], "n_citation": 0, "title": "An ISO 9001: 2000 certificate and quality awards from outside - : What's inside? -a case study", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65fb4cfd-4c7f-4be2-b710-5afac732d3ae"}
{"abstract": "In this paper a new Experimental and Explorative Research (EER) research strategy is proposed. It combines experimental software engineering with exploratory research of new technologies. EER is based on several years experience of using and developing the approach in research of future mobile applications. In large international projects explorative application research includes quite often both industrial software developers and experienced researchers. This kind of an experimental research environment resolves the subject problem found in student experiments. It also does not have the difficulties found in experimental design and control of industrial projects that are constrained by strict commercial conditions. EER strategy provides benefits for both worlds: (1) experimental software engineering research benefits from almost industry level projects that can be used as experimentation environments, and (2) future mobile telecom application research benefits from better control and understanding of the characteristics of the applications and their development methods and processes.", "authors": ["Markku Oivo", "Pasi Kuvaja", "Petri Pulli", "Jouni Simil\u00e4"], "n_citation": 5, "title": "Software engineering research strategy: Combining Experimental and Explorative Research (EER)", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "680a959d-0f4a-4e4f-86a3-a3b0c86af88f"}
{"abstract": "As a key activity in product line engineering (PLE), instantiation is a task to generate target applications by resolving variability embedded in core assets. However, instantiation is often conducted in manual and ad-hoc fashion, largely replying on domain knowledge and experience. Hence, it can easily lead to technical problems in precisely specifying decision model consisting of product-specific variation points and variants, and in handling inter-variant conflicts/dependency. To overcome this difficulty, it is desirable to develop a systematic process which includes a set of systematic activities, detailed instructions, and concrete specification of artifacts. In this paper, we first propose a meta-model of a core asset to specify its key elements. Then, we represent a comprehensive process that defines key instantiation activities, representations of artifacts, and work instructions. With the proposed process, one can instantiate core assets more effectively and systematically.", "authors": ["Soo Ho Chang", "Soo Dong Kim", "Sung Yul Rhew"], "n_citation": 0, "title": "A variability-centric approach to instantiating core assets in product line engineering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6c08536e-54e1-42e8-a686-c94dc3973ef9"}
{"abstract": "Case Based Reasoning is a feasible approach for recognizing and predicting behavior of agents within the RoboCup domain. Using the method described here, on average 98.4 percent of all situations within a game of virtual robotic soccer have been successfully classified as part of a behavior pattern. Based on the assumption that similar triggering situations lead to similar behavior patterns, a prediction accuracy of up to 0.54 was possible, compared to 0.17 corresponding to random guessing. Significant differences are visible between different teams, which is dependent on the strategic approaches of these teams.", "authors": ["Jan Wendler", "Joscha Bach"], "n_citation": 0, "title": "Recognizing and predicting agent behavior with Case Based Reasoning", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "73565fbf-d08e-4216-a6be-4f69ccdd6533"}
{"authors": ["Florian Mendel", "Christian Rechberger", "Martin Schl\u00e4ffer", "S\u00f8ren S. Thomsen"], "n_citation": 0, "title": "The Rebound Attack: Cryptanalysis of Reduced Whirlpool and Gr\u00f8stl", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "743461a1-07f9-45a3-8cac-130c5a6eb3c5"}
{"abstract": "The Nicholl-Lee-Nicholl (NLN) algorithm for clipping line segments against a rectangular window in the plane (Computer Graphics 21,4 pp 253-262) was proved to be optimal recently in terms of the minimum and maximum number of comparisons and the number of predicates used. A new algorithm is proposed that does not use predicates, but calculates intersections speculatively. Surprisingly, this approach not only leads to a much simpler algorithm, but also takes fewer operations in many cases, including the worst case. It is proved that the new algorithm never takes more operations than the optimal algorithm. Experimental results demonstrate that the new algorithm is 80% to 560% faster than long-established, widely known algorithms.", "authors": ["Frank D\u00e9vai"], "n_citation": 50, "title": "A Speculative Approach to Clipping Line Segments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "74781c14-5d55-4c26-a0c4-63a5bf142da2"}
{"abstract": "Oceanographers from the IFREMER institute have an hypothesis that the presence of so-called retentive meso-scale vortices in ocean and coastal waters could have an influence on watery fauna's demography. Up to now, identification of retentive hydro-dynamical structures on stream maps has been performed by experts using background knowledge about the area. We tackle this task with filters induced by Genetic Programming, a technique that has already been successfully used in pattern matching problems. To overcome specific difficulties associated with this problem, we introduce a refined scheme that iterates the filters classification phase while giving them access to a memory of their previous decisions. These iterative filters achieve superior results and are compared to a set of other methods.", "authors": ["Marc Segond", "Denis Robilliard", "Cyril Fonlupt"], "n_citation": 0, "title": "Iterative filter generation using genetic programming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "75917a94-74ee-4383-a909-17c6a01803a2"}
{"abstract": "Searching neighboring points around a point in a point set has been important for various applications and there have been extensive studies such as the minimum spanning tree, relative neighborhood graph, Delaunay triangulation, Gabriel graph, and so on. Observing the fact that the previous approaches of neighbor search may possibly sample severely biased neighbors in a set of unevenly distributed points, an elliptic Gabriel graph has recently been proposed. By extending the influence region from a circle to an ellipse, the elliptic Gabriel graph generalizes the ordinary Gabriel graph. Hence, the skewness in the sampled neighbors is rather reduced. In this paper, we present a simple observation which allows to compute the correct elliptic Gabriel graph efficiently by reducing the search space.", "authors": ["Chang-Hee Lee", "Dong-Uk Kim", "Hayong Shin", "Deok-Soo Kim"], "n_citation": 0, "title": "Efficient Computation of Elliptic Gabriel Graph", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "88ad2ff5-1c76-4825-92d8-e1647d620f6f"}
{"abstract": "Many modem new product development (NPD) embedded software projects are required to be run under turbulent conditions. Both the business and the technological environments are often volatile. Uncertainty is then an inherent part of the project management. In such cases, traditional detailed up-front planning with supporting risk management is often inadequate, and more adaptive project management tools are needed. This industrial paper investigates the typical problem space of those embedded software projects. Based on a literature survey coupled with our practical experiences, we compose an extensive structured matrix of different potential project problem factors, and propose a method for assessing the project's problem profile with the matrix. The project manager can then utilize that information for problem-conscious project management. Some industrial case examples of telecommunications products embedded software development are illustrated.", "authors": ["Petri Kettunen"], "n_citation": 50, "title": "Troubleshooting large-scale new product development embedded software projects", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8c80ac17-7eb7-46d7-bb25-b980d74b81e7"}
{"abstract": "A new model for evolving crossover operators for evolutionary function optimization is proposed in this paper. The model is a hybrid technique that combines a Genetic Programming (GP) algorithm and a Genetic Algorithm (GA). Each GP chromosome is a tree encoding a crossover operator used for function optimization. The evolved crossover is embedded into a standard Genetic Algorithm which is used for solving a particular problem. Several crossover operators for function optimization are evolved using the considered model. The evolved crossover operators are compared to the human-designed convex crossover. Numerical experiments show that the evolved crossover operators perform similarly or sometimes even better than standard approaches for several well-known benchmarking problems.", "authors": ["Laura Diosan", "Mihai Oltean"], "n_citation": 50, "title": "Evolving crossover operators for function optimization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8cd922da-2e8d-4bb5-a5bd-69c32f114310"}
{"abstract": "Software maintenance is the most expensive activity in software development. Many software companies spent a large amount of cost to maintain the existing software systems. In perfective maintenance, refactoring has often been applied to the software to improve the understandability and complexity. One of the targets of refactoring is code clone. A code clone is a code fragment in a source code that is identical or similar to another. In an actual software development process, code clones are introduced because of various reasons such as reusing code by 'copy-and-paste' and so on. Code clones are one of the factors that make software maintenance difficult. In this paper, we propose a method which removes code clones from object oriented software by using existing refactoring patterns, especially Extract Method and Pull Up Method. Then, we have implemented a refactoring supporting tool based on the proposed method. Finally, we have applied the tool to an open source program and actually perform refactoring.", "authors": ["Yoshiki Higo", "Toshihiro Kamiya", "Shinji Kusumoto", "Katsuro Inoue"], "n_citation": 0, "title": "Refactoring support based on code clone analysis", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "94f6928c-3c06-419a-9c05-5f1d6115aa84"}
{"abstract": "Current approaches to automated black-box testing of components tend to focus on reducing the effort required to reveal component's properties by partially automating interface probing. This often leads to the development of test cases, which make too many assumptions about interfaces. Aspect-oriented component engineering uses the concept of different system capabilities to reason about component provided and required services. Aspect information is used to help implement better component interfaces and to encode knowledge of a component's capability. We describe and illustrate a proposal on the use of aspect-oriented component engineering techniques and notations to search for components inputs on which the component properties are revealed using a combination of existing test generation methods for black-box testing and a categorisation of component services.", "authors": ["Alejandra Cechich", "Macario Polo"], "n_citation": 0, "title": "Black-box evaluation of COTS components using aspects and metadata", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9df98583-ed6e-40cc-bf10-e56ca26caa7f"}
{"abstract": "We propose a novel agent framework to describe behaviors of the general public in rescue simulations and implement an application for Risk-Communication for disaster rescur. Conventional agent description languages are designed to model intellectual behaviors of human that solve a task to achieve a single goal. In a disaster situation, however, it is difficult to model civilians' behaviors such as goal-oriented problem-solving. Instead of such a formalization, we introduce the Parallel Scenario Description approach that models agents' behavior as an action pattern or plan of situations. We call these Scenarios. In the proposed framework, behaviors are divided into multiple scenarios for each goal by Posit and Posit operator, in which behavior rules are grouped based on situations where the rules are active. The problem solver PS 2  constructs a rule-set of behavior dynamically according to the situation of the environment and the agent's state. The framework is implemented as civilian agents for RoboCupRescue Simulation to adapt to a general civilian simulation. Moreover, we implemented refuge simulation for disaster rescue simulations to realize Risk-Communication.", "authors": ["Kousuke Shinoda", "Itsuki Noda", "Masayuki Ohta", "Susumu Kunifuji"], "n_citation": 0, "title": "Application of parallel Scenario description for RoboCupRescue civilian agent", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a10835c5-3b3f-4791-a6e4-81e13f819b6f"}
{"abstract": "The paper is dedicated to the description of the algorithm of calculation of the equilibrium gas-phase concentration at the surface of the aerosol particle - a parameter, which defines the rate of mass transport between the gas and aerosol phases in the kinetic models of atmospheric aerosol dynamics. Some problems concerning deducing of the surface equilibrium gas-phase concentrations from thermodynamic equilibrium aerosol models are discussed. It is shown that computational algorithm should be split in two steps; the sought quantity is defined on the second step, after ion concentration calculation.", "authors": ["Elena N. Stankova"], "n_citation": 0, "title": "On the Algorithm of Calculation of the Equilibrium Gas-Phase Concentration at the Particle Surface in the Kinetic Models of Aerosol Dynamics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a1f7a090-f139-4197-840e-a7081181f953"}
{"abstract": "This paper outlines a process for software system modernization decisions. The rationale of the process is explained and the process is defined in a way that allows its adaptation for other organizations and situations. The process is a light-weight one and is based on the use of objective data. The procedures for collecting the data are explained. The process has been used to solve a real industrial decision making situation in which the process was successful.", "authors": ["Jarmo J. Ahonen", "Henna Sivula", "Jussi Koskinen", "Heikki Lintinen", "Tero Tilus", "Irja Kankaanp\u00e4\u00e4", "P\u00e4ivi Juutilainen"], "n_citation": 0, "title": "Defining the process for making software system modernization decisions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a3b07bab-5625-4315-b95f-41d21ac3daa9"}
{"abstract": "Certain recursive oracles can force the polynomial hierarchy to collapse to any fixed level. All collections of such oracles associated with each collapsing level form an infinite hierarchy, called the collapsing recursive oracle polynomial (CROP) hierarchy. This CROP hierarchy is a significant part of the extended low hierarchy (note that the assumption P = NP makes the both hierarchies coincide). We prove that all levels of the CROP hierarchy are distinct by showing strong types of separation. First, we prove that each level of the hierarchy contains a set that is immune to its lower level. Second, we show that any two adjacent levels of the CROP hierarchy can be separated by another level of the CROBPP hierarchy-a bounded-error probabilistic analogue of the CROP hierarchy. Our proofs extend the circuit lower-bound techniques of Yao, Hastad, and Ko.", "authors": ["Tomoyuki Yamakami"], "n_citation": 0, "title": "Collapsing recursive oracles for relativized polynomial hierarchies", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "aa5e04e9-9968-4e82-b8d6-2b74d930e58e"}
{"abstract": "In the context of Solomonoff's Inductive Inference theory, Induction operator plays a key role in modeling and correctly predicting the behavior of a given phenomenon. Unfortunately, this operator is not algorithmically computable. The present paper deals with a Genetic Programming approach to Inductive Inference, with reference to Solomonoff's algorithmic probability theory, that consists in evolving a population of mathematical expressions looking for the 'optimal' one that generates a collection of data and has a maximal a priori probability. Validation is performed on Coulomb's Law, on the Henon series and on the Arosa Ozone time series. The results show that the method is effective in obtaining the analytical expression of the first two problems, and in achieving a very good approximation and forecasting of the third.", "authors": ["Ivanoe De Falco", "Antonio Della Cioppa", "Domenico Maisto", "Ernesto Tarantino"], "n_citation": 0, "title": "A genetic programming approach to solomonoff's probabilistic induction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "af0b3748-a57f-4e3a-900e-78b99045a82b"}
{"abstract": "Fatty acids and cholesterol are important substances for the living matter, especially for the biological membrane [1]. Since the liquid crystal state of these substances can give information on some membrane mechanism [2], their answer to some external stimuli within the mesomorphism interval has been widely studied. The possibility of inducing a non-linearity in such systems could lead to a radical change of their dynamics. Interesting non-linear optical laser based answers were obtained in different thin film samples. We analyzed these effect answers and the measurement procedures. For simulating the generation of delayed pulses inside organic mixtures a mathematical model based on practical test-functions has been used. The input pulse (usually represented by an optical or electromagnetic pulse) generates a delayed pulse inside the material medium, and thus a modulated input pulse represented by a gaussian function modulated by a sine function has been used for simulation, with good results.", "authors": ["Bogdan Lazar", "Andreea Sterian", "Stefan Pusca", "Viorel Paun", "Cristian Toma", "Cristian Morarescu"], "n_citation": 0, "title": "Simulating Delayed Pulses in Organic Materials", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b05ed85d-d73e-4e75-9a3d-dea5766fe9d2"}
{"abstract": "With the increasing usage of XML database, XML update has become an important issue in the database community. How updates affect the XML documents need to be investigated further. In this paper we propose a methodology to maintain the integrity of updated XML documents by maintaining the consistency of XML Link. XLink and its subsequent XPointer are W3C standards and used to provide referential purpose among XML documents or nodes. Since XML Link is embedded as an attribute in an XML instance, our proposal can be used for schema-less documents and for instance-based references. Our proposal is targeted for Object-Relational Storage, one of the most widely used repositories for XML document. While the XML documents are stored as a CLOB XML Type, our update methodologies are implemented as a set of functions that perform checking mechanism before updates.", "authors": ["Eric Pardede", "J. Wenny Rahayu", "David Taniar"], "n_citation": 0, "title": "Towards a High Integrity XML Link Update in Object-Relational Database", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ba2bc024-6661-43ee-b87a-d41b623346ee"}
{"abstract": "In recent years, the Chaos Project at the University of Washington has analyzed and simulated a dozen routing algorithms. Three new routing algorithms have been invented; of these, the chaotic routing algorithm (a.k.a. Chaos) has been the most successful. Although the Chaos router was developed for multicomputer routing, the project has recently directed its attention towards the application of Chaos technology to LAN switching. The present task is to implement a gigabit LAN called ChaosLAN, based on a centralized switch (hub) and high speed serial links to workstations. The switch itself is a fully-populated two-dimensional torus network of Chaos routers. The host adapter is Digital's PCI Pamette card. To evaluate the performance of ChaosLAN, we are supporting the Global Memory System (GMS), a type of distributed virtual memory also developed at UW. We also describe an application involving real-time haptic rendering used in a surgical simulator.", "authors": ["Neil McKenzie", "Kevin Bolding", "Carl Ebeling", "Lawrence Snyder"], "n_citation": 50, "title": "ChaosLAN : Design and implementation of a Gigabit LAN using chaotic routing", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "bee9ee4b-8049-444e-90ed-42757f936f3e"}
{"abstract": "We present the Prolog Cafe system that translates Prolog into Java via the WAM. Prolog Cafe provides multi-threaded Prolog engines. A Prolog Cafe thread seems to be conceptually an independent Prolog evaluator and communicates with each other through shared Java objects. Prolog Cafe also has the advantages of portability, extensibility, smooth interoperation with Java, and modularity. In performance, our translator generates faster code for a set of classical Prolog benchmarks than an existing Prolog-to-Java translator j Prolog.", "authors": ["Mutsunori Banbara", "Naoyuki Tamura", "Katsumi Inoue"], "n_citation": 50, "title": "Prolog cafe : A prolog to java translator system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c80d7f10-5423-4292-b223-9688e48d9e79"}
{"authors": ["Ismail Fahmi", "Junte Zhang", "Henk Ellermann", "Gerlof Bouma"], "n_citation": 50, "title": "SWHi System Description: A Case Study in Information Retrieval, Inference, and Visualization in the Semantic Web", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "cc0ac1df-bd73-4d19-a475-0eae9b63f11b"}
{"abstract": "The first moves to structure the COMPCHEM Grid virtual organization of the computational chemistry community are discussed. A tool based on a credit system developed to guarantee its sustainability is presented.", "authors": ["Antonio Lagan\u00e0", "Antonio Riganelli", "Osvaldo Gervasi"], "n_citation": 0, "title": "On the Structuring of the Computational Chemistry Virtual Organization COMPCHEM", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d6414255-464c-4930-b89b-68c99266d19d"}
{"abstract": "We report on a knowledge-based system for process planning in cold forging. In this system, a forged product is represented as an aggregation of forming patterns that consists of cylindrical pillar parts. The basic principle of the inference method we propose is to adjust the diameters of neighboring stepped cylinders so that they are identical. Plural deformable process plans are generated using expert knowledge about working limits, die configurations, and metal flow. This system can eliminate ineffective plans by using the knowledge of how to combine plural forming patterns into a single process. Moreover, it can evaluate process plans and interactively select the optimal one by considering production costs, the forming load, the effective strain in the product, the equipment, and other factors. We applied this system to actual forged products. As a result, this system is widely applicable to various shapes and types of equipment and can improve both maintenance and operation.", "authors": ["Osamu Takata", "Yuji Mure", "Yasuo Nakashima", "Masuharu Ogawa", "Masanobu Umeda", "Isao Nagasawa"], "n_citation": 0, "title": "A knowledge-based system for process planning in cold forging using the adjustment of stepped cylinder method", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "de9c5db9-552e-40b4-93e6-4eb3527f9963"}
{"abstract": "This paper proposes the multi-version based spatial record management technique for non-blocking search-operation for managing moving objects. The storage manager used for LBS should consider the efficient and concurrent control of searching and updating operations of moving objects. Traditionally, the In-place update method with lock is used for updating records in storage manager. But, this method cannot guarantee that each transaction conflicts on the same object. Unlike this, the multi-version concurrency control is a standard technique for avoiding conflicts between reads and writes of the same object. When multi-version technique is applied to spatial database systems, search and update-transactions can access different versions individually. But, the storage will be wasted as the version of whole spatial record is needed to be stored even if only the aspatial data is updated. In multi-version based spatial record management technique, each of aspatial data versions and spatial data versions are managed separately to improve concurrency and reduce the wastage of storage.", "authors": ["Ho Seok Kim", "Hee Taek Kim", "Myung Keun Kim", "Gyoung Bae Kim", "Hae Young Bae"], "n_citation": 0, "title": "Versioning Based Spatial Record Management Technique for Non-blocking Search Operations of Moving Objects", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e2694226-ad10-4fd3-a4eb-33d5dfb4f006"}
{"abstract": "Incremental development, software reuse, product families and component-based development seem to be the potent technologies to achieve benefits in productivity, quality and maintainability, and to reduce the risks of changes. These approaches have multiple and crosscutting impacts on development practices and quality attributes. Empirical studies in industry answer questions about why and when certain approaches are chosen, how these are applied with impact on single instances and how to generalize over classes or systems. Large, long-lived systems place more demands on software engineering approaches. Complexity is increased, systems should have the correct subset of functionality and be maintainable for several years to return the investment.The research in this thesis is based on several empirical studies performed at Ericsson in Grimstad, Norway and in the context of the Norwegian INCO project (INcremental and COmponent-Based Software Development). A product family with two large-scale products that have been developed incrementally is described. The work aimed to assess the impact of development approaches on quality and improve the practice in some aspects. The research has been a mixed-method design and the studies use qualitative data collected from sources such as web pages, text documents and own studies, as well as quantitative data from company\u2019s data repositories for several releases of one product. The thesis contains five main novel contributions:C1. Empirical verification of reuse benefits. Quantitative analyses of defect reports, change requests and component size showed reuse benefits in terms of lower defect-density, higher stability between releases, and no significant difference in change-proneness between reused and non-reused components.C2. Increased understanding of the origin and type of changes in requirements in each release and changes of software between releases. A quantitative analysis of change requests showed that most changes are initiated by the organization. Perfective changes to functionality and quality attributes are most common. Functionality is enhanced and improved in each release, while quality attributes are mostly improved and have fewer changes in form of new requirements.C3. Developing an effort estimation method using use case specifications and the distribution of effort in different phases of incremental software development. The estimation method is tailored for complex use case specifications, incremental changes in these and reuse of software from previous releases. Historical data on effort spent in two releases are used to calibrate and validate the method.C4. Identifying metrics for a combination of reuse of software components and incremental development. Results of quantitative and qualitative studies are used to relate quality attributes to development practices and approaches, and to identify metrics for a combination of software reuse and incremental development.C5. Developing a data mining method for exploring industrial data repositories based on experience from the quantitative studies. This thesis also proposes how to improve the software processes for incremental development of product families. These are considered minor contributions:C6a. Adaptation of the Rational Unified Process for reuse to improve consistency between practice and the software process model.C6b. Improving techniques for incremental inspection of UML models to improve the quality of components. A controlled industrial experiment is performed.", "authors": ["Jingyue Li", "Reidar Conradi", "Parastoo Mohagheghi", "Odd Arne S\u00e6hle", "\u00d8ivind Wang", "Erlend Naalsund", "Ole Anders Walseth"], "n_citation": 0, "title": "A Study of Developer Attitude to Component Reuse in Three IT Companies", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e26cd0e9-95d7-40d6-bd04-04aee6011e4b"}
{"abstract": "An estimation method based on use cases, the use case points method, has given promising results. However, more knowledge is needed about the contexts in which the method can be applied and how it should be adapted to local environments to improve the estimation process. We applied the use case points method to several projects in a Scandinavian software development company as the first activity in a software process improvement project on improving estimation. The second activity of the improvement project was to conduct interviews with project managers and senior developers about how to obtain continued and more widespread use of the method in the company. Based on the interviews, we propose a tailored, potentially improved version of the method and suggest how estimation practices can be improved by applying it. We believe that these experiences may be of interest to other companies that consider applying use case models as part of their estimation practices.", "authors": ["Bente Anda", "Endre Angelvik", "Kirsten Ribu"], "n_citation": 0, "title": "Improving estimation practices by applying use case models", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e5a93992-f39e-42b5-bf78-ae860801fc5f"}
{"abstract": "Automatic extraction of semantic relationships between entity instances in an ontology is useful for attaching richer semantic metadata to documents. In this paper we propose an SVM based approach to hierarchical relation extraction, using features derived automatically from a number of GATE-based open-source language processing tools. In comparison to the previous works, we use several new features including part of speech tag, entity subtype, entity class, entity role, semantic representation of sentence and WordNet synonym set. The impact of the features on the performance is investigated, as is the impact of the relation classification hierarchy. The results show there is a trade-off among these factors for relation extraction and the features containing more information such as semantic ones can improve the performance of the ontological relation extraction task.", "authors": ["Ting Wang", "Yaoyong Li", "Kalina Bontcheva", "Hamish Cunningham", "Ji Wang"], "n_citation": 64, "title": "Automatic Extraction of Hierarchical Relations from Text", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e74b4990-1f5b-4438-a2c7-84bc274f3f87"}
{"abstract": "We present a software process model that can be effectively applied to the agile software development context. Our approach builds an ideal agile software process model starting from the principles established by the agile community in what is known as the Agile Manifesto. The practices defined by the ideal model can be used to assess the various agile methods to determine any missing or uttder-emphasized practices or practices needing improvement. We compared the'practices defined for two of the most popular agile methods (XP and Scrum) with the ideal model and found that these two methods do not explicitly address in an organized way all the principles in the Manifesto. While these approaches do include practices to regularly review the effectiveness of the particular method and tune the method's behavior accordingly for a particular product being developed, they lack explicit practices to perform a retrospective focusing on the process, with the goal of adapting and improving the method itself and its future application. We propose a simple practice that could be added to these methods that would address this apparent oversight. This practice would also provide the ability to leverage what was learned from previous projects to the future projects,. We believe the proposed ideal model is complete, flexible, and can be used to assess and propose simple process improvement actions for agile methods. We also point out some open questions about the best way to share the knowledge gained from retrospectives and to do end of project reviews.", "authors": ["Marcello Visconti", "Curtis R. Cook"], "n_citation": 0, "title": "An ideal process model for Agile methods", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "efe7938a-fdab-427a-8017-8f27721072d5"}
{"abstract": "In the mobile industry, system platforms are offered to device developers to enable rapid product development while sharing expensive technology development investments. This paper presents a framework for assessment of requirements engineering collaboration related to statements-of-compliance negotiation in platform subcontracting. The framework includes a classification of platform compliance scenarios and results from analysis of interviews with engineers at two collaborating companies, a device vendor and a platform vendor. Case study findings particular to the compliance scenarios of the framework are provided. The purpose of the framework is to provide a basis for process improvement in collaborative requirements engineering.", "authors": ["Bj\u00f6rn Regnell", "Hans O. Olsson", "Staffan Mossberg"], "n_citation": 50, "title": "Assessing requirements compliance scenarios in system platform subcontracting", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f0a80e1b-e0ab-4068-8639-d17f0a73f71f"}
{"abstract": "This article introduces the TURTLE++ library which combines constraint-based and imperative paradigms and enables in this way constraint imperative programming (CIP) with C++. Integrating CIP into C++ allows to exploit the powerful expressiveness of the CIP paradigm within a language widely used and accepted in practice. We discuss the main concepts and implementation and illustrate programming with TURTLE++ by means of typical examples.", "authors": ["Petra Hofstedt", "Olaf Krzikalla"], "n_citation": 0, "title": "TURTLE++ -A CIP-library for C++", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f40a6379-05fd-434e-b112-37112e30738f"}
{"authors": ["J Dekeyser", "Dirk Roose"], "n_citation": 0, "title": "Adaptive irregular multiple grids on a distributed memory multiprocessor", "venue": "Lecture Notes in Computer Science", "year": 1991, "id": "f45d26d6-561f-4477-b1a8-1bb3762e1ff1"}
{"authors": ["Aysajan Abidin", "Eduard Marin", "Dave Singel\u00e9e", "Bart Preneel"], "n_citation": 0, "title": "Towards quantum distance bounding protocols", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "fe31f3e3-4974-4a16-9d52-52f127d8edc7"}
{"abstract": "This paper discusses a visual process modeling language called Virtual Reality Process Modeling Language (VRPML). Novel features have been introduced in VRPML to investigate perceived weaknesses in existing PMLs, and include support for the integration of a virtual environment, and dynamic creation and assignment of tasks and resources at the PML level. The paper describes the VRPML syntax and semantics.", "authors": ["Kamal Zuhairi Zamli", "Peter Lee"], "n_citation": 0, "title": "Exploiting a Virtual environment in a visual PML", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fe72839b-cac2-4306-8ba8-009c82476140"}
{"abstract": "In this paper, a methodology for integrating heterogeneous industrial information systems is presented. The methodology is strongly based on the extensive reuse of already-made components and is conceptually divided in three levels, one for each kind of designer that is typically involved in this type of projects. To accomplish a better integration of the activities and tools necessary to develop industrial information systems with the proposed methodology, three appropriate organizational configurations are adopted.", "authors": ["Ricardo J. Machado", "Jo\u00e3o M. Fernandes"], "n_citation": 0, "title": "Heterogeneous information systems integration: Organizations and methodologies", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ff6a81ea-1ed4-4151-80d9-604e3e35c0aa"}
{"abstract": "The adoption of agile software development methodologies may appear to be a rather straightforward process yielding instantly improved software in less time and increasingly satisfied customers. This paper will show that such a notion is a misunderstanding and can be harmful to small software development organisations. A more reasonable approach involves a careful risk assessment and framework for introducing agile practices to address specific risks. A case study with a small software development organisation is provided to show the assessment in practice and the resulting risk mitigation strategies for process improvement.", "authors": ["Philip S. Taylor", "Paul Sage", "Gerry Coleman", "Kevin McDaid", "Ian Lawthers", "Ronan Corr", "Desmond Greer"], "n_citation": 50, "title": "Applying an agility/discipline assessment for a small software organisation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ff73370a-5d6f-40bb-a7a9-c97256494349"}
{"abstract": "Access control is a significant issue in any secure database system. In this paper, we develop a logic programming based approach for temporal decentralized authorization administration in which users can be delegated, granted or forbidden some access rights for restricted periods of time. Three major aspects are taken into consideration for the semantics of the program, the temporal authorization delegation correctness, temporal authorization propagation and temporal authorization conflict resolution. In particular, a conflict resolution method based on the underlying delegation relation and temporal relation is presented, which can support controlled temporal delegation, temporal authorization suspension or exception and the automatic authorization update. The approach provides users a useful way to express complex security policy with time constraints.", "authors": ["Chun Ruan", "Vijay Varadharajan"], "n_citation": 0, "title": "Decentralized temporal authorization administration", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "039a6971-655b-480e-b1be-5e4fd91e0dca"}
{"authors": ["Oscar Reparaz"], "n_citation": 0, "title": "Detecting flawed masking schemes with leakage detection tests", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "03c88d70-0c51-4744-a819-f961c9da2ab5"}
{"abstract": "This paper describes our experiences with the installation of a number of software quality assurance programs at DaimlerChrysler's Passenger Car Development Unit (PCD). Our goal was to establish software quality assurance processes that have to run in parallel with the overall car development process. Depending on the actual problem domain (i.e., powertrain, chassis control, body control, or telematics) we developed four generic software development processes. Along with the development of these processes we wrote an experience based handbook containing general descriptions and a list of best practices for software quality assurance. We describe how we developed the generic processes, and how these processes and the documented best practices are used in our daily work. We concentrate on measures to co-ordinate the co-operation between DaimlerChrysler and its suppliers.", "authors": ["Ton Vullinghs", "Thomas Gantner", "Stephan Steinhauer", "Thomas Weber"], "n_citation": 0, "title": "Experiences on lean techniques to manage software suppliers", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "143a1b92-023b-4843-ba6e-62fbb61275c7"}
{"abstract": "Adequate bandwidth allocations and strict delay requirements are required for real time flows, such as streaming audio and video data. Most of commonly known packet scheduling algorithms like Weighted Fair Queueing (WFQ) and Start-Time Fair Queueing (SFQ) were mainly designed to ensure the bandwidth reservation function. They only guarantee the queueing delay under a certain threshold. It may cause unsteady packet latencies and introduces extra handling overheads for streaming applications. A few packet scheduling algorithms were proposed in recent years to address this problem like Low Latency Queueing (LLQ) which may suffer from low priority traffic starvation problem. In this paper, we will show the unsteady queueing delay problem, The buffer underrun problem for most well known packet scheduling algorithms. We propose a novel packet scheduling algorithm with history support, LLEPS, to ensure low latency and efficient packet scheduling for streaming applications via monitoring the behavior of queues and traffics.", "authors": ["Eric Hsiao-Kuang Wu", "Ming-I Hsieh", "Hsu-Te Lai"], "n_citation": 50, "title": "A novel low latency packet scheduling scheme for broadband networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "17fbbef1-f2e5-4c17-a0d8-9a8cf58e5242"}
{"abstract": "Functional descriptions are a central pillar of Semantic Web services. Disregarding details on how to invoke and consume the service, they shall provide a black box description for determining the usability of a Web service for some request or usage scenario with respect to the provided functionality. The creation of sophisticated semantic matchmaking techniques as well as exposition of their correctness requires clear and unambiguous semantics of functional descriptions. As existing description frameworks like OWL-S and WSMO lack in this respect, this paper presents so-called Abstract State Spaces as a rich and language independent model of Web services and the world they act in. This allows giving a precise mathematical definition of the concept of Web Service and the semantics of functional descriptions. Finally, we demonstrate the benefit of applying such a model by means of a concrete use case: the semantic analysis of functional descriptions which allows to detect certain (un)desired semantic properties of functional descriptions. As a side effect, semantic analysis based on our formal model allows us to gain a formal understanding and insight in matching of functional descriptions during Web service discovery.", "authors": ["Uwe Keller", "Holger Lausen", "Michael Stollberg"], "n_citation": 0, "title": "On the Semantics of Functional Descriptions of Web Services", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1dc8d858-fe0e-4791-ab33-e71ddf53b450"}
{"abstract": "We show that if a language is recognized within certain error bounds by constant-depth quantum circuits over a finite family of gates, then it is computable in (classical) polynomial time. In particular, for 0   6 (for acceptance). We show that BQNC 0 \u2208 ,\u03b4  C P, provided that 1 - S < 2 -2d (1-\u2208), where d is the circuit depth. On the other hand, we adapt and extend ideas of Terhal & DiVincenzo [1] to show that, for any family F of quantum gates including Hadamard and CNOT gates, computing the acceptance probabilities of depth-five circuits over F is just as hard as computing these probabilities for arbitrary quantum circuits over F. In particular, this implies that NQNC\u00b0 = NQACC = NQP = coC=P, where NQNC\u00b0 is the constant-depth analog of the class NQP. This essentially refutes a conjecture of Green et al. that NQACC \u2286 TC 0  [2].", "authors": ["Stephen A. Fenner", "Frederic Green", "Steven Homer", "Yong Zhang"], "n_citation": 50, "title": "Bounds on the power of constant-depth quantum circuits", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1fa4717f-1d90-45e3-b5d6-7d200e5c01f1"}
{"abstract": "Policies are pervasive in web applications. They play crucial roles in enhancing security, privacy and usability of distributed services. There has been extensive research in the area, including the Semantic Web community, but several aspects still exist that prevent policy frameworks from widespread adoption and real world application. This paper discusses important requirements and open research issues in this context, focusing on policies in general and their integration into trust management frameworks, as well as on approaches to increase system cooperation, usability and user-awareness of policy issues.", "authors": ["Piero A. Bonatti", "Claudiu Duma", "Norbert E. Fuchs", "Wolfgang Nejdl", "Daniel Olmedilla", "Joachim Peer", "Nahid Shahmehri"], "n_citation": 90, "title": "Semantic Web Policies : A Discussion of Requirements and Research Issues", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "251753ff-ed74-416c-87a5-d5b67d563740"}
{"abstract": "Ten years after the presentation of the tutorial The Experience Factory: How to Build and Run One at ICSE 17 in 1995 [4], the idea of building such a Learning Software Organization (LSO) is in wide spread use..Meanwhile, the Experience Factory (EF) concept [2], i.e., the systematic goal-oriented utilization of experience, is also being successfully applied outside the domain of Software Engineering [11], [12]. However, defining and implementing a successful Experience Factory is still a challenge [9]. In this tutorial we take a look at existing concepts on how to identify and structure the content of the experience base (EB), discuss solutions for how to implement an EB, and present processes on how to setup, run, evaluate, and maintain an EF in an organization. The tutorial is based on the authors' organizations' experiences with implementing EFs in research, industry, and government environments.", "authors": ["Frank Bomarius", "Raimund L. Feldmann"], "n_citation": 0, "title": "Get your experience factory ready for the next decade : Ten years after how to build and run one", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2b059153-a0bf-4f2f-a5a3-48c29c6abd41"}
{"abstract": "We present a new object-based algorithm for morphing between two shapes with an arbitrary number of polygons and arbitrarily different topology. Many solutions have been proposed for morphing between two polygons. However, there has been little attention to morphing between different numbers of polygons, across a change in topology. A modified conforming Delaunay triangulation is used to build the vertex correspondence. Polygon evolution is used to smooth the morph. The morph requires no user interaction, avoids self-intersection, uses dynamic vertex correspondence, and follows nonlinear vertex paths.", "authors": ["Xiaqing Wu", "John K. Johnstone"], "n_citation": 0, "title": "Delaunay-Based Polygon Morphing Across a Change in Topology", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d337b1b-a5c5-4205-ad00-c26b594802ee"}
{"abstract": "In this paper we deal with one of the art gallery problems, namely the problem of fault tolerant guarding of grids, which is defined as the problem of finding two disjoint guard sets in a grid. Although determining the existence of such a structure is easy in general grids, the task of minimising the number of guards taken over both teams is shown to be NP-hard even for subcubic grids. Moreover, we propose a 6/5-approximation algorithm for solving the fault tolerant guard problem in grids.", "authors": ["Adrian Kosowski", "Micha\u0142 Ma\u0142afiejski", "Pawee Zylinski"], "n_citation": 50, "title": "Fault Tolerant Guarding of Grids", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2f0a86b2-2615-4657-9601-ef14bfccf632"}
{"abstract": "We explore the extremal properties of the Localized Delaunay Triangulation over networks with heterogeneous ranges. We find theoretical bounds on these properties and compare them with those found via experimentation.", "authors": ["Mark Watson", "J. Mark Keil"], "n_citation": 0, "title": "Routing Properties of the Localized Delaunay Triangulation over Heterogeneous Ad-Hoc Wireless Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "359a9d51-5219-4686-ab79-23e063bc3987"}
{"abstract": "The management and deployment of reuse-driven and architecture-centric requirements engineering processes have become common in many organizations adopting Enterprise Resource Planning solutions. Yet, little is known about the variety of reusability aspects in ERP projects at the level of requirements. Neither, we know enough how exactly ERP adopters benefit from reuse as part of the requirements engineering process. This paper sheds some light into these questions and suggests a practical approach to applied ERP requirements reuse measurement by incorporating reuse metrics planning as part of the implementation of metrics on an ERP project. Relevant process integration challenges are resolved in the context of SAP R/3 implementation projects in which the author participated while being employed at the second largest telecommunication company in Canada.", "authors": ["Maya Daneva"], "n_citation": 0, "title": "Integrating reuse measurement practices into the ERP requirements engineering process", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37874639-d0ac-4596-baee-d565f920abc6"}
{"abstract": "This paper addresses the problem of indexing high dimensional normalized histogram data, i.e., D-dimensional feature vectors H where \u03a3 D i=1 H i  = 1. These are often used as representations for multimedia objects in order to facilitate similarity query processing. By analyzing properties that are induced by the above constraint and that do not hold in general multi-dimensional spaces we design a new split policy. We show that the performance of similarity queries for normalized histogram data can be significantly improved by exploiting such properties within a simple indexing framework. We are able to process nearest-neighbor queries up to 10 times faster than the SR-tree and 3 times faster than the A-tree.", "authors": ["Alexandru Coman", "J\u00f6rg Sander", "Mario A. Nascimento"], "n_citation": 0, "title": "Efficient indexing of high dimensional normalized histograms", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "387d2032-113f-4dad-ac2d-fd2f37f27394"}
{"abstract": "Recently, design support tools have been used to improve the efficiency of design work. These tools support design verification at each design stage, and succeed to some extent in improving the efficiency of the design work. However, the management of scattered design information and data conversion have lately become difficult because designers often require two or more tools. In this paper, we focus on the detailed design of mechanism parts made by injection molding and used in precision consumer products. We propose a design product model which describes the detailed design of such parts. The model describes the mechanism parts based on sets of faces, which are basic units in design work. In this way, the model can express design information in both two- and three-dimensional forms. We also define the procedures used to operate the model. Finally, the effectiveness of this model is shown by applying it to an experimental system.", "authors": ["Tatsuichiro Nagai", "Isao Nagasawa", "Masanobu Umeda", "Tatsuji Higuchi", "Yasuyuki Nishidai", "Yusuke Kitagawa", "Tsuyoshi Tsurusaki", "Masahito Ohhashi", "Osamu Takata"], "n_citation": 0, "title": "A design product model for mechanism parts by injection molding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "38994d95-eeb6-41b4-a264-d0c8377f4840"}
{"abstract": "Over the last eight years, different approaches have been used to diagnose the performance in ABB organizations developing software. The efforts build to a large degree on methods from the Software Engineering Institute (SEI). In this paper we examine the experiences from five organizations through a description of the pathways that we have observed in the maturity development. We also propose a way to classify organizations based on two organizational characteristics, maturity and openness. Based on this classification, a simple method for the selection of how to collect performance data from the organizations is described.", "authors": ["Stig Larsson", "Fredrik Ekdahl"], "n_citation": 0, "title": "Selecting CMMI appraisal classes based on maturity and openness", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3dff26f7-5db8-40c7-8d00-793bd4e062fb"}
{"abstract": "In this paper, a wavelet packets watermarking algorithm based on chaos encryption for still digital images is presented. The watermarking is changed into bit sequence by modular arithmetic, at the same time, a random sequence is gotten by using a chaos map and deal with it also by modular arithmetic. After getting these two sequences, we do XOR arithmetic, then getting a new encrypted watermarking sequence. At the embedding stage, wavelet packets decomposition is used for original image, then embedding encrypted watermarking sequence into some frequency bands of original image by repeatedly embedding four times. At the extracting stage, wavelet packets decomposition for original image and watermarked image are used to inverse process. The experiment results demonstrated the new algorithm is robust for scaling, cropping, JPEG compression and noise attack.", "authors": ["Jin Cong", "Yan Jiang", "Zhiguo Qu", "Zhongmei Zhang"], "n_citation": 0, "title": "A Wavelet Packets Watermarking Algorithm Based on Chaos Encryption", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3e7ecc7a-65b0-457f-a73b-bd331da4647a"}
{"abstract": "Thermoforming is a manufacturing process widely used to produce thin thermoplastic parts. In this process a previously extruded thermoplastic sheet is clamped and then heated and formed into a mold cavity using a differential pressure. In this paper a finite element model of the thermoforming process of an ABS sheet is proposed and numerical results are compared to data from literature. Thermoplastic sheet is modelled according to the membrane formulation. An implicit time scheme has been adopted for the integration algorithm. Mechanical behaviour of the processing material is assumed as hyperelastic, according to the two parameters Mooney-Rivlin model. Mathematical formulation of the mechanical model is exposed. The proposed model allows to evaluate material thinning, stresses, strains and contact status between the processing material and the die.", "authors": ["Pierpaolo Carlone", "Gaetano S. Palazzo"], "n_citation": 50, "title": "Finite Element Analysis of the Thermoforming Manufacturing Process Using the Hyperelastic Mooney-Rivlin Model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "495acd4d-cb6f-4fd9-bbed-c7ff64b774c5"}
{"abstract": "Graph decompositions such as decomposition by clique separators and modular decomposition are of crucial importance for designing efficient graph algorithms. Clique separators in graphs were used by Tarjan as a divide-and-conquer approach for solving various problems such as the Maximum Weight Stable Set (MWS) Problem, Coloring and Minimum Fill-in. The basic tool is a decomposition tree of the graph whose leaves have no clique separator (so-called atoms), and the problem can be solved efficiently on the graph if it is efficiently solvable on its atoms. We give new examples where the clique separator decomposition works well for the MWS problem which also improves and extends various recently published results. In particular, we describe the atom structure for some new classes of graphs whose atoms are P 5 -free (the P 5  is the induced path with 5 vertices) and obtain new polynomial time results for MWS.", "authors": ["Andreas Brandst\u00e4dt", "Van Bang Le", "Suhail Mahfud"], "n_citation": 0, "title": "New applications of clique separator decomposition for the maximum weight stable set problem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4fad2112-97fd-430f-b6ca-61c1bf45ca02"}
{"authors": ["Antoon Bosselaers", "Ren\u00e9 Govaerts", "Joos Vandewalle"], "n_citation": 50, "title": "SHA: a design for parallel architectures?", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "5b1ad4f5-d2f2-432f-8189-6d632317efde"}
{"abstract": "Software Process Improvement (SPI) methods have been used for years as means to try to solve the problems in software development. Number of SPI life cycle models exists, and some of them take a wider look to the problems. However, little information exists about how the SPI models apply to global level SPI programs in a multi-site environment. This article takes a historical look into one such case and compares how well IDEAL and ISO 15504-7 models match the actual activities. The results are naturally only indicative, but suggest that the literature models do not readily scale up and that a separate model may be needed to support setting up and guiding the execution of a multi-site SPI program.", "authors": ["Atte Kinnula", "Marianne Kinnula"], "n_citation": 0, "title": "Comparing global (multi-site) SPI program activities to SPI program models", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5b3fca7e-c6f8-446e-a9de-337cb99593be"}
{"authors": ["Gabriela Gheorghe", "Bruno Crispo", "Daniel Schleicher", "Tobias Anstett", "Frank Leymann", "Ralph Mietzner", "Ganna Monakova"], "n_citation": 0, "title": "Combining Enforcement Strategies in Service Oriented Architectures", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "5bdac007-3b08-4d60-8df1-c1d3bb3c6e77"}
{"abstract": "The reuse of software engineering assets has been proposed as the most promising alternative for improving productivity and software quality. The improvement of reuse requires understanding of suitable reuse strategies and the software process. In four industrial cases the reuse process is analyzed for the purpose of its improvement and remarkable differences between successful processes are found. Those differences are due to differences in the products and businesses of the analyzed companies. In some cases the product line approach fits the business very well and high level of reuse can be achieved by using it. In other cases the black-box approach to reuse has turned out to suit the business better.", "authors": ["Jarmo J. Ahonen", "Heikki Lintinen", "Sanna-Kaisa Taskinen"], "n_citation": 0, "title": "Improving the reuse process is based on understanding the business and the products: Four case studies", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "607132ce-1929-4ec8-a77a-35d12d4aed62"}
{"abstract": "Clique-width of graphs is a major new concept with respect to efficiency of graph algorithms. The notion of clique-width extends the one of treewidth, since bounded treewidth implies bounded clique-width. We give a complete classification of all graph classes defined by forbidden induced subgraphs of at most four vertices with respect to bounded or unbounded clique-width.", "authors": ["Andreas Brandst\u00e4dt", "Joost Engelfriet", "Hoang-Oanh Le", "Vadim V. Lozin"], "n_citation": 0, "title": "Clique-width for four-vertex forbidden subgraphs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6438ec09-18a5-4784-bb4a-8ee62d18643b"}
{"abstract": "Many papers deal with the approximability of multi-criteria optimization problems but only a small number of non-approximability results, which rely on NP-hardness, exist in the literature. In this paper, we provide a new way of proving non-approximability results which relies on the existence of a small size good approximating set (i.e. it holds even in the unlikely event of P = NP). This method may be used for several problems but here we illustrate it for a multi-criteria version of the traveling salesman problem with distances one and two (TSP(1, 2)). Following the article of Angel et al. (FCT 2003) who presented an approximation algorithm for the bi-criteria TSP(1, 2), we extend and improve the result to any number k of criteria.", "authors": ["Eric Angel", "Evripidis Bampis", "Laurent Gourv\u00e8s", "J\u00e9r\u00f4me Monnot"], "n_citation": 0, "title": "(Non)-approximability for the multi-criteria TSP(1,2)", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "65b384ba-778c-4ee9-9e93-471cd4c4fe4d"}
{"abstract": "In this paper, we propose a technique for establishing process lines, which are sets of common processes in particular problem domains, and process line architectures that incorporate commonality and variability. Process line architectures are used as a basis for deriving process lines from the perspective of overall optimization. The proposed technique includes some extensions to the Software Process Engineering Metamodel for clearly expressing the commonality and variability in the process workflows described as UML activity diagrams. As a result of applying the proposed technique to hardware/software co-design processes in an embedded system domain, it is found that the proposed technique is useful for defining consistent and project-specific processes efficiently.", "authors": ["Hironori Washizaki"], "n_citation": 0, "title": "Building software process line architectures from bottom up", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6a3a4562-1019-431a-b201-38e704c5e519"}
{"abstract": "A knowledge-based system is suitable for realizing advanced functions that require domain-specific expert knowledge in enterprise-mission-critical information systems (enterprise applications). This paper describes a newly implemented multi-threaded Prolog system that evolves single-threaded Inside Prolog. It is intended as a means to apply a knowledge-based system written in Prolog to an enterprise application. It realizes a high degree of parallelism on an SMP system by minimizing mutual exclusion for scalability essential in enterprise use. Also briefly introduced is the knowledge processing server which is a framework for operating a knowledge-based system written in Prolog with an enterprise application. Experimental results indicated that on an SMP system the multi-threaded Prolog could achieve a high degree of parallelism while the server could obtain scalability. The application of the server to clinical decision support in a hospital information system also demonstrated that the multi-threaded Prolog and the server were sufficiently robust for use in an enterprise application.", "authors": ["Masanobu Umeda", "Keiichi Katamine", "Isao Nagasawa", "Masaaki Hashimoto", "Osamu Takata"], "n_citation": 0, "title": "Multi-threading inside prolog for knowledge-based enterprise applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6edac9d4-0013-4136-9d8a-472e796e9e52"}
{"abstract": "A recent paper, published in Algorithms-ESA2004, presented examples designed to illustrate that using floating-point arithmetic in algorithms for computational geometry may cause implementations to fail. The stated purpose was to demonstrate, to students and implementors, the inadequacy of floating-point arithmetic for geometric computations. The examples presented were both useful and insightful, but certain of the accompanying remarks were misleading. One such remark is that researchers in numerical analysis may believe that simple approaches are available to overcome the problems of finite-precision arithmetic. Another is the reference, as a general statement, to the inadequacy of floating-point arithmetic for geometric computations. In this paper it will be shown how the now-classical backward error analysis can be applied in the area of computational geometry. This analysis is relevant in the context of uncertain data, which may well be the practical context for computational-geometry algorithms such as, say, those for computing convex hulls. The exposition will illustrate the fact that the backward error analysis does not pretend to overcome the problem of finite precision: it merely provides a tool to distinguish, in a fairly routine way, those algorithms that overcome the problem to whatever extent it is possible to do so. It will also be shown, by using one of the examples of failure presented in the principal reference, that often the situation in computational geometry is exactly parallel to other areas, such as the numerical solution of linear equations, or the algebraic eigenvalue problem. Indeed, the example mentioned can be viewed simply as an example of an unstable algorithm, for a problem for which computational geometry has already discovered provably stable algorithms.", "authors": ["Di Jiang", "Neil F. Stewart"], "n_citation": 0, "title": "Backward Error Analysis in Computational Geometry", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6f43d1ee-b6a4-4978-8400-fbc1dadc4a86"}
{"abstract": "Genetic Programming (GP) can be used to identify the nonlinear differential equations of dynamical systems. If, however, the fitness function is chosen in a classical way, the optimization will not work very well. In this article, we explain the reasons for the failure of the GP approach and present a solution strategy for improving performance. Using more than one identification criterion (fitness function) and switching based on the information content of the data enable standard GP algorithms to find better solutions in shorter times. A computational example illustrates that identification criteria switching has a bigger influence on the results than the choice of the GP parameters has.", "authors": ["Thomas Buchsbaum", "Siegfried V\u00f6ssner"], "n_citation": 0, "title": "Information-dependent switching of identification criteria in a genetic programming system for system identification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6ffb655a-1602-45b4-9854-511bbb73c954"}
{"abstract": "Evolutionary algorithms have already been more or less successfully applied to a wide range of optimisation problems. Typically, they are used to evolve a population of complete candidate solutions to a given problem, which can be further refined by some problem-specific heuristic algorithm. In this paper, we introduce a new framework called Iterative Prototype Optimisation with Evolved Improvement Steps. This is a general optimisation framework, where an initial prototype solution is being improved iteration by iteration. In each iteration, a sequence of actions/operations, which improves the current prototype the most, is found by an evolutionary algorithm. The proposed algorithm has been tested on problems from two different optimisation problem domains -binary string optimisation and the traveling salesman problem. Results show that the concept can be used to solve hard problems of big size reliably achieving comparably good or better results than classical evolutionary algorithms and other selected methods.", "authors": ["Jiri Kubalik", "Jan Faigl"], "n_citation": 50, "title": "Iterative prototype optimisation with evolved improvement steps", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "71865535-5de6-439a-b0ff-08e3ac7d0833"}
{"authors": ["Wim H. Hesselink", "B. M\u00f6ller"], "n_citation": 0, "title": "Angelic termination in Dijkstra's calculus", "venue": "Lecture Notes in Computer Science", "year": 1995, "id": "7226d85e-b011-472a-ad4c-13db24f9eb37"}
{"abstract": "The use of Constructive Induction (CI) methods for the generation of high-quality attributes is a very important issue in Machine Learning. In this paper, we present a CI method based in Genetic Programming (GP). This method is able to evolve projections that transform the dataset, constructing a new coordinates space in which the data can be more easily predicted. This coordinates space can be smaller than the original one, achieving two main goals at the same time: on one hand, improving classification tasks; on the other hand, reducing dimensionality of the problem. Also, our method can handle classification and regression problems. We have tested our approach in two financial prediction problems because their high dimensionality is very appropriate for our method. In the first one, GP is used to tackle prediction of bankruptcy of companies (classification problem). In the second one, an IPO Underpricing prediction domain (a classical regression problem) is confronted. Our method obtained in both cases competitive results and, in addition, it drastically reduced dimensionality of the problem.", "authors": ["C\u00e9sar Est\u00e9banez", "Jos\u00e9 Mar\u00eda Valls", "Ricardo Aler"], "n_citation": 50, "title": "Projecting financial data using genetic programming in classification and regression tasks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "73d1862e-ca3f-455c-aa79-d646c3efdab1"}
{"abstract": "The CMMI model for Software Engineering provides guidance for improving an organization's processes and the ability to develop software systems. The CenPRA test process is a generic software testing model defined by selecting software testing best practices; it evolved over the last years and has been published in specific forums. The CenPRA test process, which defines a set of partially ordered activities and test artifacts, has been validated and improved based on the experience of its application at software development companies in Brazil. In this work we carried out an evaluation of the CenPRA test process under the perspective of CMMI. We evaluated essentially which aspects of CMMI are taken into account by the CenPRA test process. We also evaluate how the CenPRA model can be used to supplement software testing related aspects of CMMI. Our results pointed to improvements in the CenPRA test process, and also identify testing tasks and artifacts not considered by CMMI, which can significantly improve an organization testing practices.", "authors": ["Paulo Marcos Siqueira Bueno", "Adalberto Nobiato Crespo", "Mario Jino"], "n_citation": 0, "title": "Analysis of an artifact oriented test process model and of testing aspects of CMMI", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7692b3cf-78fe-4e51-b338-b483cec1d7da"}
{"abstract": "The shrinking two-pushdown automaton is known to charactize the class of growing context-sensitive languages, while its deterministic variant accepts the Church-Rosser languages. Here we study the expressive power of shrinking pushdown automata with more than two pushdown stores, obtaining a close correspondence to linear time-bounded multi-tape Turing machines.", "authors": ["Markus Holzer", "Friedrich Otto"], "n_citation": 0, "title": "Shrinking multi-pushdown automata", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "784a4730-c3b6-48a7-a5dd-0f521d1b66fc"}
{"abstract": "Genetic Programming (GP) [1] often uses a tree form of a graph to represent solutions. An extension to this representation, Automatically Defined Functions (ADFs) [1] is to allow the ability to express modules. In [2] we proved that the complexity of a function is independent of the primitive set (function set and terminal set) if the representation has the ability to express modules. This is essentially due to the fact that if a representation can express modules, then it can effectively define its own primitives at a constant cost. This is reminiscent of the result that the complexity of a bit string is independent of the choice of Universal Turing Machine (UTM) (within an additive constant) [3], the constant depending on the UTM but not on the function. The representations typically used in GP are not capable of expressing recursion, however a few researchers have introduced recursion into their representations. These representations are then capable of expressing a wider classes of functions, for example the primitive recursive functions (PRFs). We prove that given two representations which express the PRFs (and only the PRFs), the complexity of a function with respect to either of these representations is invariant within an additive constant. This is in the same vein as the proof of the invariants of Kolmogorov complexity [3] and the proof in [2].", "authors": ["John R. Woodward"], "n_citation": 0, "title": "Invariance of function complexity under primitive recursive functions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7a7b0756-4515-49ad-bcc3-9037a5977b23"}
{"abstract": "Despite of their advertisement as task independent representations, the reuse of ontologies in different contexts is difficult. An explanation for this is that when developing an ontology, a choice is made with respect, to what aspects of the world are relevant. In this paper we deal with the problem of reusing ontologies in a context where only parts of the originally encoded aspects are relevant. We propose the notion of a viewpoint on an ontology in terms of a subset of the complete representation vocabulary that is relevant in a certain context. We present an approach of implementing different viewpoints in terms of an approximate subsumption operator that only cares about a subset of the vocabulary. We discuss the formal properties of subsumption with respect to a subset of the vocabulary and show how these properties can be used to efficiently compute different viewpoints on the basis of maximal sub- vocabularies that support. subsumption between concept pairs.", "authors": ["Heiner Stuckenschmidt"], "n_citation": 50, "title": "Toward Multi-viewpoint Reasoning with OWL Ontologies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7c021b5b-40fd-49c0-a943-3b2e9040f06a"}
{"abstract": "This paper addresses the ongoing inventory activities within the ITEA MOOSE project. The inventory result will be a complete view on the application of methods, techniques and tools for software production within some of the leading European industrial companies within the embedded system field, such as Philips, Oce, ASML and Nokia. The current results are remarkable, as they confirm the cautiousness of industry to adopt recent state of the art development technologies, even as the production of in-time reliable software products becomes more and more an unreachable target.", "authors": ["B.A. Graaf", "Marco Lormans", "Hans Toetenel"], "n_citation": 50, "title": "Software technologies for embedded systems: An industry inventory", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "831746f0-bb1d-4dc0-9483-91406ab59c13"}
{"abstract": "A number of structural measures have been suggested to support the assessment and prediction of software quality attributes. The aim of our study is to investigate how class-level measures of structural properties can be used to assess the maintainability of a software product as a whole. We survey, structure and discuss current practices on this topic, and apply alternative strategies on four functionally equivalent systems that were constructed as part of a multi-case study. In the absence of historical data needed to build statistically based prediction models, we apply elements of judgment in the assessment. We show how triangulation of alternative strategies as well as sensitivity analysis may increase the confidence in assessments that contain elements of judgment. This paper contributes to more systematic practices in the application of structural measures. Further research is needed to evaluate and improve the accuracy and precision of judgment-based strategies.", "authors": ["Hans Christian Benestad", "Bente Anda", "Erik Arisholm"], "n_citation": 0, "title": "Assessing software product maintainability based on class-level structural measures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "91e909e4-a996-4a00-8a94-64a10ae549da"}
{"abstract": "We present several new standard and differential approximation results for P 4 -partition problem by using the algorithm proposed in Hassin and Rubinstein (Information Processing Letters, 63: 63-67, 1997), for both minimization and maximization versions of the problem. However, the main point of this paper is the robustness of this algorithm, since it provides good solutions, whatever version of the problem we deal with, whatever the approximation framework within which we estimate its approximate solutions.", "authors": ["J\u00e9r\u00f4me Monnot", "Sophie Toulouse"], "n_citation": 5, "title": "Approximation results for the weighted P4 partition problems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9217d582-7828-4062-9ee1-00d1d9a88a0c"}
{"authors": ["Olga Kovalenko", "Christophe Debruyne", "Estefan\u00eda Serral Asensio", "Stefan Biffl"], "n_citation": 0, "title": "Evaluation of Technologies for Mapping Representation in Ontologies", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "94d19541-570e-4abb-bc64-e09f764512f2"}
{"abstract": "IEE, Univ Perugia, Univ Calgary, Univ Minnesota, Queens Univ Belfast, ERCIM, OptimaNumerics, INTEL, AMD", "authors": ["Jia Li", "Zhang B", "Zhang S", "Wang Zm"], "n_citation": 0, "title": "An algorithm on extraction of saline-alkalized land by image segmentation based on ETM+ image", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "98b5a289-1c26-48ba-a1d8-6991c9798494"}
{"abstract": "Recently much research has been done in applying database technology to tracking moving objects. Most techniques assume a predictable linear motion. We present a new application for moving object databases, characterized by repetitive, unpredictable motion with very high data rates. In particular, the domain of athletic and auto races is presented. We found existing moving object methods do not adequately address this application area. A data model is presented for efficiently storing the data. A spatiotemporal index is developed for fast retrieval of data. We give a set of queries likely to be of interest in this application domain. A study is presented showing our implementation has better performance than those based on relational DBMSs.", "authors": ["Paul Werstein", "Jenny McDonald"], "n_citation": 0, "title": "A database for repetitive, unpredictable moving objects", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a48c144b-68a1-4829-aad6-2a118a96b497"}
{"authors": ["Junte Zhang", "Ismail Fahmi", "Henk Ellermann", "Gerlof Bouma"], "n_citation": 0, "title": "Mapping Metadata for SWHi: Aligning Schemas with Library Metadata for a Historical Ontology", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "a4ec019a-a1c6-4f41-b6d0-24a095b5e419"}
{"abstract": "Informed and qualified decisions are key factors for project failure or success. The idea of decision support always arises when timely decisions must be made in unstructured or semi-structured problem domains, where multiple stakeholders are involved, and when the information available is uncertain. Release planning (RP) addresses decisions related to the selection and assignment of features to a sequence of consecutive product releases such that the most important technical, resource, budget, and risk constraints are met. Release planning is an important and integral part of any type of incremental product development. The objective of this tutorial is to describe and position the 'art and science' of software release planning. The art of release planning refers to relying on human intuition, communication, and capabilities to negotiate between conflicting objectives and constraints. The science of release planning refers to emphasizing formalization of the problem and applying computational algorithms to generate best solutions. Both art and science are important for achieving meaningful release planning results. We investigate the release planning process and propose a hybrid planning approach that integrates the strength of computational intelligence with the knowledge and experience of human experts.", "authors": ["G\u00fcnther Ruhe", "Omolade Saliu"], "n_citation": 0, "title": "Art and science of system release planning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a5616474-5f1e-4df5-a5d2-8770e6212e10"}
{"abstract": "It has been recognized that it is a challenging problem to deal with the situation where learners have diverse computing backgrounds and the learning content to be covered is also in the broad coverage. In the case, it's required to devise a sophisticated diagnostic model for applying a proper teaching-learning method. We have drawn a scheme which can be applied to that case efficiently by using clustering algorithms based on web technology. In our approach, we focus on finding out methods for classifying both learners and learning content on the web. To make classification and manipulation of learning content ease, we reorganize learning content in order to have discrete form by introducing the concept of the knowledge unit which is extracted from each topic. Also, to make classification and diagnostic ease, we develop questions to measure them and analyze each question using item response theory (IRT) on the web. From the experiment of students sampled using our method, we show that learners with various backgrounds and the learning content with distribution on the broad range can be categorized effectively into the groups with homogeneous property. Also, we describe how to apply our proposed scheme to the introductory courses at postsecondary level.", "authors": ["Seong Baeg Kim", "Kyoung Mi Yang", "Cheol Min Kim"], "n_citation": 0, "title": "A Diagnostic Model Using a Clustering Scheme", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa7016c9-0293-4004-8196-4e27ebd6a35c"}
{"abstract": "SUPSIM is a web application, based on the Java Servlet technologies, to compute ab-initio potential energy surfaces of molecular systems for subsequent dynamical properties calculation. We describe the architecture of SUPSIM, its current implementation, and its possible future developments to tackle chemical systems of increasing complexity.", "authors": ["Loriano Storchi", "F. Tarantelli", "Antonio Lagan\u00e0"], "n_citation": 50, "title": "Computing Molecular Energy Surfaces on a Grid", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ae410b14-d383-4543-938a-d9474bfeed90"}
{"abstract": "Software development processes such as the Waterfall process and Extreme Programming are project management methods (PMMs) which are well known and widely used. However, conventional project management (PM) lacks the process concepts expressed in PMMs, and the connection between PMMs and PM is not much explored in the literature. We present data models for PM and PMM, in a framework that can articulate the PM-to-PMM relationship, illustrating with simple examples. A java/XML implementation of this framework can create and then revise a PMM-aware project, conforming to a specified PMM. In terms of the framework, we describe a simple project data visualization and associated method that can be used to synthesize a PMM for a project instance that was initially created without reference to any PMM.", "authors": ["Tony Dale", "Neville Churcher", "Warwick Irwin"], "n_citation": 0, "title": "A framework for linking projects and project management methods", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ae9bdb37-72f1-4b9e-9e7f-c52f7effb356"}
{"abstract": "Mentor programs are important mechanisms that serve functions such as career development as well as knowledge transfer. Many see mentor programs as an efficient, inexpensive, flexible and tailored way of transferring technical knowledge from experts to less experienced employees. We have investigated how a mentor program works in a small software consultancy company, and propose that the learning effect of the program could be improved by introducing methods to increase the employees level of reflection.", "authors": ["Finn Olav Bj\u00f8rnson", "Torgeir Dings\u00f8yr"], "n_citation": 0, "title": "A study of a mentoring program for knowledge transfer in a small software consultancy company", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "aeff17a3-a79b-422c-b255-63efcd768557"}
{"abstract": "It has been shown recently in [5] that the visual secret sharing scheme proposed in [1] leads to the largest possible visual contrast among all schemes that perfectly reconstruct black pixels. The main purpose of this paper is to demonstrate that the largest optimal contrast (for this kind of schemes) equals the smallest possible error when we try to approximate a polynomial of degree k on k + 1 interpolation points by a polynomial of degree k - 1. Thus, the problem of finding a contrast-optimal scheme with perfect reconstruction of black pixels boils down to a well-known problem (with a well-known solution) in Approximation Theory. A second purpose of this paper is to present a tight asymptotic analysis for the contrast parameter. Furthermore, the connection between visual cryptography and approximation theory discussed in this paper (partially known before) may also find some interest in its own right.", "authors": ["Hans Ulrich Simon"], "n_citation": 0, "title": "Perfect reconstruction of black pixels revisited", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b04dc6c2-fa14-4a2d-b427-d5cff8d8e597"}
{"abstract": "Recently the computing environment has been moved to open architectures that include Web technologies. Web Service is one of import component of the new paradigm. This paper presents a design and implementation of GIS Web Service for mobile devices. As many mobile devices are equipped with GPS (Global Positioning System), it is required to handle the position information more effectively. We have extended the proxy program in the client device to actively send the context information to the server. Based on the context information the server determines the optimal service mode to a particular client. A working example of location-based GIS Web Service is also presented. By using Web Service standards and XML messages we can achieve the maximal interoperability for heterogeneous mobile devices.", "authors": ["Xun Li", "Woochul Shin", "Li Li", "Sang Bong Yoo"], "n_citation": 0, "title": "GIS Web Service Using Context Information in Mobile Environments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b17aa110-8d82-41e9-9e57-21f0caa569ed"}
{"abstract": "In 2005, Peyravian-Jeffries proposed secure password-based protocols for remote user authentication, password change, and session key establishment over insecure networks. These protocols, however, are still susceptible to a stolen-verifier attack. Accordingly, the current paper demonstrates the vulnerability of their protocols to a stolen-verifier attack and then, a simple solution to resolve such a problem is presented. In contrast to these protocols, the proposed solution can resist the stolen-verifier attack.", "authors": ["Eun-Jun Yoon", "Kee-Young Yoo"], "n_citation": 0, "title": "Various Types of Attacks and Solutions Regarding Secure Remote User Access over Insecure Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b3df3c45-7b88-4b34-99a9-224b5bb82c00"}
{"abstract": "We consider the problem of estimating vector-valued variables from noisy relative measurements. The measurement model can be expressed in terms of a graph, whose nodes correspond to the variables being estimated and the edges to noisy measurements of the difference between the two variables. This type of measurement model appears in several sensor network problems, such as sensor localization and time synchronization. We consider the optimal estimate for the unknown variables obtained by applying the classical Best Linear Unbiased Estimator, which achieves the minimum variance among all linear unbiased estimators. We propose a new algorithm to compute the optimal estimate in an iterative manner, the Overlapping Subgraph Estimator algorithm. The algorithm is distributed, asynchronous, robust to temporary communication failures, and is guaranteed to converges to the optimal estimate even with temporary communication failures. Simulations for a realistic example show that the algorithm can reduce energy consumption by a factor of two compared to previous algorithms, while achieving the same accuracy.", "authors": ["Prabir Barooah", "Neimar Machado da Silva", "Jo\u00e3o Pedro Hespanha"], "n_citation": 64, "title": "Distributed optimal estimation from relative measurements for localization and time synchronization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c048ccc3-7fad-4c5e-9fa9-a26327900302"}
{"abstract": "non-functional requirements (NFRs) of software-intensive systems that are under continuous evolution should be evaluated during early development phases in order to be able to improve those systems and achieve 'time-to-market'. However, current evaluations are often done during late stages, like coding and testing. In this paper we propose an approach to evaluate NFRs earlier. The requirements for this approach are the use of flexible and reusable quality models, which can deal with little data, that are transparent and measurement-based. Our approach, called Prometheus, is a way of modeling NFRs that should cope with those requirements. Prometheus applies the quality modeling concept from the SQUID approach, the probability concept of Bayesian Belief Nets (BBNs) and the specification concepts of the Goal Question Metric (GQM) approach.", "authors": ["Teade Punter", "Adam Trendowicz", "Peter Kaiser"], "n_citation": 0, "title": "Evaluating evolutionary software systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c3d0df41-4a44-42d8-92fa-c2d86235c50b"}
{"abstract": "Designing experiments to be carried out with students as subjects in an XP setup is a difficult task: Students lack experiences with XP, there are limited resources, the experiment might not be taken seriously and other effects interfere. This paper presents an experiment using student subjects examining test-first in comparison to classical-testing. We proved several hypotheses about test coverage, number of test-cases, contacts with customer, acceptance for test-first, development speed and not required features. While designing the experiment we noticed that it is useful to include some additional XP techniques on top of test first, because of our special setup and the demands we had. Despite careful planning and conduction of the experiment we still faced a number of problems. In this paper we also discuss the problems with our experimental setup.", "authors": ["Thomas Flohr", "Thorsten Schneider"], "n_citation": 50, "title": "An XP experiment with students : Setup and problems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c543292b-d688-4465-9bad-e8868cfec757"}
{"abstract": "Effective usage of a general purpose modeling language in software engineering poses a need for language customization- adaptation of the language for a specific purpose. In the context of the Unified Modeling Language (UML) the customization could be done using two mechanisms: developing profiles and extending the metamodel of UML. This paper presents an industrial case study on the choice between metamodel extensions and profiles as well as the influence of the choice on the quality of products based on the extensions. The results consist of a set of nine prioritized industrial criteria which complement six theoretical criteria previously identified in the literature. The theoretical criteria are focused on the differences between the extension mechanisms of UML while the industrial criteria are focused on development of products based on these extensions. The case study reveals that there are considerable differences in effort required to develop comparable products using each mechanism and that the quality (measured as correctness of a product) is different for these comparable products by an order of magnitude.", "authors": ["Miroslaw Staron", "Claes Wohlin"], "n_citation": 50, "title": "An industrial case study on the choice between language customization mechanisms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d026acd0-9122-4e52-820b-afbe0986c174"}
{"abstract": "We investigate the complexity of finding uniformly mixed Nash equilibria (that is, equilibria in which all played strategies are played with the same probability). We show that, even in very simple win/lose bimatrix games, deciding the existence of uniformly mixed equilibria in which the support of one (or both) of the players is at most or at least a given size is an NP-complete problem. Motivated by these results, we also give NP-completeness results for problems related to finding a regular induced subgraph of a certain size or regularity in a given graph, which can be of independent interest.", "authors": ["Vincenzo Bonifaci", "Ugo Di Iorio", "Luigi Laura"], "n_citation": 0, "title": "On the complexity of uniformly mixed nash equilibria and related regular subgraph problems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d047be32-8173-430d-b9ae-761bfa551517"}
{"abstract": "Changing a consistent ontology may turn the ontology into an inconsistent state. It is the task of an approach supporting ontology evolution to ensure an ontology evolves from one consistent state into another consistent state. In this paper, we focus on checking consistency of OWL DL ontologies. While existing reasoners allow detecting inconsistencies, determining why the ontology is inconsistent and offering solutions for these inconsistencies is far from trivial. We therefore propose an algorithm to select the axioms from an ontology causing the inconsistency, as well as a set of rules that ontology engineers can use to resolve the detected inconsistency.", "authors": ["Peter Plessers", "Olga De Troyer"], "n_citation": 0, "title": "Resolving Inconsistencies in Evolving Ontologies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d4039028-ffee-4c15-9c54-7e585aed2a67"}
{"abstract": "In the BSS model of real number computations we prove a concrete and explicit semi-decidable language to be undecidable yet not reducible from (and thus strictly easier than) the real Halting Language. This solution to Post's Problem over the reals significantly differs from its classical, discrete variant where advanced diagonalization techniques are only known to yield the existence of such intermediate Turing degrees. Then we strengthen the above result and show as well the existence of an uncountable number of incomparable semi-decidable Turing degrees below the real Halting problem in the BSS model. Again, our proof will give concrete such problems representing these different degrees.", "authors": ["Klaus Meer", "Martin Ziegler"], "n_citation": 0, "title": "An explicit solution to Post's Problem over the reals", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d510dc94-b9e9-4215-aeb6-f56746a2ee12"}
{"abstract": "We propose dynamic brush stroke generation for an impressionist effect of source images, using reference data. Colors used are formed by actual palette colors from artists. To create the palette, we have referred mostly to colors used in Van Gogh's works and determined the color of brush strokes by transferring it to the most similar one, through comparing colors used in source images and the palette colors. Also, by referring to the edge orientation of source images, we have applied a brush stroke direction that surrounds the edges. The sizes were determined depending on the different sizes of the objects from wide to narrow brushes. Finally, we applied spline curve and line shapes. The brush strokes created in such method, were applied separately according to its segmented images, and composed after rendering.", "authors": ["Youngsup Park", "Kyunghyun Yoon"], "n_citation": 0, "title": "Dynamic Brush Stroke Generation for an Impressionist Effect", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dca5fcdf-625b-4329-acad-b6bad3bba2bf"}
{"authors": ["Pablo Fernando Vanegas Peralta", "Dirk Cattrysse", "Jos Van Orshoven"], "n_citation": 50, "title": "Compactness in Spatial Decision Support: A Literature Review", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "dde71d4a-091c-4f6c-9876-242956daed36"}
{"abstract": "Product line models of requirements can be used to drive the selection of requirements of new systems in the product line. Validating any selected single system is difficult because product line models are large and complex. However by modelling variability and dependency between requirements using propositional connectives a logical expression can be developed for the model and then selection validation can be achieved by satisfying the logical expression. This approach can be used to validate the model as a whole. A case study with nearly 800 requirements is presented and the computational aspects of the approach are discussed.", "authors": ["Mike Mannion", "Javier C\u00e1mara"], "n_citation": 0, "title": "Theorem proving for product line model verification", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e1212e1a-e463-4ba4-9ac4-60eaef9e07a3"}
{"abstract": "Several recent papers have investigated a relational approach to store XML data and there is a growing evidence that schema-conscious approaches are a better option than schema-oblivious techniques as far as query performance is concerned. This paper studies three strategies for storing XML document including one representing schema-conscious approach (SHARED-INLINING ) and two representing schema-oblivious approach (XParent and SUCXENT++). We implement and evaluate each approach using benchmark non-recursive XQueries. Our analysis shows an interesting fact that schema-conscious approaches are not always a better option than schema-oblivious approaches! In fact, it is possible for a schema-oblivious approach (SUCXENT++) to be faster than a schema-conscious approach (SHARED-INLINING ) for 55% of the benchmark queries (the highest observed factor being 87.8 times). SUCXENT++ also outperforms XParent by up to 1700 times.", "authors": ["Sandeep Prakash", "Sourav S. Bhowmick"], "n_citation": 0, "title": "A tale of two approaches : Query performance study of XML storage strategies in relational databases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e6889232-aebf-4586-8d23-4ce2fe779786"}
{"abstract": "In this paper, we propose a computation-efficient DCT-based encoding algorithm for the standard video encoders. Most video coding standards consist of DCT and associated modules such as quantization, zigzag scan, VLC (Variable Length Coders). Commercial and standard video encoders implement the modules independently. In this paper, the relationship between the function modules is investigated and the unnecessary operations are removed. The simulation results show that the proposed method reduces the computation complexity of IDCT by 40 % compared with standard implementation.", "authors": ["Kook-Yeol Yoo"], "n_citation": 0, "title": "A Fast Video Encoding Algorithm by Skipping the Operations on Zero-Valued DCT Coefficients", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e9177a02-c319-4855-9017-e5ec5d229510"}
{"abstract": "Quality is measurable - also in the case of software. Properly introduced metrics are the basis for efficient project- and quality management. This tutorial presents the basic concepts of measurement and gives guidelines on how to apply measurement in practice. Numerous examples included in the tutorial help quality managers, developers, and project leaders to understand the concepts presented and to select optimal metric sets for their specific organizational needs.", "authors": ["Jiirgen M\u00fcnch", "Dirk Hamann"], "n_citation": 0, "title": "Software product metrics - : Goal-oriented software product measurement", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "faec1d6a-5427-4b73-b093-a34b63df3399"}
{"authors": ["Olga Kovalenko", "Estefan\u00eda Serral Asensio", "Marta Sabou", "Fajar J. Ekaputra", "Dietmar Winkler", "Stefan Biffl"], "n_citation": 0, "title": "Automating Cross-Disciplinary Defect Detection in Multi-disciplinary Engineering Environments", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "fb0dbead-daeb-4589-8490-3151c91ec7dd"}
{"abstract": "Sequence comparison is a fundamental task in pattern matching. Its applications include file comparison, spelling correction, information retrieval, and computing (dis)similarities between biological sequences. A common scheme for sequence comparison is the longest common subsequence (LCS) metric. This paper considers the fully incremental LCS computation problem as follows: For any strings A,B and characters a,b, compute LCS(aA, B), LCS(A,bB), LCS(Aa, B), and LCS(A,Bb), provided that L = LCS(A,B) is already computed. We present an efficient algorithm that computes the four LCS values above, in O(L) or O(n) time depending on where a new character is added, where n is the length of A. Our algorithm is superior in both time and space complexities to the previous known methods.", "authors": ["Yusuke Ishida", "Shunsuke Inenaga", "Ayumi Shinohara", "Masayuki Takeda"], "n_citation": 0, "title": "Fully incremental LCS computation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fc038a63-5b34-4cfe-a740-b9ffbfab9989"}
{"abstract": "The implementation of a prototype Internet portal devoted to the fitting of ab initio potential energy values for three atom reactions is discussed. The application has been designed to run as a part of a Grid simulator of molecular processes.", "authors": ["Leonardo Arteconi", "Antonio Lagan\u00e0", "Leonardo Pacifici"], "n_citation": 50, "title": "A Web Based Application to Fit Potential Energy Functionals to ab Initio Values", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fc45e28e-af35-4260-92f2-7daeced03523"}
{"abstract": "A common technique for improving performance in a database is to decluster the database among multiple disks so that data retrieval can be parallelized. In this paper we focus on answering range queries in a multidimensional database (such as a GIS), where each of its dimensions is divided uniformly to obtain tiles which are placed on different disks; there has been a significant amount of research for this problem (a subset of which is [1,2,3,4,5,6,7,8,9,11,12,13,14,15]). A declustering scheme would be optimal if any range query could be answered by doing no more than [# of tiles inside the range/# of disks ] retrievals from any one disk. However, it was shown in [1] that this is not achievable in many cases even for two dimensions, and therefore much of the research in this area has focused on developing schemes that performed close to optimal. Recently, the idea of using replication (i.e. placing records on more than one disk) to increase performance has been introduced. [7, 12,13,15]. If replication is used, a retrieval schedule (i.e. which disk to retrieve each tile from) must be computed whenever a query is being processed. In this paper we introduce a class of replicated schemes where the retrieval schedule can be computed in time O(# of tiles inside the query's range), which is asymptotically equivalent to query retrieval for the non-replicated case. Furthermore, this class of schemes has a strong performance advantage over non-replicated schemes, and several schemes are introduced that are either optimal or are optimal plus a constant additive factor. Also presented in this paper is a strictly optimal scheme for any number of colors that requires the lowest known level of replication of any such scheme.", "authors": ["Mikhail Atallah", "Keith B. Frikken"], "n_citation": 0, "title": "Replicated parallel I/O without additional scheduling costs", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "03627e9a-56ed-4f65-a629-a213b579811e"}
{"abstract": "Datacubes are specially useful for answering efficiently queries on data warehouses. Nevertheless the amount of generated aggregated data is incomparably more voluminous than the initial data which is itself very large. Recently, research work has addressed the issue of a concise representation of datacubes in order to reduce their size. The approach presented in this paper fits in a similar trend. We propose a concise representation, called Partition Cube, based on the concept of partition and define an algorithm to compute it. Various experiments are performed in order to compare our approach with methods fitting in the same trend. This comparison relates to the efficiency of algorithms computing the representations, the main memory requirements, and the storage space which is necessary.", "authors": ["Alain Casali", "Rosine Cicchetti", "Lotfi Lakhal", "Noel Novelli"], "n_citation": 50, "title": "Lossless reduction of datacubes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "047812f8-8cba-45b6-a316-372b4f7ee6fb"}
{"abstract": "Software engineers use abstraction to better understand, model and reason about the surrounding world. Recently Architecture Description Languages (ADLs) introduced new levels of abstraction with potential use at run-time to support system evolution. In this paper we propose the FORMAware architecture that blends run-time architectural representation with a reflective programming model to address adaptation issues and promote the proximity between design and development. Reflection opens up composition architecture through a replaceable default style manager that permits to execute architecture reconfigurations. This manager enforces the structural integrity of the architecture through a set of style rules that developers may change to meet other architectural strategies. Each reconfiguration runs in the scope of a transaction that we may commit or rollback.", "authors": ["Rui S. Moreira", "Gordon S. Blair", "Eurico Carrapatoso"], "n_citation": 0, "title": "FORMAware: Framework of reflective components for managing architecture adaptation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1215dfe6-bdd2-4cd0-98b0-12b82937366d"}
{"abstract": "The need for explicit time management in workflow environments has been recently identified. Although the concept of time is inherent in workflow applications, time management until now has been treated by the same general-purpose structures of the workflow system. This traditional approach does not allow for explicitly specifying timing correctness requirements, temporal consistency checking, immediate control over the diverse set of time constraints that workflow applications exhibit and timely monitoring of the environment. In this paper, we extend the existing workflow specification based on the requirements posed by real-life applications such as health systems. We argue that time management should be an integral part of the workflow management system and not performed by a general-purpose temporal reasoner. We incorporate a subset of interval algebra that allows for efficient consistency checking, while providing expressiveness of temporal constraints. We show that static scheduling for meeting temporal constraints is inadequate for a large class of workflow applications. We show how global scheduling based on temporal constraints can be combined with agent scheduling policies. We demonstrate through examples the working of the scheduling algorithms.", "authors": ["Eleanna Kafeza", "Kamalakar Karlapalem"], "n_citation": 50, "title": "Gaining Control Over Time in Workflow Management Applications", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "1971bd0a-1eeb-45a1-99fa-0d41b1200305"}
{"authors": ["Andrey Bogdanov", "Gregor Leander", "Kaisa Nyberg", "Meiqin Wang"], "n_citation": 0, "title": "Integral and Multidimensional Linear Distinguishers with Correlation Zero", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "19d0d4f4-5604-4ec1-8002-e2501a866298"}
{"abstract": "In this paper we introduce the concept of quality-dependent time slots in the project scheduling literature. Quality-dependent time slots refer to pre-defined time windows where certain activities can be executed under ideal circumstances (optimal level of quality). Outside these time windows, there is a loss of quality due to detrimental effects. The purpose is to select a quality-dependent time slot for each activity, resulting in a minimal loss of quality. The contribution of this paper is threefold. First, we show that an R&D project from the bio-technology sector can be transformed to a resource-constrained project scheduling problem (RCPSP). Secondly, we propose an exact search procedure for scheduling this project with the aforementioned quality restrictions. Finally, we test the performance of our procedure on a randomly generated problem set.", "authors": ["Mario Vanhoucke"], "n_citation": 50, "title": "Scheduling an R&D project with quality-dependent time slots", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1b71efdb-7037-41a6-8431-2512c329d7ff"}
{"abstract": "It is important for the strategy of product sales to investigate the customer's sensibility and preference degree in the environment that the process of material development has been changed focusing on the customer center. In this paper we identify collaborative filtering and content-based filtering as independent technologies for information filtering. We propose the Fashion Design Recommender Agent System of textile design applying two-way combined filtering technologies as one of methods in the material development centered on customer's representative sensibility and preference. We build the database founded on the sensibility adjective to develop textile design by extracting the representative sensibility adjective form user's sensibility and preference about textiles. Our system recommends textile designs to a customer who has a similar propensity about textile. Ultimately, this paper suggests empirical applications to verify the adequacy and the validity on this system.", "authors": ["Kyung-Yong Jung", "Na Youngjoo", "Jung-Hyun Lee"], "n_citation": 0, "title": "FDRAS: Fashion Design Recommender Agent System using the extraction of representative sensibility and the two-way combined filtering on textile", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "218b4c53-8e98-42c0-a86a-14eeef90c3cf"}
{"abstract": "In this paper, we present incremental and intelligent query answering techniques using a multi-layered database (MLDB) in a mobile environment. We discuss static and dynamic ways of generating MLDB and explore the issues of join and updates in maintaining MLDB. We explore various issues in answering queries incrementally and intelligently.", "authors": ["Sanjay Kumar Madria", "Yongjian Fu", "Sourav S. Bhowmick"], "n_citation": 0, "title": "Incremental query answering using a multi-layered database model in a mobile computing environment", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2b7e8510-b3cb-42d7-b345-301ebf27f029"}
{"abstract": "This research is to make a series of NURBS surfaces for the virtual 3D conceptual design and the styling process by applying arbitrary free-hand strokes. The surface can be modified in real-time calligraphic stroke based free form deformation. The suggested algorithm is used to create 3D NURBS surfaces for styling object using free-hand strokes with the posture information of the input wand. The algorithm presented in this paper can help product designers in the conceptual engineering stage, even if he or she has no idea about the shape of a target product.", "authors": ["Jung-Hoon Kwon", "Han-wool Choi", "Jeong-In Lee", "Youngho Chai"], "n_citation": 50, "title": "Free-hand stroke based NURBS surface for sketching and deforming 3D contents", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2bb89c54-6724-4797-8e01-01a43ff6e582"}
{"abstract": "Online 3D games require efficient and fast user interaction support over network, and the networking support is usually implemented using network game engine. The network game engine should minimize the network delay and mitigate the network traffic congestion. To minimize the network traffic between game users, a client-based prediction (dead reckoning algorithm) is used. Each game entity uses the algorithm to estimates its own movement (also other entities' movement), and when the estimation error is over threshold, the entity sends the UPDATE (including position, velocity, etc) packet to other entities. As the estimation accuracy is increased, each entity can minimize the transmission of the UPDATE packet. To improve the prediction accuracy of dead reckoning algorithm, we propose the Kalman filter based dead reckoning approach. To show real demonstration, we use a popular network game (BZFlag), and improve the game optimized dead reckoning algorithm using Kalman filter. We improve the prediction accuracy and reduce the network traffic by 12 percents.", "authors": ["Hyon-Gook Kim", "Seong-Whan Kim"], "n_citation": 50, "title": "An improvement of dead reckoning algorithm using kalman filter for minimizing network traffic of 3D on-line games", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "36c04885-4764-428a-a8e3-f42d8570c61e"}
{"authors": ["Antonio Kung", "Francesco Furfari", "Mohammad-Reza Tazari", "Atta Badii", "Petra Turkama"], "n_citation": 0, "title": "Workshop: integration of AMI and AAL platforms in the future Internet (FI) platform initiative", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "37a28a30-fd8d-4034-bd56-952925f77b4e"}
{"abstract": "In the bare public-key model, introduced by Canetti et al. [STOC 2000], it is only assumed that each verifier deposits during a setup phase a public key in a file accessible by all users at all times. As pointed out by Micali and Reyzin [Crypto 2001], the notion of soundness in this model is more subtle and complex than in the classical model. Indeed Micali and Reyzin have introduced four different notions which are called (from weaker to stronger): one-time, sequential, concurrent and resettable soundness. In this paper we introduce the counter public-key model (the cPK model for short), an augmentation of the bare public-key model in which each verifier is equipped with a counter and, like in the original bare public-key model, the key of the verifier can be used for any polynomial number of interactions with provers. In the cPK model, we give a three-round concurrently-sound resettable zero-knowledge argument of membership for NP. Previously similar results were obtained by Micali and Reyzin [EuroCrypt 2001] and then improved by Zhao et al. [EuroCrypt 2003] in models in which, roughly speaking, each verifier is still equipped with a counter, but the key of the verifier could only be used for a fixed number of interactions.", "authors": ["Giovanni Di Crescenzo", "Giuseppe Persiano", "Ivan Visconti"], "n_citation": 0, "title": "Improved setup assumptions for 3-round resettable zero knowledge", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "39707c8d-8df3-479c-b14a-aa75ea938a7d"}
{"abstract": "Both event and stream data processing models have been researched independently and are utilized in diverse application domains. Although they complement each other in terms of their functionality, there is a critical need for their synergistic integration to serve newer class of pervasive and sensor-based monitoring applications. For instance, many advanced applications generate interesting simple events as a result of stream processing that need to be further composed and detected for triggering appropriate actions. In this paper, we present EStream, an approach for integrating event and stream processing for monitoring changes on stream computations and for expressing and processing complex events on continuous queries (CQs). We introduce masks for reducing uninteresting events and for detecting events correctly and efficiently. We discuss stream modifiers, a special class of stream operators for computing changes over stream data. We also briefly discuss architecture and functional modules of EStream.", "authors": ["Vihang Garg", "Raman Adaikkalavan", "Sharma Chakravarthy"], "n_citation": 0, "title": "Extensions to stream processing architecture for supporting event processing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "40a8cf6f-f1d3-4092-9641-ad8f453712f0"}
{"abstract": "Adamec and Nesetfil [1] proposed a new the so called fractional length criterion for measuring the aesthetics of (artistic) drawings. They proposed to apply the criterion to the aesthetic drawing of graphs. In the graph drawing community, it is widely believed and even experimentally confirmed that the number of crossings is one of the most important aesthetic measures for nice drawings of graphs [6]. The aim of this note is to demonstrate on two standard graph drawing models that in provably good drawings, with respect to the crossing number measure, the fractional length criterion is closely related to the crossing number criterion.", "authors": ["Ondrej Sykora", "L\u00e1szl\u00f3 A. Sz\u00e9kely", "Imrich Vrto"], "n_citation": 50, "title": "Fractional lengths and crossing numbers", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "47469616-75cb-4e55-9183-6bc3fc1c5b62"}
{"abstract": "In this study we present a production-inventory model for deteriorating item with vendor-buyer integration. A periodic delivery policy for a vendor and a production-inventory model with imperfect quality for a buyer are established. Such implicit assumptions (deteriorating items, imperfect quality) are reasonable in view of the fact that poor-quality items do exist during production. Defective items are picked up during the screening process. Shortages are completely backordered. The study shows that our model is a generalization of the models in current literatures. An algorithm and numerical analysis are given to illustrate the proposed solution procedure. Computational results indicate that our model leads to a more realistic result.", "authors": ["Hui Ming Wee", "Jonas C. P. Yu", "K. J. Wang"], "n_citation": 26, "title": "An Integrated Production-Inventory Model for Deteriorating Items with Imperfect Quality and Shortage Backordering Considerations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f03bbb6-582f-49a1-a319-1ded95524501"}
{"abstract": "In in-band wavelet video coding schemes, motion prediction is applied in the spatial subband domain. Compared to motion prediction in full-resolution image domain, in-band schemes suffer coding performance loss at full resolution. One reason is that signals of the subband at low resolution are predicted from the reference frames at low resolution, which has comparatively low quality. However, if signals of the subband at high resolution are involved in the prediction of signals at low resolution, mismatch will occur in decoding low-resolution video when the corresponding signals of high resolution are not available at the decoder. This paper first analyzes the mismatch error propagation when low-resolution video is decoded. Then based on the analysis we propose a frame-based cross-resolution leaky prediction scheme for in-band wavelet video coding to make a good trade-off between reducing mismatch error of low resolution and improving coding performance of high resolution. Experimental -results show that, the proposed scheme can dramatically reduce the mismatch error by about 0.3\u223c2.5dB at different bit rates for the low resolution, while for the high resolution, the performance loss is marginal.", "authors": ["Dongdong Zhang", "Jizheng Xu", "Feng Wu", "Wenjun Zhang", "Hongkai Xiong"], "n_citation": 0, "title": "A cross-resolution leaky prediction scheme for in-band wavelet video coding with spatial scalability", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5157d14d-9a4d-4c65-b57f-a673eaa99e34"}
{"abstract": "With the advent of wearable cameras attached with wearable computers, we can record our daily lives as a large amount of video data. As a consequence, automatic creation of indexes on such video data is becoming important to support query retrieval or digest creation on them. This is because it is unrealistic to create indexes on long-sustained video data by hand. To cope with this problem, we propose a method to automatically create index on such video data on the basis of geographic objects. In this scheme, users wear a wearable computer, a GPS receiver, and a wearable video camera with gyroscope to record various kinds of context information as well as video data. The system then makes analysis of those data, and automatically creates indexes of the video data. To this end, we propose two kinds of weighting schemes for geographic objects, namely, distance- and direction-based weights. We introduce some variations of distance-based weighting functions, and a method to adaptively decide reasonable indexing area with respect to the position of the user and geographic objects.", "authors": ["Takamasa Ueda", "Toshiyuki Amagasa", "Masatoshi Yoshikawa", "Shunsuke Uemura"], "n_citation": 0, "title": "A system for retrieval and digest creation of video data based on geographic objects", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "54167e01-adfd-4fa0-8c70-d8d0bc5a2b20"}
{"abstract": "We discuss a group protocol which supports applications with group communication service in change of QoS supported by networks and required by applications. An autonomic group protocol is realized by cooperation of multiple autonomous agents. Each agent autonomously takes a class of each protocol function. Classes taken by an agent are required to be consistent with but might be different from the others. We make clear what combination of classes can be autonomously taken by agents. We also present how to change retransmission ways.", "authors": ["Tomoya Enokido", "Makoto Takizawa"], "n_citation": 0, "title": "Autonomic group protocol for peer-to-peer (P2P) systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5d5616c8-72cc-49fa-a6e2-4fa658abbd25"}
{"abstract": "This paper presents an algorithm for termination static analysis of active rules with priorities. Active rules termination is an undecidable problem. Several recent works have suggested proving termination by using the concept of triggering graph. We propose here a refinement of these works, exploiting the priorities defined between rules. We introduce the notions of path set and destabilizing set. We show how to determine the priority of a path set. The triggering graph can then be reduced thanks to considerations about priorities of the path sets. Much more termination situations can be detected, since priorities are exploited.", "authors": ["Alain Couchot"], "n_citation": 0, "title": "Termination analysis of active rules with priorities", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5e2937fa-897f-4f62-b597-f0b780f8fef7"}
{"authors": ["Bart Mennink"], "n_citation": 0, "title": "XPX: Generalized Tweakable Even-Mansour with Improved Security Guarantees", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "63a4e446-4802-4175-9cc4-bf4329d3913d"}
{"abstract": "We present a parameterized representation of virtual organs for surgery simulation purpose. Random 3D input mesh are parameterized and resampled into a regular 2D parameterized model. With this parameterized representation, a high resolution 3D organ mesh can be reconstructed and deformed interactively with a simple and fast free-form deformation method. The amount of deformation and force feed-back can be calculated rapidly. Therefore, haptic rendering can be achieved. In addition, the parameterized mesh can be used to handle collision detection and the contact between multi-objects in an efficient way. With the parameterized mesh, realistic visual and haptic rendering can be provided for interactive surgery simulation.", "authors": ["Qiang Liu", "Edmond C. Prakash"], "n_citation": 0, "title": "Cyber surgery : Parameterized mesh for multi-modal surgery simulation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "63c77c16-b5f9-4f7c-b8f6-552eaed72310"}
{"abstract": "It becomes too expensive computationally to compare a query protein with protein structures in a 3D structure databases for determining their similarity. Therefore, we emphasize that solving structural similarity search is to develop fast structure comparison algorithms. We propose a new method for comparing the structural similarity in protein structure databases with a given query protein by using topological pattern of proteins. In our approach, the geometry of SSEs(Secondary Structure Elements) is represented by spatial data types and indexed using an Rtree. We discover topological patterns of SSEs in 3D space using 9IM topological relations accelerated by Rtree index join to all the structures in 3D structure databases. A similarity search algorithm compares topological patterns of a query protein with those of proteins in the structure database. Experimental results show that execution time of our method is 3 times faster than DALITE while keeping the accuracy similar. This study identifies that similarity search based on spatial databases can find the similar structures rapidly and generate small candidate sets for the generalized alignment tools such as DALI and SSAP.", "authors": ["Sung-Hee Park", "Keun Ho Ryu"], "n_citation": 0, "title": "Fast similarity search for protein 3D Structure databases using spatial topological patterns", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6f5696ae-43d1-4ee0-920e-bda012f0412b"}
{"abstract": "We developed a novel web application called My Portal Viewer (MPV), which automatically categorizes and integrates meta-data from many news pages based on the user's preferences after gathering these news pages from various news sites. Our unique approach is based on two points: one is an automatic categorization of collected information based on user's interests and knowledge, and the other is the look and feel of the MPV page, which is applied to the user's favorite news portal page, and part of the original content is replaced by the integrated content. Whenever a user accesses the MPV page after browsing news pages, he/she can obtain the desired content efficiently because the MPV presents pages refreshed based on the user's behavior through his/her favorite page layout, which reflects his/her interests and knowledge. In this paper, we describe the MPV framework, and methods that are based on the user's preferences for replacing and categorizing content have been developed using an HTML table model and a vector matching model.", "authors": ["Yukiko Kawai", "Daisuke Kanjo", "Katsumi Tanaka"], "n_citation": 0, "title": "My portal viewer : Integration system based on user preferences for news web sites", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "73539248-8f86-4bc0-af59-20b650f2851c"}
{"abstract": "Nivat et al [3] introduced Context-free Puzzle grammars for generating connected picture arrays in the two-dimensional plane. Basic Puzzle grammars [6] constitute a subclass of these grammars. In this note we consider the Cooperating Array Grammar Systems introduced by Dassow et al [2] with Basic Puzzle grammar rules in the components instead of array grammar rules and examine the picture generating power of the resulting system, called, Cooperating Basic Puzzle Grammar System, in the maximal mode.", "authors": ["K. G. Subramanian", "R. Saravanan", "P. Helen Chandra"], "n_citation": 0, "title": "Cooperating basic puzzle grammar systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "77a84446-106b-4956-a4ba-88b0ed62858b"}
{"abstract": "We present a batch version of Schnorr's identification scheme. Our scheme uses higher degree polynomials that enable the execution of several Schnorr's protocol at a cost very close to that of a single execution. We present a full proof of security that our scheme is secure against impersonation attacks. The main application of this result is a very efficient way for a party to prove that it holds several secret keys (i.e. identities), where each identity is linked to a specific authorization. This approach protects the privacy of the prover allowing her to prove only the required set of authorizations required to perform a given task, without disclosing whether she is in possession of other privileges or not. We also show that our scheme is suitable to be implemented on low-bandwidth communication devices. We present an implementation of a smart card employing recent technology for the use of LEDs (Light Emitting Diodes) for bidirectional communication. Another contribution of our paper is to show that this new technology allows the implementation of strong cryptography.", "authors": ["Rosario Gennaro", "Darren Leigh", "Ravi Sundaram", "William S. Yerazunis"], "n_citation": 0, "title": "Batching schnorr identification scheme with applications to privacy-preserving authorization and low-bandwidth communication devices", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "794ee3b9-1f7a-4ec8-ae15-e0dcc66e6335"}
{"abstract": "The relational model, as proposed by Codd, contained the concept of relations as tables composed of tuples of single valued attributes taken from a domain. In most of the early literature this domain was assumed to consist of elementary items such as simple (atomic) values, defined complex data types or arbitrary length binary objects. Subsequent to that the nested relational or non-first normal form model allowing set-valued or relation-valued attributes was proposed. Within this model an attribute could take multiple values or complete relations as values. This paper presents a further extension to the relational model which allows domains to be defined as a hierarchy (specifically a lattice) of concepts, shows how different types of imperfect knowledge can be represented in attributes defined over such domains, and demonstrates how lattices allow the accommodation of some forms of inductive queries. While our model is applied to flat relations, many of the results given are applicable also to nested relations. Necessary extensions to the relational algebra and SQL, a justification for the extension in terms of application areas and future research areas are also discussed.", "authors": ["Sally Rice", "John F. Roddick"], "n_citation": 0, "title": "Lattice-structured domains, imperfect data and inductive queries", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "79a6a989-ef1b-44a7-9b9a-f847b1a346c0"}
{"authors": ["Ari Harju", "Topi Siro", "Filippo Federici Canova", "Samuli Hakala", "Teemu Rantalaiho"], "n_citation": 50, "title": "Computational Physics on Graphics Processing Units", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "7a2d67b9-0f47-4cb6-86c8-f8a1f3c86d78"}
{"abstract": "To explore to what extent emotional intentions can be conveyed through musicians' movements, video recordings were made of a marimba player performing the same piece with the intentions Happy, Sad, Angry and Fearful. 20 subjects were presented video clips, without sound, and asked to rate both the perceived emotional content as well as the movement qualities. The video clips were presented in different conditions, showing the player to different extent. The observers' ratings for the intended emotions confirmed that the intentions Happiness, Sadness and Anger were well communicated, while Fear was not. Identification of the intended emotion was only slightly influenced by the viewing condition. The movement ratings indicated that there were cues that the observers used to distinguish between intentions, similar to cues found for audio signals in music performance.", "authors": ["Sofia Dahl", "Anders Friberg"], "n_citation": 50, "title": "Expressiveness of musician's body movements in performances on marimba", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7a6f22cc-09de-4928-b055-da1e2bdab301"}
{"abstract": "A lot of work has been done on extracting the model of web user behavior. Most of them target server-side logs that cannot track user behavior outside of the server. Recently, a novel way has been developed to collect web browsing histories, using the same method for determining TV audience ratings; i.e., by collecting data from randomly selected users called panels. The logs collected from panels(called panel logs) cover an extremely broad URL-space, and it is difficult to capture the global behaviors of the users. Here we utilize mining results of web community to group those URLs into easily understandable topics. We also use search keywords in search engine sites because user behavior is deeply related to search keyword according to preliminary experiments on panel logs. We develop a prototype system to extract user access patterns from the panel logs and to capture the global behavior based on web communities.", "authors": ["Shingo Otsuka", "Masashi Toyoda", "Jun Hirai", "Masaru Kitsuregawa"], "n_citation": 0, "title": "Extracting user behavior by web communities technology on global web logs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "863435ab-484b-4614-bd3b-dde056a106d3"}
{"abstract": "It's a fact of life that organizations love to reorganize. Reorganizations have a profound effect on the way product lines are governed. We introduce the concept of the Responsibility, Authority, and Accountability (RAA) network. An RAA network assists in the governance process of product lines for internal information systems, even in the face of massive reorganization. Armour (Reorg Cycle [1]) describes the pressures of reorganization to balance the dimensions of organization (e.g., geography, customers, product technology); we apply polarity management to balance the dimensions. Armour describes the difficulty of applying hierarchical organization charts-single dimension management structures-to the above multidimensional environments; we apply lean RAA networks to span organization charts and provide the multidimensional view needed for product lines. Armour observes that network organization approaches do not have a good track record; our experience is that lean, resilient RAA networks document the product line's governance architecture. These governance architecture patterns are applied repeatedly to the strata of products in the product line. We present the governance architect's RAA to define, monitor, and sustain governance health using these tools: polarity maps, polarity networks, RAA maps, and RAA networks.", "authors": ["Truman M. Jolley", "David J. Kasik", "Tammy R. Ben"], "n_citation": 0, "title": "Governing software product lines and reorganizations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8bbb8a01-4f21-4461-ad51-6ab27ba5769c"}
{"abstract": "A two-argument function is computed privately by two parties if after the computation, no party should know anything about the other inputs except for what he is able to deduce from his own input and the function value. In [1] Bar-Yehuda, Chor, Kushilevitz, and Orlitsky give a complete characterisation of two-argument functions which can be computed privately (in the information-theoretical sense) in the Honest-But-Curious model and study protocols for non-private functions revealing as little information about the inputs as possible. The authors define a measure which determines for any function f the additional information e(f) required for computing f and claim that f is privately-computable if and only if e(f) = 0. In our paper we show that the characterisation is false: we give a privately-computable function f with e(f) \u00ac= 0 and another function g with e(g) = 0 that is not privately-computable. Moreover, we show some rather unexpected and strange properties of the measure for additional information given by Bar-Yehuda et al. and we introduce an alternative measure. We show that for this new measure the minimal leakage of information of randomized and deterministic protocols are equal. Finally, we present some general relations between the information gain of an optimal protocol and the communication complexity of a function.", "authors": ["Andreas Jakoby", "Maciej Liskiewicz"], "n_citation": 0, "title": "Revealing additional information in two-party computations", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8ffb8e1c-40fb-4edb-a123-82e53e3be8f9"}
{"abstract": "A functional electrical stimulation (FES)-based tracking controller is developed to enable cycling based on a strategy to yield force direction efficiency by exploiting antagonistic bi-articular muscles. Given the input redundancy naturally occurring among multiple muscle groups, the force direction at the pedal is explicitly determined as a means to improve the efficiency of cycling. A model of a stationary cycle and rider is developed as a closed-chain mechanism. A strategy is then developed to switch between muscle groups for improved efficiency based on the force direction of each muscle group. Stability of the developed controller is analyzed through Lyapunov-based methods.", "authors": ["Hiroyuki Kawai", "Matthew J. Bellman", "Ryan J. Downey", "Warren E. Dixon"], "n_citation": 19, "references": ["050ac6db-08f2-4b11-b6e3-ce5e286d9bba", "09a907be-2099-4c40-9a18-ab76bf0ce310", "13813736-024a-4c7f-982c-9192368be31b", "187540a9-bcaf-490b-b4a6-52b5574c8f7e", "4356426f-818d-43ea-b5bc-355fab4b2510", "477ef83c-c35d-49c0-91b4-81dea5284f0f", "a5cc6f54-18b7-4bb2-9a7f-6d41c3895004"], "title": "Tracking Control for FES-Cycling based on Force Direction Efficiency with Antagonistic Bi-Articular Muscles", "venue": "advances in computing and communications", "year": 2014, "id": "947b7a04-e7ce-4041-b753-21c990bccb67"}
{"abstract": "In this paper, a high performance asynchronous on-chip bus designed in a Globally Asynchronous Locally Synchronous (GALS) style is proposed. The asynchronous on-chip bus is capable of handling multiple outstanding transactions and in-order completion to achieve a high performance, which is implemented with distributed and modularized control unit in a layered interface. The architecture of asynchronous on-chip bus is discussed and implemented for simulations. Simulation results show that throughput of the proposed asynchronous on-chip bus with multiple outstanding transactions and in-order transaction completion is increased by 31.3%, while power consumption overhead is only 6.76%, as compared to an asynchronous on-chip bus with a single outstanding transaction.", "authors": ["Eun-Gu Jung", "Eonpyo Hong", "Kyoung-Son Jhang", "Jeong-A Lee", "Dongsoo Har"], "n_citation": 0, "title": "Self-timed interconnect with layered interface based on distributed and modularized control for multimedia SoCs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9afde341-10f6-4462-9123-ba159c09582d"}
{"abstract": "A new strategy for discovering action rules (or interventions) is presented in this paper. The current methods [14], [12], [8] require to discover classification rules before any action rule can be constructed from them. Several definitions of action rules [8], [13], [9], [3] have been proposed. They differ in the generality of their classification parts but they are always constructed from certain pairs of classification rules. Our new strategy defines the classification part of an action rule in a unique way. Also, action rules are constructed from single classification rules. We show how to compute their confidence and support. Action rules are used to reclassify objects. In this paper, we propose a method for measuring the level of reclassification freedom for objects in a decision system.", "authors": ["Zbigniew W. Ras", "Agnieszka Dardzigska"], "n_citation": 50, "title": "Action Rules Discovery, a New Simplified Strategy", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9cb7b098-f6b1-47ef-8fb0-ac4cae06ed3a"}
{"abstract": "In this paper, we present an information retrieval system for tourism information that allows query formulation in natural language. We describe a knowledge representation model, based on associative networks, for defining semantic relationships of terms. The relatedness of terms is taken into account and we show how a fuzzy search strategy, performed by a constrained spreading activation algorithm, yields beneficial results and recommends closely related matches to users' queries. Thus, spreading activation implicitly implements query expansion.", "authors": ["Helmut Berger", "Michael Dittenbachl", "Dieter Merkl"], "n_citation": 0, "title": "Activation on the move: Querying tourism information via spreading activation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9ed0048b-3e68-4fb3-b98e-c68798d17a5e"}
{"abstract": "Khazad is a new block cipher initially proposed as a candidate to the NESSIE project. Its design is very similar to Rijndael, although it is a 64-bit block cipher. In this paper, we propose a new attack that can be seen as an extension of the Square attack. It takes advantage of redundancies between the round key derivation and the round function, and also exploits some algebraic observations over a few rounds. As a result, we can break 5 rounds of Khazad faster than exhaustive key search. This is the best known cryptanalytic result against Khazad.", "authors": ["Fr\u00e9d\u00e9ric Muller"], "n_citation": 0, "title": "A new attack against Khazad", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "a188cc91-d630-4e09-bd6e-a17b76864eb0"}
{"abstract": "Recently, there have been some efforts to add XML full-text retrievals and XML updates into new standardization of XML queries. XML full-text retrievals play an important role in XML query languages. Unlike tables in the relational model, an XML document has a complex and unstructured nature. XML update is another core function that an XML query should have in order to be a full-fledged query language for XML documents. In this paper we propose an inverted index to support XML updates and XML full-text queries in relational environment. Performance comparisons exhibit that our approach maintains a comparable size of inverted indexes and it supports many full-text retrieval functions very well. Foremost our approach handles XML updates efficiently by removing cascading effects.", "authors": ["Dong-Kweon Hong", "Kweon-Yang Kim"], "n_citation": 0, "title": "Update conscious inverted indexes for XML queries in relational databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a4475d06-03f7-42b4-9070-a033f960ddcb"}
{"authors": ["Mokrane Bouzeghoub", "Bernadette Farias L\u00f3scio", "Zoubida Kedad", "Assia Soukane"], "n_citation": 50, "title": "Heterogeneous data source integration and evolution", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a49af275-008a-45f4-aa78-d82fc46e1b99"}
{"authors": ["Juhani Karhum\u00e4ki", "Svetlana Puzynina"], "n_citation": 0, "title": "On k-abelian palindromic rich and poor words", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "a8689759-db7c-4f6a-8d43-d48de97369c8"}
{"abstract": "It is a fact that XML update has become more important with the rise of XML Database usage. How update operations affect XML documents needs to be investigated further. In this paper we propose a methodology to accommodate update without violating the XML document's constraints. The constraints maintained are those that are defined using XML linking language: xlink and xpointer. This language, which is standardized by W3C, is used to provide referential purpose among XML documents or nodes. Since XML link is embedded as an attribute in an XML instance, our proposal can be used for schema-less documents and for instance-based reference. We propose a set of functions that perform checking mechanisms before updates. The proposed method can be implemented in various ways, and in this case we use XQuery language.", "authors": ["Eric Pardede", "J. Wenny Rahayu", "David Taniar"], "n_citation": 0, "title": "On maintaining XML linking integrity during update", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b27585b0-0453-4372-8847-277a6c409f80"}
{"abstract": "Adaptive policies contain parameters and take into consideration performance feedback to modify values of these parameters adaptively. We propose a method (applicable to the domain of Information Lifecycle Management) to automatically adapt these parameters. Design issues such as selection of sensors, desired ranges for sensors and effects of parameter sensitivity such as over-correction and under-correction are discussed.", "authors": ["Rohit Lotlikar", "Mukesh K. Mohania"], "n_citation": 0, "title": "Adaptive policies in information lifecycle management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b2ac9ae1-5686-4357-8574-a810794eb639"}
{"abstract": "A summary of the development of the database and expert system applications research of this decade I given. The different trends and challenges are briefly analysed. The increasing shortening of the time-span between publication of an idea and the implementation and commercialisation of the idea is described.", "authors": ["A Min Tjoa", "Roland Wagner"], "n_citation": 0, "title": "Database and expert systems 2002 quo vadis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "bbb4804d-4a0c-4ef3-bcce-a46657bd403c"}
{"abstract": "Even and Mansour [EM97] proposed a block cipher construction that takes a publicly computable random permutation oracle P and XORs different keys prior to and after applying P: C = k 2  \u25cb+ P(M \u25cb+ k 1 ). They did not, however, describe how one could instantiate such a permutation securely. It is a fundamental open problem whether their construction could be proved secure outside the random permutation oracle model. We resolve this question in the affirmative by showing that the construction can be proved secure in the random function oracle model. In particular, we show that the random permutation oracle in their scheme can be replaced by a construction that utilizes a four-round Feistel network (where each round function is a random function oracle publicly computable by all parties including the adversary). Further, we prove that the resulting cipher is super pseudorandom - the adversary's distinguishing advantage is at most 2q 2 /2 n  if he makes q total queries to the cipher, its inverse, as well as any random oracles. Even and Mansour, on the other hand, only showed security against inversion and forgery. One noteworthy aspect of this result is that the cipher remains secure even though the adversary is permitted separate oracle access to all of the round functions. One can achieve a two-fold and four-fold reduction respectively in the amount of key material by a closer inspection of the proof and by instantiating the scheme using group operations other than exclusive-OR. On the negative side, a straightforward adaption of an advanced slide attack recovers the 4n-bit key with approximately \u221a2 . 2 n  work using roughly \u221a2 . 2 n  known plaintexts. Finally, if only three Feistel rounds are used, the resulting cipher is pseudorandom, but not super pseudorandom.", "authors": ["Craig Gentry", "Zulfikar Ramzan"], "n_citation": 0, "title": "Eliminating random permutation oracles in the even-mansour cipher", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c93be9f1-9255-4832-a4b3-546bcfd9d1d7"}
{"abstract": "In this paper, we propose a framework, called XAR-Miner, for mining ARs from XML documents efficiently and effectively. In XAR-Miner, raw XML data are first transformed to either an Indexed Content Tree (IX-tree) or Multi-relational databases (Multi-DB), depending on the size of XML document and memory constraint of the system, for efficient data selection in the AR mining. Concepts that are relevant to the AR mining task are generalized to produce generalized meta-patterns. A suitable metric is devised for measuring the degree of concept generalization in order to prevent under-generalization or over-generalization. Resultant generalized meta-patterns are used to generate large ARs that meet the support and confidence levels. An efficient AR mining algorithm is also presented based on candidate AR generation in the hierarchy of generalized meta-patterns. The experiments show that XAR-Miner is more efficient in performing a large number of AR mining tasks from XML documents than the state-of-the-art method of repetitively scanning through XML documents in order to perform each of the mining tasks.", "authors": ["Ji Zhang", "Tok Wang Ling", "Robert M. Bruckner", "A Min Tjoa", "Han Liu"], "n_citation": 0, "title": "On efficient and effective association rule mining from XML data", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c9a21929-bf81-40d2-bf22-9778cf451f00"}
{"abstract": "In this paper, we introduce a new cryptanalysis method for stream ciphers based on T-functions and apply it to the TSC family which was proposed by Hong et al.. Our attack are based on linear approximations of the algorithms (in particular of the T-function). Hence, it is related to correlation attack, a popular technique to break stream ciphers with a linear update, like those using LFSR's. We show a key-recovery attack for the two algorithms proposed at FSE 2005: TSC-1 in 2 25.4  computation steps, and TSC-2 in 2 48.1  steps. The first attack has been implemented and takes about 4 minutes to recover the whole key on an average PC. Another algorithm in the family, called TSC-3, was proposed at the ECRYPT call for stream ciphers. Despite some differences with its predecessors, it can be broken by similar techniques. Our attack has complexity of 2 42  known keystream bits to distinguish it from random, and about 2 66 steps of computation to recover the full secret key. An extended version of this paper can be found on the ECRYPT website [23].", "authors": ["Fr\u00e9d\u00e9ric Muller", "Thomas Peyrin"], "n_citation": 50, "title": "Linear cryptanalysis of the TSC family of stream ciphers", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cadc595c-5eb4-4a93-941f-b3646a20317c"}
{"abstract": "We introduce a primitive called Hierarchical Identity-Coupling Broadcast Encryption (HICBE) that can be used for constructing efficient collusion-resistant public-key broadcast encryption schemes with extended properties such as forward-security and keyword-searchability. Our forward-secure broadcast encryption schemes have small ciphertext and private key sizes, in particular, independent of the number of users in the system. One of our best two constructions achieves ciphertexts of constant size and user private keys of size O(log 2  T), where T is the total number of time periods, while another achieves both ciphertexts and user private keys of size O(logT). These performances are comparable to those of the currently best single-user forward-secure public-key encryption scheme, while our schemes are designed for broadcasting to arbitrary sets of users. As a side result, we also formalize the notion of searchable broadcast encryption, which is a new generalization of public key encryption with keyword search. We then relate it to anonymous HICBE and present a construction with polylogarithmic performance.", "authors": ["Nuttapong Attrapadung", "Jun Furukawa", "Hideki Imai"], "n_citation": 50, "title": "Forward-Secure and Searchable Broadcast Encryption with Short Ciphertexts and Private Keys", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cb1d48a0-93f4-42fa-918d-10b5959da2ec"}
{"abstract": "We describe an approach to fill missing values in decision trees during classification. This approach is derived from the ordered attribute trees method, proposed by Lobo and Numao in 2000, which builds a decision tree for each attribute and uses these trees to fill the missing attribute values. It is based on the Mutual Information between the attributes and the class. Our approach primarily extends this method on three points: 1) it does not impose an order of construction; 2) a probability distribution is used for each missing attribute instead of the most probable value; 3) the result of the classification process is a probability distribution instead of a single class. Moreover, our method takes the dependence between attributes into account. We present Lobo's approach and our extensions, we compare them, and we discuss some perspectives.", "authors": ["Lamis Hawarah", "Ana Simonet", "Michel Simonet"], "n_citation": 0, "title": "A probabilistic approach to classify incomplete objects using decision trees", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d43e6aa7-3634-488b-8b13-8c62f8fb63bc"}
{"abstract": "This paper presents a framework for terminological reasoning by taking into account geometric representation of objects. We consider objects having shapes which are described by means of semi-algebraic sets 1 . We show how geometric representations can be used to define concepts. The formal semantics as well as the reasoning algorithms are given. We present a calculus for deciding satisfiability of a constraint knowledge base described in our language. The proposed language can be seen as an extensible core for applications such as multimedia databases or spatial and geographic databases.", "authors": ["Hicham Hajji", "Evimaria Terzi"], "n_citation": 0, "title": "A framework for reasoning on objects with geometric constraints", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d5e4e85d-a8ab-46f8-b76b-800baacb7455"}
{"abstract": "To ensure accurate predictions of loss given default it is necessary to test the goodness-of-fit of the recovery rate data to the Beta distribution, assuming that its parameters are unknown. In the presence of unknown parameters, the Cramer-von Mises test statistic is neither asymptotically distribution free nor parameter free. In this paper, we propose to compute approximated critical values with a parametric bootstrap procedure. Some simulations show that the bootstrap procedure works well in practice.", "authors": ["J. Samuel Baixauli", "Susana Alvarez"], "n_citation": 0, "title": "On the Performance of Recovery Rate Modeling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d69c3fbb-4f07-4462-8f8a-2b053817fb3b"}
{"abstract": "A range query applies an aggregation operation over all selected cells of an OLAP data cube where selection is specified by the range of contiguous values for each dimension. Many works have focused on efficiently computing range sum or range max queries. Most of these algorithms use a uniformly partitioning scheme for the data cube. In this paper, we improve on query costs of some of these existing algorithms by noting two key areas. First, end-user range queries usually involve repetitive query patterns, which provide a variable sized partitioning scheme that can be used to partition the data cubes. Query costs are reduced because pre-computation is retrieved for entire partitions, rather than computed for a partial region in many partitions, which requires large amounts of cell accesses to the data cube. Second, data in the data cube can be arranged such that each partition is stored in as few physical storage blocks as possible, thus reducing the I/O costs for answering range queries.", "authors": ["Tok Wang Ling", "Wai Chong Low", "Zhong Wei Luo", "Sin Yeung Lee", "Hua Gang Li"], "n_citation": 0, "title": "Variable sized partitions for range query algorithms", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d73856f8-3fa2-4ab1-b68c-c14e1d5b21c8"}
{"abstract": "Clustering is currently one of the most crucial techniques for dealing (e.g. resources locating, information interpreting) with massive amount of heterogeneous information on the web, which is beyond human being's capacity to digest. In this paper, we discuss the shortcomings of pervious approaches and present a unifying clustering algorithm to cluster web search results for a specific query topic by combining link and contents information. Especially, we investigate how to combine link and contents analysis in clustering process to improve the quality and interpretation of web search results.The proposed approach automatically clusters the web search results into high quality, semantically meaningful groups in a concise, easy-to-interpret hierarchy with tagging terms. Preliminary experiments and evaluations are conducted and the experimental results show that the proposed approach is effective and promising. snippet.", "authors": ["Yitong Wang", "Masaru Kitsuregawa"], "n_citation": 0, "title": "On combining link and contents information for web page Clustering", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e31eef3f-c567-46a1-a0d9-158715bfeda6"}
{"abstract": "It has been recently acknowledged [4,6,9] that the use of double bases representations of scalars n, that is an expression of the form n = \u03a3 e,s,t  (-1) e A s B t  can speed up significantly scalar multiplication on those elliptic curves where multiplication by one base (say B) is fast. This is the case in particular of Koblitz curves and supersingular curves, where scalar multiplication can now be achieved in o(logn) curve additions. Previous literature dealt basically with supersingular curves (in characteristic 3, although the methods can be easily extended to arbitrary characteristic), where A, B \u2208 N. Only [4] attempted to provide a similar method for Koblitz curves, where at least one base must be non-real, although their method does not seem practical for cryptographic sizes (it is only asymptotic), since the constants involved are too large. We provide here a unifying theory by proposing an alternate recoding algorithm which works in all cases with optimal constants. Furthermore, it can also solve the until now untreatable case where both A and B are non-real. The resulting scalar multiplication method is then compared to standard methods for Koblitz curves. It runs in less than log n/ log log n elliptic curve additions, and is faster than any given method with similar storage requirements already on the curve K-163, with larger improvements as the size of the curve increases, surpassing 50% with respect to the T-NAF for the curves K-409 and K-571. With respect of windowed methods, that can approach our speed but require 0(log(n)/ log log(n)) precomputations for optimal parameters, we offer the advantage of a fixed, small memory footprint, as we need storage for at most two additional points.", "authors": ["Roberto Maria Avanzi", "Vassil S. Dimitrov", "Christophe Doche", "Francesco Sica"], "n_citation": 53, "title": "Extending Scalar Multiplication Using Double Bases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e6c48f4a-2696-46cb-825a-367dfb400f8f"}
{"abstract": "In these note we review some basic approaches and algorithms for discrete plane/hyperplane recognition. We present, analyze, and compare related theoretical and experimental results and discuss on the possibilities for creating algorithms with higher efficiency.", "authors": ["David Coeurjolly", "Valentin E. Brimkov"], "n_citation": 50, "title": "Computational aspects of digital plane and hyperplane recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e702dc19-43db-4066-af13-fdbcf33ee5ee"}
{"abstract": "A Distributed Heterogeneous Database System (DHDBS) is constituted of different kinds of autonomous databases connected to the network. A distributed transaction in such a system involves many sub-transactions and data movements among database sites. For the time being, most of the commercial database products implement their distributed transactions using the traditional client/server model that is suffering from enormous data movements. This paper proposes a new distributed transaction model which uses mobile agent technology to reduce data traffics in distributed transactions. The idea is backed by the well-known characteristics, such as mobility, autonomy, and concurrency, of mobile agents in supporting distributed computations. The aim is to boost the performance of distributed transactions of a heterogeneous database system in a loosely coupled environment (such as the Internet). An procedure is designed for distributed query decomposition. Some principles are observed for the path planning of a mobile agent roaming the network to carry out various sub-transactions.", "authors": ["Ding Yen Ye", "Ming Che Lee", "Tzone I. Wang"], "n_citation": 0, "title": "Mobile agents for Distributed transactions of a Distributed Heterogeneous Database System", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e7410923-3e6d-456a-b1b8-4fba9e13e309"}
{"abstract": "In this paper, we present a novel frequent generalized pattern mining algorithm, called GP-Close, for mining generalized associations from RDF metadata. To solve the over-generalization problem encountered by existing methods, GP-Close employs the notion of generalization closure for systematic over-generalization reduction. Empirical experiments conducted on real world RDF data sets show that our method can substantially reduce pattern redundancy and perform much better than the original generalized association rule mining algorithm Cumulate in term of time efficiency.", "authors": ["Tao Jiang", "Ah-Hwee Tan"], "n_citation": 50, "title": "Mining RDF metadata for generalized association rules", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ea0f2412-170f-4ce8-ba32-6b7014a12a8f"}
{"abstract": "This paper presents a system prototype implementing NP Data log, a Datalog-like language for expressing NP search and optimization problems. NP Datalog extends DATALOG (DATALOG with stratified negation) with intuitive and efficient constructs, i.e. constraints and a restricted form of (exclusive) disjunction used to define (nondeterministically) subsets (or partitions) of relations. The system translates NP Datalog queries into OPL programs, then solves them by using the ILOG Solver [16]. Thus, it combines an easy formulation of problems, expressed by means of a declarative logic language, and an efficient execution of the ILOG Solver. Several experiments show the effectiveness of this approach.", "authors": ["Sergio Greco", "Cristian Molinaro", "Irina Trubitsyna"], "n_citation": 50, "title": "Implementation and experimentation of the logic language NP datalog", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f4be9d54-f130-4edf-88dc-a00d37ec0233"}
{"abstract": "In the mobile computing area, short-range wireless communication technologies make it possible to envision direct interactions between mobile devices. In the scope of data access, devices can now be considered as both data providers and data consumers. Thus, each device can be provided with a remote access to data its neighbours agree to share. Such a service enables applications to consult a set of data providers which dynamically evolves according to the mobility of the neighbouring devices. The set of data sources an application may access by this way is therefore representative of its physical neighbourhood. In this context, we propose to design a tool making possible the continuous consultation of neighbouring shared data. We present, in this paper, the PERSEND system we develop in this scope. Based on relational databases systems, PERSEND enables applications to define continuous queries over neighbouring data.", "authors": ["David Touzet", "Fr\u00e9d\u00e9ric Weis", "Michel Banatre"], "n_citation": 0, "title": "Sensing and filtering surrounding data: The PERSEND approach", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f695b1f5-1fd9-4e3a-8a54-fb19ad77a199"}
{"abstract": "The Access Grid (AG) for collaborative environment exploits IP multicast, which is quite a promising technology for sharing network bandwidth. IP multicast however, faces lots of challenges such as unreliable data transport and immature network infrastructure. As a step to improve reliability and stability in IP multicast, it is essential to properly monitor and debug multicast networks. In this paper we introduce MEET (Multicast debugging toolkit with End-to-End packet Trace), an user-oriented multicast debugging toolkit for AG. MEET provides proactive ways of debugging multicast reachablility and measuring end-to-end delivery statistics. We are sure that MEET supports multicast users with a simple yet extensive view of end-to-end multicast reachability and delivery statistics.", "authors": ["Jinyong Jo", "Jaiseung Kwak", "Okhwan Byeon"], "n_citation": 0, "title": "Meet : Multicast debugging toolkit with end-to-end packet trace", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f6a768f6-22f6-4fed-beee-b6122ec2ebd7"}
{"abstract": "We investigate decision problems like reachability and boundedness for extensions of PB systems with volatile membranes. Specifically, we prove that reachability and boundedness are decidable for PB systems extended with rules for membrane dissolution. For PB systems extended with membrane creation, reachability is still decidable whereas boundedness becomes undecidable. Furthermore, we show that both problems are undecidable for PB systems extended with both dissolution and creation rules. Finally, we prove that reachability and boundedness become decidable for PB systems with dissolution rules and in which only one instance of each type of membrane can be created during a computation. Our work extends the results in [4] obtained by Dal Zilio and Formenti for PB systems with static membrane structure. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "authors": ["Giorgio Delzanno", "Laurent Van Begin"], "n_citation": 50, "title": "On the dynamics of PB systems with volatile membranes", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "00100380-e432-4adc-8ad2-25e629cde0ba"}
{"abstract": "Clustering is to group similar objects into clusters. Until now there are a lot of approaches using Self-Organizing Feature Maps(SOFMs). But they have problems with a small output-layer nodes and initial weight. This paper suggests one-dimensional output-layer nodes in SOFMs. The number of output-layer nodes is more than those of clusters intended to find and the order of output-layer nodes is ascending in the sum of the output-layer node's weight. We can find input data in SOFMs output node and classify input data in output nodes using the Euclidean Distance. The suggested algorithm was tested on well-known IRIS data and machine-part incidence matrix. The results of this computational study demonstrate the superiority of the suggested algorithm.", "authors": ["Jong-Sub Lee", "Maing-Kyu Kang"], "n_citation": 0, "title": "A Clustering Algorithm Using the Ordered Weight Sum of Self-Organizing Feature Maps", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "021d66fc-992b-41f4-b859-0a1ec52e0049"}
{"abstract": "In a large-scale dynamic system with multiple distributed entities, each with their own set of interests, there is a need to find a globally acceptable and optimal solution state. This solution state is, by definition, efficient to all entities with respect to their own individual goals and to the system as a whole. In these dynamic environments, this solution state can be achieved by utilizing software techniques from the field of game theory in order to make optimal decisions. We present an application built upon a generalized optimization framework that can be applied to a number of domains, such as cargo or network traffic algorithms. In this research, we used a market-based approach to air traffic flow management through a modeling and simulation environment. The aim is to allow individual aircraft a certain degree of local autonomy, much like cars on a highway. Our system is able to cope in real time with failures such as node loss and adjust system parameters accordingly to optimize results based on the goals of the involved agents. We describe tradeoffs between different agent interaction frameworks with respect to their performance in market mechanism auctions. We also discuss lessons learned while implementing this application. This research has built upon our previously reported work [20, 21] on route optimizations and airspace sector design in an air traffic control network, by adding in the goals of interested entities, e.g. airlines, aircraft, and airports, maximizing the payoff to each player (agent). It is intended that the results of our work will be directly used in this domain. In addition, we envision our work being leveraged for other optimization tasks such as data traffic on a network, first responder / disaster relief efforts, and other tasks where rapid solving of large-scale optimization problems is essential.", "authors": ["Thomas Castelli", "Joshua Lee", "Waseem Naqvi"], "n_citation": 50, "title": "An applied optimization framework for distributed air transportation environments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "092147f6-f0da-4bc3-8539-6d6303136f2d"}
{"abstract": "The Resource Description Framework (RDF) is the basic standard for representing information in the Semantic Web. It is mainly designed to be machine-readable and -processable. This paper takes the opposite side of view: RDF is investigated as a logic system designed for the needs of humans. RDF is developed as a logic system based on mathematical graphs, i.e., as diagrammatic reasoning system. As such, is has humanly-readable, diagrammatic representations. Moreover, a sound and complete calculus is provided. Its rules are suited to act on the diagrammatic representations. Finally, some normalforms for the graphs are introduced, and the calculus is modified to suit them.", "authors": ["Frithjof Dau"], "n_citation": 0, "title": "RDF as Graph-Based, Diagrammatic Logic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0d65fc02-a0f1-4483-9084-bb7ade8e29ac"}
{"abstract": "Many traditional cost- time trades off models are computationally expensive to use due to the complexity of algorithms especially for large scale problems. We present a new approach to adapt linear programming to solve cost time trade off problems. The proposed approach uses two different modeling flowshop scheduling into a leveled project management network. The first model minimizes makespan subject to budget limitation and the second model minimizes total cost to determine optimum makespan over production planning horizon.", "authors": ["Morteza Bagherpour", "Siamak Noori", "S. Jafar Sadjadi"], "n_citation": 0, "title": "Cost -Time Trade Off Models Application to Crashing Flow Shop Scheduling Problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "184e76e8-4636-48aa-8e89-b28dd2508eba"}
{"abstract": "Many variants of Chaum and van Antwerpen's undeniable signatures have been proposed to achieve specific properties desired in real-world applications of cryptography. Among them, directed signatures were introduced by Lim and Lee in 1993. Directed signatures differ from the well-known confirmer signatures in that the signer has the simultaneous abilities to confirm, deny and individually convert a signature. The universal conversion of these signatures has remained an open problem since their introduction in 1993. This paper provides a positive answer to this quest by showing a very efficient design for universally convertible directed signatures (UCDS) both in terms of computational complexity and signature size. Our construction relies on the so-called xyz-trick applicable to bilinear map groups. We define proper security notions for UCDS schemes and show that our construction is secure in the random oracle model, under computational assumptions close to the CDH and DDH assumptions. Finally, we introduce and realize traceable universally convertible directed signatures where a master tracing key allows to link signatures to their direction.", "authors": ["Fabien Laguillaumie", "Pascal Paillier", "Damien Vergnaud"], "n_citation": 0, "title": "Universally convertible directed signatures", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1a7369d3-2e05-4dc4-ac11-51be9e5debd9"}
{"abstract": "A range query applies an aggregation operation over all selected cells of an OLAP data cube where the selection is specified by ranges of continuous values for numeric dimensions. Much work has been done with one type of aggregations: SUM. But little work has been done with another type of aggregations: MAX/MIN besides the tree-based algorithm. In this paper, we propose a new method which partitions the given data cube, stores precomputed max/min over partitions and location of the max/min of the partitions. We also use some techniques to reduce the chance of accessing the original data cube when answering ad hoc queries at run time. The experiment results demonstrate that our method outperforms the tree-based algorithm.", "authors": ["Hua Gang Li", "Tok Wang Ling", "Sing Yeung Lee"], "n_citation": 0, "title": "Range-max/min query in OLAP data cube", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "1acb766c-54b5-4af8-8f35-69ab686f269b"}
{"abstract": "To reduce the effort developers have to make for crash debugging, researchers have proposed several solutions for automatic failure reproduction. Recent advances proposed the use of symbolic execution, mutation analysis, and directed model checking as underling techniques for post-failure analysis of crash stack traces. However, existing approaches still cannot reproduce many real-world crashes due to such limitations as environment dependencies, path explosion, and time complexity. To address these challenges, we present EvoCrash, a post-failure approach which uses a novel Guided Genetic Algorithm (GGA) to cope with the large search space characterizing real-world software programs. Our empirical study on three open-source systems shows that EvoCrash can replicate 41 (82%) of real-world crashes, 34 (89%) of which are useful reproductions for debugging purposes, outperforming the state-of-the-art in crash replication.", "authors": ["Mozhan Soltani", "Annibale Panichella", "Arie van Deursen"], "n_citation": 0, "references": ["05dd259f-bc12-4b6f-8290-49f9edc589a2", "06a0ca5c-cef2-4398-b5c2-a01b3d2a43ca", "06d8afe6-f71f-4017-8953-9583c3bcf35e", "07956a14-67b2-4e7f-98f1-7b7fb28cc74f", "096ce984-07c5-46fc-acd4-baa1698b4c60", "0f74dc6a-3d94-41a1-ac8d-b2c184b28bdf", "1a943cdb-eeba-4cf4-b756-f08f1ec83f97", "1ad2b1db-9f31-401c-b44e-c2d23d20b1bd", "1d1de684-6666-46ec-8f96-ae94c4caf8a9", "252b9e3e-4e30-426b-b4fc-2859b4e5503c", "2872c6e4-a55c-4248-aec6-ffb53f2e0884", "3aab0644-1570-44d2-8582-029f5f5e3a4f", "3b003b3e-7722-4e75-b48b-734723d2b56f", "4056eb66-b7f6-49c0-b945-3a8ad61f300e", "4b286ff1-0f78-4cef-915e-51a2601ffe3a", "588bb249-47ea-45e5-9c32-12059ce2585b", "58b3db58-6dcb-4265-8fd8-22127acd1884", "60210e7d-48c4-4ad6-ab86-96b834738854", "6456ad7f-5c52-4898-a843-0ab4832a1fc2", "75f207ac-46cd-4695-8db2-c2b13ec9ff92", "8f06238c-41b4-4422-a250-b4a7e716b77e", "a6eedfc4-c126-4e37-b038-18eeaa961716", "b3bb17a3-b83b-4a83-851e-ced329d69d8d", "bf6fee36-499e-470a-b353-24968127ff69", "c9cb9420-be8d-43b6-a920-94f43d420619", "d26dad9b-4fca-4fc7-a87d-2972a5b73023", "d80a8d9a-dacc-4656-b560-4923b8d8f908", "de0526f8-c7a6-4134-9b60-5d76d46e080e", "e674d1cd-ffbc-420e-8abc-30853bfb10ff", "e9ef683e-6fa9-435c-907c-66b47c75f8fa", "ea566275-b87e-48c4-9257-1abde04a2f57", "ede8fe17-8695-4ad8-8c1b-aaed40f712d6", "f0b82d3f-379b-4504-a2e5-449f5852d8b0", "f4acaf30-f596-4cce-beaa-14d1a8d4fbed", "fad6f4a0-ccf7-4dee-b1a8-d22f1995f817"], "title": "A guided genetic algorithm for automated crash reproduction", "venue": "international conference on software engineering", "year": 2017, "id": "30ad21c8-0abb-4533-af58-86c4d05b1b32"}
{"abstract": "A significant and ever-increasing amount of data is accessible only by filling out HTML forms to query an underlying Web data source. While this is most welcome from a user perspective (queries are relatively easy and precise) and from a data management perspective (static pages need not be maintained and databases can be accessed directly), automated agents must face the challenge of obtaining the data behind forms. In principle an agent can obtain all the data behind a form by multiple submissions of the form filled out in all possible ways, but efficiency concerns lead us to consider alternatives. We investigate these alternatives and show that we can estimate the amount of remaining data (if any) after a small number of submissions and that we can heuristically select a reasonably minimal number of submissions to maximize the coverage of the data. Experimental results show that these statistical predictions are appropriate and useful.", "authors": ["Stephen W. Liddle", "David W. Embley", "Del T. Scott", "Sai Ho Yau"], "n_citation": 0, "title": "Extracting data behind Web forms", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "34785629-d914-4f96-a1d3-42f475916757"}
{"abstract": "Consider a finite collection of subsets of a metric space and ask for a system of representatives which are pairwise at a distance at least q, where q is a parameter of the problem. In discrete spaces this generalizes the well known problem of distinct representatives, while in Euclidean metrics the problem reduces to finding a system of disjoint balls. This problem is closely related to practical applications like scheduling or map labeling. We characterize the computational complexity of this geometric problem for the cases of L 1  and L 2  metrics and dimensions d = 1, 2. We show that for d = 1 the problem can be solved in polynomial time, while for d = 2 we prove that it is NP-hard. Our NP-hardness proof can be adjusted also for higher dimensions.", "authors": ["Jir\u00ed Fiala", "Jan Kratochv\u00edl", "Andrzej Proskurowski"], "n_citation": 50, "title": "Geometric systems of disjoint representatives", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3dab0e64-1294-4e1d-84cd-d16ec4a2c2c5"}
{"abstract": "Preamble based channel estimation is widely used in OFDM based wireless multimedia system. However, the channel response varies significantly during one transmission packet due to high mobility. We propose new channel estimation method that tracks the channel variation along the time. We take advantage of the pilots occupied for the phase tracking to catch the variation of the channel responses with slightly modified pilot structure. For the accurate channel tracking, the variation trend of the channel is examined by correlation analysis. In here, we see the linear channel variation and so we use the linear interpolation for channel tracking. Without loss of data rate, the method reduces the error floor of bit error rate (BER) dramatically. Simulation results demonstrate the capability of the proposed channel tracking scheme even though the Doppler shift is very large. Moreover, the proposed method shows more competitive performance in convolutional coded systems used in many wireless multimedia application like IEEE 802.11a.", "authors": ["Kwanghoon Kim", "Haelyong Kim", "Hyuncheol Park"], "n_citation": 0, "title": "An efficient channel tracking method for OFDM based high mobility wireless multimedia system", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3e8c7125-832d-4964-865b-e03ee8313040"}
{"abstract": "Automatic liver segmentation from abdominal computed tomography (CT) images is one of the most important steps for computer-aided diagnosis (CAD) for liver CT. However, the liver must be separated manually or semi-automatically since surface features of the liver and partial-volume effects make automatic discrimination from other adjacent organs or tissues very difficult. In this paper, we present an unsupervised liver segmentation algorithm with three steps. In the preprocessing, we simplify the input CT image by estimating the liver position using a prior knowledge about the location of the liver and by performing multilevel threshold on the estimated liver position. The proposed scheme utilizes the multiscale morphological filter recursively with region-labeling and clustering to detect the search range for deformable contouring. Most of the liver contours are positioned within the search range. In order to perform an accurate segmentation, we produce the gradient-label map, which represents the gradient magnitude in the search range. The proposed algorithm performed deformable contouring on the gradient-label map by using regular patterns of the liver boundary. Experimental results are comparable to those of manual tracing by radiological doctors and shown to be efficient.", "authors": ["Seong-Jae Lim", "Yong-Yeon Jeong", "Yo-Sung Ho"], "n_citation": 0, "title": "Segmentation of the liver using the deformable contour method on CT images", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3fac0af6-16db-4c91-b65e-4c4a013b1474"}
{"abstract": "We propose a new public key trace and revoke scheme secure against adaptive chosen ciphertext attack. Our scheme is more efficient than the DF scheme suggested by Y. Dodis and N. Fazio[9]. Our scheme reduces the length of enabling block of the DF scheme by (about) half. Additionally, the computational overhead of the user is lower than that of the DF scheme; instead, the computational overhead of the server is increased. The total computational overhead of the user and the server is the same as that of the DF scheme, and therefore, our scheme is more practical, since the computing power of the user is weaker than that of the server in many applications. In addition, our scheme is secure against adaptive chosen ciphertext attack under only the decision Diffie-Hellman (DDH) assumption and the collision-resistant hash function H assumption, whereas the DF scheme also needs the one-time MAC (message authentication code) assumption.", "authors": ["Chong Hee Kim", "Yong Ho Hwang", "Pil Joong Lee"], "n_citation": 0, "title": "An efficient public key trace and revoke scheme secure against adaptive chosen ciphertext attack", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "4257f550-8959-45ba-85b3-084d030a55a5"}
{"abstract": "Often, real world business processes are constantly changing and dynamic in nature. These runtime changes may stem from various requirements, such as changes to the goals of the business process, changes to the business rules of the organization, or exceptions arising during the workflow execution. Unfortunately, traditional workflow management systems do not provide sufficient flexibility to accommodate such dynamic and adaptive workflows that support run-time changes of in-progress workflow instances. Moreover, traditional workflow management is accomplished by a single centralized workflow management engine, which may not only be a performance bottleneck, but also unsuitable for the emerging internet-based commerce and service environments where workflows may span many organizations that are autonomous. In this paper, we propose a formal model for a decentralized workflow change management (DWFCM) that uses a rules topic ontology and a service ontology to support the needed run-time flexibilty. We present a system architecture and the workflow adaptation process that generates a new workflow that is migration consistent with the original workflow.", "authors": ["Vijayalakshmi Atluri", "Soon Ae Chun"], "n_citation": 0, "title": "Handling dynamic changes in decentralized workflow execution environments", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "46fd50f1-bf80-48f9-bee0-6dc6c3c2e86b"}
{"abstract": "A web service is a modular application that is published, advertised, discovered, and invoked across a network, i.e., an intranet or the Internet. It is based on a software-as-services model and may participate as a component of other web services and applications. Binary and XML are two popular encoding/decoding mechanisms for network messages. Binary encoding is used when performance is critical and XML encoding is employed when interoperability with other web services and applications is essential. With each, one may employ compression to reduce message size prior to its transmission across the network. These decisions have a significant impact on response time and throughput. This paper reports on our experiences with a decision support benchmark, TPGH, using these alternatives on different hardware platforms. We focus on queries and make the following observations. First, compression reduces the message size and enhances the throughput of a shared network. With XML, we present numbers from XMill, a compression technique that employs XML semantics. For queries that produce more than one megabyte of XML data, XMill compressed XML messages are almost always smaller than the Zip compressed Binary messages. While this improves the throughput of a networked environment with a fixed bandwidth, the response time of XMill compressed messages are at times twice slower than Zip compressed Binary messages. The processor speed has a significant impact on the observed response times.", "authors": ["Min Cai", "Shahram Ghandeharizadeh", "Rolfe R. Schmidt", "Saihong Song"], "n_citation": 0, "title": "A comparison of alternative encoding mechanisms for web services", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5af5cf67-8a52-4a3b-928c-9c419221fd25"}
{"abstract": "We demonstrate an approach to transform keyword queries automatically into queries that combine keywords appropriately by boolean operations, such as and and or. Our approach is based on an analysis of relationships between the keywords using a taxonomy. The transformed queries will be sent to a search engine, and the returned results will be presented to the user. We evaluate the effectiveness of our approach by comparing the precision of the results returned for the generated query with the precision of the result for the original query. Our experiments indicate that our approach can improve the precision of the results considerably.", "authors": ["Martin Erwig", "Jianglin He"], "n_citation": 0, "title": "KeyQuery: A front end for the automatic translation of keywords into structured queries", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "60d3c2f9-94c1-4bdb-8aa3-b3e7090370d7"}
{"abstract": "Automated rendering technique in oriental painting style is demanding in the entertainment industry such as game, animation since this makes their contents more unique. To simulate the oriental painting more vivid, intrinsic color expression is critical. Generally actual artists put 3 colors on the different part of the brush at the same time and they express the volume and diffusion effect with this brush using peculiar color pigments. However most existing works have only focused on black ink painting and they rarely discuss about how to simulate the effect of actual pigments and their layered application. This paper presents a novel algorithm which can express the volume and diffusion of 3D objects using oriental color-ink model constructed from the real artists' standpoint. This model consists of 3 layers according to different color tones. They are combined using Kubelka-Munk (KM) composition model where optical parameters are extracted from the real painting media. We implemented our model on a GPU and the results show real-time rendering performance in arbitrarily given 3D scenes.", "authors": ["Crystal S. Oh", "Yang-Hee Nam"], "n_citation": 0, "title": "Oriental color-ink model based painterly rendering for realtime application", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "64464302-0652-4adb-98f0-f4a88fd516d7"}
{"abstract": "We compare two approaches to event definition, deriving from the Active Database and Knowledge Representation communities. We relate these approaches by taking a system of the former kind, displaying some of its shortcomings, and rectifying them by remodelling the system in the latter style. We further show the extent to which the original system can be recreated within the remodelled system. This bridge between the two approaches should provide a starting point for fruitful interaction between the two communities.", "authors": ["Antony Galton", "Juan Carlos Augusto"], "n_citation": 0, "title": "Two approaches to event definition", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6959740a-0a4c-4a47-a8a4-50a215d94d23"}
{"abstract": "This work presents a method founded on instance-based learning algorithms for inductive (memory-based) reasoning on ABoxes. The method, which exploits a semantic dissimilarity measure between concepts and instances, can be employed both to infer class membership of instances and to predict hidden assertions that are not logically entailed from the knowledge base and need to be successively validated by humans (e.g. a knowledge engineer or a domain expert). In the experimentation, we show that the method can effectively help populating an ontology with likely assertions that could not be logically derived.", "authors": ["Claudia d'Amato", "Nicola Fanizzi"], "n_citation": 0, "title": "Lazy Learning from Terminological Knowledge Bases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6b1baa9f-bcfb-445c-900c-5793b791875c"}
{"abstract": "Currently, the best and only evidence of the security of the OAEP encryption scheme is a proof in the contentious random oracle model. Here we give further arguments in support of the security of OAEP. We first show that partial instantiations, where one of the two random oracles used in OAEP is instantiated by a function family, can be provably secure (still in the random oracle model). For various security statements about OAEP we specify sufficient conditions for the instantiating function families that, in some cases, are realizable through standard cryptographic primitives and, in other cases, may currently not be known to be achievable but appear moderate and plausible. Furthermore, we give the first non-trivial security result about fully instantiated OAEP in the standard model, where both oracles are instantiated simultaneously. Namely, we show that instantiating both random oracles in OAEP by modest functions implies non-malleability under chosen plaintext attacks for random messages. We also discuss the implications, especially of the full instantiation result, to the usage of OAEP for secure hybird encryption (as required in SSL/TLS, for example).", "authors": ["Alexandra Boldyreva", "Marc Fischlin"], "n_citation": 0, "title": "On the Security of OAEP", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6b9f4882-911d-45fa-bac5-ef134c06b06c"}
{"abstract": "Public-key cryptography is fast becoming the foundation for those applications that require security and authentication in open networks. But the widespread use of a global public-key cryptosystem requires that public-key certificates are always available and up-to-date. Problems associated to digital certificates management, like storage, retrieval, maintenance, and, specially, revocation, require special procedures that ensure reliable features because of the critical significance of inaccuracies. Most of the existing systems use a Certificate Revocation List, a repository of certificates that have been revoked before their expiration date. The need to access CRLs in order to check certificate revocations becomes a performance handicap. Furthermore, they introduce a source of vulnerability in the whole security infrastructure, as it is impossible to produce a new CRL each time a revocation takes place. This paper introduces an alternative for the storage of digital certificates that avoids the use of CRLs. The system is designed to provide a distributed management of digital certificates by using Certification Authorities that, while being part of a whole Public-Key Infrastructure, operate over local certificates databases. Communication protocols between local databases have been designed to minimize network traffic without a lack of security and efficiency.", "authors": ["Javier Lopez", "Antonio Ma\u00f1a", "Juan J. Ortega", "Jos\u00e9 M. Troya"], "n_citation": 0, "title": "Distributed storage and revocation in digital Certificate databases", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "75d817b3-fb93-4063-8e6b-859716cf7cc3"}
{"abstract": "AbstracL In this article, we propose an object-oriented approach to rewrite queries using views. Our approach is based on the object-oriented classification reasoning mechanism which is possible thanks to the representation of queries as classes. In our approach, we used results of classification of query-classes, to generate possible rewritings for the corresponding queries. We have proposed two types of rewritings: elementary rewritings and recursive rewritings. Then, query rewriting is possible without limitation of query types. In fact, authorized queries are OQL expressions and query rewriting process can generate query expressions using all existing views.", "authors": ["Abdelhak Seriai"], "n_citation": 0, "title": "Object oriented mechanisms to rewriting queries using views", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "78cc9de7-b602-4c0e-8c0a-2a75649ebb0b"}
{"authors": ["Gilles Diguglielmo", "Eric Durocher", "Philippe Kaplan", "Georg Sander", "Adrian Vasiliu"], "n_citation": 50, "title": "Graph layout for workflow applications with ILOG JViews", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7f60cce4-599e-4cea-b662-774a91636694"}
{"abstract": "This work addresses the automatic generation of conceptual models for XML-oriented databases, which in many cases have little or no support for schemata. Our techniques are based on both an incremental clustering algorithm, which groups together the incoming XML documents according to their structural similarities, and a schema inference method, which maintains dynamically the schema of each detected document cluster. Our proposal takes into consideration the schema evolution. For this purpose, we have adapted the TOODOR document model that describes the temporal properties of the XML document types.", "authors": ["Ismael Sanz", "Juan Manuel P\u00e9rez", "Rafael Berlanga", "Mar\u00eda Jos\u00e9 Aramburu"], "n_citation": 0, "title": "XML schemata inference and evolution", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "89c2b0a6-a978-41c6-9b59-b03fd2093033"}
{"abstract": "More recently, embedded databases and smartcard databases have emerged. To make such small databases practically useful, only a scale-down approach to a general-purpose relational database will be insufficient; some specializations for each application domain are also required. In this paper, we design a small database based on the experience of our electronic ticket project. The design includes a new data model that well represents a variety of small volume tickets, a role-based access control applied to each ticket, and a logging and rollback operations externalized in a way interacting with a preprocessor. We present that database functionality on a smartcard are helpful to build multi-party ticket applications.", "authors": ["Kimio Kuramitsu", "Ken Sakamura"], "n_citation": 50, "title": "Electronic tickets on contactless smartcard database", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8a23046b-3182-4682-a518-d9fa07e5974d"}
{"abstract": "Association rules are typically evaluated in terms of support and confidence measures, which ensure that discovered rules have enough positive evidence. However, in real-world applications, even considering only those rules with high confidence and support it is not true that all of them are interesting. It may happen that the presentation of all discovered rules can discourage users from interpreting them in order to find nuggets of knowledge. Association rules interpretation can benefit from discovering group of similar rules, where (dis)similarity is estimated on the basis of syntactic or semantic characteristics. In this paper, we resort to the multi-dimensional scaling to support a visual exploration of association rules by means of bi-dimensional scatter-plots. An application in the domain of biomedical literature is reported. Results show that the use of this visualization technique is beneficial.", "authors": ["Margherita Berardi", "Annalisa Appice", "Corrado Loglisci", "Pietro Leo"], "n_citation": 0, "title": "Supporting Visual Exploration of Discovered Association Rules Through Multi-Dimensional Scaling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "91fd4eb6-07f4-459d-85ed-b9c33a003f3f"}
{"abstract": "In the paper we propose a new approach to the concept of dynamic object roles. The approach assumes, among others, that: a role is a distinguished subobject for an object; a role dynamically inherits attributes' values and methods of its parent object; objects can be accessed by their names as well as by the names of their roles. The paper focuses on implications of this concept for an object data model and for an object data store. We also explain how it fits with other modern notions from the conceptual modeling field. Finally, we discuss some issues concerning a query language for the object data model with roles.", "authors": ["Andrzej Jod\u0142owski", "Piotr Habela", "Jacek P\u0142odzien", "Kazimierz Subieta"], "n_citation": 0, "title": "Objects and roles in the stack-based approach", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9638df33-68db-4bb7-a219-8115df72ce0d"}
{"abstract": "This paper examines the problem of transaction management in pervasive computing environments and presents a new approach to address them. We represent each entity as a mobile or static semi-autonomous device. The purpose of each device is to satisfy user queries based on its local data repository and interactions with other devices currently in its vicinity. Pervasive environments, unlike traditional mobile computing paradigm, do not differentiate between clients and servers that are located in a fixed, wired infrastructure. Consequently, we model all devices as peers. These environments also relax other assumptions made by mobile computing paradigm, such as the possibility of reconnection with a given device, support from wired infrastructure, or the presence of a global schema. These fundamental characteristics of pervasive computing environments limit the use of techniques developed for transactions in a mobile computing environments. We define an alternative optimistic transaction model whose main emphasis is to provide a high rate of successful transaction terminations and to maintain a neighborhood-based consistency. The model accomplishes this via the help of active witnesses and by employing an epidemic voting protocol. The advantage of our model is that it enables two or more peers to engage in a reliable and consistent transaction while in a pervasive environment without assuming that they can talk to each other via infrastructure such as base stations. The advantage of using active witnesses and an epidemic voting protocol is that transaction termination does not depend on any single point of a failure. Additionally, the use of an epidemic voting protocol does not require all involved entities to be simultaneously connected at any time and, therefore, further overcomes the dynamic nature of the environments. We present the implementation of the model and results from simulations.", "authors": ["Filip Perich", "Anupam Joshi", "Yelena Yesha", "Tim Finin"], "n_citation": 0, "title": "Neighborhood-consistent transaction management for pervasive computing environments", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "99ad7440-2ff6-47ce-b1ac-677ee6f2cdd8"}
{"abstract": "In this paper, a new panoramic image generation algorithm is proposed based on more realistic image formation processes. Perspective projection, lens distortion, vignetting and illumination effects are incorporated into the proposed panoramic modeling. Intrinsic and extrinsic camera parameters are estimated by the proposed stable camera parameter estimation algorithm derived from panning camera constraints. This paper shows that accurate panoramic images can be reconstructed based on the proposed camera modeling and parameters estimation. The effectiveness of the proposed algorithm is also shown with several image sequences in terms of reconstruction error from the generated panoramic image.", "authors": ["Dong-Gyu Sim"], "n_citation": 0, "title": "New panoramic image generation based on modeling of vignetting and illumination effects", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a1fa19a6-aff2-41a4-8006-5848bf198f30"}
{"abstract": "Traditional database systems assume that clients always consume the results of queries from the beginning. In various new applications especially in WWW, however, clients frequently need a small part of the result from the middle, e.g. retrieving a page in a bulletin board in WWW. To process this partial retrieval, traditional database systems should find all the records and discard unnecessary ones. Although several algorithms for top-k queries have been proposed, there has been no research effort for partial retrieving from the middle of an ordered result. In this paper, we define a mid-(k,n) query, which retrieves n records from the k th  record of an ordered result. We also propose an efficient algorithm for mid-(k,n) queries using a slightly modified B + -Tree, named the B +c -Tree. We provide the theoretical analysis and the experimental results that the proposed technique evaluates mid-(k,n) queries efficiently.", "authors": ["Dongseop Kwon", "Taewon Lee", "Sukho Lee"], "n_citation": 0, "title": "Evaluating mid-(k, n) queries using B+-tree", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a2d494b6-5b1f-4e20-b3b3-5bf8a8eb41e9"}
{"abstract": "This paper deals with an optimization of Random Forests which aims at: adapting the concept of forest for learning imbalanced data as well as taking into account user's wishes as far as recall and precision rates are concerned. We propose to adapt Random Forest on two levels. First of all, during the forest creation thanks to the use of asymmetric entropy measure associated to specific leaf class assignation rules. Then, during the voting step, by using an alternative strategy to the classical majority voting strategy. The automation of this second step requires a specific methodology for results quality assessment. This methodology allows the user to define his wishes concerning (1) recall and precision rates for each class of the concept to learn, and, (2) the importance he wants to confer to each one of those classes. Finally, results of experimental evaluations are presented.", "authors": ["Julien Thomas", "Pierre-Emmanuel Jouve", "Nicolas Nicoloyannis"], "n_citation": 0, "title": "Optimisation and Evaluation of Random Forests for Imbalanced Datasets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa35263a-f42a-48a4-a573-75cff88d1e03"}
{"abstract": "We consider the problem of hiding sender and receiver of classical and quantum bits (qubits), even if all physical transmissions can be monitored. We present a quantum protocol for sending and receiving classical bits anonymously, which is completely traceless: it successfully prevents later reconstruction of the sender. We show that this is not possible classically. It appears that entangled quantum states are uniquely suited for traceless anonymous transmissions. We then extend this protocol to send and receive qubits anonymously. In the process we introduce a new primitive called anonymous entanglement, which may be useful in other contexts as well.", "authors": ["Matthias Christandl", "Stephanie Wehner"], "n_citation": 0, "title": "Quantum anonymous transmissions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "abbc96a2-63a6-4367-b10b-68f82ea6ca5d"}
{"abstract": "With constant advances in information technology, more and more information is available and users' information needs are becoming more diverse. Most conventional information systems only attempt to provide information that meets users' specific interests. In contrast, we are working on ways of discovering information from the viewpoints of both interest and necessity. For example, we are trying to discover complementary information that provides additional knowledge on the users' topics of interest, not just information that is similar to the topic. In previous work, which was based on extracting topic structures from closed-caption data, we proposed methods of searching for information to complement TV program content; that is, to provide users with more detailed information or different viewpoints. In this paper, we focus on the features of text streams (closed-caption data, etc.) and propose a method for context-sensitive retrieval of complementary information. We modified our topic-structure model for content representation and consider the context of a text stream in searching for complementary information. The context of the text stream is considered to be a series of topic structures. Based on such kind of context, we propose methods of searching for complementary information for TV programs, including query-type selection, query modification, and computation of the degree of complementarity. The experiment results showed that, comparing to our previous methods, the context-sensitive method could provide more additional information and avoid information overlap.", "authors": ["Qiang Ma", "Katsumi Tanaka"], "n_citation": 0, "title": "Context-sensitive complementary information retrieval for text stream", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b3b6e0f2-0bd2-40ef-bfd2-6ccf5c5e03be"}
{"abstract": "We study private computations in information-theoretical settings on networks that are not 2-connected. Non-2-connected networks are non-private in the sense that most functions cannot privately be computed on them. We relax the notion of privacy by introducing lossy private protocols, which generalize private protocols. We measure the information each player gains during the computation. Good protocols should minimize the amount of information they lose to the players. Throughout this work, privacy always means 1-privacy, i.e. players are not allowed to share their knowledge. Furthermore, the players are honest but curious, thus they never deviate from the given protocol. By use of randomness by the protocol the communication strings a certain player can observe on a particular input determine a probability distribution. We define the loss of a protocol to a player as the logarithm of the number of different probability distributions the player can observe. For optimal protocols, this is justified by the following result: For a particular content of any player's random tape, the distributions the player observes have pairwise fidelity zero. Thus the player can easily distinguish the distributions. The simplest non-2-connected networks consists of two blocks that share one bridge node. We prove that on such networks, communication complexity and the loss of a private protocol are closely related: Up to constant factors, they are the same. Then we study 1-phase protocols, an analogue of 1-round communication protocols. In such a protocol each bridge node may communicate with each block only once. We investigate in which order a bridge node should communicate with the blocks to minimize the loss of information. In particular, for symmetric functions it is optimal to sort the components by increasing size. Then we design a 1-phase protocol that for symmetric functions simultaneously minimizes the loss at all nodes where the minimum is taken over all 1-phase protocols. Finally, we prove a phase hierarchy. For any k there is a function such that every (k - 1)-phase protocol for this function has an information loss that is exponentially greater than that of the best k-phase protocol.", "authors": ["Markus Bl\u00e4ser", "Andreas Jakoby", "Maciej Liskiewicz", "Bodo Manthey"], "n_citation": 0, "title": "Privacy in non-private environments", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "be1b3e69-d948-401a-89fe-f9302931de96"}
{"abstract": "With the growth of the Internet, communication and network security have been the focus of much attention. In addition, deployment of resource intensive security protocols in battery-powered mobile devices has raised power consumption to a significant design basis of network design. In this paper, we propose a power-efficient secure communication restart mechanism for a wireless network and analyze the power consumed while restarting a secure communication. An experimental test bed was developed to inspect the proposed mechanism and to evaluate it in terms of power consumption relative to that of conventional secure communication restart mechanisms. Using our enhanced mechanism, we were able to reduce the power consumed during a secure communication restart by up to 60% compared with conventional restart mechanisms.", "authors": ["Ki-Hong Kim", "Jinkeun Hong", "Jongin Lim"], "n_citation": 0, "title": "Analysis of the power consumption of secure communication in wireless networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c2a05482-e77d-46bf-bec4-107a740ab6e5"}
{"abstract": "This article describes a method to insert virtual objects into a real video stream based on feature tracking and camera pose estimation from a set of single-camera video frames. To insert or modify 3D shapes to target video frames, the transformation from the 3D objects to the projection of the objects onto the video frames should be revealed. It is shown that, without a camera calibration process, the 3D reconstruction is possible using multiple images from a single camera under the fixed internal camera parameters. The proposed approach is based on the simplification of the camera matrix of intrinsic parameters and the use of projective geometry. The method is particularly useful for augmented reality applications to insert or modify models to a real video stream. Several experimental results are presented on real-world video streams, demonstrating the usefulness of our method for the augmented reality applications.", "authors": ["Jong-Seung Park", "Mee Young Sung", "Sung-Ryul Noh"], "n_citation": 0, "title": "Virtual object placement in video for augmented reality", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cc977992-dc5c-4315-9dd9-b1d241662fe3"}
{"abstract": "XML is arising as a new format to represent the enterprises' business processes, since it is a flexible and interchangeable language. In the workflow area, one of the most discussed questions is the evolution of the workflow representations, in order to meet new requirements. Despite the number of existing proposals, none of them deals with evolution of workflows that use the XML syntax. In this paper, we present a proposal for workflow evolution in which the workflow schema is represented in XML. The proposal is based on versioning concepts, that allow the storage and use of different versions of the same workflow schema. An evolution architecture is proposed, in order to separate the involved concepts in a set of managers. Two of these managers are discussed in this paper - Version Manager and Modification Manager.", "authors": ["F\u00e1bio Zschornack", "Nina Edelweiss"], "n_citation": 0, "title": "On evolution of XML workflow schemata", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ccfd85a3-07fc-4cd6-8c05-64a57ae0495e"}
{"abstract": "Temporal constraints are part of the specification of many complex application frameworks including activities to be scheduled, real-time components, temporal data management, e-commerce applications, and workflow management. This paper investigates the problem of creating an abstract view over a set of temporal constraints that may have been specified in terms of different granularities. The level of ion is determined by a specific time granularity chosen by the user among the ones in the system or created on purpose. An expressive formal model for time granularities is assumed including common granularities like hours and days as well as non-standard granularities like business days and academic semesters. The view derivation is based on a conversion technique exploiting the periodicity of time granularities.", "authors": ["Claudio Bettini", "Simone Ruffini"], "n_citation": 0, "title": "Deriving abstract views of multi-granularity temporal constraint networks", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "cffe61ae-765b-491e-bce4-8e41fe3c226c"}
{"abstract": "Orderly spanning trees seem to have the potential of becoming a new and promising technique capable of unifying known results as well as deriving new results in graph drawing. Our exploration in this paper provides new evidence to demonstrate such a potential. Two applications of the orderly spanning trees of plane graphs are investigated. Our first application deals with Podevs drawing, i.e., planar orthogonal drawing with equal vertex size, introduced by Fossmeier and Kaufmann. Based upon orderly spanning trees, we give an algorithm that produces a Podevs drawing with half-perimeter no more than [3n/2] + 1 and at most one bend per edge for any n-node plane graph with maximal degree A, a notable improvement over the existing results in the literature in terms of the size of the drawing area. The second application is an alternative proof for the sufficient and necessary condition for a graph to admit a rectangular dual, i.e., a floor-plan using only rectangles.", "authors": ["Ho-Lin Chen", "Chien-Chih Liao", "Hsueh-I Lu", "Hsu-Chun Yen"], "n_citation": 50, "title": "Some applications of orderly spanning trees in graph drawing", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d2bdc040-b9bc-4b55-8fd0-79d8db8b4d0b"}
{"abstract": "In this paper 1  we propose a new symmetric block cipher with the following paradoxical traceability properties: it is computationally easy to derive many equivalent secret keys providing distinct descriptions of the same instance of the block cipher. But it is computationally difficult, given one or even up to k equivalent keys, to recover the so called meta-key from which they were derived, or to find any additional equivalent key, or more generally to forge any new untraceable description of the same instance of the block cipher. Therefore, if each legitimate user of a digital content distribution system based on encrypted information broadcast (e.g. scrambled pay TV, distribution over the Internet of multimedia content, etc.) is provided with one of the equivalent keys, he can use this personal key to decrypt the content. But it is conjectured infeasible for coalitions of up to k traitors to mix their legitimate personal keys into untraceable keys they might redistribute anonymously to pirate decoders. Thus, the proposed block cipher inherently provides an efficient traitor tracing scheme [4]. The new algorithm can be described as an iterative block cipher belonging to the class of multivariate schemes. It has advantages in terms of performance over existing traitor tracing schemes and furthermore, it allows to restrict overheads to one single block (i.e. typically 80 to 160 bits) per encrypted content payload. Its strength relies upon the difficulty of the Isomorphism of Polynomials problem [17], which has been extensively investigated over the past years. An initial security analysis is supplied.", "authors": ["Olivier Billet", "Henri Gilbert"], "n_citation": 0, "title": "A traceable block cipher", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d3e3d3d7-9359-41a3-ab2f-b0ff76e9ff25"}
{"abstract": "With the exploding volume of traffic and expanding Quality of Service (QoS) requirements from emerging multimedia applications, many research efforts have been carried out to establish multi-class network service model in next-generation Internet. To successfully support multiple classes of service, network resources must be managed effectively to ensure end-to-end QoS while simultaneously sustaining stable network QoS. First, we present a scalable and adaptive QoS mapping control (SAQM) framework over the differentiated services network focusing reactive edge-to-edge QoS control in class-based. Secondly, under SAQM framework, end-to-end QoS control for per-flow service guarantee is proposed through incorporating relative priority index (RPI)-based video streaming and a special access node called media gateway (MG) at network edge. The SAQM framework is composed of the functionalities of proactive and reactive QoS mapping controls to provide reliable and consistent service guarantee. In our framework, edge-to-edge active monitoring is utilized to obtain measures reflecting each class performance and then measurement-based reactive mapping control for relative network QoS provisioning is performed at MG located in the ingress edges. Simulation results demonstrate the feasibility of an edge-based QoS control and show how to enhance the QoS performance of video streaming in proposed SAQM framework.", "authors": ["Gooyoun Hwang", "Jitae Shin", "Jong-Won Kim"], "n_citation": 0, "title": "Scalable and adaptive QoS mapping control framework for packet video delivery", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d51e70b5-ccf4-42a3-8280-540380d11680"}
{"abstract": "The aim of steganalysis is to uncover the concealed secret message in the multimedia carrier. Now, most steganalysis approaches focus on two issues: one is detecting whether there existed a secret message, and the other is estimating the length of the secret message. In this paper, we present a new secret message location steganalysis based on local coherences of hue (LCH) to determine the stego-bearing regions for color digital images. For the stego-image, which is sequentially embedded with messages in spatial domain, stego-bearing regions can be determined by analyzing the changes of coherence of hue. Experimental results show that the proposed LCH steganalysis has high detection accuracy.", "authors": ["Xiangwei Kong", "Wen-Feng Liu", "Xingang You"], "n_citation": 50, "title": "Secret message location steganalysis based on local coherences of hue", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d9a38fe2-d826-45bf-aaa4-89ea723f60a9"}
{"abstract": "In this paper, we presented a routing strategy for requests in unstructured peer-to-peer networks. The strategy is based on the adaptive routing Q-routing. The strategy uses reinforcement learning to estimate the cost of routing a request. Such a strategy is scalable only if the routing indices are of reasonable size. We proposed and comparatively evaluated three methods for the pruning for the pruning of the routing indices. Our experiments confirm the validity of the adaptive routing and the scalability of a pruning approach based on a pruning strategy considering the popularity of the resources.", "authors": ["Chu Yee Liau", "Achmad Nizar Hidayanto", "St\u00e9phane Bressan"], "n_citation": 0, "title": "Adaptive peer-to-peer routing with proximity", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "da0416c5-2d24-4b1b-8850-2d6ff096d748"}
{"abstract": "Intelligent search agent is popularly used for searching relevant information in the Internet and there are lots of tools that are used to satisfy the needs of the users. Since there is no sufficient cooperation among the agents and they are independent to each other, it is difficult to make an efficient search of information in the distributed environment. Therefore, a typical search agent is difficult to use and can contain irrelevant information for the users. To solve these problems, we use the CORBA architecture to create an agency in the broker agent and provide more reliable information to the users. Also, the proposed intelligent information search system use the NFC and filtering techniques through the multi-agents for fast and reliable search of information.", "authors": ["Subong Yi", "Bobby D. Gerardo", "Young-Seok Lee", "Jaewan Lee"], "n_citation": 0, "title": "Intelligent Information Search Mechanism Using Filtering and NFC Based on Multi-agents in the Distributed Environment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dc9ddade-6a99-4022-9d64-89d96907ec81"}
{"abstract": "We give an O(\u03a6 k  . n 2 ) algorithm for the 1-SIDED CROSSING MINIMIZATION problem, thus showing that the problem is Fixed Parameter Tractable. The constant \u03a6 in the running time is the golden ratio \u03a6 = 1+\u221a5/2 \u2243 1.618. The constant k is the parameter of the problem: the number of allowed edge crossings.", "authors": ["Vida Dujmovi\u0107", "Sue Whitesides"], "n_citation": 0, "title": "An efficient Fixed Parameter Tractable algorithm for 1-sided crossing minimization", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "dde09e41-1caf-43e0-b637-7252a011843e"}
{"abstract": "Effective daily processing of large amounts of paper documents in office environments requires the application of semantic-based indexing techniques during the transformation of paper documents to electronic format. For this purpose a combination of both XML and knowledge technologies can be used. XML distinguishes between data, its structure and semantics, allowing the exchange of data elements that carry descriptions of their meaning, usage and relationship. Moreover, the combination with XSLT enables any browser to render the original layout structure of the paper documents accurately. However, an effective transformation of paper documents into XML format is a complex process involving several steps. In this paper we propose the application of knowledge technologies to many document processing steps, namely rule-based systems for semantic indexing of documents and the extraction of the necessary knowledge by means of machine learning techniques. This approach has been implemented in the system Wisdom++, which is currently used in the European project COLLATE (Collaboratory for Annotation, Indexing and Retrieval of Digitized Historical Archive Material) to provide film archivists with a tool for the automated annotation of historical documents in film archives.", "authors": ["Donato Malerba", "Michelangelo Ceci", "Margherita Berardi"], "n_citation": 0, "title": "XML and knowledge technologies for semantic-based indexing of paper documents", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e14b0216-aa99-4df3-8e77-4860467f6976"}
{"abstract": "In this paper, we present a systematic approach towards decision making for variability in product families in the context of uncertainty. Our approach consists of the following ingredients: a suitable set of architectural views that bridge the gap between customer needs and available technology, a multi-view variation modeling technique, the selection of several scenarios of different kinds, and a quantitative analysis of quality aspects for these scenarios.", "authors": ["Pierre America", "Dieter K. Hammer", "Mugurel T. Ionita", "Henk Obbink", "Eelco Rommes"], "n_citation": 0, "title": "Scenario-based decision making for architectural variability in product families", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e3352ae2-d8dd-47d6-b0a2-dc17f59637ff"}
{"abstract": "A vast amount of information is available on the WWW. There are a lot of Web pages whose content is 'local' and interesting for people in a very narrow regional area. Usually, users search for information with search engines, even though finding or excluding local information may still be difficult. In this paper, we propose a new information retrieval method that is based on the localness degree for discovering or excluding local information from the WWW. Localness degree is a new notion for estimating the local dependence and ubiquitous nature of Web pages. The localness degree is computed by 1) a content analysis of the Web page itself, to determine the frequency of occurrence of geographical words, and the geographical area (i.e., latitude and longitude) covered by the location information given on the page, and 2) a comparison of the Web page with other pages with respect to daily (ubiquitous) content. We also show some results of our preliminary experiments of the retrieval method based on the localness degree.", "authors": ["Chiyako Matsumoto", "Qiang Ma", "Katsumi Tanaka"], "n_citation": 0, "title": "Web information retrieval based on the localness degree", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e3704a33-0083-44b2-b57e-bde9765478eb"}
{"abstract": "Workflow Management Systems (WFMS) coordinate execution of logically related multiple tasks in an organization. A workflow schema is defined using a set of tasks that are coordinated using dependencies. Workflows instantiated from the same schema may differ with respect to the tasks executed. An important issue that must be addressed while designing a workflow is to decide what tasks are needed for the workflow to complete - we refer to this set as the completion set. Since different tasks are executed in different workflow instances, a workflow schema may be associated with multiple completion sets. Incorrect specification of completion sets may prohibit some workflow from completing. Manually generating these sets for large workflow schemas can be an error-prone and tedious process. Our goal is to automate this process. We investigate the factors that affect the completion of a workflow. Specifically, we study the impact of control-flow dependencies on completion sets and show how this knowledge can be used for automatically generating these sets. Finally, we provide an algorithm that can be used by application developers to generate the completion sets associated with a workflow schema.", "authors": ["Tai Xin", "Indrakshi Ray", "Parvathi Chundi", "Sopak Chaichana"], "n_citation": 50, "title": "On the completion of workflows", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f9144486-8a2a-4edc-b85e-f7641b7a00dd"}
{"abstract": "The past few years have seen significant work in mobile data management, typically based on the client/proxy/server model. Mobile/wireless devices are treated as clients that are data consumers only, while data sources are on servers that typically reside on the wired network. With the advent of pervasive computing environments an alternative scenario arises where mobile devices gather and exchange data from not just wired sources, but also from their ethereal environment and one another. This is accomplished using ad-hoc connectivity engendered by Bluetooth like systems. In this new scenario, mobile devices become both data consumers and producers. We describe the new data management challenges which this scenario introduces. We describe the design and present an implementation prototype of our framework, MoGATU, which addresses these challenges. An important component of our approach is to treat each device as an autonomous entity with its goals and beliefs, expressed using a semantically rich language. We have implemented this framework over a combined Bluetooth and Ad-Hoc 802.11 network with clients running on a variety of mobile devices. We present experimental results validating our approach and measure system performance.", "authors": ["Filip Perich", "Sasikanth Avancha", "Dipanjan Chakraborty", "Anupam Joshi", "Yelena Yesha"], "n_citation": 0, "title": "Profile driven data management for pervasive environments", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fb30a65b-5a2a-4cfe-8ea6-09ff54ac65ea"}
{"abstract": "This article introduces our research efforts to build the Essex Rovers'01 robot soccer team participated in the RoboCup-2001 competition. A modular design for implementing a behavior-based hierarchy is introduced, which consists of three modules such as Perception module, Cognition module and Action module. This architecture is used for the team to achieve intelligent actions in real time. The implementation aspect of these three modules are briefly described.", "authors": ["Huosheng Hu", "Dongbing Gu", "Dragos Golubovic", "Bo Li", "Zhengyu Liu"], "n_citation": 0, "title": "Essex Rovers 2001 team description", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fd2d9fba-1b73-444f-b1ad-da2f2a98c74c"}
{"abstract": "Along the way, acquire technical expertise and a master's degree, even while changing positions and companies.", "authors": ["Daniel J. Mazzola", "Robert D. St. Louis", "Mohan Tanniru"], "n_citation": 0, "references": ["65b7f124-f292-407a-818e-69b37f5a0a92", "90883a1c-b1c1-497e-a141-553001ad4382", "db7a87de-56bc-45f6-89cc-94bb6a7b30c4", "f7d719c3-2b4d-466a-91ee-f42883c9e422"], "title": "The path to the top: insights from career histories of top CIOs", "venue": "Communications of The ACM", "year": 2017, "id": "07f9b4c4-9f21-4ba4-8b4f-ef21f22a636c"}
{"abstract": "The recording and analysis of process descriptions from running surgical interventions is a very new and promising field named Surgical Workflows. Surgical Workflows fulfill two major objectives: they form the base of scientific evaluation and rapid prototyping of surgical assist systems, and they pave the road for the entering of workflow management systems into the operating room for intraoperative support of the surgeon. In this paper we describe how process descriptions from surgical interventions can be obtained for Surgical Process Modelling (SPM) as a specific domain of Business Process Modelling (BPM). After the introduction into the field of Surgical Workflows and the motivation of the research efforts, we deal with theoretical considerations about surgical interventions and the identification of classifications. Based on that, we propose the extendable structure for computational data acquisition support and conclude with use cases. The presented approach was applied to more than 200 surgical interventions of 10 different intervention types from otorhinolaryngology, neurosurgery, heart surgery, eye surgery, and interventional radiology, and it represents an ongoing project.", "authors": ["Thomas Neumuth", "Gero Straub", "J\u00fcrgen Meixensberger", "Heinz U. Lemke", "Oliver Burgert"], "n_citation": 0, "title": "Acquisition of process descriptions from surgical interventions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "13a7666b-2a6b-454d-bedb-b2fa776e22cd"}
{"abstract": "In Eurocrypt 2003, Boneh et al. presented a novel cryptographic primitive called aggregate signatures. An aggregate signature scheme is a digital signature that supports aggregation: i.e. given k signatures on k distinct messages from k different users it is possible to aggregate all these signatures into a single short signature. Applying the above concept to verifiably encrypted signatures, Boneh et al. introduced a new complexity assumption called the k-Element Aggregate Extraction Problem. In this paper we show that the k-Element Aggregate Extraction Problem is nothing but a Computational Diffie-Hellman Problem in disguise.", "authors": ["Jean-S\u00e9bastien Coron", "David Naccache"], "n_citation": 0, "title": "Boneh et al.'s k-Element Aggregate Extraction assumption is equivalent to the Diffie-Hellman assumption", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "159c528d-a7dd-47b6-ba3f-a5d2c1e2366e"}
{"abstract": "Broadcasting architectures in general and broadcast disks in particular outperform traditional client/server architectures when many clients read data from few servers. Yet several issues arise when the broadcasting model allows updates by the clients. These issues, most of which are related to the control of the concurrent access (isolation and consistency), are rooted in the asynchronous nature of the broadcast model and in the replication of data on the broadcast channel and in the caches of the clients. In this paper we study the design and evaluate the performance of a simple update model controlled by a basic locking mechanism for broadcast disks architecture that involves replication on the broadcast channel and client caches.", "authors": ["St\u00e9phane Bressan", "Guo Yuzhi"], "n_citation": 0, "title": "Performance evaluation of a simple update model and a basic locking mechanism for broadcast disks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1694f96e-fa6a-464d-ab49-f3d56768e327"}
{"authors": ["Thomas M. Philip"], "n_citation": 0, "references": [], "title": "Learning with mobile technologies", "venue": "Communications of The ACM", "year": 2017, "id": "1e0076bc-4fdc-44cf-81f0-f3d1de36efe2"}
{"abstract": "This paper presents ANDROMEDA (Astronomical Data Resources Mediation), an XML-based data mediation system that enables transparent access to astronomical data sources. Transparent access is achieved by a global view that expresses the requirements of a community of users (i.e., astronomers) and data integration mechanisms adapted to astronomical data characteristics. Instead of providing an ad hoc mediator, ANDROMEDA can be configured for giving access to different data sources according to user requirements (data types, content, data quality, and provenance). ANDROMEDA can be also adapted when new sources are added or new requirements are specified. Furthermore, in ANDROMEDA the data integration process is done in a distributed manner, taking advantage of the available computing resources and reducing data communication costs.", "authors": ["Victor Cuevas-Vicenttfn", "Jos\u00e9 Luis Zechinelli-Martini", "Genoveva Vargas-Solar"], "n_citation": 0, "title": "Andromeda : Building e-science data integration tools", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "20a6714a-07e1-4344-aec0-a9e783cd01fc"}
{"abstract": "A data integration system typically creates a target XML schema to represent an application domain and source schemas are mapped to the target schema. A user poses a query over the target schema, and the system rewrites the query into a set of queries over the data sources. Existing algorithms generate a set of static rules based on the target schema and mappings, and rewrite the target query using these rules. We design a flexible and dynamic approach that rewrites XML queries directly based on the mappings between the target and source schemas. Theoretical analysis and experiments on both synthetic and real-world datasets indicate that the proposed approach is efficient and scalable.", "authors": ["Ling Li", "Mong Li Lee", "Wynne Hsu"], "n_citation": 50, "title": "Rewriting queries for XML integration systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "29a04cb4-e680-4808-a6d9-17274ccfead0"}
{"abstract": "A decentralized switched controller is developed for dynamic agents to perform global formation configuration convergence while maintaining network connectivity and avoiding collision within agents and between stationary obstacles, using only local feedback under limited and intermittent sensing. Due to the intermittent sensing, constant position feedback may not be available for agents all the time. Intermittent sensing can also lead to a disconnected network or collisions between agents. Using a navigation function framework, a decentralized switched controller is developed to navigate the agents to the desired positions while ensuring network maintenance and collision avoidance.", "authors": ["T. Cheng", "Zhen Kan", "Joel A. Rosenfeld", "Warren E. Dixon"], "n_citation": 50, "references": ["084fac90-fef4-4e53-b5c5-6ff818b0bf35", "109232c4-71d1-49fd-98fc-5b56179f62dd", "12a52019-89a5-4c41-b732-6a519446eec3", "2768199c-b9d6-4001-94d3-e6429c93bc5f", "383a833d-640c-4162-ae03-d291229be5e4", "3b974c8d-569b-4500-b6a8-1bc856882a87", "5c9483d5-4401-48fb-a9a1-c154625a626f", "6f4225ac-a69e-4236-8868-262678fca345", "8f453f0d-d119-4171-b5d0-9fde40380499", "9d6894c9-ca53-41a9-83a9-105c17d741ab", "a2c023fc-90b5-4e0d-bb53-83d0195d84a3", "a618cf33-3cc9-4a13-ac21-38c6217220c5", "aaebcbc1-e785-4bf7-b492-40c1cda2b2fc", "ab35dc68-62bd-4c54-81d3-9a8406827489", "aca928ab-a742-4ff7-bf7c-7ee7f5df4866", "d00d8e40-85ec-414b-8c0a-0ab738204236", "d9162547-fd7f-4605-855d-0a3173c4b08e", "ea1d5c21-fba6-4fae-9fdc-ee7679ee46c9", "f37f8fb3-9b86-47b0-a453-264ac13bfd00", "fe8e5e2c-7e75-4a20-9197-64e9cf944221"], "title": "Decentralized formation control with connectivity maintenance and collision avoidance under limited and intermittent sensing", "venue": "advances in computing and communications", "year": 2014, "id": "2c0fdcf4-d059-4639-8ff6-69481709fd64"}
{"abstract": "Modern computer applications, from business decision support to scientific data analysis, utilize visualization techniques to support exploratory activities. However, most existing visual exploration tools do not scale well for large data sets, i.e., the level of cluttering on the screen is typically unacceptable and the performance is poor. To solve the cluttered interface problem, visualization tools have recently been extended to support hierarchical views of the data, with support for focusing and drilling-down using interactive brushes. To solve the scalability problem, we now investigate how best to couple such a near real-time responsive visualization tool with a database management system. This integration must be done carefully, since the direct implementation of the visual user interactions on hierarchical datasets corresponds to recursive query processing and thus is highly inefficient. For this problem, we have developed a tree labeling method, called MinMax tree, that allows the movement of the on-line recursive processing into an off-line precomputation step. Thus at run time, the recursive processing operations translate into linear cost range queries. Secondly, we employ a main memory access strategy to support incremental loading of data into the main memory. The techniques have been incorporated into XmdvTool, a visual exploration tool, to achieve scalability. Lastly, we report experimental results that illustrate the impact of the proposed techniques on the system's overall performance.", "authors": ["Ionel Stroe", "Elke A. Rundensteiner", "Matthew O. Ward"], "n_citation": 0, "title": "Scalable visual hierarchy exploration", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "2f49f44f-0b39-4860-bc34-c90ae3120f6a"}
{"abstract": "We present an approach to model supervisory control systems based on extended behaviour networks. In particular, we employ them to formalize the control theory of the supervisor. By separating the reasoning in the supervisor and the action implementation in the controller, the overall system architecture becomes modular, and therefore easily changeable and modifiable.", "authors": ["Pierangelo Dell'Acqua", "Anna Lombardi", "Lu\u00eds Moniz Pereira"], "n_citation": 0, "title": "A Logic-Based Approach to Model Supervisory Control Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3640dbd4-376c-4fb8-ae22-b77c1c8952ec"}
{"abstract": "Data model clustering is the process of dividing large and complex models into parts of manageable size, in order to improve understanding and simplify documentation and maintenance. Based on theories of human cognition, a previous paper proposed connectivity (defined as the number of relationships an entity participates in) as a basis for clustering data models. This paper describes a series of laboratory experiments which evaluate the validity of this metric as a basis for clustering compared to hierarchical levelling, which has been the predominant approach used in previous research. The first two experiments investigate the relationship between the metrics and perceptions of importance, while the third experiment investigates their relationship to how people intuitively cluster entities. The results show that connectivity provides an empirically valid basis for clustering data models, which closely matches human perceptions of importance and chunking behaviour. No significant results were found for hierarchical level in any of the experiments. The high levels of statistical significance and effect size of the results for connectivity, together with their consistency across different domains and sample populations, suggests the possible discovery of a natural law governing data models.", "authors": ["Daniel L. Moody"], "n_citation": 0, "title": "Entity connectivity vs. hierarchical levelling as a basis for data model clustering: An experimental analysis", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "38c7d93e-6e39-43b9-98e7-940ab340b8fe"}
{"abstract": "This paper is situated in the area of the so-called fuzzy databases, i.e., databases containing imprecise information. It is now recognized that querying databases containing imperfect information raises several problems, including complexity. In this paper, we consider a specific kind of queries, called possibilistic queries, of the form to what extent is it possible that a given tuple t belongs to the answer of Q (a regular relational query). The major objective is to show that a reasonable complexity can be expected for a specific (although not too restricted) subset of possibilistic queries.", "authors": ["Patrick Bosc", "Laurence Duval", "Olivier Pivert"], "n_citation": 0, "title": "About selections and joins in possibilistic queries addressed to possibilistic databases", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4ad40b7e-41e7-4faa-86f5-7af71f3c1d95"}
{"abstract": "Composition of Web services is a cornerstone step in the development of interoperable systems. However, Web services still face data-heterogeneity challenges, although several attempts of using semantics. In addition, the context in which Web services evolve is still somehow ignored, hampering their adaptability to changes in composition situations. In this paper, we argue how context permits to determine the semantics of interfaces that Web services expose to third parties. We show the need for a context- and semantic-based approach for Web services composition.", "authors": ["Michael Mrissa", "Chirine Ghedira", "Djamal Benslimane", "Zakaria Maamar"], "n_citation": 50, "title": "Context and semantic composition of web services", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "513e7861-c1a8-43bb-93a4-9784d24f1fa3"}
{"abstract": "In the last time, several efforts were made in order to remove the polarization of membranes from P systems with active membranes; the present paper is a contribution in this respect. In order to compensate the loss of power represented by avoiding polarizations, we introduce tables of rules: each membrane has associated several sets of rules, one of which is non-deterministically chosen in each computation step. Three universality results for tabled P systems are given, trying to use rules of as few as possible types. Then, we consider tables with obligatory rules - rules which must be applied at least once when the table is applied. Systems which use tables with at most one obligatory rule are proven to be able to solve SAT problem in linear time. Several open problems are also formulated.", "authors": ["Gheorghe Paun", "Mario J. P\u00e9rez-Jim\u00e9nez", "Agust\u00edn Riscos-N\u00fa\u00f1ez"], "n_citation": 0, "references": ["0fe50cba-358b-4c30-806e-e32f0e1f62b1", "42207093-184f-4052-9b3b-507c43b8118a", "e12b2a8d-6755-472c-83e6-ba62463934aa"], "title": "P Systems with Tables of Rules.", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "541d773b-07e8-4fff-b8b2-65049576fd3f"}
{"abstract": "Moving-objects databases need a spatio-temporal indexing scheme for moving objects to efficiently process queries over continuously changing locations of the objects. A simple extension of the R-tree that employs time as the third dimension of the data universe shows low space utilization and poor search performance because of overlapping index regions. In this paper, we propose a variant of the 3-dimensional R-tree called the Adaptive 3DR-tree. The dynamic splitting policies of the Adaptive 3DR-tree significantly reduce the overlap rate, and this, in turn, results in improved query performance. The results of our extensive experiments show that the Adaptive 3DR-tree outperforms the original 3D R-tree and the TB-tree typically by a big margin.", "authors": ["Bong-Gi Jun", "Bonghee Hong", "Byunggu Yu"], "n_citation": 7, "title": "Dynamic splitting policies of the Adaptive 3DR-tree for indexing continuously moving objects", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "58b27958-7188-49ae-9be2-9086625a4675"}
{"abstract": "There is increasing interest from large organisations who are requiring access to and manipulation of data distributed over the multiple sites of a computer network, and furthermore, they want to share and reuse database resources and services that are not available in their original systems. In the context of Object-Oriented Database Systems (OODBSs), services are the set of methods that describe the behaviour of a particular class in a database system. Sharing these services saves effort, cost and time where the investment made in developing them can be exploited again by the original owner of these services and also by new users in the interoperation context. Different users have different needs for data integration, and the same user might want to integrate the same data with different services (i.e. behaviours). Therefore, multiple integration views with multiple behaviours are required to capture this diversity. This paper describes the theoretical framework we are using in the construction of the (MVMBS) Multiple Views supported by Multiple Behaviours System. MVMBS offers the potential for users to work in terms of integrated and customised global views supported by multiple behaviours. Our goals for MVMBS include flexibility and customisability, to this end we have developed a semantically-rich integration operator language. The user is able to generate global view(s) from scratch by means of these operators.", "authors": ["Mohamed Basel Al-Mourad", "W. A. Gray", "N. J. Fiddian"], "n_citation": 0, "title": "Multiple Views with Multiple Behaviours for interoperable Object-Oriented Database Systems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5a924d99-a261-4792-bc3b-ab5cce2ea1c6"}
{"abstract": "In this paper, we study the problem of clustering textual units in the framework of helping an expert to build a specialized ontology. This work has been achieved in the context of a French project, called BIOTIM, handling botany corpora. Building an ontology, either automatically or semi-automatically is a difficult task. We focus on one of the main steps of that process, namely structuring the textual units occurring in the texts into classes, likely to represent concepts of the domain. The approach that we propose relies on the definition of a new non-symmetrical measure for evaluating the semantic proximity between lemma, taking into account the contexts in which they occur in the documents. Moreover, we present a non-supervised classification algorithm designed for the task at hand and that kind of data. The first experiments performed on botanical data have given relevant results.", "authors": ["Guillaume Cleuziou", "Sylvie Billot", "Stanislas Lew", "Lionel Martin", "Christel Vrain"], "n_citation": 0, "title": "A Proximity Measure and a Clustering Method for Concept Extraction in an Ontology Building Perspective", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65e30cbb-afb7-46e1-a928-01bbc00eaa04"}
{"abstract": "A three-year research project is investigating the evolution of social (business) networks in an eMarket environment. To address this issue a complete, immersive, distributed virtual trading environment has been constructed. That environment is described here. Virtual worlds technology provides an immersive environment in which traders are represented as avatars that interact with each other, and have access to market data and general information that is delivered by data and text mining machinery. To enrich this essentially social market place, synthetic bots have also been constructed. They too are represented by avatars, and provide informed idle chatter so enriching the social fabric. They acquire their information with text and data mining machinery that continually scans market data and general financial news feeds. The middle-ware in this environment is based on powerful multiagent technology that manages all processes including the information delivery and data mining. The investigation of network evolution leads to the development of network mining techniques.", "authors": ["John Debenham"], "n_citation": 0, "title": "Interacting with electronic institutions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "668b782b-f9da-4fcb-b061-738a84126a26"}
{"abstract": "Building a data warehouse is a very challenging issue because compared to software engineering it is quite a young discipline and does not yet offer well-established strategies and techniques for the development process. Current data warehouse development methods can fall within three basic groups: data-driven, goal-driven and user-driven. All three development approaches have been applied to the Process Warehouse that is used as the foundation of a process-oriented decision support system, which aims to analyse and improve business processes continuously. In this paper we evaluate all three development methodologies by various assessment criteria. The aim is to establish a link between the methodology and the requirement domain.", "authors": ["Beate List", "Robert M. Bruckner", "Karl Machaczek", "Josef Schiefer"], "n_citation": 0, "title": "A comparison of data Warehouse development methodologies case study of the Process Warehouse", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "67b2bf3a-4b3f-4aa1-b8df-3996e9086631"}
{"abstract": "In this paper, we propose a new feature selection criterion. It is based on the projections of data set elements onto each attribute. The main advantages are its speed and simplicity in the evaluation of the attributes. The measure allows features to be sorted in ascending order of importance in the definition of the class. In order to test the relevance of the new feature selection measure, we compare the results induced by several classifiers before and after applying the feature selection algorithms.", "authors": ["Roberto Ruiz", "Jos\u00e9 C. Riquelme", "Jes\u00fas S. Aguilar-Ruiz"], "n_citation": 0, "title": "NLC: A measure based on projections", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "70397564-b1d7-4305-87cc-62b5b35f7e95"}
{"abstract": "Time stamping is a cryptographic technique providing us with a proof-of-existence of a message/document at a given time. Several timestamping schemes have already been proposed [1-10]. In this paper, we first define a new timestamping scheme which is based on skip lists [11]. Then, we show that our scheme offers nice properties and optimal performances.", "authors": ["Kaouthar Blibech", "Alban Gabillon"], "n_citation": 0, "title": "A New Timestamping Scheme Based on Skip Lists", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "73749cea-6267-43a0-ac04-cf363a66502e"}
{"abstract": "The first step in finding an efficient way to solve any difficult problem is making a complete, possibly formal, problem specification. This paper introduces a formal specification for the problem of semantic XML schema matching. Semantic schema matching has been extensively researched, and many matching systems have been developed. However, formal specifications of problems being solved by these systems do not exist, or are partial. In this paper, we analyze the problem of semantic schema matching, identify its main components and deliver a formal specification based on the constraint optimization problem formalism. Throughout the paper, we consider the schema matching problem as encountered in the context of a large scale XML schema matching application.", "authors": ["Marko Smiljanic", "Maurice van Keulen", "Willem Jonker"], "n_citation": 50, "title": "Formalizing the XML schema matching problem as a constraint optimization problem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "74b11800-5e0b-4e32-9606-0ea0754c5d43"}
{"abstract": "A production plan, which describes how core assets are used to develop products, has an important role in product line engineering as a communications medium between core asset developers and product developers. Recently, there have been efforts to address issues related to production planning, most of which focus on the process and business/management aspects of production planning; not much emphasis is given to technical issues such as deciding which features will be made as core assets and what their granularity will be. In this paper, we introduce a feature-based approach to product line production planning and illustrate how our approach addresses these technical issues. In our approach, a feature model and feature-binding information are used as primary input to production plan development. A product line production plan created using our approach could be customized easily to a product-specific production plan, because when we developed the approach, we considered units of product configurations as well as their integration techniques.", "authors": ["Jaejoon Lee", "Kyo Chul Kang", "Sajoong Kim"], "n_citation": 0, "title": "A feature-based approach to product line production planning", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "79c73892-3fdf-47f1-bfb2-a73cc8643d66"}
{"authors": ["Yuan Gao", "Panpan Xu", "Lu Lu", "He Liu", "Siyuan Liu", "Huamin Qu"], "n_citation": 0, "title": "Visualization of taxi drivers' income and mobility intelligence", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "7d3c6357-6655-462a-a383-62de20838847"}
{"abstract": "This paper proposes a hierarchical clustering method based on the relative mobility pattern in mobile ad hoc environment. The relative mobility pattern between two nodes is evaluated by using a received message and the cluster is created by grouping nodes with a relative mobility below a specific threshold. Also, we create a hierarchical clustering structure by allowing a merge among clusters based on the mobility pattern. In this way, the proposed mechanism can increase a continuity of total cluster structure. Since we allow the combination of clusters, we can reduce the number of cluster and message required for a routing. To evaluate a performance of our mechanism, we compared ours with the existing LCC and WCA by a Glomosim. The simulation results show that our scheme can provide the higher stability and efficiency than existing schemes.", "authors": ["Sulyun Sung", "Yuhwa Seo", "Yongtae Shin"], "n_citation": 50, "title": "Hierarchical Clustering Algorithm Based on Mobility in Mobile Ad Hoc Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8015339b-be99-4266-98f6-8919bfdcc9a6"}
{"abstract": "A human face is a complex object with features that can vary over time. Face recognition systems have been investigated while developing biometrics technologies. This paper presents a face recognition system that uses eyes, nose and mouth approximations for training a neural network to recognize faces in different expressions such as natural, smiley, sad and surprised. The developed system is implemented using our face database and the ORL face database. A comparison will be drawn between our method and two other face recognition methods; namely PCA and LDA. Experimental results suggest that our method performs well and provides a fast, efficient system for recognizing faces with different expressions.", "authors": ["Adnan Khashman", "Akram A. Garad"], "n_citation": 50, "title": "Multi-expression Face Recognition Using Neural Networks and Feature Approximation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "864d503b-8cc0-4be9-a4b9-13c56e83501a"}
{"abstract": "This paper reconsiders the established Merkle-Damgard design principle for iterated hash functions. The internal state size w of an iterated n-bit hash function is treated as a security parameter of its own right. In a formal model, we show that increasing w quantifiably improves security against certain attacks, even if the compression function fails to be collision resistant. We propose the wide-pipe hash, internally using a w-bit compression function, and the double-pipe hash, with w = 2n and an n-bit compression function used twice in parallel.", "authors": ["Stefan Lucks"], "n_citation": 235, "title": "A failure-friendly design principle for hash functions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "875f85c4-aa7b-43d1-8d3e-ef436de5c06f"}
{"abstract": "Mining association rules is one of the important research problems in data mining. So, many algorithms have been proposed to find association rules in databases with either binary or quantitative attributes. One of these approaches is fuzzy association rules mining. However, most of the earlier algorithms proposed for mining fuzzy association rules assume that fuzzy sets are given. In this paper, we propose an automated method for autonomous mining of both fuzzy sets and fuzzy association rules. For this purpose, we first find fuzzy sets by using an efficient clustering algorithm, namely CURE, and then determine their membership functions. Finally, we decide on interesting fuzzy association rules. Experimental results show the efficiency of the presented approach for synthetic transactions.", "authors": ["Mehmet Kaya", "Reda Alhajj", "Faruk Polat", "Ahmet Arslan"], "n_citation": 0, "title": "Efficient automated mining of fuzzy association rules", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8afd7b27-72af-4752-b64d-34d0e19c8bbf"}
{"abstract": "With the rapidly increasing popularity of XML, an effective method to transmit XML data over wireless broadcasting environments is urgently required. In this paper, a novel XML data streaming method for wireless broadcasting environments, is proposed. An XML stream is organized to enable a selective access scheme for simple XPath queries, by borrowing the path summary technique, which was originally devised for indexing semi-structured data. In order to utilize structure information as an XML stream index, the structure information and text values of an XML document are separated. In experimental results, the proposed method demonstrates superior performance over previous approaches with regard to both access and tuning time.", "authors": ["Sang-Hyun Park", "Jaeho Choi", "Sangkeun Lee"], "n_citation": 0, "title": "An effective, efficient XML data broadcasting method in a mobile wireless network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "932bf7c1-6cb0-4b22-b8c9-fd9879cefdf2"}
{"abstract": "The increasing amount of multimedia data enforces the development of automatic video analysis techniques. In this paper, a method of ToC generation is proposed for educational video contents. The proposed method consists of two parts: scene segmentation followed by scene annotation. First, video sequence is divided into scenes by the proposed scene segmentation algorithm which considers the characteristic of educational video. Then each shot in the scene is annotated in terms of scene type, existence of enclosed caption and main speaker of the shot. The experimental result showed that the proposed method can generate ToC for educational video with high accuracy.", "authors": ["Gwang-Gook Lee", "Eui-Jin Kim", "Jung Won Kang", "Jae-Gon Kim", "Whoi-Yul Kim"], "n_citation": 0, "title": "A method of generating table of contents for educational videos", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "983071f1-b7d3-45a4-b100-3a99aa6aea29"}
{"authors": ["Luca Anselma", "Luca Piovesan", "Abdul Sattar", "Bela Stantic", "Paolo Terenziani"], "n_citation": 0, "title": "A General Approach to Represent and Query Now-Relative Medical Data in Relational Databases", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "991acd2f-cd70-4e7a-850c-47d5538103ba"}
{"abstract": "We consider scenarios in which two parties, each in possession of a graph, wish to compute some algorithm on their joint graph in a privacy-preserving manner, that is, without leaking any information about their inputs except that revealed by the algorithm's output. Working in the standard secure multi-party computation paradigm, we present new algorithms for privacy-preserving computation of APSD (all pairs shortest distance) and SSSD (single source shortest distance), as well as two new algorithms for privacy-preserving set union. Our algorithms are significantly more efficient than generic constructions. As in previous work on privacy-preserving data mining, we prove that our algorithms are secure provided the participants are honest, but curious.", "authors": ["Justin Brickell", "Vitaly Shmatikov"], "n_citation": 0, "title": "Privacy-preserving graph algorithms in the semi-honest model", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9bb97a5f-4eff-4d13-b55b-a65b24c48099"}
{"abstract": "In this paper, we present the storage management of the WHOWEDA web warehousing system, which warehouses historical web information. To facilitate inter-table and intra-table sharing of web pages, we propose a three-layer storage architecture, that consists of tuple, table, and pool layers of storage modules storing different parts of warehoused web information. To improve retrieval efficiency, we.have chosen to replicate some node attributes across web tables in the table layer while keeping only unique copies of web pages at the pool layer. The separation of table and pool layer storage also allows different valid times to be maintained by multiple web tables for the same web pages due to different schedules of global coupling across web tables. As the sharing of web pages may lead to valid time inconsistency between different web tables, we propose an update synchronization scheme to resolve the valid time differences on user request.", "authors": ["Yinyan Cao", "Ee Peng Lim", "Wee Keong Ng"], "n_citation": 0, "title": "Storage management of a historical web warehousing system", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "9f132560-54e6-46c8-a694-20b0990a0b42"}
{"abstract": "XML is a representation of data which may require huge amounts of storage space and query processing time. Summarized representations of XML data provide succinct information which can be directly queried, either when fast yet approximate answers are sufficient, or when the actual dataset is not available. In this work we show which kinds of XQuery expressions admit a partial answer by using association rules extracted from XML datasets. Such partial information provide intensional answers to queries formulated as XQuery expressions.", "authors": ["Simone Gaspwini", "Elisa Quintarelli"], "n_citation": 0, "title": "Intensional query answering to XQuery expressions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9fbae0af-4644-4b7e-97db-452b0f9789ea"}
{"abstract": "Large-scale scientific experiments or simulation programs often generate large amounts of multidimensional data. Data volume may reach hundreds of terabytes (up to petabytes). In the present and the near future, the only practicable way for storing such large volumes of multidimensional data are tertiary storage systems. But commercial (multidimensional) database systems are optimized for performance with primary and secondary memory access. So tertiary storage memory is only in an insufficient way supported for storing or retrieval of multidimensional array data. To combine the advantages of both techniques, storing large amounts of data on tertiary storage media and optimizing data access for retrieval with multidimensional database management systems is the intention of this paper. We introduce concepts for efficient hierarchical storage support and management for large-scale multidimensional array database management systems and their integration into the commercial array database management system RasDaMan.", "authors": ["Bernd Reiner", "Karl Hahn", "Gabriele H\u00f6fling", "Peter Baumann"], "n_citation": 0, "title": "Hierarchical storage support and management for large-scale multidimensional array database management systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a1a491bb-52e0-4720-9643-9ca485886f15"}
{"abstract": "The technological advances and the use of the internet have favoured the appearance of a great diversity of web applications, among them Web Portals. Through them, organizations develop their businesses in a really competitive environment. A decisive factor for this competitiveness is the assurance of data quality. In the last years, several research works on Web Data Quality have been developed. However, there is a lack of specific proposals for web portals data quality. Our aim is to develop a data quality model for web portals focused on three aspects: data quality expectations of data consumer, the software functionality of web portals and the web data quality attributes recompiled from a literature review. In this paper, we will present the first version of our model.", "authors": ["Ang\u00e9lica Caro", "Coral Calero", "Ismael Caballero", "Mario Piattini"], "n_citation": 0, "title": "A First Approach to a Data Quality Model for Web Portals", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a242c9e9-7e74-4928-85f1-d35d81c41b8f"}
{"abstract": "KASUMI is an 8-round Feistel block cipher used in the confidentiality and the integrity algorithms of the 3GPP mobile communications. As more and more 3GPP networks are being deployed, more and more users use KASUMI to protect their privacy. Previously known attacks on KASUMI can break up to 6 out of the 8 rounds faster than exhaustive key search, and no attacks on the full KASUMI have been published. In this paper we apply the recently introduced related-key boomerang and rectangle attacks to KASUMI, resulting in an attack that is faster than exhaustive search against the full cipher. We also present a related-key boomerang distinguisher for 6-round KASUMI using only 768 adaptively chosen plaintexts and ciphertexts encrypted or decrypted under four related keys. Recently, it was shown that the security of the entire encryption system of the 3GPP networks cannot be proven using only the ordinary assumption that the underlying cipher (KASUMI) is a Pseudo-Random Permutation. It was also shown that if we assume that KASUMI is also secure with respect to differential-based related-key attacks then the security of the entire system can be proven. Our results show that theoretically, KASUMI is not secure with respect to differential-based related-key attacks, and thus, the security of the entire encryption system of the 3GPP cannot be proven at this time.", "authors": ["Eli Biham", "Orr Dunkelman", "Nathan Keller"], "n_citation": 103, "title": "A related-key rectangle attack on the full KASUMI", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a3506971-807c-4e2d-a1da-3fc2822de34f"}
{"abstract": "This paper develops and applies sequential pattern mining to a corpus of songs for the bagana, a large lyre played in Ethiopia. An important aspect of this repertoire is the unique availability of rare motifs that have been used by a master bagana teacher in Ethiopia. The method is applied to find antipatterns: patterns that are surprisingly rare in a corpus of bagana songs. In contrast to previous work, this is performed without an explicit set of background pieces. The results of this study show that data mining methods can reveal with high significance these antipatterns of interest based on the computational analysis of a small corpus of bagana songs.", "authors": ["Darrell Conklin", "St\u00e9phanie Weisser"], "n_citation": 0, "title": "Antipattern discovery in ethiopian bagana songs", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "b2dc692a-af47-48f7-b4d2-63904691688f"}
{"abstract": "In this paper we evaluate several in-memory algorithms for efficient and scalable processing of continuous range queries over collections of moving objects. Constant updates to the index are avoided by query indexing. No constraints are imposed on the speed or path of moving objects. We present a detailed analysis of a grid approach which shows the best results for both skewed and uniform data. A sorting based optimization is developed for significantly improving the cache hit ratio. Experimental evaluation establishes that indexing queries using the Grid index yields orders of magnitude better performance than other index structures such as R*-trees.", "authors": ["Dmitri V. Kalashnikov", "Sunil Prabhakar", "Susanne E. Hambrusch", "Walid G. Aref"], "n_citation": 125, "title": "Efficient evaluation of continuous range queries on moving objects", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b74339c5-f9cf-402b-a4b3-e2e0eb3a1f27"}
{"abstract": "In the paper, a new method for cost-sensitive learning of decision trees is proposed. Our approach consists in extending the existing evolutionary algorithm (EA) for global induction of decision trees. In contrast to the classical top-down methods, our system searches for the whole tree at the moment. We propose a new fitness function which allows the algorithm to minimize expected cost of classification defined as a sum of misclassification cost and cost of the tests. The remaining components of EA i.e. the representation of solutions and the specialized genetic search operators are not changed. The proposed method is experimentally validated and preliminary results show that the global approach is able to effectively induce cost-sensitive decision trees.", "authors": ["Marek Kretowski", "Marek Grzes"], "n_citation": 0, "title": "Evolutionary Induction of Cost-Sensitive Decision Trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b83d95f2-53c3-45bd-96f3-c80c708c9ec2"}
{"abstract": "Realistic broadcasting is considered as a next generation broadcasting system supporting user-friendly interactions. In this paper, we define multi-modal immersive media and introduce technologies for a realistic broadcasting system, which are developed at Realistic Broadcasting Research Center (RBRC) in Korea. In order to generate three-dimensional (3-D) scenes, we acquire immersive media using a depth-based camera or multi-view cameras. After converting the immersive media into broadcasting contents, we send the immersive contents to the clients using high-speed and high-capacity transmission techniques. Finally, we can experience realistic broadcasting represented by the 3-D display, 3-D sound, and haptic interaction. Demonstrations show two types of broadcasting systems: the system using a depth-based camera and the system using multi-view cameras. From the realistic broadcasting system, we can generate new paradigms for the next generation digital broadcasting.", "authors": ["Sung-Yeol Kim", "Seung-Uk Yoon", "Yo-Sung Ho"], "n_citation": 0, "title": "Realistic broadcasting using multi-modal immersive media", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bf344027-0b80-403a-83b1-b2400f73bfee"}
{"authors": ["Surya Bahadur Kathayat", "Hien Nam Le", "Rolv Br\u00e6k"], "n_citation": 0, "title": "A Model-Driven Framework for Component-based Development", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "c81b55b8-a64e-4378-80de-1b5bc19ecdad"}
{"authors": ["Jiageng Chen", "Atsuko Miyaji"], "n_citation": 0, "title": "How to Find Short RC4 Colliding Key Pairs", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "c9d8043a-e827-4774-a6ed-13b4b79ccd69"}
{"abstract": "Storage and transfer of information as well as interfaces for accessing this information have undergone a remarkable evolution. Nevertheless, information systems are still not intelligent in the sense that they understand the information they store, manipulate, and present to their users. A case in point is the world wide web and search engines allowing to access the vast amount of information available there. Web-pages are mostly written for human consumption and the mark-up provides only rendering information for textual and graphical information. Search engines are usually based on keyword search and often provide a huge number of answers, many of which are completely irrelevant, whereas some of the more interesting answers are not found. In contrast, the vision of a semantic web aims for machine-understandable web resources, whose content can then be comprehended and processed both by automated tools, such as search engines, and by human users. The content-based representation of information requires representation formalisms with a well-defined formal semantics since otherwise there cannot be a common understanding of the represented information. This semantics can elegantly be provided by a translation into an appropriate logic or the use of a logic-based formalism in the first place. This logical approach has the additional advantage that logical inferences can then be used to reason about the represented information, thus detecting inconsistencies and computing implicit information. However, in this setting there is a fundamental tradeoff between the expressivity of the representation formalism on the one hand, and the efficiency of reasoning with this formalism on the other hand. This motivates the engineering of logics, i.e., the design of logical formalisms that are tailored to specific representation tasks. This also encompasses the formal investigation of the relevant inference problems, the development of appropriate inferences procedures, and their implementation, optimization, and empirical evaluation. Another important topic in this context is the combination of logics and their inference procedures since a given application my require the use of more than one specialized logic. The talk will illustrate this approach with the example of so-called Description Logics and their application as ontology languages for the semantic web.", "authors": ["Franz Baader"], "n_citation": 0, "title": "Engineering of Logics for the content-based representation of information", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "cb3aa343-c305-4b6b-8965-372d972b202a"}
{"abstract": "In the joint scalable video model (JSVM), selection of quantization parameters for mode decision (QPMD) and bit rate control (QPBC) is important for efficient coding performance. For quality (SNR) scalability, QPMD is adjusted only to the base layer QPBC in JSVM. Thus, it reduces coding efficiency in the enhancement layer. In this paper, we propose a new method for selecting optimum quantization parameters for mode decision (OQPMD) in order to improve coding efficiency for both the base and the enhancement layers. For the base layer, we propose optimum scaling factors in each decomposition stage. We also propose an offset quantization parameter for the enhancement layer. Experimental results show that the proposed method increases the average PSNR value up to 0.8dB.", "authors": ["Seung-Hwan Kim", "Yo-Sung Ho"], "n_citation": 0, "title": "Optimum quantization parameters for mode decision in scalable extension of H.264/AVC video codec", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d91c921b-7793-40ec-a840-4a59ce7dfb9c"}
{"authors": ["Srinivasan Keshav"], "n_citation": 0, "references": ["4ac80067-bbea-4eaf-8b7a-89c97db7ecfe"], "title": "Technical Perspective: The power of wi-fi to deliver power", "venue": "Communications of The ACM", "year": 2017, "id": "da343831-a150-431a-9e40-24eec8b81ec7"}
{"abstract": "We present an extended Mumford-Shah regularization for blind image deconvolution and segmentation in the context of Bayesian estimation for blurred, noisy images or video sequences. The Mumford-Shah functional is extended to have cost terms for the estimation of blur kernels via a newly introduced prior solution space. This functional is minimized using \u0393-convergence approximation in an embedded alternating minimization within Neumann conditions. Accurate blur identification is the basis of edge-preserving image restoration in the extended Mumford-Shah regularization. One output of the finite set of curves and object boundaries are grouped and partitioned via a graph theoretical approach for the segmentation of blurred objects. The chosen regularization parameters using the L-curve method is presented. Numerical experiments show that the proposed algorithm is efficiency and robust in that it can handle images that are formed in different environments with different types and amounts of blur and noise.", "authors": ["Hongwei Zheng", "Olaf Hellwich"], "n_citation": 50, "title": "Extended mumford-shah regularization in bayesian estimation for blind image deconvolution and segmentation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dae82ee9-ec76-42fb-ab3b-770ae5d0ca0f"}
{"abstract": "In this paper, an eye tracking method is presented using a neural network (NN) and mean-shift algorithm that can accurately detect and track user's eyes under the cluttered background. In the proposed method, to deal with the rigid head motion, the facial region is first obtained using skin-color model and connected-component analysis. Thereafter the eye regions are localized using neural network (NN)-based texture classifier that discriminates the facial region into eye class and non-eye class, which enables our method to accurately detect users' eyes even if they put on glasses. Once the eye region is localized, they are continuously and correctly tracking by mean-shift algorithm. To assess the validity of the proposed method, it is applied to the interface system using eye movement and is tested with a group of 25 users through playing a 'aligns games.' The results show that the system process more than 30 frames/sec on PC for the 320x240 size input image and supply a user-friendly and convenient access to a computer in real-time operation.", "authors": ["Eun Yi Kim", "Sin Kuk Kang"], "n_citation": 0, "title": "Eye Tracking Using Neural Network and Mean-Shift", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "db393313-d013-44a9-8701-58125aa24320"}
{"abstract": "Two major tasks in process management are the planning of a process and the coordination of its execution. Workflow management systems support and automate the coordination of a process, but planning the process remains a manual task. This paper argues that planning and coordination are closely interrelated and therefore restricting the attention to the automation of coordination can lead to problems concerning consistency and performance. A concept is presented to avoid these problems by adding automated planning. It is shown how planning algorithms from Artificial Intelligence can be combined with workflow management concepts to build an integrated workflow planning and coordination system. Advantages, limitations and areas of application of this approach are discussed.", "authors": ["Hilmar Schuschel", "Mathias Weske"], "n_citation": 0, "title": "Integrated workflow planning and coordination", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "dd0ccaa0-4331-4f53-b118-5b176ccff460"}
{"abstract": "In ad hoc networks, data accessibility decreases due to network divisions. To solve this problem, it is effective that each mobile host creates replicas of data items held by others. In this paper, we assume an environment where mobility and access characteristics of mobile hosts have the locality and propose a method that locally relocates replicas just before a network division occurs.", "authors": ["Hideki Hayashi", "Takahiro Hara", "Shojiro Nishio"], "n_citation": 50, "title": "A replica allocation method adapting to topology changes in ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ddb4d9d0-866a-4061-8d35-8a64c7f827cc"}
{"abstract": "Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. Holistic twig join algorithm has showed its superiority over binary decompose based approach due to efficient reducing intermediate results. The existing holistic join algorithms, however, cannot deal with ordered twig queries. A straightforward approach that first matches the unordered twig queries and then prunes away the undesired answers is obviously not optimal in most cases. In this paper, we study a novel holistic-processing algorithm, called OrderedTJ, for ordered twig queries. We show that OrderedTJ can identify a large query class to guarantee the I/O optimality. Finally, our experiments show the effectiveness, scalability and efficiency of our proposed algorithm.", "authors": ["Jiaheng Lu", "Tok Wang Ling", "Tian Yu", "Changqing Li", "Wei Ni"], "n_citation": 0, "title": "Efficient processing of ordered XML twig pattern", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e6c4982b-eac8-4c04-a440-9868dc32a5e7"}
{"abstract": "In this paper we present and evaluate two approaches for the generation of Semantic Fields, which are used as a tool for resource discovery in the Semantic Web. We mainly concern ourselves with semantic networks that describe their interests and resources by means of ontologies. Semantic Fields are intended to help users to locate these resources by specifying a brief description (also as an ontology). We propose two ways to automatically build Semantic Fields. The first one is used in the KREIOS approach, which is based on the pre-computation of distances between all the ontology pairs. The second one is based on a fast incremental clustering algorithm, which groups together similar ontologies as they are published. These groups constitute a pre-computed set of Semantic Fields.", "authors": ["I. Navas", "Ismael Sanz", "J. F. Aldana", "Rafael Berlanga"], "n_citation": 0, "title": "Automatic generation of Semantic Fields for resource discovery in the Semantic Web", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ea761409-97fd-4c59-8ca0-6dae3cdbf5cd"}
{"abstract": "This paper presents the performance study of a Genetic Algorithm applied to a mixed model non-permutation flowshop production line. Resequencing is permitted where stations have access to intermittent or centralized resequencing buffers. The access to the buffers is restricted by the number of available buffer places and the physical size of the products. Characteristics such as the difference between the intermittent and the centralized case, the number of buffer places and the distribution of the buffer places are analyzed. Improvements that come with the introduction of constrained resequencing buffers are highlighted.", "authors": ["Gerrit Hartmut F\u00e4rber", "Anna Maria Coves Moreno"], "n_citation": 50, "title": "Performance Study of a Genetic Algorithm for Sequencing in Mixed Model Non-permutation Flowshops Using Constrained Buffers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "eab56f04-bd5e-4617-b526-80d403c0e656"}
{"abstract": "Formal Concept Analysis is an unsupervised machine learning technique that has successfully been applied to document organisation by considering documents as objects and keywords as attributes. The basic algorithms of Formal Concept Analysis then allow an intelligent information retrieval system to cluster documents according to keyword views. This paper investigates the scalability of this idea. In particular we present the results of applying spatial data structures to large datasets in formal concept analysis. Our experiments are motivated by the application of the Formal Concept Analysis idea of a virtual filesystem [11,17,15]. In particular the libferris [1] Semantic File System. This paper presents customizations to an RD-Tree Generalized Index Search Tree based index structure to better support the application of Formal Concept Analysis to large data sources.", "authors": ["Ben Martin", "Peter W. Eklund"], "n_citation": 3, "title": "Asymmetric Page Split Generalized Index Search Trees for Formal Concept Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "efd4084c-0ccc-469d-bb97-7fd00b0b785b"}
{"abstract": "We consider the problem of counting the interior edge crossings when a bipartite graph G = (V, E) with node set V and edge set E is drawn such that the nodes of the two shores of the bipartition are drawn as distinct points on two parallel lines and the edges as straight line segments. The efficient solution of this problem is important in layered graph drawing. Our main observation is that it can be reduced to counting the inversions of a certain sequence. This leads to an O(|E| + |C|) algorithm, where C denotes the set of pairwise interior edge crossings, as well as to a simple O(|E| log |V small |) algorithm, where V small  is the smaller cardinality node set in the bipartition of the node set V of the graph. We present the algorithms and the results of computational experiments with these and other algorithms on a large collection of instances.", "authors": ["Wilhelm Barth", "Michael J\u00fcnger", "Petra Mutzel"], "n_citation": 50, "title": "Simple and efficient bilayer cross counting", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f1c396ab-8753-46f0-9213-df866356841a"}
{"abstract": "The Global Forest Information Service (GFIS), an initiative of internationally recognized forestry institutions lead by the International Union of Forest Research Organizations (IUFRO), primarily aims as an electronic system that utilises the Internet to facilitate forest information resource discovery. For describing the content of resources, the GFIS metadata has been designed and implemented as a generalization of the Dublin Core metadata element set. In this context, a set of multilingual controlled forest vocabularies has been established that allows GFIS to focus on a forest special application domain. Hereafter, we have studied and implemented the GFIS prototype, which supports interoperability search among a class of GFIS nodes by means of GFIS-Dublin Core Metadata and the multi host search (MS) tool.", "authors": ["Thanh Binh Nguyen", "Mohamed T. Ibrahim"], "n_citation": 0, "title": "GFIS Pro: A tool for managing forest information resources", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f74cd47b-409f-4f5e-aaa4-8b030ec0d924"}
{"abstract": "Stream ciphers play an important role in symmetric cryptology because of their suitability in high speed applications where block ciphers fall short. A large number of fast stream ciphers or pseudorandom bit generators (PRBG's) can be found in the literature that are based on arrays and simple operations such as modular additions, rotations and memory accesses (e.g. RC4, RC4A, Py, Py6. ISAAC etc.). This paper investigates the security of array-based stream ciphers (or PRBG's) against certain types of distinguishing attacks in a unified way. We argue, counter-intuitively, that the most useful characteristic of an array, namely, the association of array-elements with unique indices, may turn out to be the origins of distinguishing attacks if adequate caution is not maintained. In short, an adversary may attack a cipher simply exploiting the dependence of array-elements on the corresponding indices. Most importantly, the weaknesses are not eliminated even if the indices and the array-elements are made to follow uniform distributions separately. Exploiting these weaknesses we build distinguishing attacks with reasonable advantage on five recent stream ciphers (or PRBG's), namely, Py6 (2005, Biham et al.), IA, ISAAC (1996, Jenkins Jr.), NGG, GGHN (2005, Gong et al.) with data complexities 2 68.61 , 2 32.59 , 2 16.89 , 2 32.89  and 2 32.89  respectively. In all the cases we worked under the assumption that the key-setup algorithms of the ciphers produced uniformly distributed internal states. We only investigated the mixing of bits in the keystream generation algorithms. In hindsight, we also observe that the previous attacks on the other array-based stream ciphers (e.g. Py, etc.), can also be explained in the general framework developed in this paper. We hope that our analyses will be useful in the evaluation of the security of stream ciphers based on arrays and modular addition.", "authors": ["Souradyuti Paul", "Bart Preneel"], "n_citation": 0, "title": "On the (In)security of Stream Ciphers Based on Arrays and Modular Addition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0ef4db44-ae84-4534-b2cb-df04ac38bc11"}
{"abstract": "This article gives a comprehensive study on SMO-type (Sequential Minimal Optimization) decomposition methods for training support vector machines. We propose a general and flexible selection of the two-element working set. Main theoretical results include 1) a simple asymptotic convergence proof, 2) a useful explanation of the shrinking and caching techniques, and 3) the linear convergence of this method. This analysis applies to any SMO-type implementation whose selection falls into the proposed framework.", "authors": ["P. Chen", "Rong-En Fan", "Chih-Jen Lin"], "n_citation": 0, "title": "Training support vector machines via SMO-type decomposition methods", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "12c0dbea-ed38-4e1a-abac-5cb24c3934ad"}
{"abstract": "Permutation-only image ciphers encrypt images by permuting the positions of all pixels in a secret way, which are unfortunately frail under known-text attack. In view of the weakness of permutation-only algorithms, a color image encryption algorithm based on magic cube transformation and a new modular arithmetic operation is designed. First, a natural number chaotic sequence is created with the secret key. For the sake of higher security, all secret keys are generated by different chaotic maps, and thus increase the security for decryption. Second, we implement the position permutation algorithm by magic cube transformation with chaotic sequences. Third, the pixel-substitution algorithm is realized by changing the image pixel value, with a XOR plus mod diffuse operation and a modular arithmetic operation. Finally, experimental results are given to demonstrate the efficiency and high security of our novel algorithm.", "authors": ["Jianbing Shen", "Xiaogang Jin", "Chuan Zhou"], "n_citation": 50, "title": "A color image encryption algorithm based on magic cube transformation and modular arithmetic operation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1a2bcf7b-2cf1-401b-bb01-9f605c13aad8"}
{"abstract": "A concrete reconstruction problem arising in Computer Vision motivates our investigation of combinatorial variations on the problem of drawing a graph with given direction vectors associated to its edges. We formulate solutions in dimension 2 and report on experimental results done with simple implementations.", "authors": ["Ileana Streinu", "Elif Tosun"], "n_citation": 0, "title": "Camera position reconstruction and tight direction networks", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1a9df5b9-5ea1-477c-b346-9c329aa56203"}
{"abstract": "In this paper, we propose a new collaborative caching strategy in a push-based broadcast environment where clients construct a peer-to-peer network by connecting with each other. In the proposed strategy, a client takes into account its own access probabilities and information on queries issued by other clients, and caches data items with large benefits of the response time. We confirm that the proposed strategy reduces the average response time by simulation experiments.", "authors": ["Kazuhiko Maeda", "Wataru Uchida", "Takahiro Hara", "Shojiro Nishio"], "n_citation": 0, "title": "On a collaborative caching in a peer-to-peer network for push-based broadcast", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1d91ae5a-0477-4632-ac4b-391553626c98"}
{"abstract": "The paper presents an efficient way of designing lighting setup for rendering 3D face model. Specifically, we focus on obtaining lighting direction for Rembrandt lighting. A Rembrandt patch is a triangle defined as the bright region surrounded by self and cast shadows on a check area, and we use the self- and cast-shadow curves for computing the direction of main lighting. A user graphically specifies a Rembrandt patch on a 3D model. From the user input, lighting directions are estimated from the cast- and self-shadow geometry on a 3D face model. The final lighting direction is decided among the candidates predicted by the self and cast shadows. The presented method lets a user interactively design and achieve Rembrandt lighting by alleviating repetitive manual search for the light direction by trial and error. Experimental results show the effectiveness of the presented method. It suggests appropriate Rembrandt lighting directions quickly and easily.", "authors": ["Hongmi Joe", "Kyoung Chin Seo", "Sang Wook Lee"], "n_citation": 0, "title": "Interactive rembrandt lighting design", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1f9d03ab-8cf0-4d2b-a658-d1c4066e0ef9"}
{"abstract": "Unlike processing snapshot queries in a traditional DBMS, the processing of continuous queries in a data stream management system (DSMS) needs to satisfy quality requirements such as processing delay. When the system is overloaded, quality degrades significantly thus load shedding becomes necessary. Maintaining the quality of queries is a difficult problem because both the processing cost and data arrival rate are highly unpredictable. We propose a quality adaptation framework that adjusts the application behavior based on the current system status. We leverage techniques from the area of control theory in designing the quality adaptation framework. Our simulation results demonstrate the effectiveness of the control-based quality adaptation strategy. Comparing to solutions proposed in previous works, our approach achieves significantly better quality with less waste of resources.", "authors": ["Tong Yi-cheng", "Mohamed Hefeeda", "Yuni Xia", "Sunil Prabhakar", "Song Liu"], "n_citation": 29, "title": "Control-based quality adaptation in data stream management systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1fceaae4-a294-4ad2-a597-be5a48ef8a71"}
{"abstract": "Space-time block codes (STBC) are well-known for use in the co-located multi-antenna systems to combat the adverse effects of fading channel. However, their application is limited in some scenarios where users can not support multiple transmit antennas. Therefore, we propose a solution of space-time block coded cooperative transmission among single-antenna users in the multiple access wireless environments to obtain the powerful benefits of multi-antenna system without the demand for physical arrays. The closed-form BER expression was also derived and compared to the simulation results to verify the validity of the suggested method. A variety of numerical results demonstrated the superiority of cooperative transmission over direct transmission under flat Rayleigh fading channel plus AWGN (Additive White Gaussian Noise).", "authors": ["Ho Van Khuong", "Hyung-Yun Kong"], "n_citation": 0, "title": "Proposal of space-time block coded cooperative wireless transmission in rayleigh fading channels", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "250d5db9-4937-4da9-8559-6136be1bef7b"}
{"abstract": "Separate-and-conquer classifiers strongly depend on the criteria used to choose which rules will be included in the classification model. When association rules are employed to build such classifiers (as in ART [3]), rule evaluation can be performed attending to different criteria (other than the traditional confidence measure used in association rule mining). In this paper, we analyze the desirable properties of such alternative criteria and their effect in building rule-based classifiers using a separate-and-conquer strategy.", "authors": ["Fernando Berzal", "Juan-Carlos Cubero", "Nicol\u00e1s Mar\u00edn", "Jos\u00e9-Luis Polo"], "n_citation": 0, "title": "An Overview of Alternative Rule Evaluation Criteria and Their Use in Separate-and-Conquer Classifiers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "28c7425f-db6a-4280-8e07-206bd18dde05"}
{"authors": ["Grazia Bombini", "Raquel Ros", "Stefano Ferilli", "Ramon L\u00f3pez de M\u00e1ntaras"], "n_citation": 50, "title": "Analysing the behaviour of robot teams through relational sequential pattern mining", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "31025035-c94e-49f8-87f4-f9587f1224e7"}
{"abstract": "In this paper, we present a methodology to express, in a formal way, the requirements of products belonging to a product line. We relied on a formalism allowing the representation of variabilities at the family level and the in. stantiation of them in order to move to the requirements of a single product. The proposed methodology also allows the formalization of the family constraints to be taken into account for the construction of the products belonging to it, along with the verification of the compliance to those constraints of a single product requirements document. This approach is promising due to its simplicity and effectiveness for being supported by automatic tools.", "authors": ["Alessandro Fantechi", "Stefania Gnesi", "Giuseppe Lami", "E. Nesti"], "n_citation": 0, "title": "A methodology for the derivation and verification of use cases for product lines", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "31ad2fae-a3ac-4360-90d1-6e5bec73c2a7"}
{"abstract": "In this paper we present a new XML-based query language for XML documents denoted XRL. This language expresses database conditions concerning the attributes and the structure of documents, as well as Information Retrieval conditions over their contents and their relevance. XRL queries can be stored into a XML repository and manipulated as any other XML document. In order to illustrate the usefulness of this language, we also describe some general guidelines of its current implementation, and present an example application. This application consists in a subscription/notification service of news articles, which are periodically retrieved from a digital library of newspapers according to the preferences of each user.", "authors": ["Juan Manuel P\u00e9rez", "Mar\u00eda Jos\u00e9 Aramburu", "Rafael Berlanga"], "n_citation": 0, "title": "XRL: A XML-based query language for advanced services in digital libraries", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "526f3bf0-1e0f-46a8-b0e6-fb2db5b51771"}
{"abstract": "Several approaches to the development of shared artefacts in software product families exist. Each has advantages and disadvantages, but there is no clear framework for selecting among these alternatives. As a consequence, mismatches between the optimal approach and the one currently used by an organization may lead to several problems, such as a high degree of erosion, mismatches between product needs and shared components, organizational noise and inefficient knowledge management. This paper (1) presents the problems resulting from the aforementioned mismatch, (2) presents the relevant decision dimensions that define the space of alternatives, (3) discusses the advantages and disadvantages of each alternative and (4) presents a framework for selecting the best alternative for each decision dimension based on a three-stage adoption model.", "authors": ["Jan Bosch"], "n_citation": 0, "title": "On the development of software product-family components", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5a06d246-e984-40a2-92b6-37ffda5f531b"}
{"abstract": "We give improved upper bounds on the communication complexity of optimally-resilient secure multiparty computation in the cryptographic model. We consider evaluating an n-party randomized function and show that if f can be computed by a circuit of size c, then O(cn 2 k) is an upper bound for active security with optimal resilience t < n/2 and security parameter k. This improves on the communication complexity of previous protocols by a factor of at least n. This improvement comes from the fact that in the new protocol, only O(n) messages (of size O(k) each) are broadcast during the whole protocol execution, in contrast to previous protocols which require at least 0(n) broadcasts per gate. Furthermore, we improve the upper bound on the communication complexity of passive secure multiparty computation with resilience t < n from O(cn 2 k) to O(cnk). This improvement is mainly due to a simple observation.", "authors": ["Martin Hirt", "Jesper Buus Nielsen"], "n_citation": 0, "title": "Upper bounds on the communication complexity of optimally resilient cryptographic multiparty computation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5f502732-b865-4c62-b213-73e19aab45c0"}
{"abstract": "Finite turn-based safety games have been used for very different problems such as the synthesis of linear temporal logic (LTL), the synthesis of schedulers for computer systems running on multiprocessor platforms, and also for the determinisation of timed automata. In these contexts, games are implicitly defined, and their size is at least exponential in the size of the input. Nevertheless, there are natural relations between states of arenas of such games. We first formalise the properties that we expect on the relation between states, thanks to the notion of alternating simulation. Then, we show how such simulations can be exploited to (1) improve the running time of the OTFUR algorithm to compute winning strategies and (2) obtain a succinct representation of a winning strategy. We also show that our general theory applies to the three applications mentioned above.", "authors": ["Gilles Geeraerts", "Jo\u00ebl Goossens", "Am\u00e9lie Stainer"], "n_citation": 1, "references": ["2230c340-e108-418b-a1ab-c40ef3eaefe5", "23edbafc-3897-4144-ad1a-3a92bfab467c", "298ac21c-8d14-45e3-943f-36695f269a2a", "47c9336f-9b26-409b-8630-d4d835d9ec79", "74e3fd8b-f955-4fde-aad8-0a705f05e27e", "854eca6f-8b4b-4384-a8c2-77bee2bc00de", "8df98de7-cda4-4c8a-9abe-73f5d7ef23d5", "9138f800-cf0d-45ad-9700-6cdcea3e04a7", "9849d9c4-a97f-452f-882c-42a8c6cab0b5", "cd192c55-ae65-4c98-9345-4b6dc79af008", "d7f2c945-b293-46f9-95dd-f30e82d1dfda", "df0f05c2-0595-4828-ae7f-96119e066d60", "ef292a03-dfc6-48b7-86e4-7047d7a4a38f"], "title": "Synthesising Succinct Strategies in Safety Games", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "6171bd93-9372-479d-910d-a327ec5c2ca0"}
{"abstract": "As the network-related technology develops the number of both internet users and the usage are explosively increasing. The networking traffic is increasing in the campus as the networking system inside universities, following the trend, adds more nodes and various networking services. Nonetheless, the quality of services for users has been degraded. Accordingly, core problems, which can cause troubles for network management, design and expansion of the network, and the cost policy, have developed. To effectively cope with the problems an analysis of a great number of technicians, tools, and budget are needed. However, it is not possible for mid and small-sized colleges to spend such a high expenditure for professional consulting. To reduce the cost and investment of creating an optimized environment, analysis of the replacement of the tools, changing the network structure, and performance analysis about capacity planning of networking is necessary. For this reason, in this paper, framework-based performance management tools are used for all steps that are related to the subject of the analysis for the network management. As the major research method, the current data in detailed categories are collected, processed, and analyzed to provide a meaningful solution to the problems. As a result we will be able to manage the network, server, and application more systematically and react efficiently to errors and degrading of performance that affect the networking tasks. Also, with the scientific and organized analyses the overall efficiency is upgraded by optimizing the cost for managing the operation of entire system.", "authors": ["Seong-Man Choi", "Cheol-Jung Yoo", "Ok-Bae Chang"], "n_citation": 0, "title": "An Efficient Management of Network Traffic Performance Using Framework-Based Performance Management Tool", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65f18426-57cb-48ca-b811-15a2d9b80d67"}
{"abstract": "Contourlet is a new image representation method, which can efficiently represent contours and textures in images. In this paper, we analyze the distribution of significant contourlet coefficients in different subbands and propose a contourlet image coding algorithm by constructing a virtual low frequency subband and adjusting coding method of SPIHT (Set Partitioning in Hierarchical Trees) algorithm according to the structure of contourlet coefficients. The proposed coding algorithm can provide an embedded bit stream, which is very desirable in heterogeneous networks. Our experiments demonstrate that the proposed coding algorithm can achieve better or competitive compression performance when compared with traditional wavelet transform with SPIHT and wavelet-based contourlet transform with SPIHT, which both are embedded image coding algorithms based on two non-redundant transforms. At the same time, benefiting from genuine contourlet adopted in the proposed coding algorithm, more contours and textures in the coded images are preserved to ensure superior subjective quality.", "authors": ["Haohao Song", "Songyu Yu", "Li Song", "Hongkai Xiong"], "n_citation": 0, "title": "Contourlet image coding based on adjusted SPIHT", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6cad6850-f3ca-4b7a-9b5e-77b0e6b65120"}
{"abstract": "We present time-efficient algorithms for encoding (and decoding) planar orthogonal drawings of degree-4 and degree-3 biconnected and triconnected planar graphs using small number of bits. We also present time-efficient algorithms for encoding (and decoding) turn-monotone planar orthogonal drawings.", "authors": ["Amrita Chanda", "Ashim Garg"], "n_citation": 0, "title": "Compact encodings of planar orthogonal drawings", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6f0cbaa0-b6aa-40b4-8439-53ccb0ac7754"}
{"abstract": "The time element inherent in normative systems has become a central topic of the cultural and political debate and is of fundamental concern to legal computer science. The law is under increasing pressure to keep pace with social change: normative texts and amendments follow one another in time and get overlapped. Given this background the Norma-System project, presented in this paper, seeks to use the theoretical, legistic, and legimatic models for facilitating the task of identifying and determining what is the law in force in order to face the multiple problems from which the Italian legal system is currently suffering 1 .", "authors": ["Monica Palmirani", "Raffaella Brighi"], "n_citation": 0, "title": "Norma-system: A legal document system for managing consolidated acts", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6f43fd71-a3b8-4d15-ade7-5543fd0bedd0"}
{"authors": ["Ashish Amresh", "John Femiani", "Cristoph Fuenfzig"], "n_citation": 0, "title": "Methods for Approximating Loop Subdivision Using Tessellation Enabled GPUs", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "7224b7d8-1a0e-4af9-a205-f7816e7fb0c4"}
{"abstract": "In this paper, we proposed a new method named adjacency based ordering to order sentences for summarization tasks. Given a group of sentences to be organized into the summary, connectivity of each pair of sentences is learned from source documents. Then a top-first strategy is implemented to define the sentence ordering. It provides a solution of ordering texts while other information except the source documents is not available. We compared this method with other existing sentence ordering methods. Experiments and evaluations are made on data collection of DUC04. The results show that this method distinctly outperforms other existing sentence ordering methods. Its low input requirement also makes it capable to most summarization and text generation tasks.", "authors": ["Yu Nie", "Donghong Ji", "Lingpeng Yang"], "n_citation": 0, "title": "An adjacency model for sentence ordering in multi-document summarization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7a73f668-92aa-4149-a124-ebb11468eba9"}
{"abstract": "In this paper, we propose an adaptive congestion control scheme to control congestion caused in wireless/mobile access networks of ubiquitous computing environment. Significantly, this scheme includes a new reverse congestion avoidance phase, which can classify packet loss due to bit error or network congestion. Also, it includes a new slow stop phase which can minimize an wasted bandwidth due to previous congestion control schemes. And, this scheme controls network congestion more adaptive than previous congestion control schemes by a new method of congestion measurement and various phases in the adaptive congestion control scheme. The new method of congestion measurement classifies degree of congestion by a relation of the number of loss packets and increment size of a congestion window. This scheme is designed based on DCCP(Datagram Congestion Control Protocol) being proposed by IETF(Internet Engineering Task Force). In our simulation, this scheme provides a good bandwidth throughput not in wireless/mobile access networks but also in wired networks.", "authors": ["Si-Yong Park", "Sung-Min Kim", "Tae-Hoon Lee", "Ki-Dong Chung"], "n_citation": 0, "title": "Adaptive congestion control scheme based on DCCP for wireless/mobile access networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7ba927b2-f5b2-4207-bc1d-185f8928fdc0"}
{"abstract": "In digitizing 3D objects one wants as much as possible object properties to be preserved in its digital reconstruction. One of the most fundamental properties is topology. Only recently a sampling theorem for cubic grids could be proved which guarantees topology preservation [1]. The drawback of this theorem is that it requires more complicated reconstruction methods than the direct representation with voxels. In this paper we show that face centered cubic (fcc) and body centered cubic (bcc) grids can be used as an alternative. The fcc and bcc voxel representations can directly be used for a topologically correct reconstruction. Moreover this is possible with coarser grid resolutions than in the case of a cubic grid. The new sampling theorems for fcc and bcc grids also give absolute bounds for the geometric error.", "authors": ["Peer Stelldinger", "Robin Strand"], "n_citation": 0, "title": "Topology preserving digitization with FCC and BCC grids", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8445a0aa-480e-402e-908f-54319d35e245"}
{"abstract": "Let S be a set of n + m sites, of which n are red and have weight wR, and m are blue and weigh wB. The objective of this paper#R##N#is to calculate the minimum value of the red sites\u2019 weight such that the union of the red Voronoi cells in the weighted Voronoi diagram of S is a connected region. This problem is solved for the multiplicativelyweighted#R##N#Voronoi diagram in O((n+m)2 log(nm)) time and for both the additively-weighted and power Voronoi diagram in O(nmlog(nm)) time", "authors": ["Manuel Abellanas", "Ant\u00f3nio Leslie Bajuelos", "Santiago Canales", "Merc\u00e8 Claverol Aguas", "Gregorio Hern\u00e1ndez", "In\u00eas Matos"], "n_citation": 0, "title": "Connecting Red Cells in a Bicolour Voronoi Diagram", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "92b0571c-85b8-4394-b5e6-ab4b43489cab"}
{"abstract": "Knowledge and information spanning multiple information sources, multiple media, multiple versions and multiple communities challenge the capabilities of existing knowledge and information management infrastructures by far -primarily in terms of intellectually exploiting the stored knowledge and information. In this paper we present some semantic web technologies of the EU integrated project X-Media that build bridges between the various information sources, the different media, the stations of knowledge management and the different communities. Core to this endeavour is the combination of information extraction with formal ontologies as well as with semantically lightweight folksonomies.", "authors": ["Steffen Staab", "Thomas Franz", "Olaf G\u00f6rlitz", "Carsten Saathoff", "Simon Schenk", "Sergej Sizov"], "n_citation": 0, "title": "Lifecycle Knowledge Management : Getting the Semantics Across in X-Media", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "97c26cb7-672a-4a0d-9f8e-ce0bac459f50"}
{"abstract": "Identifying temporal information of topics from a document set typically involves constructing a time decomposition of the time period associated with the document set. In an earlier work, we formulated several metrics on a time decomposition, such as size, information loss, and variability, and gave dynamic programming based algorithms to construct time decompositions that are optimal with respect to these metrics. Computing information loss values for all subintervals of the time period is central to the computation of optimal time decompositions. This paper proposes several algorithms to assist in more efficiently constructing an optimal time decomposition. More efficient, parallelizable algorithms for computing loss values are described. An efficient top-down greedy heuristic to construct an optimal time decomposition is also presented. Experiments to study the performance of this greedy heuristic were conducted. Although lossy time decompositions constructed by the greedy heuristic are suboptimal, they seem to be better than the widely used uniform length decompositions.", "authors": ["Parvathi Chundi", "Rui Zhang", "Daniel J. Rosenkrantz"], "n_citation": 0, "title": "Efficient algorithms for constructing time decompositions of time stamped documents", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "97db3ead-fa91-4a6b-8abb-e2dba2e32244"}
{"authors": ["Mengyao Ma", "Oscar C. Au", "Liwei Guo", "Zhiqin Liang", "Gary Shueng Han Chan"], "n_citation": 0, "title": "Error Concealment for INTRA-frame losses over packet loss channels", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "a2738588-41a3-4e0f-930b-2a4f90710511"}
{"abstract": "In modem universal database management systems (DBMSs) user-defined data types along with extensible indexing structures try to bridge the gap between standard data-independent DBMS implementations and the requirement of specialized access methods for efficient domain-specific data retrieval, maintenance, and storage. However, these approaches often suffer from restricting the degree of freedom in the implementation and limiting the availability of crucial database features. Due to their design concepts, these extensible indexing frameworks are not intended to be suitable for rapid development and evaluation of research prototypes, as they lack essential generalization, completeness, and depth of their integration into the host DBMS. We discuss the advantages and drawbacks of available extensible indexing techniques and present several methods that can be easily combined into a powerful and flexible framework for storing, indexing, and manipulating domain specific data from any data source. We demonstrate that this framework comprises all properties truly extensible indexing should have. A prototype implementation of this framework was integrated into the relational DBMS Transbase\u00ae.", "authors": ["Ralph Acker", "Roland Pieringer", "Rudolf Bayer"], "n_citation": 50, "title": "Towards truly extensible database systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a34b5396-7890-4499-9734-64e25eaa87ed"}
{"abstract": "This article presents preliminary quantitative results from movement analysis of several clarinet performers with respect to non-obvious or ancillary gestures produced while playing a piece. The comparison of various performances of a piece by the same clarinetist shows a high consistency of movement patterns. Different clarinetists show different overall patterns, although clear similarities may be found, suggesting the existence of various levels of information in the resulting movement. The relationship of these non-obvious gestures to material/physiological, structural and interpretative parameters is highlighted.", "authors": ["Marcelo M. Wanderley"], "n_citation": 0, "title": "Quantitative analysis of non-obvious performer gestures", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b3325915-14f5-44ba-8eae-32e5ea3c74af"}
{"authors": ["Vinton G. Cerf"], "n_citation": 0, "title": "Grumpy old cells", "venue": "Communications of The ACM", "year": 2017, "id": "b480b859-40f1-4598-b603-656c50172348"}
{"abstract": "The paper gives an overview of an inter-disciplinary research project whose goal is to elucidate the complex phenomenon of expressive music performance with the help of machine learning and automated discovery methods. The general research questions that guide the project are laid out, and some of the most important results achieved so far are briefly summarized (with an emphasis on the most recent and still very speculative work). A broad view of the discovery process is given, from data acquisition issues through data visualization to inductive model building and pattern discovery. It is shown that it is indeed possible for a machine to make novel and interesting discoveries even in a domain like music. The report closes with a few general lessons learned and with the identification of a number of open and challenging research problems.", "authors": ["Gerhard Widmer"], "n_citation": 50, "title": "In search of the horowitz factor: Interim report on a musical discovery project", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "bceecf9e-e2b2-45c9-b198-6fcf088e97dd"}
{"abstract": "In this paper, we discuss issues related to metrics for Web page quality. These metrics are used for ranking the quality and relevance of Web pages in response to user needs. We focus on the problem of ascertaining the statistical distribution of some well-known hyperlink based Webpage quality metrics. Based on empirical distributions of Webpage degrees, we derived analytically the probability distribution for the Pagerank metric. We found out that it follows the familiar inverse polynomial law reported for Webpage degrees. We verified the theoretical exercise with experimental results that suggest a highly concentrated distribution of the metric.", "authors": ["Devanshu Dhyani", "Sourav S. Bhowmick", "Wee Keong Ng"], "n_citation": 0, "title": "Deriving and verifying statistical distribution of a hyperlink-based Web page quality metric", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ca7d4736-ade1-4612-9424-739749690a67"}
{"abstract": "We give a linear-time algorithm to decide whether a graph has a planar LL-drawing, i.e. a planar drawing on two parallel lines. This has previously been known only for trees. We utilize this result to obtain planar drawings on three lines for a generalization of bipartite graphs, also in linear time.", "authors": ["Sabine Cornelsen", "Thomas Schank", "Dorothea Wagner"], "n_citation": 0, "title": "Drawing graphs on two and three lines", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ce7388e5-83a8-4ca4-96c2-efacb4817274"}
{"abstract": "We introduce a new cryptographic tool: multiset hash functions. Unlike standard hash functions which take strings as input, multiset hash functions operate on multisets (or sets). They map multisets of arbitrary finite size to strings (hashes) of fixed length. They are incremental in that, when new members are added to the multiset, the hash can be updated in time proportional to the change. The functions may be multiset-collision resistant in that it is difficult to find two multisets which produce the same hash, or just set-collision resistant in that it is difficult to find a set and a multiset which produce the same hash. We demonstrate how set-collision resistant multiset hash functions make an existing offline memory integrity checker secure against active adversaries. We improve on this checker such that it can use smaller time stamps without increasing the frequency of checks. The improved checker uses multiset-collision resistant multiset hash functions.", "authors": ["Dwaine E. Clarke", "Srinivas Devadas", "Marten van Dijk", "Blaise Gassend", "G. Edward Suh"], "n_citation": 0, "title": "Incremental multiset hash functions and their application to memory integrity checking", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d138faaa-09ac-4a6a-8e49-f6ebed0e92c7"}
{"abstract": "Neighborhood graphs are an effective and very widespread technique in several fields. But, in spite of the neighborhood graphs interest, their construction algorithms suffer from a very high complexity what prevents their implementation for great data volumes processing applications. With this high complexity, the update task is also affected. These structures constitute actually a possible representation of the point location problem in a multidimensional space. The point location on an axis can be solved by a binary research. This same problem in the plan can be solved by using a voronoi diagram, but when dimension becomes higher, the location becomes more complex and difficult to manage. We propose in this paper an effective method for point location in a multidimensional space with an aim of effectively and quickly updating neighborhood graphs.", "authors": ["Hakim Hacid", "Abdelkader Djamel Zighed"], "n_citation": 0, "title": "An effective method for locally neighborhood graphs updating", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d3c6ae75-9fd3-4976-8d9f-4d6fe8d93c38"}
{"abstract": "Machine classification of Polish language emails into user-specific folders is considered. We experimentally evaluate the impact of different approaches to construct data representation of emails on the accuracy of classifiers. Our results show that language processing techniques have smaller influence than an appropriate selection of features, in particular ones coming from the email header or its attachments.", "authors": ["Jerzy Stefanowski", "Marcin Zienkowicz"], "n_citation": 50, "title": "Classification of Polish Email Messages : Experiments with Various Data Representations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d53a0c8a-c6d4-4170-bb0e-28876b266603"}
{"abstract": "We have developed an application called My Portal Viewer (MPV)[1] that effectively integrates many articles collected from multiple news sites and presents these integrations through a familiar interface such as a page the user has often visited. MPV dynamically determines keywords of interest that a user might potentially be interested in based on the history of the articles the user has read and creates categories based on these interest words. MPV and many other similar integration systems, however, cause problems where users cannot find only their interest articles in each category because they are only ranked by frequency and the cooccurrence of keywords. We propose a new method of selecting further articles from each category using a user's impressions of articles. The improved MPV, called MPV Plus, selects and recommends more desirable articles using the method we propose. This paper presents the design concept and process flow of MPV Plus and reports on its effectiveness as evaluated in experiments.", "authors": ["Yukiko Kawai", "Tadahiko Kumamoto", "Katsumi Tanaka"], "n_citation": 0, "title": "User preference modeling based on interest and impressions for news portal site systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "da5e0b66-919c-4329-9d52-dd396181060b"}
{"abstract": "A fundamental reason for investing in product families is to minimize the application engineering costs. Several organizations that employ product families, however, are becoming increasingly aware of the fact that, despite the efforts in domain engineering, deriving individual products from their shared software assets is a time- and effort-consuming activity. In this paper, we present a collection of product derivation problems that we identified during a case study at two large and mature industrial organizations. These problems are attributed to the lack of methodological support for application engineering, and to underlying causes of complexity and implicit properties. For each problem, we provide a description and an example, while for each cause we present a description, consequences, solutions, and research issues. The discussions in this paper are relevant outside the context of the two companies, as the challenges they face arise in, for example, comparable or less mature organizations.", "authors": ["Sybren Deelstra", "Marco Sinnema", "Jan Bosch"], "n_citation": 0, "title": "Experiences in software product families: Problems and issues during product derivation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ec9d25ad-4d8f-4842-b88c-0a527dc2d660"}
{"abstract": "The data warehousing approach intends to exploit a very large volume of data to make relevant decisions. In this paper, we deal with object-oriented data warehouse design. More precisely, we present an object-oriented data warehouse model, integrating temporal and archive data. We provide functions allowing the administrator to specify a data warehouse from a global source schema.", "authors": ["Franck Ravat", "Olivier Teste"], "n_citation": 0, "title": "A temporal object-oriented data warehouse model", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "f8f60402-e90f-4c91-8448-64850b0a163b"}
{"abstract": "Queries in relational DBMS are getting more and more complex especially in the decision support environment. Also, tools generate most SQL queries received by commercial DBMS engines with not much user control. Query rewrite plays an important role in optimizing these complex and tool-generated queries. One important such technique is eliminating redundant joins whenever possible. In this paper, we present a new solution for outer join elimination. This solution was implemented in the Teradata DBMS part of V2R5.1. As an example, we show how applications based on vertical partitioning and universal views can greatly benefit from our outer join elimination technique. Finally, we describe performance improvement results of applying these techniques to the TPC-H benchmark.", "authors": ["Ahmad Ghazal", "Alain Crolotte", "Ramesh Bhashyam"], "n_citation": 0, "title": "Outer join elimination in the Teradata RDBMS", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fa3d9e80-1513-4c20-972f-f7ccbb00491f"}
{"abstract": "Desktop Search, the search across local storage such as a personal computer, is a common practice among computer users. There has been much activity in Web-related Information Retrieval, but Desktop Search has only recently increased in popularity. As the structure and accessibility of data in a local environment is different to the Web, new algorithmic possibilities arise for Desktop Search. We apply a connectivity analysis approach to the local environment a filesystem. We describe how it can be used in parallel with existing tools to provide more useful ranked results. Our evaluation reveals that such an approach has promise, and we conclude that exploiting the organization of a filesystem is beneficial for Desktop Search.", "authors": ["Alex Penev", "Matthew Gebski", "Raymond K. Wong"], "n_citation": 50, "title": "Topic distillation in desktop search", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "08500dff-c4c4-4437-bee7-a0cbf4c960de"}
{"authors": ["Kyungsook Han", "Byong-Hyon Ju", "Jong H. Park"], "n_citation": 0, "title": "InterViewer: Dynamic visualization of protein-protein interactions", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "085b64db-a1ba-40ec-ba96-b73ebae1c49f"}
{"authors": ["Patrizio Angelini", "Till Bruckdorfer", "Marco Chiesa", "Fabrizio Frati", "Michael Kaufmann", "Claudio Squarcella"], "n_citation": 50, "title": "On the Area Requirements of Euclidean Minimum Spanning Trees", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "11a985bc-e164-4607-a0c5-628cafe4a5ea"}
{"abstract": "Due to the limitation of broadcasting service, in general, TV programs with commercial advertisements are scheduled to be broadcasted by demographics. The uniformly provided commercial can not draw many TV viewers' interest, which is not correspondent to the goal of the commercial. In order to solve the problem, a novel target advertisement technique is proposed in this paper. The target advertisement is a personalized advertisement according to TV viewers' profile such as their age, gender, occupation, etc. However, viewers are usually reluctant to inform their profile to the TV program provider or the advertisement company because their information can be used on some bad purpose by unknown people. Our target advertisement technique estimates a viewer's profile using Normalized Distance Sum and Inner product method. In the experiment, our method is evaluated for estimating the TV viewers' profile using TV usage history provided by AC Neilson Korea.", "authors": ["Mun-Jo Kim", "Sanggil Kang", "Munchurl Kim", "Jae-Gon Kim"], "n_citation": 0, "title": "Target advertisement service using TV viewers' profile inference", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "15e90b7c-80e7-4c5d-8ff9-9a45957919e3"}
{"abstract": "Declustering techniques reduce query response times through parallel I/O by distributing data among multiple devices. Except for a few cases it is not possible to find declustering schemes that are optimal for all spatial range queries. As a result of this, most of the research on declustering have focused on finding schemes with low worst case additive error. Recently, constrained declustering that maximizes the threshold k such that all spatial range queries < k buckets are optimal is proposed. In this paper, we extend constrained declustering to high dimensions. We investigate high dimensional bound diagrams that are used to provide upper bound on threshold and propose a method to find good threshold-based declustering schemes in high dimensions. We show that using replicated declustering with threshold N, low worst case additive error can be achieved for many values of N. In addition, we propose a framework to find thresholds in replicated declustering.", "authors": ["Ali Saman Tosun"], "n_citation": 0, "title": "Threshold based declustering in high dimensions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "17bbf8f1-5c00-4915-a314-c99dfbb7281d"}
{"abstract": "Mining knowledge from structured data has been extensively addressed in the few past years. However, most proposed approaches are interested in flat structures. With the growing popularity of the Web, the number of semi-structured documents available is rapidly increasing. Structure of these objects is irregular and it is judicious to assume that a query on documents structure is almost as important as a query on data. Moreover, manipulated data is not static since it is constantly being updated. The problem of maintaining such sub-structures then becomes as much of a priority as researching them because, every time data is updated, found sub-structures could become invalid. In this paper we propose a system, called A.U.S.M.S. (Automatic Update Schema Mining System), which enables us to retrieve data, identify frequent sub-structures and keep up-to-date extracted knowledge after sources evolutions.", "authors": ["Pierre-Alain Laur", "Maguelonne Teisseire", "Pascal Poncelet"], "n_citation": 0, "title": "AUSMS: An environment for frequent sub-structures extraction in a semi-structured object collection", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "190c9f41-aa57-4305-b735-eb4a7db98b98"}
{"abstract": "Stream cipher Hiji-Bij-Bij (HBB) was proposed by Sarkar at Indocrypt'03. This cipher uses cellular automata (CA). The algorithm has two modes: a basic mode (B) and a self-synchronizing mode (SS). This article presents the first attack on B mode of HBB using 128 bit secret key. This is a known-pliantext guess-then-determine attack. The main step in the attack guesses 512 bits of unknown out of the 640 bits of the initial internal state. The guesses are done sequentially and the attack uses a breadth-first-search-type algorithm so that the time complexity is 2 50 .", "authors": ["Joydip Mitra"], "n_citation": 0, "title": "A near-practical attack against B mode of HBB", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "19e256bb-a19e-413d-a841-ada36472c10a"}
{"authors": ["Mukesh K. Mohania", "Inderpal Narang"], "n_citation": 0, "title": "Policy based enterprise (active) information integration", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2b14f834-6d66-4eae-bcb6-2cfe6771c860"}
{"authors": ["Giuseppe Andreoni", "M. Rabuffetti", "Antonio Pedotti"], "n_citation": 0, "title": "Simulation of complex human movement through the modulation of observed motor tasks.", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "2c925956-8d0a-439f-9d26-e9a645bb8760"}
{"abstract": "The XSL algorithm is a method for solving systems of multivariate polynomial equations based on the linearization method. It was proposed in 2002 as a dedicated method for exploiting the structure of some types of block ciphers, for example the AES and Serpent. Since its proposal, the potential for algebraic attacks against the AES has been the source of much speculation. Although it has attracted a lot of attention from the cryptographic community, currently very little is known about the effectiveness of the XSL algorithm. In this paper we present an analysis of the XSL algorithm, by giving a more concise description of the method and studying it from a more systematic point of view. We present strong evidence that, in its current form, the XSL algorithm does not provide an efficient method for solving the AES system of equations.", "authors": ["Carlos Cid", "Ga\u00ebtan Leurent"], "n_citation": 89, "title": "An analysis of the XSL algorithm", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2ced1c22-ad4f-4886-9db5-a99ba25084bd"}
{"abstract": "The recent interest in three dimensional graph drawing has been motivating studies on how to extend two dimensional techniques to higher dimensions. A common approach for computing a 2D orthogonal drawing of a graph separates the task of defining the shape of the drawing from the task of computing its coordinates. First results towards finding a three-dimensional counterpart of this approach are presented in [8, 9], where characterizations of orthogonal representations of paths and cycles are studied. In this note we show that the known characterization for cycles does not immediately extend to even seemingly simple graphs such as theta graphs. A sufficient condition for recognizing three-dimensional orthogonal representations of theta graphs is also presented.", "authors": ["Emilio Di Giacomo", "Giuseppe Liotta", "Maurizio Patrignani"], "n_citation": 0, "title": "Orthogonal 3D shapes of theta graphs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2fe614bf-7229-4d70-97aa-6ad881d4d039"}
{"abstract": "Recently, Bellare and Palacio succeeded in defining the plaintext awareness, which is also called PA2, in the standard model. They propose three valiants of the standard model PA2 named perfect, statistical, and computational PA2. In this paper, we study the relationship between the standard model PA2 and the property about message hiding, that is, IND-CPA. Although it seems that these two are independent notions at first glance, we show that all of the perfect, statistical, and computational PA2 in the standard model imply the IND-CPA security if the encryption function is oneway. By using this result, we also showed that PA2 + Oneway => IND-CCA2. This result shows the all-or-nothing aspect of the PA2. That is, a standard model PA2 secure public-key encryption scheme either satisfies the strongest message hiding property. IND-CCA2, or does not satisfy even the weakest message hiding property, onewayness. We also showed that the computational PA2 notion is strictly stronger than the statistical one.", "authors": ["Isamu Teranishi", "Wakaha Ogata"], "n_citation": 50, "title": "Relationship Between Standard Model Plaintext Awareness and Message Hiding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "325820d1-3919-455e-b858-ca551bc044c1"}
{"abstract": "More work is needed to make synthesized speech more natural, easier to understand, and more pleasant to hear.", "authors": ["Neil Savage"], "n_citation": 0, "references": [], "title": "Thinking deeply to make better speech", "venue": "Communications of The ACM", "year": 2017, "id": "34026b2c-6c7f-4a23-be64-2b727b4a3a76"}
{"abstract": "Many problems encountered in practice involve the prediction of a continuous attribute associated with an example. This problem, known as regression, requires that samples of past experience with known continuous answers are examined and generalized in a regression model to be used in predicting future examples. Regression algorithms deeply investigated in statistics, machine learning and data mining usually lack measures to give an indication of how good' the predictions are. Tolerance regions, i.e., a range of possible predictive values, can provide a measure of reliability for every bare prediction. In this paper, we focus on tree-based prediction models, i.e., model trees, and resort to the inductive inference to output tolerance regions in addition to bare prediction. In particular, we consider model trees mined by SMOTI (Stepwise Model Tree Induction) that is a system for data-driven stepwise construction of model trees with regression and splitting nodes and we extend the definition of trees to build tolerance regions to be associated with each leaf. Experiments evaluate validity and quality of output tolerance regions.", "authors": ["Annalisa Appice", "Michelangelo Ceci"], "n_citation": 0, "title": "Mining Tolerance Regions with Model Trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "38b9ff05-5711-4248-801d-faf25158a910"}
{"abstract": "This paper proposes a word sense language model based method for information retrieval. This method, differing from most of traditional ones, combines word senses defined in a thesaurus with a classic statistical model. The word sense language model regards the word sense as a form of linguistic knowledge, which is helpful in handling mismatch caused by synonym and data sparseness due to data limit. Experimental results based on TREC-Mandarin corpus show that this method gains 12.5% improvement on MAP over traditional tf-idf retrieval method but 5.82% decrease on MAP compared to a classic language model. A combination result of this method and the language model yields 8.92% and 7.93% increases over either respectively. We present analysis and discussions on the not-so-exciting results and conclude that a higher performance of word sense language model will owe to high accurate of word sense labeling. We believe that linguistic knowledge such as word sense of a thesaurus will help IR improve ultimately in many ways.", "authors": ["Liqi Gao", "Yu Zhang", "Ting Liu", "Guiping Liu"], "n_citation": 0, "title": "Word sense language model for information retrieval", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3a1f45f1-7033-43d1-bab4-d2171ab9d77f"}
{"abstract": "In this paper we describe a visual, ontology-based query paradigm. It has two novel features: visually specifying aggregate table queries and table layout in a single process, and providing users with an ontology guide for formulating statistical data analysis tasks as table queries which are composed of the terms defined in ontologies. We describe the role of the fundamental concept of ontology in the content representation of distributed databases with large numbers of multi-valued attributes, along with the methods and techniques developed for representing and manipulating ontologies, and for understanding semantics of database contents and query formulation and processing.", "authors": ["Yaxin Bi", "David A. Bell", "Joanne Lamb", "Kieran Greer"], "n_citation": 0, "title": "Visual querying with ontologies for distributed statistical databases", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "3a67e549-524e-4ef8-88dc-0dfa23680227"}
{"abstract": "Mobile devices are capable of retrieving and processing data from remote databases. In a wireless data transmission environment, users are typically charged by the size of transferred data, rather than the amount of time they stay connected. We propose algorithms that join information from non-collaborative remote databases on mobile devices. Our methods minimize the data transferred during the join process, by also considering the limitations of mobile devices. Experimental results show that our approach can perform join processing on mobile devices effectively.", "authors": ["Lucas \u00c9ric", "Nikos Mamoulis", "David W. Cheung", "Ws Ho", "Panos Kalnis"], "n_citation": 50, "title": "Processing Ad-Hoc joins on mobile devices", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3bd12f1f-2bb4-4871-803c-84186e895f7f"}
{"authors": ["Margus Veanes", "Nikolaj Bj\u00f8rner"], "n_citation": 0, "title": "Equivalence of Finite-Valued Symbolic Finite Transducers", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "3c59bca4-c9bd-4197-8185-cf2068bcfc9c"}
{"abstract": "Continuously monitoring large-scale aggregates over data streams is important for many stream processing applications, e.g. collaborative intelligence analysis, and presents new challenges to data management systems. The first challenge is to efficiently generate the updated aggregate values and provide the new results to users after new tuples arrive. We implemented an incremental aggregation mechanism for doing so for arbitrary algebraic aggregate functions including user-defined ones by keeping up-to-date finite data summaries. The second challenge is to construct shared query evaluation plans to support large-scale queries effectively. Since multiple query optimization is NP-complete and the queries generally arrive asynchronously, we apply an incremental sharing approach to obtain the shared plans that perform reasonably well. The system is built as a part of ARGUS, a stream processing system atop of a DBMS. The evaluation study shows that our approaches are effective and efficient on typical collaborative intelligence analysis data and queries.", "authors": ["Chun Jin", "Jaime G. Carbonell"], "n_citation": 0, "title": "Incremental Aggregation on Multiple Continuous Queries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3d2c7647-a6c7-4b3e-bf87-92f09594c786"}
{"abstract": "In the research field of document information retrieval, the unit of retrieval results returned by IR systems is a whole document or a document fragment, like a paragraph in passage retrieval. IR systems based on the vector space model compute feature vectors of the units and calculate the similarities between the units and the query. However, the unit of retrieval results are not suitable for document information retrieval since they are not congruent with the information which users are searching for. Therefore, the unit of retrieval results should be a portion of the XML document, such as a chapter, section, or subsection. That is, we think the most important concern of document information retrieval is to define the unit of retrieval results, that is meaningful for users. It is easy to construct the appropriate portion of XML documents as retrieval results because XML is a standard document format on the Internet and because XML documents consist of contents and document structures. In this paper, we propose an effective IR system for XML documents that automatically defines an appropriate unit of retrieval results by analyzing the XML document structure. We performed experimental evaluations and verified the effectiveness of our XML IR system. In addition, we also defined new recall and precision measures for XML information retrieval in order to evaluate our XML IR system.", "authors": ["Kenii Hatano", "Hiroko Kinutani", "Masatoshi Yoshikawa", "Shunsuke Uemura"], "n_citation": 0, "title": "Information retrieval system for XML documents", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3fb4b20f-5bc2-44f6-82e2-c04eb098bd4c"}
{"authors": ["Bogdan Groza", "Bogdan Warinschi"], "n_citation": 50, "title": "Revisiting Difficulty Notions for Client Puzzles and DoS Resilience", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "3fe94998-cd19-424e-92c7-d27014edf923"}
{"authors": ["Dimitry S. Ananichev", "Alessandra Cherubini", "Mikhail V. Volkov"], "n_citation": 0, "title": "An inverse automata algorithm for recognizing 2-collapsing words", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "41ef5238-2dab-4508-8a94-62ba2a19c4b6"}
{"abstract": "In the modem information society, multimedia libraries are increasingly essential core components of the information systems managing our digital assets. The effective and efficient management of large amounts of multimedia information involves the extraction of relevant features from unstructured multimedia documents, images, videos, and sound recordings, as well as the organization, classification, and retrieval of these multimedia documents. A particularly important aspect is the opportunity to combine a variety of diverse features. In this paper we are interested in a feature rarely considered in such systems: the environmental noise. We design, implement, present, and evaluate an experimental multimedia library system for video clips and sound recordings in which scenes are indexed, classified and retrieved according to their environmental noise. Namely, after adequate training, the system is able distinguish between such scenes as traffic scenes, canteen scenes, and gunfight scenes, for instance. We show how we improved existing techniques for the classification of sound to reach an accuracy of up to 90% in the recognition of environmental noise.", "authors": ["St\u00e9phane Bressan", "Boon Tiang Tan"], "n_citation": 0, "title": "Environmental noise classification for multimedia libraries", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "41efd778-3276-4bc6-9d11-5e415e8cdad9"}
{"abstract": "Frequent pattern mining has become a fundamental technique for many data mining tasks. Many modern frequent pattern mining algorithms such as FP-growth adopt tree structure to compress database into on-memory compact data structure. Recent studies show that the tree structure can be efficiently mined using frequent pattern growth methodology. Higher level of performance improvement can be expected from parallel execution. In particular, PC cluster is gaining popularity as the high cost-performance parallel platform for data extensive task like data mining. However, we have to address many issues such as space distribution on each node and skew handling to efficiently mine frequent patterns from tree structure on a shared-nothing environment. We develop a framework to address those issues using novel granularity control mechanism and tree remerging. The common framework can be enhanced with temporal constrain to mine web access patterns. We invent improved support counting procedure to reduce the additional communication overhead. Real implementation using up to 32 nodes confirms that good speedup ratio can be achieved even on skewed environment.", "authors": ["Iko Pramudiono", "Masaru Kitsuregawa"], "n_citation": 0, "title": "Tree structure based parallel frequent pattern mining on PC cluster", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "46da2317-815b-4969-b195-15eccb8ba827"}
{"abstract": "The aim of feature subset selection is to reduce the complexity of an induction system by eliminating irrelevant and redundant features. Selecting the right set of features for classification task is one of the most important problems in designing a good classifier. In this paper we propose a feature selection approach based on rough set based feature weighting. In the approach the features are weighted and ranked in descending order. An incremental forward interleaved selection process is used to determine the best feature set with highest possible classification accuracy. The approach is experimented and tested using some standard datasets. The experiments carried out are to evaluate the influence of the feature pre-selection on the prediction accuracy of the rough classifier. The results showed that the accuracy could be improved with an appropriate feature pre-selection phase.", "authors": ["Qasem A. Al-Radaideh", "Nasir Sulaiman", "Mohd Hasan Selamat", "Hamidah Ibrahim"], "n_citation": 0, "title": "Feature selection by ordered rough set based feature weighting", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4f2f788e-2ef2-4942-8bdb-85a7c55241c8"}
{"abstract": "Recent works in XML change detection have focused on detecting changes to ordered or unordered XML documents. However, in real life XML documents may not always be purely ordered or purely unordered. It is indeed possible to have both ordered and unordered nodes in the same XML document (such documents are called hybrid XML). In this paper, we present a technique for detecting the changes to hybrid XML documents. In our approach, old and new versions of XML documents are first stored in a relational database. Then, the order learning module is used to determine the node types in hybrid XML. The change detection module then uses the knowledge of node types to detect the changes by issuing SQL queries. Our experimental results show that our approach produces better result quality compared to existing approaches.", "authors": ["Erwin Leonardi", "Sri L. Budiman", "Sourav S. Bhowmick"], "n_citation": 0, "title": "Detecting changes to hybrid XML documents using relational databases", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "59c21d19-5351-44d0-8a96-84c4eda21b8b"}
{"abstract": "We present new results in the framework of secure multiparty computation based on homomorphic threshold cryptosystems. We introduce the conditional gate as a special type of multiplication gate that can be realized in a surprisingly simple and efficient way using just standard homomorphic threshold ElGamal encryption. As addition gates are essentially for free, the conditional gate not only allows for building a circuit for any function, but actually yields efficient circuits for a wide range of tasks.", "authors": ["Berry Schoenmakers", "Pim Tuyls"], "n_citation": 0, "title": "Practical two-party computation based on the conditional gate", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5f0c4b28-8607-4bed-9381-8b04c0c07655"}
{"abstract": "This paper proposes a family of key management schemes for broadcast encryption based on a novel underlying structure - Time Varying Heterogeneous Logical Key Hierarchy (TVH-LKH). Note that the main characteristics of the previously reported key management schemes include the following: employment of a static underlying structure for key management, and addressing the subset covering problem over the entire underlying structure. Oppositely, the main underlying ideas for developing of the novel key management schemes based on TVH-LKH include the following: (i) employment of a reconfigurable underlying structure; and (ii) employment of a divide-and-conquer approach related to the underlying structure and an appropriate communications-storage-processing trade-off (for example, a small increase of the communication overload and large reduction of the storage and processing overload) for addressing the subset covering problem and optimization of the overloads. The design is based on a set of static keys at a receiver (stateless receiver) which are used in all possible reconfiguration of the underlying structure for key management, and accordingly, in a general case, a key plays different roles depending on the employed underlying structure. A particular family of the components for developing TVH-LKH, is also proposed and discussed. The proposed technique is compared with the recently reported schemes, and the advantages of the novel one are pointed out.", "authors": ["Miodrag J. Mihaljevic"], "n_citation": 0, "title": "Key management schemes for stateless receivers based on Time varying Heterogeneous Logical Key Hierarchy", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "6515ee4f-2f69-4ade-a6f1-8b7e889f1f5c"}
{"abstract": "We study the class of masking based domain extenders for UOWHFs. Our first contribution is to show that any correct masking based domain extender for UOWHF which invokes the compression UOWHF s times must use at least [log 2 s] masks. As a consequence, we obtain the key expansion optimality of several known algorithms among the class of all masking based domain extending algorithms. Our second contribution is to present a new parallel domain extender for UOWHF. The new algorithm achieves asymptotically optimal speed-up over the sequential algorithm and the key expansion is almost everywhere optimal, i.e., it is optimal for almost all possible number of invocations of the compression UOWHF. Our algorithm compares favourably with all previously known masking based domain extending algorithms.", "authors": ["Palash Sarkar"], "n_citation": 0, "title": "Masking based domain extenders for UOWHFS: Bounds and constructions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6ceb73c8-97c3-482a-923d-eab80b877199"}
{"abstract": "Page revisiting is a popular browsing activity in the Web. In this paper we describe a method for improving page revisiting by detecting and highlighting the information on browsed Web pages that is fresh for a user. Content freshness is determined based on comparison with the previously viewed versions of pages. Any new content for the user is marked, enabling the user to quickly spot it. We also describe a mechanism for visually informing users about the degree of freshness of linked pages. By indicating the freshness level of content on linked pages, the system enables users to navigate the Web more effectively. Finally, we propose and demonstrate the concept of determining user-dependent, subjective age of page contents. Using this method, elements of Web pages are annotated with dates indicating the first time the elements were accessed by the user.", "authors": ["Adam Jatowt", "Yukiko Kawai", "Katsumi Tanaka"], "n_citation": 50, "title": "Personalized detection of fresh content and temporal annotation for improved page revisiting", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "715cc7ac-9f21-468b-89c3-e6c44e2b8c83"}
{"abstract": "With limitations in nature, current workflow management systems are insufficient to meet the arising requirements from the evolving e-Business applications such as global running, process migration, inter-organisational cooperation etc. For this reason, this paper proposes a new architecture for workflow management systems, with an emphasis on supporting inter-organisational interoperability, flexibility, reliability and scalability, based on the Web service technology. With provision of a novel agreement management service and an enhanced definition template library, this architecture offers full support to business process collaborations among participating organisations, including both the horizontal and vertical partitioning schemes.", "authors": ["Xiaohui Zhao", "Chengfei Liu", "Yun Yang"], "n_citation": 0, "title": "Web service based architecture for workflow management systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "71af5807-08b0-4ea6-8149-90abb12f2c2e"}
{"abstract": "An important property of a hash function is the performance. We study fast iterated hash functions based on block ciphers. These hash functions and their compression functions are analyzed in the standard black-box model. We show an upper bound on rate of any collision resistant hash function. In addition, we improve known bound on the rate of collision resistant compression functions.", "authors": ["Martin Stanek"], "n_citation": 0, "title": "Analysis of Fast Blockcipher-Based Hash Functions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "75020db1-fb97-4df5-a1a3-a0dfb95f3800"}
{"abstract": "A new efficient standard discrete line recognition method is presented. This algorithm incrementally computes in linear time all straight lines which cross a given set of pixels. Moreover, pixels can be considered in any order and do not need to be connected. A new invertible 2D discrete curve reconstruction algorithm based on the proposed recognition method completes this paper. This algorithm computes a polygonal line so that its standard digitization is equal to the discrete curve. These two methods are based on the definition of a new generalized preimage and the framework is the discrete analytical geometry.", "authors": ["Martine Dexet", "Eric Andres"], "n_citation": 0, "title": "Linear discrete line recognition and reconstruction based on a generalized preimage", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "78d9dddd-4cad-4863-b277-d7d797432526"}
{"authors": ["Teuvo Kohonen"], "n_citation": 50, "title": "Data management by self-organizing maps", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "7bf50a9c-7e2f-4ef4-941c-00fd62e98e5e"}
{"abstract": "The search for an optimal node elimination sequence for the triangulation of Bayesian networks is an NP-hard problem. In this paper, a new method, called the TAGA algorithm, is proposed to search for the optimal node elimination sequence. TAGA adjusts the probabilities of crossover and mutation operators by itself, and provides an adaptive ranking-based selection operator that adjusts the pressure of selection according to the evolution of the population. Therefore the algorithm not only maintains the diversity of the population and avoids premature convergence, but also improves on-line and off-line performances. Experimental results show that the TAGA algorithm outperforms a simple genetic algorithm, an existing adaptive genetic algorithm, and simulated annealing on three Bayesian networks.", "authors": ["Hao Wang", "Kui Yu", "Xindong Wu", "Hongliang Yao"], "n_citation": 0, "title": "Triangulation of Bayesian Networks Using an Adaptive Genetic Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "88004908-bb0d-42b0-bfd5-d508bd003ad1"}
{"abstract": "Broadcast has been often used to disseminate the frequently requested data efficiently to a large volume of mobile units over a single or multiple channels. Since the mobile units have limited battery power, the minimization of the access time for the broadcast data is an important problem. In this paper, we studied an efficient index allocation method for the broadcast data over multiple physical channels to minimize the access time. Previously proposed index allocation techniques either require the equal size of index and data or have a performance degradation problem when the number of given physical channels is not enough. To cope with these problems, we propose an efficient tree-structured index allocation method for the broadcast data with different access frequencies over multiple physical channels.", "authors": ["Byungkyu Lee", "Sungwon Jung"], "n_citation": 0, "title": "An efficient tree-structure index allocation method over multiple broadcast channels in mobile environments", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "88df9f01-6419-4db4-b40d-9e9b18ca84e9"}
{"abstract": "Trees are usually drawn planar, i.e. without any crossings. In this paper, we investigate the area requirement of (non-upward) planar straight-line grid drawings of binary trees. Let T be a binary tree with n nodes. We show that T admits a planar straight-line grid drawing with area O(n) and with any pre-specified aspect ratio in the range [1,n \u03b1 ], where a is a constant such that 0 < a < 1. We also show that such a drawing can be constructed in O(n log n) time.", "authors": ["Ashim Garg", "Adrian Rusu"], "n_citation": 50, "title": "Straight-line drawings of binary trees with linear area and arbitrary aspect ratio", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8e9c88e5-483f-40c9-a5e2-6fb0424ec505"}
{"abstract": "Content based retrieval is an important paradigm in multimedia applications. It heavily relies on k-Nearest-Neighbor (k-NN) queries applied to high dimensional feature vectors representing objects. Dimensionality Reduction (DR) of high-dimensional datasets via Principal Component Analysis - PCA is an effective method to reduce the cost of processing k-NN queries on multi-dimensional indices. The distance information loss is quantifiable by the Normalized Mean Square Error (NMSE), which is determined by the number of retained dimensions (n). For smaller n the cost of accessing the index (an SR-tree is used in our study) for k-NN search is lower, but the postprocessing cost to achieve exact query processing is higher. The optimum value n opt  can be determined experimentally by considering cost as a function of n. We concern ourselves with a local DR method, which applies DR to clusters of the original dataset. Clusters are obtained via a PCA-friendly clustering method, which also determines the number of clusters. For a given NMSE we use an algorithm developed in conjunction with the Clustered SVD - CSVD method to determine the vector of the number of dimensions retained in all clusters (n). The NMSE is varied to determine the optimum n, which minimizes the number of accessed pages. To verify the robustness of our methodology we experimented with one synthetic and three real-world datasets. It is observed that the NMSE yielding the optimum n varies over a narrow range and that the optimal value is expected to be applicable to datasets with similar characteristics.", "authors": ["Yue Li", "Alexander Thomasian", "Lijuan Zhang"], "n_citation": 0, "title": "Optimal subspace dimensionality for k-NN search on clustered datasets", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9216582b-0fa7-4efb-87cb-5c08bb039b9c"}
{"abstract": "In this paper, we proposed the image processing techniques for extracting the cracks in a concrete surface crack image and the ART2-based radial basis function neural network for recognizing the directions of the extracted cracks. The image processing techniques used are the closing operation of morphological techniques, the Sobel masking used to extract edges of the cracks, and the iterated binarization for acquiring the binarized image from the crack image. The cracks are extracted from the concrete surface image after applying two times of noise reduction to the binarized image. We proposed the method for automatically recognizing the directions (horizontal, vertical, -45 degree, 45 direction degree) of the cracks with the ART2-based RBF(Radial Basis Function) neural network. The proposed ART2-based RBF neural network applied ART2 to the learning between the input layer and the middle layer and the Delta learning method to the learning between the middle layer and the output layer. The experiments using real concrete crack images showed that the cracks in the concrete crack images were effectively extracted and the proposed ART2-based RBF neural network was effective in the recognition of the extracted cracks directions.", "authors": ["Kwang-Baek Kim", "Hwang-Kyu Yang", "Sang-Ho Ahn"], "n_citation": 0, "title": "Recognition of Concrete Surface Cracks Using ART2-Based Radial Basis Function Neural Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "950ee2be-7abf-4db2-8708-902c2d4e6d31"}
{"abstract": "This paper proposes a way to overcome locking problems in interactive database applications by using awareness concepts. Parallel long running editing sessions in interactive database applications often cause locking conflicts. The occurrence of conflicts can be drastically decreased by giving users means to be aware of each other and to communicate. This paper explains how this is done in database applications for the social area, which are prone to locking conflicts due to their use of relations with a very large number of attributes and long running transactions. Additionally it shows how high scalability can be achieved with the help of dynamic partitioning schemes. We present MAL, a development system for interactive database applications, which allows to develop applications that automatically include awareness-based locking in a network environment.", "authors": ["G\u00fcnther Specht", "P. F. H. Von Enderndorf", "Thomas Kahabka", "Franz Peterander"], "n_citation": 0, "title": "Awareness in interactive database applications", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "9f749d8e-1032-4fd3-9f8a-ec0cb1dbef6b"}
{"abstract": "Considering a protocol of Tseng, we show that a group key agreement protocol that resists attacks by malicious insiders in the authenticated broadcast model, loses this security when it is transfered into an unauthenticated point-to-point network with the protocol compiler introduced by Katz and Yung. We develop a protocol framework that allows to transform passively secure protocols into protocols that provide security against malicious insiders and active adversaries in an unauthenticated point-to-point network and, in contrast to existing protocol compilers, does not increase the number of rounds. Our protocol particularly uses the session identifier to achieve the security. By applying the framework to the Burmester-Desmedt protocol we obtain a new 2 round protocol that is provably secure against active adversaries and malicious participants.", "authors": ["Jens-Matthias Bohli"], "n_citation": 50, "title": "A Framework for Robust Group Key Agreement", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a1cafaa5-68d1-435a-97d6-f4d0ae6543a9"}
{"authors": ["Pawe\u0142 Betli\u0144ski"], "n_citation": 0, "title": "Markov Blanket Approximation Based on Clustering", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "a5170160-b8e3-4c68-b62a-89220505fabe"}
{"abstract": "There is a growing trend to use general-purpose operating systems like Linux in embedded systems. Previous research focused on using compaction and specialization techniques to adapt a general-purpose OS to the memory-constrained environment, presented by most, embedded systems. However, there is still room for improvement: it has been shown that even after application of the aforementioned techniques more than 50% of the kernel code remains unexecuted under normal system operation. We introduce a new technique that reduces the Linux kernel code memory footprint, through on-demand code loading of infrequently executed code, for systems that support virtual memory. In this paper, we describe our general approach, and we study code placement algorithms to minimize the performance impact of the code loading. A code, size reduction of 68% is achieved, with a 2.2% execution speedup of the system-mode execution time, for a case study based on the MediaBench II benchmark suite.", "authors": ["Dominique Chanet", "Javier Cabezas", "Enric Morancho", "Nacho Navarro", "Koen De Bosschere"], "n_citation": 50, "title": "Linux kernel compaction through cold code swapping", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "ac7b21ca-7221-44bf-ad06-c4e9d5623997"}
{"abstract": "The increasing demand for streaming video applications on the Internet motivates the problem of building an adaptive rate control scheme for the time-varying network condition. The time-varying available bandwidth and latency make the real-time streaming applications difficult to achieve high bandwidth utilization and video quality at receivers. MPEG-4 is a widely-used streaming protocol and a MPEG-4 video streaming system should be able to provide an adaptive rate control mechanism to satisfy these changing conditions. In this paper we analyze the problem of transmitting MPEG-4 video streaming over IP network with some well-known TCP-friendly rate control protocols. This paper is focused on solving the challenging issues in application layer, compression layer to reduce the frame dropping rate. With this aim, we provide a solution called SARS (Smoothed and Adaptive Rate-control Scheme) in compression layer and application layer for an MPEG-4 video streaming system and discuss the characteristics of its components. Furthermore, we assess the effects of SARS by simulation with the well-known solution for MPEG-4 transmission.", "authors": ["Eric Hsiao-Kuang Wu", "Ming-I Hsieh", "Chung-Yuan Knight Chang"], "n_citation": 0, "title": "SARS : A linear source model based adaptive rate-control scheme for TCP-friendly real-time MPEG-4 video streaming", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b612aaa7-a3b5-434b-b3f5-b4c8f4896fe3"}
{"abstract": "The security of key agreement protocols has traditionally been notoriously hard to establish. In this paper we present a modular approach to the construction of proofs of security for a large class of key agreement protocols. By following a modular approach to proof construction, we hope to enable simpler and less error-prone analysis and proof generation for such key agreement protocols. The technique is. compatible with Bellare-Rogaway style models as well as the more recent models of Bellare et al. and Canetti and Krawczyk. In particular, we show how the use of a decisional oracle can aid the construction of proofs of security for this class of protocols and how the security of these protocols commonly reduces to some form of Gap assumption.", "authors": ["Caroline Kudla", "Kenneth G. Paterson"], "n_citation": 117, "title": "Modular security proofs for key agreement protocols", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b672de75-65af-4fd7-ac81-1c389c763b29"}
{"abstract": "Provable security usually makes the assumption that a source of perfectly random and secret data is available. However, in practical applications, and especially when smart cards are used, random generators are often far from being perfect or may be monitored using probing or electromagnetic analysis. The consequence is the need of a careful evaluation of actual security when idealized random generators are implemented. In this paper, we show that Esign signature scheme, like many cryptosystems, is highly vulnerable to so called partially known nonces attacks. Using a 1152-bit modulus, the generation of an Esign signature requires to draw at random a 768-bit integer. We show that the exposure of only 8 bits out of those 768 bits, for 57 signatures, is enough to recover the whole secret signature key in a few minutes. It should be clear that we do not cryptanalyze a good implementation of Esign nor do we find a theoretical flaw. However, our results show that random data used to generate signatures must be very carefully produced and protected against any kind of exposure, even partial. As an independent result, we show that the factorization problem is equivalent to the existence of an oracle returning the most or least significant bits of S mod p, on input S randomly chosen in Z pq .", "authors": ["Pierre-Alain Fouque", "Nick Howgrave-Graham", "G. Martinet", "Guillaume Poupard"], "n_citation": 0, "title": "The insecurity of Esign in practical implementations", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b8862c40-3d03-458f-aa85-bbef19ba3634"}
{"abstract": "In this paper, we propose a method called LocalRank to rank web pages by integrating the web and a user database containing information on a specific geographical area. LocalRank is a rank value for a web page to assess its relevance degree to database entries considering geographical locality and its popularity on a local web space. In our method, we first construct a linked graph structure using entries contained in the database. The nodes of this graph consist of database entries and their related web pages. The edges in the graph are composed of semantic links including geographical links between these nodes, in addition to conventional hyperlinks. Then a link analysis is performed to compute a LocalRank value for each node. LocalRank can represent user's interest since this graph effectively integrates the web and the user database. Our experimental results for a local restaurant database shows that local web pages related to the database entries are highly ranked based on our method.", "authors": ["Jianwei Zhang", "Yoshiharu Ishikawa", "Sayumi Kurokawa", "Hiroyuki Kitagawa"], "n_citation": 50, "title": "LocalRank : Ranking web pages considering geographical locality by integrating web and databases", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b8c82434-1a24-48fa-87f6-1b82946d571b"}
{"abstract": "The problem of managing the evolution of complex and large software systems is well known. Evolution implies the reuse and modification of existing software artifacts, and this means that the related knowledge must be documented and maintained. This paper focuses on the evolution of software product families, although the same principles apply in other software development environments as well. We describe our experience gained in a case study recovering a family of six software products. We give an overview of the case study, and provide lessons learned, implicit assumptions reconstructed during the case study, and some rules we think are generally applicable. Our experience indicates that organizing architectural knowledge is a difficult task. To properly serve the various uses of this knowledge, it needs to be organized along different dimensions and tools are required. Our experience also indicates that, next to variability explicitly designed into the product family, a variation creep is caused by different, and evolving, technical and organizational environments of the products. We propose explicitly modeling invariabilities, next to variabilities, in software product lines to get a better grip on this variation creep.", "authors": ["Patricia Lago", "Hans van Vliet"], "n_citation": 0, "title": "Observations from the recovery of a software product family", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "bdd2e8e3-1b59-46b0-b91e-44d02441dbe3"}
{"abstract": "Most documents available over the web confirm to the HTML specification. They are intended to be human readable through a web browser and thus are constructed following some common conventions. Based on such common conventions, the Conceptual Model for HTML was proposed recently to automatically capture the hierarchical structure within web documents. However, certain key semantic information about the contents in the documents, which are obvious to human, are often omitted. As a result, web data processing, manipulation and integration are still quite difficult. In this paper, we discuss how to extend the Conceptual Model for HTML to capture the intended semantics of the HTML documents. We show that with the new constructs introduced, using an Intelligent Wrapper, and limited human interaction, semantics can be transferred from human into the Extended Conceptual Model so that further meaningful processing, manipulation and integration of web documents become possible.", "authors": ["Mengchi Liu"], "n_citation": 0, "title": "Capturing semantics in HTML documents", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c2489194-f4ed-4662-b45c-f9e791f01696"}
{"abstract": "Reasoning with cases has been a central focus of work in Artificial Intelligence and Law since the field began in the late eighties. Reasoning with cases is a distinctive feature of legal reasoning and is of interest because such reasoning is both inherently defeasible, and because it is an example of practical reasoning in that it aims to provide a rational basis for a choice rather than to deduce some conclusion from premises. As reasoning with cases has developed, it has moved beyond techniques for matching past cases to the current situation to consider how arguments for a position are constructed on the basis of past cases. Recently it has been argued that this should be seen as a process involving the construction, evaluation and application of theories grounded in the phenomena presented by the past cases. Our aim is to develop and refine this idea, with the ultimate goal of building a system which is able to reason with cases in this manner. This paper describes the implementation of a theory construction tool (CATE) to aid in the construction and evaluation of theories to explain the decisions obtained in legal cases, so as to give an understanding of a body of case law. CATE gives a rapid way of creating and testing different theories. Use of CATE is illustrated by showing the construction of alternative theories in a small case study. CATE is useful in itself for anyone wishing to explore their understanding of a set of cases, such as lawyers practising in the domain and knowledge engineers tasked with constructing a rule based system in the domain. We also believe that it offers good prospects for automating the process of theory construction.", "authors": ["Alison Chorley", "Trevor J. M. Bench-Capon"], "n_citation": 0, "title": "Support for constructing theories in case law domains", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c678f35f-2c5b-486f-bcd3-7ff1bb15e7f7"}
{"abstract": "This paper presents a new hybrid traffic engineering routing algorithm for bandwidth guaranteed traffic. Former traffic engineering routing algorithms mainly optimize one of the three objectives: minimizing the hop count, balancing the load and minimizing the interference between source-destination pairs. But usually there is a tradeoff among these three factors. Single objective optimizations can't get the best performance. The main contribution of this paper is a new hybrid approach to consider the three objectives together. From the simulation results, the proposed algorithm has better performance than former algorithms.", "authors": ["Zhaowei Meng", "Jinshu Su", "Stefano Avallone"], "n_citation": 0, "title": "A new hybrid traffic engineering routing algorithm for bandwidth guaranteed traffic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c6b7e60a-21bb-48a9-96d3-6b7df0f073c1"}
{"abstract": "The number of applications that are downloaded from the Internet and executed on-the-fly is increasing every day. Unfortunately, not all of these applications are benign, and, often, users are unsuspecting and unaware of the intentions of a program. To facilitate and secure this growing class of mobile code, Microsoft introduced the.NET framework, a new development and runtime environment where machine-independent byte-code is executed by a virtual machine. An important feature of this framework is that it allows access to native libraries to support legacy code or to directly invoke the Windows API. Such native code is called unmanaged (as opposed to managed code). Unfortunately. the execution of unmanaged native code is not restricted by the.NET security model, and, thus, provides the attacker with a mechanism to completely circumvent the framework's security mechanisms. The approach described in this paper uses a sandboxing mechanism to prevent an attacker from executing malicious, unmanaged code that is not permitted by the security policy. Our sandbox is implemented as two security layers, one on top of the Windows API and one in the kernel. Also, managed and unmanaged parts of an application are automatically separated and executed in two different processes. This ensures that. potentially unsafe code can neither issue system calls not permitted by the.NET security policy nor tamper with the memory of the .NET runtime. Our proof-of-concept implementation is transparent to applications and secures unmanaged code with a generally acceptable performance penalty. To the best of our knowledge, the presented architecture and implementation is the first solution to secure unmanaged code in NET.", "authors": ["Patrick Klinkoff", "Christopher Kruegel", "Engin Kirda", "Giovanni Vigna"], "n_citation": 50, "title": "Extending .NET Security to Unmanaged Code", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cbcdbf56-9e08-4835-8a3e-50b3550ebdc8"}
{"abstract": "In this paper, we propose an automated SuperSQL query formulating method based on statistical characteristics of data so that the end-users can obtain the desired information more easily without burden of web application development side. SuperSQL is an extension of SQL to generate various kinds of structured presentations and documents. In our method, first, web producers prepare a provisional SQL query and send it to a relational database. Next, the automated algorithm formulates a SuperSQL query based on statistical information of the query result. Finally the resulting SuperSQL query is executed to return the categorized table to end-users. Experimental results demonstrate that the implemented system enables web producers to construct data-intensive web sites, which have better structure with respect to the recognition of the data by end-users.", "authors": ["Jun Nemoto", "Motomichi Toyama"], "n_citation": 0, "title": "Automated SuperSQL query formulation based on statistical characteristics of data", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cc1b56e2-0550-4480-bc3c-1e9d45cfb88a"}
{"abstract": "Image-based lighting (IBL) is the process of illuminating scenes and objects with images of light from the real world. To generate a high quality synthesized image, consistency of geometry and illumination has to be taken into account. In general, lighting design for realistically rendering synthetic objects into a real-world scene, is labor intensive process and not always successful. Though many researches on IBL have been presented up to now, most of them assumed static situations that light sources and the objects were not moved. This paper presents a novel algorithm that integrates synthetic objects in the real photographs by using the global illumination model and HDR (High Dynamic Range) radiance map. We identify the camera positions and the light types, and then construct 3D illumination environment of the scene. The proposed method makes it possible to handle dynamic scenes and generate photo-realistic images.", "authors": ["Yong-Ho Hwang", "Hyun-Ki Hong", "Jun-Sik Kwon"], "n_citation": 0, "title": "Image-based relighting in dynamic scenes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ce47ae1d-8646-4abd-a0b7-d5234428d15a"}
{"abstract": "When an organization faces the need of integrating some workflow-related activities in its information system, it becomes necessary to have at hand some well-defined informational model to be used as a framework for determining the selection criteria onto which the requirements of the organization can be mapped. Some proposals exist that provide such a framework, remarkably the WfMC reference model, but they are designed to be applicable when workflow tools are selected independently from other software, and departing from a set of well-known requirements. Often this is not the case: workflow facilities are needed as a part of the procurement of a larger, composite information system and therefore the general goals of the system have to be analyzed, assigned to its individual components and further detailed. We propose in this paper the MULTSEC method in charge of analyzing the initial goals of the system, determining the types of components that form the system architecture, building quality models for each type and then mapping the goals into detailed requirements which can be measured using quality criteria. We develop in some detail the quality model (compliant with the ISO/IEC 9126-1 quality standard) for the workflow type of tools; we show how the quality model can be used to refine and clarify the requirements in order to guarantee a highly reliable selection result; and we use it to evaluate two particular workflow solutions available in the market (kept anonymous in the paper). We develop our proposal using a particular selection experience we have recently been involved in, namely the procurement of a document management subsystem to be integrated in an academic data management information system for our university.", "authors": ["Juan Pablo Carvallo", "Xavier Franch", "Carme Quer", "Nuria Rodr\u00edguez"], "n_citation": 0, "title": "A framework for selecting workflow tools in the context of composite information systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d2d36360-6afb-4c52-9735-6f0383e03d6c"}
{"abstract": "Much work has been done on characterizing the workload of a database system. Previous studies focused on providing different types of statistical summaries, and modeling the run-time behavior on the physical resource level. In this paper, we focus on characterizing the database system's workload from the view of database users. We use user access patterns to describe how a client application or a group of users accesses the data of a database system. The user access patterns include a set of user access events that represent the format of the queries and a set of user access graphs that represent the query execution orders. User access patterns can help database administrators tune the system, help database users optimize queries, and help to predict and cache future queries. In this paper, we present several approaches to using user access patterns to improve system performance, and report some experimental results.", "authors": ["Qingsong Yao", "Aijun An"], "n_citation": 0, "title": "Characterizing database user's access patterns", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d440e3f9-1ee8-403c-8df8-d4d40dce5ba0"}
{"authors": ["Bart Mennink", "Reza Reyhanitabar"], "n_citation": 0, "title": "Security of Full-State Keyed Sponge and Duplex: Applications to Authenticated Encryption", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "e029c922-c759-4169-9515-5e600c824be6"}
{"abstract": "A two-stage color quantization method is proposed in this paper. At the first stage, a palette selection scheme suitable to the quantization level requirement is chosen and an initial palette is selected. At the second stage, a fast LBG algorithm is adopted to iteratively refine the palette. Experimental results show that this approach is superior to most of the prevalent methods in terms of quantization distortion measured by the MSE metric.", "authors": ["Xin Zhang", "Zuman Song", "Yunli Wang", "Hui Wang"], "n_citation": 0, "title": "Color quantization of digital images", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e18b09e8-eddb-455c-af06-21bbc7d9cd1a"}
{"abstract": "Due to the sensitive data contained in Data Warehouses (DWs), it is essential to specify security measures from the early stages of the DWs design and enforce them. In this paper, we will present a UML profile to represent multidimensional and security aspects of our conceptual modeling. Our approach proposes the use of UML packages in order to group classes together into higher level units creating different levels of abstraction, and therefore, simplifying the final model. Furthermore, we present an extension of the relational model to consider security and audit measures represented in the conceptual modeling. To accomplish this, we based on the Relational Package of the Common Warehouse Metamodel (CWM) and extend it to properly represent all security and audit rules defined in the conceptual modeling of DWs. Finally, we will show an example to illustrate the applicability of our proposal.", "authors": ["Rodolfo Villarroel", "Emilio Soler", "Eduardo Fern\u00e1ndez-Medina", "Juan Trujillo", "Mario Piattini"], "n_citation": 0, "title": "Using UML Packages for Designing Secure Data Warehouses", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "efa4fced-60ea-4ee3-812f-6c6ea4b40d8c"}
{"abstract": "Resolving heterogeneity among various protein data sources is a crucial problem if we want to gain more information about proteomics process. Information from multiple protein databases like PDB, SCOP, and UniProt need to integrated to answer user queries. Issues of Semantic Heterogeneity haven't been addressed so far in Protein Informatics. This paper outlines protein data source composition approach based on our existing work of Protein Ontology (PO). The proposed approach enables semi-automatic interoperation among heterogeneous protein data sources. The establishment of semantic interoperation over conceptual framework of PO enables us to get a better insight on how information can be integrated systematically and how queries can be composed. The semantic interoperation between protein data sources is based on semantic relationships between concepts of PO. No other such generalized semantic protein data interoperation framework has been considered so far.", "authors": ["Amandeep S. Sidhu", "Tharam S. Dillon", "Elizabeth Chang"], "n_citation": 0, "title": "Integration of protein data sources through PO", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fb3587b1-a789-47d5-b245-85752c1e41c0"}
{"abstract": "Image completion is a method to fill the missing portions of an image caused by the removal of one or more foreground or background elements. In this paper a novel image completion algorithm is proposed for removing significant objects from natural images or photographs. The completion is realized in the following three steps. First, a gradient-based model is presented to determine the gradient-patch filling order. This step is critical because a better filling order can improve the continuation of image structures. Second, we implement the gradient-patch update strategy by measuring the exponential distance between the source patch and the target one in gradient domain. In order to find a better patch matching and propagating algorithm, we incorporate the gradient and color information together to determine the target patch. Third, a complete image is achieved by solving the Poisson equation with the updated image gradient map. Some experimental results on real-scene photographs are given to demonstrate both the efficiency and image equality of our novel method.", "authors": ["Jianbing Shen", "Xiaogang Jin", "Chuan Zhou"], "n_citation": 0, "title": "Gradient based image completion by solving poisson equation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fda0fdd2-fb52-4ffd-b422-838594f99c35"}
{"abstract": "HIICA(Highly-improved Intra CA) proposed in this paper is internal CA which highly improved by using OCSP(Online Certificate Status Protocol)and LDAP(Lightweight Directory Access Protocol). It provides more security and confidentiality than external CA. As HIICA is provided with certification policy from PCA(Policy Certificate Authority) and sets an special environment like subscriber registration process, it reduces additional expense and strengthens the security at the certificate request process through HIICA agent. Therefore, secure electronic payment system is optimized through HIICA design that removed the existing CA complex process and unsecure elements.", "authors": ["Byung-Kwan Lee", "Chang-min Kim", "Dae-Won Shin", "Seung-Hae Yang"], "n_citation": 0, "title": "A HIICA(Highly-Improved Intra CA) Design for M-Commerce", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fe0c8318-cdde-4247-b008-c08c798925f1"}
{"abstract": "A range of agent implementation technologies are reviewed according to five user-based criteria and via a comparison with object-oriented programming. The comparison with OO shows that some parts of object technology are a candidate implementation technique for some parts of agent systems. However, many other non-object-based implementation techniques may be just as useful. Also, for agents with mentalistic attitudes, the high-level specification of agent behavior requires numerous concepts outside the object paradigm; e.g. plans, communication, intentions, roles, and teams.", "authors": ["Tim Menzies", "Adrian R. Pearce", "Clinton Heinze", "Simon Goss"], "n_citation": 0, "title": "What is an agent and why should I care", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "05c16b45-373f-495f-9eca-9e04f5af33c2"}
{"abstract": "This paper presents the integration of linguistic knowledge in learning semantic user profiles able to represent user interests in a more effective way with respect to classical keyword-based profiles 1 . Semantic profiles are obtained by integrating a naive Bayes approach for text categorization with a word sense disambiguation strategy based on the WordNet lexical database (Section 2). Semantic profiles are exploited by the conference participant advisor service in order to suggest papers to be read and talks to be attended by a conference participant. Experiments on a real dataset show the effectiveness of the service (Section 3).", "authors": ["Marco Degemmis", "Pasquale Lops", "Pierpaolo Basile"], "n_citation": 50, "title": "An Intelligent Personalized Service for Conference Participants", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1127906b-98fc-49da-b08d-bd295e9d8041"}
{"abstract": "This paper proposes an adjusted-Q digital graphic equalizer employing the opposite filters. A method for designing the proposed equalizer is also presented. In the proposed adjusted-Q equalizer, we adjust the Q-factor of the equalizer filter depending on the gain, yielding an improved equalizer performance. Also, by increasing the Q-factor of the opposite filters gracefully with increasing gain, the inter-band interference can be reduced effectively in all frequency range in consideration. We shall show that the proposed equalizer can reproduce the user's gain setting faithfully.", "authors": ["Yonghee Lee", "Rin-Chul Kim", "Googchun Cho", "Seong Jong Choi"], "n_citation": 0, "title": "An adjusted-Q digital graphic equalizer employing opposite filters", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1b045ad8-d5f3-43fa-b8b4-ff50ed1a6f89"}
{"abstract": "Personnel rosters are typically constructed for a medium-term period under the assumption of a deterministic operating environment. However, organisations usually operate in a stochastic environment and are confronted with unexpected events in the short term. These unexpected events affect the workability of the personnel roster and need to be resolved efficiently and effectively. To facilitate this short-term recovery, it is important to consider robustness by adopting proactive scheduling strategies during the roster construction. In this paper, we discuss a proactive strategy that maximises the employee substitutability value in a personnel shift scheduling context. We propose a problem-specific population-based approach with local and evolutionary search heuristics to solve the resulting non-linear personnel shift scheduling problem and obtain a medium-term personnel shift roster with a maximised employee substitutability value. Detailed computational experiments are presented to validate the design of our heuristic procedure and the selection of the heuristic operators.", "authors": ["Jonas Ingels", "Broos Maenhout"], "n_citation": 0, "references": ["147af4e5-690b-4990-8ad8-f1feb48c8e09", "1dbff32d-d96d-443f-9e7b-acea5ac8f325", "2222da71-d983-4ca0-9d16-dc8631da74ea", "2b88fe4b-c660-4f6a-82a1-a294ee6afe9b", "2f18e458-d581-40c6-ad88-e66538b5fc60", "3511f2f5-9987-41ec-8107-88b64bcf71f6", "489b0409-1b56-426b-b71f-ecb07ef8aa46", "5dcb4429-fc10-4ec0-a8d6-3a6f4befa181", "6728bd67-4658-49e2-9031-2cbbeaf3ae04", "7530f687-08f6-45c2-815a-1dc089071731", "7fc1ac34-6a14-4198-93fe-e5f3305d486f", "8066b2ad-5d0a-48d6-b7aa-e11280a284cd", "aa06af3d-981d-4a56-a51e-a2fea8ef6963", "b55a5516-63f7-4be4-9cd4-63b942cb5399", "be10ea8a-eb68-4e33-a2a9-6ba3ca62adf8", "cb35b78b-79b6-4abe-bb06-bb4a5e417933", "d6b07288-2e01-4b56-b391-b01e9dfa061c", "dbe5d99d-7816-4097-9ca7-4dbeb829fce0", "e42d9020-704d-4f84-b10f-981bfb6c1bc8", "e9b123ac-5865-4fe0-96cc-e3b90d5597f9", "fdb73909-e6ff-490d-bc57-a00f40e7808d"], "title": "A Memetic Algorithm to Maximise the Employee Substitutability in Personnel Shift Scheduling", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "1c2c55bd-c2b7-444b-933a-997469d79c45"}
{"abstract": "The paper discusses how distributed logic programming can be used to define and control the hand gestures of embodied agents in virtual worlds, by using the STEP language as an interface between the constructs of logic programming and the humanoid model defined in Virtual Reality Modelling Language (VRML). By using this framework, different gesture dictionaries can be defined and variants of a hand gesture, according to dynamically changing factors, can be generated on the fly. The approach is tested on the demanding demonstrator of conducting, providing experience, also on time performance of the animation.", "authors": ["Zsofia Runkay", "Zhisheng Huang", "Anton Eli\u00ebns"], "n_citation": 0, "title": "Gestures for embodied agents with logic programming", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "22d932c0-ee6d-4ede-9733-f505ee0305df"}
{"abstract": "This paper proposes a simple pseudorandom number generator [PRNG] by using optimal normal basis. It is well known that the squaring and multiplication in finite field with optimal normal basis is very fast and the basis can be transformed to a canonical form. The suggested PRNG algorithm combines typical multiplications and exclusive-or bit operations, both operations can be easily implemented. It is shown that the algorithm passes all terms of the Diehard and the ENT tests for long sequences. This algorithm can be applied in various applications such as financial cryptography.", "authors": ["Injoo Jang", "Hyeong Seon Yoo"], "n_citation": 0, "title": "Pseudorandom Number Generator Using Optimal Normal Basis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3b01bd0e-4b73-453d-87fe-335e664e7f0f"}
{"authors": ["Alex Biryukov", "Mario Lamberger", "Florian Mendel", "Ivica Nikoli\u0107"], "n_citation": 0, "title": "Second-Order Differential Collisions for Reduced SHA-256", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "417e0377-8691-4f1f-8a9b-d5f7b20695ab"}
{"abstract": "Semantic query optimisation is the process by which a user query is transformed into a set of alternative queries each of which returns the same answer as the original. The most efficient of these alternatives is then selected, for execution, using standard cost estimation techniques. The query transformation process is based on the use of semantic knowledge in the form of rules which are generated either during the query process itself or are constructed according to defined heuristics. Previous research has tended to focus on constructing rules applicable to single relations and does not take advantage of the additional semantic knowledge, inherent in most databases, associated with relational joins. Our paper seeks to address this weakness by showing how the rule derivation process may be extended to the generation of inter-relational rules using an approach based on inductive learning.", "authors": ["Barry G. T. Lowden", "Jerome Robinson"], "n_citation": 0, "title": "Constructing inter-relational rules for Semantic query optimisation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "43cee552-1516-4a66-80da-0e2948021542"}
{"abstract": "In this paper we study the problem of declustering two-dimensional datasets with replication over parallel devices to improve range query performance. The related problem of declustering without replication has been well studied. It has been established that strictly optimal declustering schemes do not exist if data is not replicated. In addition to the usual problem of identifying a good allocation, the replicated version of the problem needs to address the issue of identifying a good retrieval schedule for a given query. We address both problems in this paper. An efficient algorithm for finding a lowest cost retrieval schedule is developed. This algorithm works for any query, not just range queries. Two replicated placement schemes are presented - one that results in a strictly optimal allocation, and another that guarantees a retrieval cost that is either optimal or 1 more than the optimal for any range query.", "authors": ["Keith B. Frikken", "Mikhail Atallah", "Sunil Prabhakar", "Rei Safavi-Naini"], "n_citation": 50, "title": "Optimal parallel I/O for range queries through replication", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "45e61865-acd5-4810-b5fd-3004e906f549"}
{"abstract": "Streaming applications, such as environment monitoring and vehicle location tracking require handling high volumes of continuously arriving data and sudden fluctuations in these volumes while efficiently supporting multi-dimensional historical queries. The use of the traditional database management systems is inappropriate because they require excessive number of disk I/O in continuously updating massive data streams. In this paper, we propose DCF (Data Stream Clustering Framework), a novel framework that supports efficient data stream archiving for streaming applications. DCF can reduce a great amount of disk I/O in the storage system by grouping incoming data into clusters and storing them instead of raw data elements. In addition, even when there is a temporary fluctuation in the amount of incoming data, it can stably support storing all incoming raw data by controlling the cluster size. Our experimental results show that our approach significantly reduces the number of disk accesses in terms of both inserting and retrieving data.", "authors": ["Kyungmin Cho", "Sungjae Jo", "Hyukjae Jang", "Su Myeon Kim", "Junehwa Song"], "n_citation": 50, "title": "DCF : An efficient data stream clustering framework for streaming applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4eb92af0-ca43-442d-b9eb-cae7aa302329"}
{"abstract": "Metamorphosis or morphing is a technique to accomplish a gradual shape transformation between a source object and a target object. This technique is usually used for animation, i.e., to create some special effects, but it can be also employed as a modeling tool where some existing shapes are combined in order to obtain new shapes. Several interactive or automatic morphing techniques were designed in the past but always with a specific object representation in mind. In this paper we propose a generic framework for 3D morphing, that allows multi-resolution surface extraction and provides a hybrid (surface-volume) dynamic representation which is suitable for encoding the static models, for supporting different morphing methods, and for encoding the intermediate 3D frames (generated during the morphing process) in a MPEG-4 stream.", "authors": ["Ioan Alexandru Salomie", "Rudi Deklerck", "Dan C. Cernea", "Aneta Markova", "Adrian Munteanu", "Peter Schelkens", "Jan Cornelis"], "n_citation": 0, "title": "Special effects : Efficient and scalable encoding of the 3D metamorphosis animation with MESHGRID", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4efab7b3-46c6-4ffe-8c3c-d7e14d3c488d"}
{"abstract": "We propose asymmetric encryption schemes for which all ciphertexts are valid (which means here reachable: the encryption function is not only a probabilistic injection, but also a surjection). We thus introduce the Full-Domain Permutation encryption scheme which uses a random permutation. This is the first IND-CCA cryptosystem based on any trapdoor one-way permutation without redundancy, and more interestingly, the bandwidth is optimal: the ciphertext is over k more bits only than the plaintext, where 2 -k  is the expected security level. Thereafter, we apply it into the random oracle model by instantiating the random permutation with a Feistel network construction, and thus using OAEP. Unfortunately, the usual 2-round OAEP does not seem to be provably secure, but a 3-round can be proved IND-CCA even without the usual redundancy m\u22250 k 1, under the partial-domain one-wayness of any trapdoor permutation. Although the bandwidth is not as good as in the random permutation model, absence of redundancy is quite new and interesting: many implementation risks are ruled out.", "authors": ["Duong Hieu Phan", "David Pointcheval"], "n_citation": 0, "title": "Chosen-ciphertext security without redundancy", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "4f65f751-ee7f-49df-b339-5673d3156a38"}
{"authors": ["Vesselin Velichkov", "Nicky Mouha", "Christophe De Canni\u00e8re", "Bart Preneel"], "n_citation": 0, "title": "UNAF: A Special Set of Additive Differences with Application to the Differential Analysis of ARX", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "5844638d-086e-430c-a89a-073559a5b67b"}
{"abstract": "A method is proposed to detect useful directional feature points other than comer points considering that the number of comer points may not be sufficient in a scene. This is achieved by directional analysis of properties of image points by virtue of the proposed gradient operators with different direction topologies. A matching criterion is also proposed to find the initial correspondence by using the feature vectors that are acquired from the results of directional analysis. For the purpose of improving the final correspondence, four constraints are employed in the system to seek and refine the correspondence.", "authors": ["Wen-Hao Wang", "Fu-Jen Hsiao", "Tsuhan Chen"], "n_citation": 0, "title": "Directional feature detection and correspondence", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "624663d5-0615-48c2-8b18-cbec9801c8a2"}
{"abstract": "In this paper, we introduce a conceptually very simple and demonstrative algorithm for finding small solutions (x, y) of ax + y = c mod N, where gcd(a, N) = 1. Our new algorithm is a variant of the Euclidian algorithm. Unlike former methods, it finds a small solution whenever such a solution exists. Further it runs in time O((log N) 3 ), which is the same as the best known previous techniques, e.g. lattice-based solutions. We then apply our algorithm to RSA-OAEP and RSA-Paillier to obtain better security proofs. We believe that there will be many future applications of this algorithm in cryptography.", "authors": ["Kaoru Kurosawa", "Katja Schmidt-Samoa", "Tsuyoshi Takagi"], "n_citation": 0, "title": "A complete and explicit security reduction algorithm for RSA-based cryptosystems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "76775342-8f5f-40d9-b0b2-4e34d5fe01ce"}
{"abstract": "We present a new lossless geometry coding method for 3D triangle-quad meshes, Adaptive Vertex Chasing. Previous localized geometry coding methods have demonstrated better compression ratios than the global approach but they are considered hard to use in practice. It is because a proper linear quantization of the local range with three inputs is time-consuming, totally dependent on a user's trials and errors. Our new localized scheme replaces this quantization with an adaptive subdivision of the range with only one input, a subdivision level. The deeper level a user choose, the closer to the original the mesh will be restored. We also present a connectivity coder improved upon the current leading Angle-Analyzer's with a context-modeling. Without losing the current level of efficiency, our new coder provides simple and systematic way to control the balance between distortions and the bit-rates.", "authors": ["Haeyoung Lee", "Sujin Park"], "n_citation": 50, "title": "Adaptive vertex chasing for the lossless geometry coding of 3D meshes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "776dd42b-eefc-4255-af1c-63a68954af86"}
{"abstract": "We propose an intelligent supply chain system collaborating with customer relationship management system in order to assess change in a supply partner's capability over a period of time. The system embeds machine learning methods and is designed to evaluate a partner's supply capability that can change over time and to satisfy different procurement conditions across time periods. We apply the system to the procurement and management of the agricultural industry.", "authors": ["Gye Hang Hong", "Sung Ho Ha"], "n_citation": 0, "title": "Developing an Intelligent Supplier Chain System Collaborating with Customer Relationship Management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7be76446-6528-45e9-a456-6a3e54227b6b"}
{"abstract": "This work proposes an efficient attack on the Shrinking Generator based on its characterization by means of Linear Hybrid Cellular Automata. The algorithm uses the computation of the characteristic polynomials of specific sub-automata and the generation of the Galois field associated to one of the Linear Feedback Shift Registers components of the generator. Both theoretical and empirical evidences for the effectiveness of the attack are given. The two main advantages of the described cryptanalysis are the determinism of bits prediction and the possible application of the obtained results to different generators.", "authors": ["Pino Caballero-Gil", "Amparo F\u00faster-Sabater"], "n_citation": 0, "title": "Practical Attack on the Shrinking Generator", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "846ab570-6a8d-4377-ab7e-1787b09f2e01"}
{"abstract": "Global marketplace and intense competition in the business environment lead organizations to focus on selecting the best R&D project portfolio among available projects using their scarce resources in the most effective manner. This happens to be a sine qua non for high technology firms to sharpen their competitive advantage and realize long-term survival with sustainable growth. To accomplish that, firms should take into account both the uncertainty inherent in R&D using appropriate valuation techniques accounting for flexibility in making investment decisions and all possible interactions between the candidate projects within an optimization framework. This paper provides a fuzzy optimization model for dealing with the complexities and uncertainties regarding the construction of an R&D project portfolio. Real options analysis, which accounts for managerial flexibility, is employed to correct the deficiency of traditional discounted cash flow valuation that excludes any form of flexibility. An example is provided to illustrate the proposed decision approach.", "authors": ["E. Ertugrul Karsak"], "n_citation": 3, "title": "A Generalized Fuzzy Optimization Framework for R&D Project Selection Using Real Options Valuation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "84fff8b1-f42a-4671-ab13-6c624341809b"}
{"abstract": "The  Communications  Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of  Communications , we'll publish selected posts or excerpts.   twitter  Follow us on Twitter at http://twitter.com/blogCACM  http://cacm.acm.org/blogs/blog-cacm   Mark Guzdial considers the steps needed to reach the goal of CS for All, while Robin K. Hill ponders the aesthetics of programming.", "authors": ["Mark Guzdial", "Robin K. Hill"], "n_citation": 0, "title": "The slow evolution of CS for all, the beauty of programs", "venue": "Communications of The ACM", "year": 2017, "id": "88d32dc3-4de5-4836-96e6-eaaf0d4b6a64"}
{"abstract": "The future System-on-Chip (SoC) design will integrate a variety of intellectual properties (IPs). The clocked bus architectures to interconnect the IPs under the deep submicron technology suffer from problems related with the clock distribution, the synchronization of all IPs, the long arbitration delay and the limited bandwidth. These problems can be resolved by adopting new interconnection architecture such as Network-on-Chip (NoC) or the asynchronous design method. In this paper, a design methodology for an asynchronous switch based on butterfly fat-tree topology is proposed. The wormhole switching technique is adopted to reduce the latency and the buffer size. The source-based routing mechanism and the output buffering strategy are used to reduce the switch design cost and increase the performance.", "authors": ["Min-Chang Kang", "Eun-Gu Jung", "Dongsoo Har"], "n_citation": 0, "title": "Design of an asynchronous switch based on butterfly fat-tree for network-on-chip applications", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8a70fe36-a597-409a-b5d6-49c26bbd98e6"}
{"abstract": "In this paper we compare two approaches for implementing Security and Privacy systems in Cyberspace: a structured approach, such as done in Mokum, where access is governed by structure (of the classes), and two principles: the epistemic and the ontologic principle. The second approach is based on the use of capabilities, such as provided by ERP systems.", "authors": ["R. V. De Riet", "Wouter Janssen", "Martin S. Olivier", "Radu Serban"], "n_citation": 0, "title": "A comparison of two architectures for implementing Security and Privacy in cyberspace", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "8e2d4f66-ff7b-416f-99f5-18d83fac550a"}
{"abstract": "In this paper we investigate the use of SAT technology for solving constraint problems. In particular, we solve many instances of several common benchmark problems for CP with different SAT solvers, by exploiting the declarative modelling language NPSPEC, and SPEC2SAT, an application that allows us to compile NPSPEC specifications into SAT instances. Furthermore, we start investigating whether some reformulation techniques already used in CP are effective when using SAT as solving engine. We present encouraging experimental results in this direction, showing that this approach can be appealing.", "authors": ["Marco Cadoli", "Toni Mancini", "Fabio Patrizi"], "n_citation": 0, "title": "SAT as an Effective Solving Technology for Constraint Problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "939e25f6-6243-4945-a622-7bf6e5972c04"}
{"abstract": "A Crossover Router (CR) based QoS provisioning mechanism under an enhanced Fast Handovers for Mobile IPv6 (FMIPv6) architecture has been proposed to provide better performance for multimedia applications in Mobile IPv6 (MIPv6) networks. In the proposal we extend the FBU and HI messages to notify the QoS requirement of Mobile Node (MN). Advance reservations along the possible future-forwarding paths are performed only between CR and MN to reduce reservation hops and signaling delays. In this paper we present the detailed performance evaluation results of this scheme compared with Mobile RSVP (MRSVP) protocol, using the network simulator (NS-2). In the simulations RSVP signaling cost, total bandwidth requirements, RSVP signaling and data packet delay in different scenarios of both schemes were considered. Results show that the proposed scheme for QoS guarantee has lower RSVP signaling cost and delay, as well as less bandwidth requirements in comparison with MRSVP. Furthermore, fluctuation and peak values of data packet delays in different scenarios are much lower.", "authors": ["Zheng Wan", "Zhengyou Wang", "Zhijun Fang", "Weiming Zeng", "Shiqian Wu"], "n_citation": 0, "title": "Evaluation of a crossover router based QoS mechanism in fast mobile IPv6 networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9a69e6f2-d8d2-4b69-8cce-ad13d234f2a7"}
{"abstract": "The most efficient collision attacks on members of the SHA family presented so far all use complex characteristics which were manually constructed by Wang et al. In this report, we describe a method to search for characteristics in an automatic way. This is particularly useful for multi-block attacks, and as a proof of concept, we give a two-block collision for 64-step SHA-1 based on a new characteristic. The highest number of steps for which a SHA-1 collision was published so far was 58. We also give a unified view on the expected work factor of a collision search and the needed degrees of freedom for the search, which facilitates optimization.", "authors": ["Christophe De Canni\u00e8re", "Christian Rechberger"], "n_citation": 267, "title": "Finding SHA-1 Characteristics: General Results and Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9b4fdabc-4c33-4fd7-945e-aef39a22d2c4"}
{"abstract": "In this paper, we propose an efficient classifier fusion for face recognition under varying illumination environments by taking classifier fusion's advantage of environment context identification. Adaptation to dynamically changing environments is very important since advanced applications become pervasive and ubiquitous. The proposed classifier fusion system, called BCF (Bayesian based classifier fusion), adopts the concept of face context awareness and evolutionary computing. But aside the difference of classifiers the training data performs main role in them consequently the results from the classifiers couldn't be so individual from each other to make decision by considering them. The system working environments are clustered and identified as face environmental context. Majority voting (MV), Maximum based fusion (MX) and Minimum based fusion (MN) are used to explore the most effective action configuration for each identified context. Once the context knowledge is constructed, the system can adapt to varying environment in real-time. The superiority of the proposed scheme is shown using three face image data sets: Inha, FERET, and Yale.", "authors": ["Mi Young Nam", "Jo Hyung Yoo", "Phill Kyu Rhee"], "n_citation": 0, "title": "An efficient classifier fusion for face recognition including varying illumination", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a1fd674d-2e1d-4b74-9cfd-19c614e9da92"}
{"abstract": "To select some valuable views for materialization is an essential challenge in OLAP system design. Several techniques proposed previously are not very scalable for systems with a large number of dimensional attributes in the very dynamic OLAP environment. In this paper, we propose two filtering methods. Our first method, the functional dependency filter, removes views with redundant summary information based on functional dependencies among the dimensional attributes. The second method, the size filter, is based on the view size to filter out any view that can be either derived from another small materialized view or has almost the same number of tuples as another materialized view from which it can be derived. More over, all useful views are selected by these two view filtering methods, other existing view selection methods can still be applied on the remaining views to further reduce other possible non-essential views from systems. We conduct performance tests to compare our method with other existing methods. The results show our method outperform the others.", "authors": ["Shi Guang Qiu", "Tok Wang Ling"], "n_citation": 50, "title": "View selection in OLAP environment", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "a2f2d4de-a0ba-45fd-a313-e6996c8e9449"}
{"abstract": "To be able to build systems by composing a variety of components dynamically, adding and removing as required, is desirable. Unfortunately systems with evolving architectures are prone to behaving in a surprising manner. In this paper we show how it is possible to generate a snapshot of the structure of a running application, and how this can be combined with behavioural specifications for components to check compatability and adherence to system properties. By modelling both the structure and the behaviour, before altering an existing system, we show how dynamic compositional systems may be put together in a predictable manner.", "authors": ["Robert Chatley", "Susan Eisenbach", "Jeff Kramer", "Jeff Magee", "Sebastian Uchitel"], "n_citation": 0, "title": "Predictable dynamic plugin systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "abdb0442-e6b3-411e-87b6-3d9a83867206"}
{"abstract": "In the role-based access control model, a role is a set of access rights. A subject doing jobs is granted roles showing the jobs in an enterprise. A transaction issued by a subject is associated with a subset of roles granted to the subject, which is named purpose. A method with a more significant purpose is performed before another method with a less significant purpose. We discuss which purpose is more significant than another purpose. We discuss two types of role-ordering (RO) schedulers SRO and PRO where multiple conflicting transactions are serializable in the significant order of subjects and purposes, respectively. We evaluate the RO schedulers compared with the traditional two-phase locking protocol in terms of throughput.", "authors": ["Youhei Tanaka", "Tomoya Enokido", "Makoto Takizawa"], "n_citation": 0, "title": "Role-based serializability for distributed object systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ac6bbfe1-dd87-4c2e-89aa-0a48438c21b3"}
{"abstract": "Common misconceptions about computer science hinder professional growth and harm the identity of computing.", "authors": ["Peter J. Denning", "Matti Tedre", "Pat Yongpradit"], "n_citation": 0, "references": ["2a933244-84bb-4d24-a79f-75ad659cae01", "6f646c86-858f-4134-85de-efc26816c3c1", "d9b32b2b-f423-4b61-9a79-fe623530d591"], "title": "Misconceptions about computer science", "venue": "Communications of The ACM", "year": 2017, "id": "b015f179-2e7c-42b0-921c-9a6842edc58c"}
{"abstract": "The best practical algorithm for class group computations in imaginary quadratic number fields (such as group structure, class number, discrete logarithm computations) is a variant of the quadratic sieve factoring algorithm. Paradoxical as it sounds, the principles of the number field sieve, in a strict sense, could not be applied to number field computations, yet. In this article we give an indication of the obstructions. In particular, we first present fundamental core elements of a number field sieve for number field computations of which it is absolutely unknown how to design them in a useful way. Finally, we show that the existence of a number field sieve for number field computations with a running time asymptotics similar to that of the genuine number field sieve likely implies the existence of an algorithm for elliptic curve related computational problems with subexponential running time.", "authors": ["Mark L. Bauer", "Safuat Hamdy"], "n_citation": 0, "title": "On class group computations using the number field sieve", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b4ca387f-297f-4342-a904-e5a230f578b2"}
{"abstract": "The necessity to management the computer security of an institution implies an evaluation phase and the most common method to carry out this evaluation it consists on the use of a set of metrics. As any system of information needs of an authentication mechanism being the most used one those based on password, in this article we propose a set of metric of password management policies based on the most outstanding factors in this authentication mechanism. Together with the metrics, we propose a quality indicator derived from these metrics that allows us to have a global vision of the quality of the password management policy used and a complete example of calculation of the proposed metric. Finally, we will indicate the future works to be performed to check the validity and usefulness of the proposed metrics.", "authors": ["Carlos Villarrubia", "Eduardo Fern\u00e1ndez-Medina", "Mario Piattini"], "n_citation": 50, "title": "Metrics of Password Management Policy", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bea67d5e-85aa-48cf-b688-ba6eedf51166"}
{"abstract": "According to deployment of broadband access networks like FTTH (Fiber-To-The-Home), many Internet Service Providers in Japan start providing video broadcasting services via IP multicast. As the service develops into higher quality and more diverse, a broadcasting system capable of handling high bit-rate contents will be required. However, typical implementation of broadcasting server may have problems because of poor packet timing control precision. In this paper, we present a new transmission control method for IP multicast that is synchronized with TCP flow control, which can be applied to QoS-guaranteed networks. We evaluate its validity using an experimental system with HDTV (High Definition TV) streams. We are able to demonstrate that the system can simultaneously distribute eight-channel HDTV streams.", "authors": ["Kazuhiro Kamimura", "Teruyuki Hasegawa", "Haruo Hoshino", "Shigehiro Ano", "Toru Hasegawa"], "n_citation": 0, "title": "A practical multicast transmission control method for multi-channel HDTV IP broadcasting system", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c9c7ec86-13aa-4fbc-a75b-ae08490167a7"}
{"abstract": "The AHEAD system supports the management of development processes for complex end products in engineering disciplines. AHEAD is based on nearly ten years of ongoing research on development processes in different engineering disciplines and the underlying concepts have been applied to the above application domains. Today, development processes tend to be distributed across organization boundaries. As the organizations which execute a shared process, usually have different goals and interests, a balance between all interests has to be achieved in a supporting concept. This paper illustrates by a short sample scenario, how the AHEAD system supports distributed development processes thereby taking the interests of all cooperation partners into account.", "authors": ["Markus Heller", "Dirk J\u00e4ger"], "n_citation": 0, "title": "Interorganizational management of development processes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cf37b899-2354-4e6c-a2cc-6c9a550c12c7"}
{"authors": ["Matti Siekkinen", "Vera Goebel"], "n_citation": 0, "title": "Non-Sticky Fingers: Policy-Driven Self-Optimization for DHTs.", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "d606e3a9-c64f-434f-b3c3-bf7c7fbf3233"}
{"abstract": "Data mining, which aims at extracting interesting information from large collections of data, has been widely used as an effective decision making tool. Mining the datasets in the presence of context factors may improve performance and efficacy of data mining by identifying the unknown factors, which are not easily detectable in the process of generating an expected outcome. This paper proposes a Context-aware data mining framework, by which contexts will be automatically captured to maximize the adaptive capacity of data mining. Context could consist of any circumstantial factors of the user and domain that may affect the data mining process. The factors that may affect the mining behavior are delineated and how each factor affects the behavior is discussed. It is also observed that a medical application of the model in wireless devices offers the advantages of Context-aware data mining. A Context-aware data mining framework is quantified through a partial implementation that would be used to test the behavior of the mining system under varied context factors. The results obtained from the implementation process are elucidated on how the prediction output or the behavior of the system changes from the similar set of inputs in view of different context factors.", "authors": ["Pravin Vajirkar", "Sachin Singh", "Yugyung Lee"], "n_citation": 0, "title": "Context-aware data mining framework for wireless medical application", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d9bb1c6f-c520-4cb5-b9d7-2a28255a59ba"}
{"abstract": "Subspace learning is one of the main directions for face recognition. In this paper, a novel unsupervised subspace learning method, Neighborhood Preserving Projections (NPP), is proposed. In contrast to traditional linear dimension reduction method, such as principal component analysis (PCA), the proposed method has good neighborhood-preserving property. The central idea is to modify the classical locally linear embedding by introducing a linear transform matrix. The transform matrix is obtained by optimizing a certain objective function. Experimental results on Yale face database and FERET face database show the effectiveness of the proposed method....", "authors": ["Yanwei Pang", "Nenghai Yu", "Houqiang Li", "Rong Zhang", "Zhengkai Liu"], "n_citation": 0, "title": "Face recognition using neighborhood preserving projections", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "db26d521-b954-4027-9110-ac97a0fa3c1d"}
{"abstract": "A successful adoption and adaptation of the particle swarm optimization (PSO) algorithm is presented in this paper. It improves the performance of Support Vector Machine (SVM) in the classification of incipient faults of power transformers. A PSO-based encoding technique is developed to improve the accuracy of classification. The proposed scheme is capable of removing misleading input features and, optimizing the kernel parameters at the same time. Experiments on real operational data had demonstrated the effectiveness and efficiency of the proposed approach. The power system industry can benefit from our system in both the accelerated operational speed and the improved accuracy in the classification of incipient faults.", "authors": ["Tsair-Fwu Lee", "Ming-Yuan Cho", "Chin-Shiuh Shieh", "Hong-Jen Lee", "Fu-Min Fang"], "n_citation": 0, "title": "Particle Swarm Optimization-Based SVM for Incipient Fault Classification of Power Transformers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dcb030e5-210e-4a49-b592-bb8d5da25b90"}
{"abstract": "Convergence of the Self-Organizing Map (SOM) and Neural Gas (NG) is usually contemplated from the point of view of stochastic gradient descent. This class of algorithms is characterized by a very slow convergence rate. However we have found empirically that One-Pass realizations of SOM and NG provide good results or even improve over the slower realizations, when the performance measure is the distortion. One-Pass realizations use each data sample item only once, imposing a very fast reduction of the learning parameters that does not conform to the convergence requirements of stochastic gradient descent. That empirical evidence leads us to propose that the appropriate setting for the convergence analysis of SOM, NG and similar competitive clustering algorithms is the field of Graduated Nonconvexity algorithms. We show they can easily be put in this framework.", "authors": ["Ana Isabel Gonz\u00e1lez", "Alicia D'Anjou", "M. Teresa Garcia-Sebastian", "Manuel Gra\u00f1a"], "n_citation": 0, "title": "SOM and Neural Gas as Graduated Nonconvexity Algorithms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e2519e69-0bda-447b-9164-bb34147d0919"}
{"authors": ["Stephen Edward Rees", "Dan Stieper Karbing", "Charlotte Aller\u00f8d", "Marianne Toftegaard", "Per Thorgaard", "Egon Toft", "S\u00f8ren Kj\u00e6rgaard", "Steen Andreassen"], "n_citation": 0, "title": "The Intelligent Ventilator project", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "e79a8617-2d5c-4ebf-80d7-d25d4e871ba7"}
{"abstract": "In this paper, we propose a methodology for embedding watermark image data into color images. At the transmitter, the signature image is encoded by a multiple description encoder. The information of the two descriptions are embedded in the host image in the spatial domain in the red and blue components. Furthermore, this scheme requires no knowledge of the original image for the recovery of the signature image, yet yields high signal-to-noise ratios for the recovered output. At the receiver, the multiple description decoder combines the information of each description and reconstructs the original signature image. We experiment the proposed scheme for embedding a gray-scale signature image of 128\u00d7128 pixels size in the spatial domain of a color host image of 512x512 pixels. Simulation results show that data embedding based on multiple description coding has low visible distortions in the host image and robustness to various signal processing and geometrical attacks, such as addition of noise, quantization, cropping and down-sampling.", "authors": ["Mohsen Ashourian", "Peyman Moallem", "Yo-Sung Ho"], "n_citation": 0, "title": "A robust method for data hiding in color images", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "eabb596c-ac08-4401-b138-81ffadd7d589"}
{"abstract": "Access control policies are security policies that govern access to resources. Real-time update of access control policies, that is, updating policies while they are in effect and enforcing the changes immediately, is necessary for many security-critical applications. In this paper, we consider real-time update of access control policies in a database system. We consider an environment in which different kinds of transactions execute concurrently some of which are policy update transactions. Updating policy objects while they are deployed can lead to potential security problems. We propose two algorithms that not only prevent such security problems, but also ensure serializable execution of transactions. The algorithms differ on the degree of concurrency provided.", "authors": ["Indrakshi Ray", "Tai Xin"], "n_citation": 0, "title": "Concurrent and real-time update of access control policies", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "ec3b7d94-9db4-49a6-ba6b-d0969f0dfb4f"}
{"abstract": "To improve the effectiveness and efficiency of CBIR systems, in this paper, we present a novel IR scheme with multiple features, the spatial color and edge percentage features, derived by way of moment-preserving edge detection. Put the above two features together, we come by an effective and efficient IR system. Experimental results show that the proposed method outperforms other similar methods in terms of accuracy and retrieval efficiency.", "authors": ["Chin-Chen Chang", "Yung-Chen Chou", "Wen-Chuan Wu"], "n_citation": 50, "title": "Image retrieval using spatial color and edge detection", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ecff3639-6fc4-40da-a2a5-f671e7ed96d3"}
{"abstract": "We investigate cardinality constraints of the form M \u2192e K, where M is a set and \u03b8 is one of the comparison operators =, \u2264, or \u2265; such a constraint states that exactly, at most, or at least, respectively, K elements out of the set M have to be chosen. We show how a set C of constraints can be represented by means of a positive-disjunctive deductive database P c , such that the models of P c  correspond to the solutions of C. This allows for embedding cardinality constraints into applications dealing with incomplete knowledge. We also present a sound calculus represented by a definite logic program P cc , which allows for directly reasoning with sets of exactly cardinality constraints (i.e., where \u03b8 is =). Reasoning with P cc  is very efficient, and it can be used for performance reasons before P c  is evaluated. For obtaining completeness, however, P c  is necessary, since we show the theoretical result that a sound and complete calculus for exactly-cardinality constraints does not exist.", "authors": ["Dietmar Seipel", "Ulrich Geske"], "n_citation": 0, "title": "Cardinality constraints in disjunctive deductive databases", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "ed738980-55c6-44fd-b0c1-d44b4069c325"}
{"abstract": "Block-based motion estimation can be regarded as a function minimization problem in a finite two-dimensional space. Therefore, fast block-based motion estimation can be achieved by using an efficient function minimization algorithm instead of using a predefined search pattern, such as the diamond search. The downhill simplex search algorithm is an efficient derivative-free function minimization algorithm. In this paper, we propose several enhanced schemes to improve the efficiency of applying the downhill simplex search algorithm to motion estimation. The proposed enhanced schemes include a new initialization process, a special rounding method, and an early-stop error function evaluation procedure. Experimental results on several benchmarking videos show superior performance of the proposed algorithm over some existing fast block matching methods.", "authors": ["Hwai-Chung Fei", "Chun-Jen Chen", "Shang-Hong Lai"], "n_citation": 0, "title": "Enhanced downhill simplex search for fast video motion estimation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ee8e5ec0-fc71-4df7-83d3-91a875263eb7"}
{"abstract": "Query Expansion has long been recognized as one of the effective methods in solving short queries and improving ranking accuracy in traditional IR research. Many variations of this method have been introduced throughout the past decades; however, few of them have incorporated web log information into the query expansion process. In this paper, we propose an expansion technique that expands document content at the initial index stage using queries extracted from the web log files. Our experimental results show that even with a minimal amount of real world log information available and a professionally cataloged knowledge structure to aid the search, there is still a significant improvement in using our query expansion method compared to the conventional query expansion ones.", "authors": ["Yun Zhu", "Le Gruenwald"], "n_citation": 0, "title": "Query expansion using web access log files", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fce08335-16e3-4179-bf9e-89ac8ee1edf4"}
{"abstract": "This paper presents a new framework for users to select relevant data from an XML document and store it in an existing relational database, as opposed to previous approaches that shred the entire XML document into a newly created database of a newly designed schema. The framework is based on a notion of XML2DB mappings. An XML2DB mapping extends a (possibly recursive) DTD by associating element types with semantic attributes and rules. It extracts either part or all of the data from an XML document, and generates SQL updates to increment an existing database using the XML data. We also provide an efficient technique to evaluate XML2DB mappings in parallel with SAX parsing. These yield a systematic method to store XML data selectively in an existing database.", "authors": ["Wenfei Fan", "Lisha Ma"], "n_citation": 0, "title": "Selectively storing XML data in relations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "08277bac-1cf2-4787-8a03-f229c9258cb4"}
{"authors": ["Leonid Libkin"], "n_citation": 0, "title": "Technical Perspective: Data distribution for fast joins", "venue": "Communications of The ACM", "year": 2017, "id": "088941e6-595a-45be-9bb7-dc703c267d00"}
{"abstract": "The success of any activity relies on its quality. There are many approaches to quality assessment and management related to software activities like specification, modelling and design of all kind of artifacts (from large systems to small Java applets, from custom-made applications to commercial software). Unfortunately, these approaches are difficult to compare, combine or select because of the lack of a widespread quality reference framework. In this paper we propose three kinds of hierarchically structured quality models in order to formalise software quality issues and deal with quality information modelling. A generic model that represents the fundamental concepts related to software quality is the root of this hierarchy. Starting from this generic model, many reference models that specialise it may be derived. Finally, reference models are refined into domain models that adapt them to a particular domain of software. In the paper, we define as example a reference model that adopts the ISO/IEC 9126-1 quality standard, classical proposals about metrics and the quality-related QML language. We then refine this model into three different domain models, for a kind of component libraries, databases and web services.", "authors": ["Xavier Burgu\u00e9s Illa", "Xavier Franch"], "n_citation": 0, "title": "Formalising software quality using a hierarchy of quality models", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1f79d246-f100-48ba-8cf3-3b4139d85dcb"}
{"abstract": "We prove that every n-vertex graph G with path-width pw(G) has a three-dimensional straight-line grid drawing with O(pw(G) 2 .n) volume. Thus for graphs with bounded path-width the volume is O(n), and it follows that for graphs with bounded tree-width, such as series-parallel graphs, the volume is O(n log 2  n). No better bound than O(n 2 ) was previously known for drawings of series-parallel graphs. For planar graphs we obtain three-dimensional drawings with O(n 2 ) volume and O(\u221an) aspect ratio, whereas all previous constructions with O(n 2 ) volume have \u03b8(n) aspect ratio.", "authors": ["Vida Dujmovi\u0107", "Pat Morin", "David R. Wood"], "n_citation": 50, "title": "Path-width and three-dimensional straight-line grid drawings of graphs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2c3a5ade-64ed-44aa-8635-e695739e394d"}
{"abstract": "This research focuses on the implementation of a real-time environment monitoring system for environment detection using wireless sensor networks. The purpose of our research is to construct the system on the real-time environment with the technology of environment monitoring systems and ubiquitous computing systems. Also, we present the monitoring system to provide a faster solution to prevent disasters through automatic machine controls in urgent situations. As the purpose of this study, we constructed simulation nodes with wireless sensor network devices and implemented a real-time monitoring system.", "authors": ["Kyung-Hoon Jung", "Seok-Cheol Lee", "Hyun-Suk Hwang", "Chang-Soo Kim"], "n_citation": 1, "title": "The Design and Implementation of Real-Time Environment Monitoring Systems Based on Wireless Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "306396b7-9483-4206-af31-332351f8775a"}
{"abstract": "This paper considers a buffer allocation problem of flexible manufacturing system composed of several parallel workstations each with both limited input and output buffers, where machine blocking is allowed and two automated guided vehicles are used for input and output material handling. Some interesting properties are derived that are useful for characterizing optimal allocation of buffers for the given FMS model. By using the properties, a solution algorithm is exploited to solve the optimal buffer allocation problem, and a variety of differently-sized decision parameters are numerically tested to show the efficiency of the algorithm.", "authors": ["Soo-Tae Kwon"], "n_citation": 0, "title": "On the Optimal Buffer Allocation of an FMS with Finite In-Process Buffers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "320c3e0d-57e6-4094-8c3c-3135a9528ace"}
{"abstract": "Despite the emergence of XML as a standard for data exchange on the Web, XML update processing has received little attention. One of the issues that need to be addressed in XML update processing is the update robustness of the XML numbering scheme employed for efficient containment query processing. In this paper, we propose an efficient update robust XML numbering scheme. It works for both ordered and unordered XML data and fits well in XML to relational mapping. The proposed scheme was implemented with a relational database as the XML stores. The experimental results showed that our scheme is more efficient than the previous one.", "authors": ["Hyunchul Kang", "Young-Hyun Kim"], "n_citation": 0, "title": "An efficient scheme of update robust XML numbering with XML to relational mapping", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "358eb80a-f49e-4db7-a2bf-4757d7e6ac94"}
{"abstract": "Differential cryptanalysis analyzes ciphers by studying the development of differences during encryption. Linear cryptanalysis is similar but is based on studying approximate linear relations. In 1994, Langford and Hellman showed that both kinds of analysis can be combined together by a technique called differential-linear cryptanalysis, in which the differential part creates a linear approximation with probability 1. They applied their technique to 8-round DES. In this paper we present an enhancement of differential-linear cryptanalysis in which the inherited linear probability is smaller than 1. We use this extension to describe a differential-linear distinguisher for a 7-round reduced-version of DES, and to present the best known key-recovery attack on a 9-round reduced-version of DES. We use our enhanced technique to attack COCONUT98 with time complexity 2 33.7  encryptions and 2 27.7  chosen plaintexts.", "authors": ["Eli Biham", "Orr Dunkelman", "Nathan Keller"], "n_citation": 0, "title": "Enhancing differential-linear cryptanalysis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "36a68067-cd56-4fb3-b1c9-b1481881a832"}
{"abstract": "Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely  F ragment  R ecommender for  A PIs with  P ageRank and  T opic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.", "authors": ["He Jiang", "Jingxuan Zhang", "Zhilei Ren", "Tao Zhang"], "n_citation": 0, "references": ["05610759-6681-4e82-86f5-6a339bc88686", "094d440f-03a1-4099-a136-76832d46e949", "2ecc2c45-feac-42dc-aa25-07cfd98c22aa", "365c4069-696f-48c1-82a6-37ffb49fa1af", "3683c706-6049-485b-a7a8-a4e2c5e13867", "3c7448bc-9918-4839-949f-922268bee3a3", "3e7d8d1c-cee4-42e0-8190-487f8e343dc7", "4135bf74-f081-4d0e-8662-b372c0a6d84a", "4c3d9531-e8eb-4d44-bd2a-27982477a4cb", "4d3c47c3-3e2e-4bd8-911e-487bc38ee048", "4d79fc0c-623c-453a-8fa4-5cdd10da8f9b", "5678121e-a587-4381-ab6b-0cf60d147edc", "580be014-f966-489f-97a7-8c23dd8747d2", "5855bdf4-d1cf-4e7b-a1e1-301b2bc57f3b", "58a81e36-73cf-4fdb-bc0c-4ece10082c20", "5f8627c5-d047-48c3-acef-aad383060438", "621cbb55-0b62-42b3-9fae-1bec1c3be7aa", "68a19821-2b16-4ac2-bf04-07ffe0f5715e", "6a7a102f-ea4e-4c1a-ba67-d62de4a1c03f", "78e92935-3d8b-4156-b211-54f58a34e692", "79252d05-c529-4d30-a426-8eddeae66ff8", "9360c6a5-ee26-4e54-96fc-8b7c6a814ecb", "9f5c7327-0258-45ff-8f8b-eb3255316a1d", "ae0d890f-6ee8-4ea7-8db1-3bfee0309fd6", "b07a7bee-4bec-4930-85a2-ad89e6f875e3", "d0da6f9f-2299-4684-98ae-0d74398791c5", "db5407cf-5783-485c-9aa7-e77523a84e0d", "eb33c4bb-530a-4a17-ba8f-2c4dc0eab142", "fef3ce1c-33db-4c53-bdbd-45bab4794acb"], "title": "An unsupervised approach for discovering relevant tutorial fragments for APIs", "venue": "international conference on software engineering", "year": 2017, "id": "4fa4f092-1b2d-47c8-84ac-d635c29a02a0"}
{"abstract": "In this paper we describe an interactive, visual knowledge discovery tool for analyzing numerical data sets. The tool combines a visual clustering method, to hypothesize meaningful structures in the data, and a classification machine learning algorithm, to validate the hypothesized structures. A two-dimensional representation of the available data allows a user to partition the search space by choosing shape or density according to criteria he deems optimal. A partition can be composed by regions populated according to some arbitrary form, not necessarily spherical. The accuracy of clustering results can be validated by using a decision tree classifier, included in the mining tool.", "authors": ["Giuseppe Manco", "Clara Pizzuti", "Domenico Talia"], "n_citation": 0, "title": "Eureka!: A tool for interactive knowledge discovery", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "54f27fc8-a24c-40cf-8180-ae32d88381bc"}
{"abstract": "Authenticated Key Establishment (AKE) protocols enable two entities, say a client (or a user) and a server, to share common session keys in an authentic way. In this paper, we review AKE protocols from a little bit different point of view, i.e. the relationship between information a client needs to possess (for authentication) and immunity to the respective leakage of stored secrets from a client side and a server side. Since the information leakage would be more conceivable than breaking down the underlying cryptosystems, it is desirable to enhance the immunity to the leakage. First and foremost, we categorize AKE protocols according to how much resilience against the leakage can be provided. Then, we propose new AKE protocols that have immunity to the leakage of stored secrets from a client and a server (or servers), respectively. And we extend our protocols to be possible for updating secret values registered in server(s) or password remembered by a client.", "authors": ["SeongHan Shin", "Kazukuni Kobara", "Hideki Imai"], "n_citation": 0, "title": "Leakage-resilient authenticated Key Establishment protocols", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5c9d3f29-75e2-4787-9714-f2711a84eb7b"}
{"abstract": "Detection of commercials in TV videos is difficult because the diversity of them puts up a high barrier to construct an appropriate model. In this work, we try to deal with this problem through a top-down approach. We take account of the domain knowledge of commercial production and extract features that describe the characteristics of commercials. According to the clues from speech-music discrimination, video scene detection, and caption detection, a multi-modal commercial detection scheme is proposed. Experimental results show good performance of the proposed scheme on detecting commercials in news and talk show programs.", "authors": ["Jun-Cheng Chen", "Jen-Hao Yeh", "Wei-Ta Chu", "Jin-Hau Kuo", "Ja-Ling Wu"], "n_citation": 0, "title": "Improvement of commercial boundary detection using audiovisual features", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5dd86349-0871-4e0b-b21f-f99ca5c84d26"}
{"abstract": "A Mobile Ad-Hoc Network (MANET) is a group of wireless, mobile, battery-powered clients and servers that autonomously form temporary networks. Three data communication modes can be provided in a MANET, data broadcast, data query, and peer-to-peer messaging. Currently, no MANET data communication protocol provides the ability to use all MANET data communication modes. The objective of this research is to develop a MANET data communication protocol, TriM (for Tri-Modal communication), capable of providing all three data communication methods. TriM was designed to accommodate node disconnection and reconnection through periodic synchronization. Each part of the protocol has minimum power consumption as a goal. Simulation showed TriM minimizes the average power consumption of servers and clients while accommodating node disconnection.", "authors": ["Leslie D. Fife", "Le Gruenwald"], "n_citation": 0, "title": "TriM: Tri-Modal data communication in Mobile Ad-Hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6510f91e-aeef-4083-adca-42cf5ae4f901"}
{"abstract": "Normal mapping is an essential rendering technique in 3D computer graphics to express detailed wrincleness and bumpy texture of the surface. As the normal mapping is increasingly utilized, compression of normal maps is becoming a significant issue. The problem is there is no quality evaluation model for lossy compressed normal maps. Therefore, in this paper, we have developed a mathematical model to analyze the characteristics between lossy compressed normal maps and 3D images rendered with them. By calculating averages of the parameters which cannot be defined uniquely and by introducing some assumptions, the model has been expressed in a simple form. The validity and generality of our model have been demonstrated by experiments. The model proposed in this paper will be helpful for deciding normal map compression strategy considering the target quality of the rendered 3D images.", "authors": ["Toshihiko Yamasaki", "Kazuya Hayase", "Kiyoharu Aizawa"], "n_citation": 0, "title": "Mathematical PSNR prediction model between compressed normal maps and rendered 3D images", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "65c40ab6-67da-4a68-a7c9-7c7875392489"}
{"abstract": "The production of effective workforce rosters is a common management problem. Rostering problems are highly constrained and require extensive experience to solve manually. The decisions made by expert rosterers are often subjective and are difficult to represent systematically. This paper presents a formal description of a new technique for capturing rostering experience using case-based reasoning methodology. Examples of previously encountered constraint violations and their corresponding repairs are used to solve new rostering problems. We apply the technique to real-world data from a UK hospital.", "authors": ["Sanja Petrovicl", "Gareth Beddoe", "Greet Vanden Berghe"], "n_citation": 50, "title": "Storing and adapting repair experiences in employee rostering", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "69815914-40f0-4b0f-bd4e-152211d09cfd"}
{"abstract": "We revisit a long-lived folklore impossibility result for factoring-based encryption and properly establish that reaching maximally secure one-wayness (i.e. equivalent to factoring) and resisting chosen-ciphertext attacks (CCA) are incompatible goals for single-key cryptosystems. We pinpoint two tradeoffs between security notions in the standard model that have always remained unnoticed in the Random Oracle (RO) model. These imply that simple RO-model schemes such as Rabin/RW-SAEP[+]/OAEP[+][+], EPOC-2, etc. admit no instantiation in the standard model which CCA security is equivalent to factoring via a key-preserving reduction. We extend this impossibility to arbitrary reductions assuming non-malleable key generation, a property capturing the intuition that factoring a modulus n should not be any easier when given a factoring oracle for moduli n'# n. The only known countermeasures against our impossibility results, besides malleable key generation, are the inclusion of an additional random string in the public key, or encryption twinning as in Naor-Yung or Dolev-Dwork-Naor constructions.", "authors": ["Pascal Paillier", "Jorge L. Villar"], "n_citation": 50, "title": "Trading One-Wayness Against Chosen-Ciphertext Security in Factoring-Based Encryption", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "69d20c36-8d6e-43d5-b7e3-842869a7edac"}
{"abstract": "In this paper we propose a scheme for ball detection and tracking in broadcast soccer video. There are two alternate procedures in the scheme: ball detection and ball tracking. In ball detection procedure, ball candidates are first extracted from several consecutive frames using color, shape, and size cues. Then a weighted graph is constructed, with each node representing a candidate and each edge linking two candidates in adjacent frames. Finally, Viterbi algorithm is employed to extract the optimal path as ball's locations. In ball tracking procedure, Kalman filter based template matching is utilized to track the ball in subsequent frames. Kalman filter and the template are initialized using detection results. In each tracking step, ball location is verified to update the template and to guide possible ball re-detection. Experimental results demonstrate that the proposed scheme is promising.", "authors": ["Dawei Liang", "Yang Liu", "Qingming Huang", "Wen Gao"], "n_citation": 0, "title": "A scheme for ball detection and tracking in broadcast soccer video", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6a4bf806-13cc-4a1b-bbfb-4db74c6d5bb6"}
{"abstract": "Continual Queries (CQs) are persistent queries that are issued once and then are run at regular intervals or when source data change until a termination condition is satisfied. Users receive new information automatically as it becomes available. CQs systems need to support a large number of CQs due to the scale of the Internet. This paper describes a novel architecture for a CQs system that scales to a large number of queries. In this system CQs are evaluated locally on the CQ server without accessing base relations after initial evaluation. Only group queries are run to retrieve auxiliary data. We optimize the retrieval of auxiliary data. A performance evaluation shows that the architecture reduces data transmission and I/O costs.", "authors": ["Sharifullah Khan", "Peter L. Mott"], "n_citation": 0, "title": "LeedsCQ: A scalable continual Queries system", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6f13305b-8ba6-4183-8d70-3b27d50202b3"}
{"abstract": "In this paper, we will propose PC-Filter (PC stands for Partition Comparison), a robust data filter for approximately duplicate record detection in large databases. PC-Filter distinguishes itself from all of existing methods by using the notion of partition in duplicate detection. It first sorts the whole database and splits the sorted database into a number of record partitions. The Partition Comparison Graph (PCG) is then constructed by performing fast partition pruning. Finally, duplicate records are effectively detected by using internal and external partition comparison based on PCG. Four properties, used as heuristics, have been devised to achieve a remarkable efficiency of the filter based on triangle inequity of record similarity. PC-Filter is insensitive to the key used to sort the database, and can achieve a very good recall level that is comparable to that of the pair-wise record comparison method but only with a complexity of O(N 4/3 ). Equipping existing detection methods with PC-Filter, we are able to well solve the Key Selection problem, the Scope Specification problem and the Low Recall problem that the existing methods suffer from.", "authors": ["Ji Zhang", "Tok Wang Ling", "Robert M. Bruckner", "Han Liu"], "n_citation": 0, "title": "PC-filter: A robust filtering technique for duplicate record detection in large databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "70d342b0-b5d0-4cc5-9b1c-7c5794eb923a"}
{"abstract": "In this paper, we present the SCIFF platform for multi-agent systems. The platform is based on Abductive Logic Programming, with a uniform language for specifying agent policies and interaction protocols. A significant advantage of the computational logic foundation of the SCIFF framework is that the declarative specifications of agent policies and interaction protocols can be used directly, at runtime, as the programs for the agent instances and for the verification of compliance. We also provide a definition of conformance of an agent policy to an interaction protocol (i.e., a property that guarantees that an agent will comply to a given protocol) and a operational procedure to test conformance.", "authors": ["Marco Alberti", "Federico Chesani", "Marco Gavanelli", "Evelina Lamma", "Paola Mello"], "n_citation": 0, "title": "A Verifiable Logic-Based Agent Architecture", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "726e3567-5891-4d9c-bdbf-6befd1bd2803"}
{"abstract": "In this paper we present a simple method for minimal distortion development of triangulated surfaces for mapping and imaging. The method is based on classical results of F. Gehring and Y. Vaisala regarding the existence of quasi-conformal and quasi-isometric mappings between Riemannian manifolds. A random starting triangle version of the algorithm is presented. A curvature based version is also applicable. In addition the algorithm enables the user to compute the maximal distortion errors. Moreover, the algorithm makes no use to derivatives, hence it is suitable for analysis of noisy data. The algorithm is tested on data obtained from real CT images of the human brain cortex.", "authors": ["Eli Appleboim", "Emil Saucan", "Yehoshua Y. Zeevi", "Ofir Zeitoun"], "n_citation": 0, "title": "Quasi-isometric and quasi-conformal development of triangulated surfaces for computerized tomography", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "72aa86f2-a527-4cd6-9b31-c114e8c94e90"}
{"abstract": "Intra-frame insertion, conventionally periodic, is inevitable to provide the decoders with the capability of random access for the streamed videos. In this paper, we propose a new non-periodic intra-frame insertion method which encodes a frame as I-picture when the propagated and accumulated uncertainties of the reference frame are greater than a threshold and show that it can be used for the purpose of the error resilient video encoding. The uncertainties of the reference frame reflect the drift noise resulting from the transmission errors. Simulation results over 3GPP/3GPP2 show that the transmission error resiliency of the proposed algorithm is superior to that of the conventional periodic intra-frame- insertion methods with the same encoding efficiency.", "authors": ["Yong Tae Kim", "Youngil Yoo", "Dong Wook Kang", "Kyeong Hoon Jung", "Ki-Doo Kim", "Seung-Jun Lee"], "n_citation": 50, "title": "Non-periodic frame refreshment based on the uncertainty models of the reference frames", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "73340e2e-8fed-42fa-b3ea-f92a395b824c"}
{"abstract": "This paper proposes decision fusion method of shape and motion information based on Bayesian framework for object classification in image sequences. This method is designed for intelligent information and surveillance guard robots to detect and track a suspicious person and vehicle within a security region. For reliable and stable classification of targets, multiple invariant feature vectors to more certainly discriminate between targets are required. To do this, shape and motion information are extracted using Fourier descriptor, gradients, and motion feature variation on spatial and temporal images, and then local decisions are performed respectively. Finally, global decision is done using decision fusion method based on Bayesian framework. The experimental results on the different test sequences showed that the proposed method obtained good classification result than any other ones using neural net and other fusion methods.", "authors": ["Heungkyu Lee", "Jungho Kim", "June Kim"], "n_citation": 50, "title": "Decision Fusion of Shape and Motion Information Based on Bayesian Framework for Moving Object Classification in Image Sequences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "76adab9f-8a28-4e9b-8c72-e82303b7784f"}
{"abstract": "We propose CLEAR (Context and Location-based Efficient Allocation of Replicas), a dynamic replica allocation scheme for improving data availability in mobile ad-hoc peer-to-peer (M-P2P) networks. To manage replica allocation efficiently, CLEAR exploits user mobility patterns and deploys a super-peer architecture, which avoids both broadcast storm during replica allocation as well as broadcast-based querying. CLEAR considers different levels of replica consistency and load as replica allocation criteria. Our performance study indicates CLEAR's overall effectiveness in improving data availability in M-P2P networks.", "authors": ["Anirban Mondal", "Sanjay Kumar Madria", "Masaru Kitsuregawa"], "n_citation": 50, "title": "CLEAR : An efficient context and location-based dynamic replication scheme for mobile-P2P networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7e2deb98-1417-480b-a695-0e8078383bc8"}
{"abstract": "Finding all occurrences of a twig pattern in an XML document is a core operation for XML query processing. The emergence of XML as a common mark-up language for data interchange has spawned great interest in techniques for filtering and content-based routing of XML data. In this paper, we aim to use the state-of-art holistic twig join technique to address multiple twig queries in a large scale XML database. We propose a new twig query technique which is specially tailored to match documents with large numbers of twig pattern queries. We introduce the super-twig to represent multiple twig queries. Based on the super-twig, we design a holistic twig join algorithm, called MTwigStack, to find all matches for multiple twig queries by scanning an XML document only once.", "authors": ["Huanzhang Liu", "Tok Wang Ling", "Tian Yu", "Ji Wu"], "n_citation": 0, "title": "Efficient processing of multiple XML twig queries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8103453f-0fd7-4344-b192-6bd1de28a6e8"}
{"abstract": "We present a framework for representing the trajectories of moving objects and the time-varying results of operations on moving objects. This framework supports the realization of discrete data models of moving objects databases, which incorporate representations of moving objects based on non-linear approximation functions.", "authors": ["Ludger Becker", "Henrik Blunck", "Klaus H. Hinrichs", "Jan Vahrenhold"], "n_citation": 0, "title": "A framework for representing moving objects", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "90e8e936-3f12-4746-9a03-fa90eaedf599"}
{"abstract": "In this paper, we present a novel technique for modeling, checking, and enforcing temporal constraints in workflow processes containing conditionally executed activities. Existing workflow time modeling proposals either do not discriminate between time constraints that apply to disparate execution paths, or they treat every execution path independently. Consequently, superfluous time constraint violations may be detected at modeling time, even when each execution path does not violate any constraints. In addition, scheduling conflicts during process execution may not be detected for activities that are common to multiple execution paths. Our approach addresses these problems by (partially) unfolding the workflow graph associated with a process that contains conditionally executed activities and, then, incorporating the temporal constraints in the time calculations performed on the unfolded graph.", "authors": ["Johann Eder", "Wolfgang Gruber", "Euthimios Panagos"], "n_citation": 0, "title": "Temporal modeling of workflows with conditional execution paths", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "90ff70ca-358b-4003-9499-d6238a5a7b9b"}
{"abstract": "Our work focuses on the simplification of MPEG-4 avatar models. Similar to other general purposed 3D models, these avatars often claim complex, highly detailed presentation to maintain a convincing level of realism. However, the full complexity of such models is not always required, especially when a client terminal - for the reason of portability and cost reduction - cannot or does not necessarily support high complex presentation. First, we deploy the well-known 3D simplification based on quadric error metric to create a simplified version of the avatar in question, taking into account that the avatar is also a 3D model based on manifold mesh. Within this general scope, we intro' duce a new weight factor to overcome an uncertainty in choosing target for decimation. Next, exploiting the biomechanical characteristic of avatars - having the underlying skeleton structure - we propose an adaptation of the simplifying technique to avatars. The concept of bones is taken into account as either a boundary constraint or a cost-component for the quadric error. Encouraging results can be obtained With these modified procedures.", "authors": ["Marius Preda", "Son Tran", "Fran\u00e7oise J. Pr\u00eateux"], "n_citation": 0, "title": "Adaptation of quadric metric simplification to MPEG-4 animated object", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "94e2948b-7016-4912-84dd-bfb088a4fbca"}
{"abstract": "In this paper, we consider the over-constrained truck dock assignment problem with time windows and capacity constraint in transshipment network through crossdocks where the number of trucks exceeds the number of docks available, the capacity of the crossdock is limited, and where the objective is to minimize the total shipping distances. The problem is first formulated as an Integer Programming (IP) model, and then we propose a Tabu Search (TS) and a Genetic algorithms (GA) that utilize the IP constraints. Computational results are provided, showing that the heuristics perform better than the CPLEX Solver in both small-scale and large-scale test sets. Therefore, we conclude that the heuristic search approaches are efficient for the truck dock assignment problem.", "authors": ["Andrew Lim", "Hong Ma", "Zhaowei Miao"], "n_citation": 50, "title": "Truck dock assignment problem with time windows and capacity constraint in transshipment network through crossdocks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "969d82bf-f97b-4fc7-b325-5e69bc4f64e1"}
{"abstract": "This paper focuses on relevance scoring for XML IR queries. We propose a novel and effective algorithm for relevance scoring, which considers both structural information and semantic. Experiments show that the algorithm can effectively improve the Precision and Recall for XML information retrieval.", "authors": ["Zhongming Han", "Jiajin Le", "Beijin Shen"], "n_citation": 50, "title": "Effectively scoring for XML IR queries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "97758648-f4fd-4120-8c12-249c73c7231d"}
{"abstract": "Multiple UAV cooperative reconnaissance is one of the most important aspects of UAV operations. This paper presents a genetic algorithm(GA) based approach for multiple UAVs cooperative reconnaissance mission planning problem. The objective is to conduct reconnaissance on a set. of targets within predefined time windows at minimum cost, while satisfying the reconnaissance resolution demands of the targets, and without violating the maximum travel time for each UAV. A mathematical formulation is presented for the problem, taking the targets reconnaissance resolution demands and time windows constraints into account, which are always ignored in previous approaches. Then a GA based approach is put forward to resolve the problem. Our GA implementation uses integer string as the chromosome representation, and incorporates novel evolutionary operators, including a subsequence crossover operator and a forward insertion mutation operator. Finally the simulation results show the efficiency of our algorithm.", "authors": ["Jing Tian", "Lincheng Shen", "Yanxing Zheng"], "n_citation": 0, "title": "Genetic Algorithm Based Approach for Multi-UAV Cooperative Reconnaissance Mission Planning Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "99012d61-f13c-4c3c-b815-ab45b7f275ab"}
{"abstract": "Iris recognition is one of the best methods in the biometric field. It includes two main processes: Iris localization and segmentation and Feature extraction and coding. We have introduced a new method based on Gabor transform for localization and segmentation of iris in eye image and also have used it to implement an Iris Recognition system. By applying the Gabor transform to an eye image, some constant templates are extracted related to the borders of pupil and iris. These features are robust and almost easy to use. There is no restriction and no tuning parameter in algorithm. The algorithm is extremely robust to the eyelids and eyelashes occlusions. To evaluate the segmentation method, we have also developed a gradient based method. The results of experimentations show that our proposed algorithm works better than the gradient based algorithm. The results of our recognition system are also noticeable. The low FRR and FAR values justify the results of segmentation method. We have also applied different Gabor Wavelet filters for feature extraction. The observations show that the threshold used to discriminate feature vectors is highly dependant on the orientation, scale and parameters of the corresponding Gabor Wavelet Transform.", "authors": ["Mohammadreza Noruzi", "Mansour Vafadoost", "M. Shahram Moin"], "n_citation": 0, "title": "Iris Recognition : Localization, Segmentation and Feature Extraction Based on Gabor Transform", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9950fea3-efbb-4c66-9d59-5630c7e91532"}
{"abstract": "For hands-free communication system, this paper describes a noise reduction method using a 2-channel microphone. Recently, the Complex Spectrum Circle Centroid (CSCC) method has been proposed. This method utilizes geometric information and estimates the spectrum of the target signal. The method is advantageous in that no adjustment of the array-processing parameters to the environment is necessary before its operation and it is effective with non-stationary noise. However, the original CSCC method requires at least three microphones to estimate the spectrum of the target signal (center of circle). In this paper, we propose a method which estimates the spectrum of the target signal using only two microphones. In experimental results, the proposed method outperforms the Delay-and-Sum approach and can restore the target signal almost completely in a simulated noisy environment.", "authors": ["Toshiya Ohkubo", "Tetsuya Takiguchi", "Yasuo Ariki"], "n_citation": 0, "title": "Two-channel-based noise reduction in a complex spectrum plane for hands-free communication system", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9b0fa713-bbd2-4dba-8fb1-214ef2005718"}
{"abstract": "In recent years more and more information has been made available on the Web. High quality information is often stored in dedicated databases of digital libraries, which are on their way to become expanding islands of well organized information. However, managing this information still poses challenges. The Semantic Web provides technologies that are about help to meet these challenges. In this article we present JeromeDL, a full fledged open-source digital library system. We exemplify how digital library content management can benefit from the Semantic Web. We define and evaluate browsing and searching features. We describe how the semantic descriptions of resources and users profiles improve the usability of a digital library. We present how digital libraries can be interconnected into one heterogeneous database with use of semantic technologies.", "authors": ["Sebastian Ryszard Kruk", "Stefan Decker", "Lech Zieborak"], "n_citation": 55, "title": "JeromeDL : Adding Semantic Web technologies to digital libraries", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9da34360-b63f-4444-af72-8db081c1f7a5"}
{"abstract": "In this paper we present a novel method for efficiently controlling the smoke's shape. Given the user-specified shape, a geometric model, our method generates a simulation in which smoke flows out somewhere and forms the shape. The geometric model serves as a black hole that applies attraction force to the fluid. The smoke is attracted to flow into the model and prevented from flowing out the model. In additional, an inner driving force term that drives the smoke from the dense region to the thin region in the geometric model, is added to the Navier-Stokes equations. Two force terms we added, need very little cost compared to the ordinary NS equations.", "authors": ["Yongxia Zhou", "Jiaoying Shi", "Jiarong Yu"], "n_citation": 50, "title": "A new method for controlling smoke's shape", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9de8cc98-9b57-4eb8-94c2-36621f50796c"}
{"abstract": "We present a variation of the index calculus attack by Gaudry which can be used to solve the discrete logarithm problem in the Jacobian of hyperelliptic curves. The new algorithm has a running time which is better than the original index calculus attack and the Rho method (and other square-root algorithms) for curves of genus > 3. We also describe another improvement for curves of genus > 4 (slightly slower, but less dependent on memory space) initially mentioned by Harley and used in a number of papers, but never analyzed in details.", "authors": ["Nicolas Th\u00e9riault"], "n_citation": 0, "title": "Index calculus attack for hyperelliptic curves of small genus", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "a0f6afc7-d149-43e6-a76f-2b135f5a598f"}
{"authors": ["Topi Siro", "Ari Harju"], "n_citation": 50, "title": "Time propagation of many-body quantum states on graphics processing units", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "aa3cb440-12d6-4d91-8b9a-3f32915f8ecd"}
{"abstract": "In the scenario of multiple homogenous intra enterprise workflow domains, several partial processes cooperate according to certain interoperability models. In this case, workflow simulation requires more flexible and powerful mathematical models. In this paper, BCMP queuing network is imported to workflow based business process simulation filed for the first time. With its special multi-class and multi-chain characteristics, we can provide a workflow simulation across multiple domains using three basic interoperability models. This simulation approach accords well with the dynamic and randomness of workflow. Thus, we can report some performance indexes and analysis before deploying.", "authors": ["Xiaohui Zhao"], "n_citation": 50, "title": "Workflow simulation across multiple workflow domains", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b65d5437-a07c-4387-931c-7526720449a6"}
{"abstract": "The Web contains a wide variety of images published by the general public. Because the image is exposed as part of a Web page, the image is specified by not only the image content, but also the Web content using the image. This generates a gap between the content keywords and the usage keywords of the image. To fill the gap, we propose a concept of context of images in the Web. The context of an image is the set of Web contents (e.g. text and images) surrounding the image. We extend the notion of surroundings to include Web contents associated with the image by Web document structure and hyperlinks. We define different types of contexts for an image in accordance with types of associations between the image and the surrounding content. By visualizing context of each image, users can distinguish reliable image content among similar images based on the usage and/or reputation of the image. In this paper, we propose methods for searching and visualizing the context of images. We also propose a method for retrieving images based on their context.", "authors": ["Koji Zettsu", "Yutaka Kidawaral", "Katsumi Tanaka"], "n_citation": 0, "title": "Image retrieval by Web context: Filling the gap between image keywords and usage keywords", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b663e136-8d1c-414e-8cb6-7d799e4e3ad9"}
{"abstract": "Wikis are becoming popular knowledge management tools. Analysing knowledge management requirements, we observe that wikis do not fully support structured search and knowledge reuse. We show how Semantic wikis address the requirements and present a general architecture. We introduce our SemperWiki prototype which offers advanced information access and knowledge reuse.", "authors": ["Eyal Oren", "Max V\u00f6lkel", "John G. Breslin", "Stefan Decker"], "n_citation": 0, "title": "Semantic wikis for personal knowledge management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c1cbdfd9-1569-40a1-b119-d3dc90e69804"}
{"authors": ["Vicki L. Hanson"], "n_citation": 0, "title": "ACM's commitment to accessibility", "venue": "Communications of The ACM", "year": 2017, "id": "c5a8fd49-516c-444b-9a0d-0c117bd12f8b"}
{"abstract": "Although many spatio-temporal conceptual models has been proposed in the last years, users must express their queries on the underlying physical data structures. In the context of the European project MurMur we developed a Query Editor tool that allows the creation of queries visually from the conceptual schema as well as the visualization of the query result. In this paper we describe the Query Translator module that is in charge of transforming the queries expressed for the conceptual schema into an operational query for a target DBMS or GIS (e.g., an SQL query or a PL/SQL program). It is composed of a Transformation module that translates a conceptual query into an equivalent logical query using only the concepts provided by the target software, and a Wrapper module that expresses the query in the Data Manipulation Language of the target platform.", "authors": ["Mohammed Minout", "Esteban Zim\u00e1nyi"], "n_citation": 0, "title": "Algebra-to-SQL query translation for spatio-temporal databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c9710149-3d7e-4c54-9fdf-5df4be64c3c9"}
{"abstract": "In this paper, we propose a new method for authentication, integrity and invertibility of digital images using invertible watermarking. While all watermarking schemes introduce some small amount of non-invertible distortion in the image, the new method is invertible in the sense that, if the image is deemed authentic, the distortion on the extraction procedure can be removed to obtain the original image data. Authentication data is produced from XOR operation between watermark signal and binary bits obtained from a hash function of one block image. If the unmodified watermarked image is used, the extraction process will return the correct watermark signal and exact original image The techniques provide new assurance tools for integrity protection of sensitive imagery such as medical images.", "authors": ["Kil-Sang Yoo", "Mi-Ae Kim", "Won-Hyung Lee"], "n_citation": 0, "title": "Invertible watermarking scheme for authentication and integrity", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cd3801c9-2cdd-4b68-9f4c-c4ddfce4bc7b"}
{"abstract": "Tonal harmony analysis is arguably one of the most sophisticated tasks that musicians deal with. It combines general knowledge with contextual cues, being ingrained with both faceted and evolving objects, such as musical language, execution style, or even taste. In the present work we introduce BREVE, a system for tonal analysis. BREVE automatically learns to analyse music using the recently developed framework of conditional models. The system is presented and assessed on a corpus of Western classical pieces from the 18 th  to the late 19 th  Centuries repertoire. The results are discussed and interesting issues in modeling this problem are drawn.", "authors": ["Daniele P. Radicioni", "Roberto Esposito"], "n_citation": 0, "title": "A Conditional Model for Tonal Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d1d339d9-cdb5-45c5-a126-ef56e352e150"}
{"authors": ["Chun Ruan", "Vijay Varadharajan"], "n_citation": 50, "title": "Reasoning on weighted delegatable authorizations", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "d27fc3c1-2b9f-4d77-b0d3-c36e2d8da641"}
{"abstract": "We present generic frameworks for constructing efficient broadcast encryption schemes in the subset-cover paradigm, introduced by Naor et.al., based on various key derivation techniques. Our frameworks characterize any instantiation completely to its underlying graph decompositions, which are purely combinatorial in nature. This abstracts away the security of each instantiated scheme to be guaranteed by the generic one of the frameworks; thus, gives flexibilities in designing schemes. Behind these are new techniques based on (trapdoor) RSA accumulators utilized to obtain practical performances. We then give some efficient instantiations from the frameworks. Our first construction improves the currently best schemes, including the one proposed by Goodrich et.al., without any further assumptions (only pseudo-random generators are used) by some factors. The second instantiation, which is the most efficient, is instantiated based on RSA and directly improves the first scheme. Its ciphertext length is of order O(r), the key size is O(1), and its computational cost is O(n 1/k log 2  n) for any (arbitrary large) constant k; where r and n are the number of revoked users and all users respectively. To the best of our knowledge, this is the first explicit collusion-secure scheme in the literature that achieves both ciphertext size and key size independent of n simultaneously while keeping all other costs efficient, in particular, sub-linear in n. The third scheme improves Gentry and Ramzan's scheme, which itself is more efficient than the above schemes in the aspect of asymptotic computational cost.", "authors": ["Nuttapong Attrapadung", "Hideki Imai"], "n_citation": 50, "title": "Graph-decomposition-based frameworks for subset-cover broadcast encryption and efficient instantiations", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d8cd2852-2eea-4eac-a1fc-f5ff93dc70ce"}
{"abstract": "In this paper, we integrate two association rule mining algorithms, Apriori and TRAND, with the F2 object-oriented database system (DBMS). The advantages of our integration are the following. Both algorithms do not need to maintain complicated data structures and use only database classes. Both algorithms do not need to manage the buffer since it is handled by the DBMS. Both algorithms store frequent itemsets in the database which can be retrieved later using the DBMS data manipulation language. In addition to that, the TRAND algorithm takes advantage of the transposed storage supported in F2. To compute the support of candidate itemsets, it applies logical AND on boolean attributes, implemented in F2.as vectors, and avoids scanning the database. This reduces significantly the number of block accesses and consequently the execution time.", "authors": ["Lina Al-Jadir"], "n_citation": 0, "title": "Integrating association rule mining algorithms with the F2 OODBMS", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "daf8079c-0af3-4b24-88e2-9518b6448b11"}
{"abstract": "A novel approach to Web browsing called WebDriving is described that automatically extracts information from the Web and places it in a 3D space, enabling children to easily view the information while driving through the 3D world. The user can visualize not only the target Web page, but also the peripheral information (that is, linked and other related pages) in the same 3D world. This makes the user aware of other relevant Web pages while browsing the target Web page. Our WebDriving browser is well suited for theme-based investigative learning, which is now being promoted at many elementary schools in Japan.", "authors": ["Mika Nakaoka", "Taro Tezuka", "Katsumi Tanaka"], "n_citation": 0, "title": "WebDriving : Web browsing based on a driving metaphor for improved children's e-learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e2006796-2940-4260-9c41-e4d334a5f4de"}
{"abstract": "Software engineering in the automotive domain faces outstanding challenges in terms of quality, cost, and functional complexity. To ensure process and product excellence, Bosch Gasoline Systems (GS) introduced a process improvement program based on Capability Maturity Model Integration (CMMI) and adopted the product line approach (PLA). Business strategies for software products, software sharing with customers, and common solutions for diesel and gasoline engine control software are inputs for the product line architecture. The steps towards the PLA started with an evaluation project followed by an evolutionary rollout. The lack of suitable mechanisms and tools for some crucial nonfunctional requirements is a major drawback for introducing the PLA. Nevertheless, GS considers it the only systematic approach to dealing with current and future challenges.", "authors": ["Mirjam Steger", "Christian Tischer", "Birgit Boss", "Andreas Muller", "Oliver Pertler", "Wolfgang Stolz", "Stefan Ferber"], "n_citation": 0, "title": "Introducing PLA at Bosch Gasoline systems: Experiences and practices", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e63ae19d-8c07-473b-a76e-5af6d4332b35"}
{"abstract": "This paper extends our previous research on e-contracts by investigating the problem of deriving business process specifications from business contracts. The aim here is to reduce the risk of behaviour leading to contract violations by encouraging the parties to a contract to follow execution paths that satisfy the policies in the contract. Our current contract monitoring prototype provides run-time checking of policies in contracts. If this system was linked to workflow systems that automate the associated business processes in the contract parties, a finer grain of control and early warning could be provided. We use an example contract to illustrate the different views and the problems of deriving business processes from contracts. We propose a set of heuristics that can be used to facilitate this derivation.", "authors": ["Roger Tagg", "Zoran Milosevic", "Sachin Kulkarni", "Simon Gibson"], "n_citation": 0, "title": "Supporting contract execution through recommended workflows", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e7c930ea-4a8a-4d3b-b0aa-065ad295893a"}
{"abstract": "P systems generating rectangular arrays have been recently introduced in [1,2,3], thus bringing together the two areas of theoretical computer science, namely membrane computing and picture grammars. In this paper, hexagonal arrays on triangular grids are considered and the capabilities of the three approaches [1,2,3] that construct P systems for generation of arrays on rectangular grids, to handle arrays on triangular grids, are demonstrated.", "authors": ["K. S. Dersanambika", "Kamala Krithivasan", "K. G. Subramanian"], "n_citation": 0, "title": "P systems generating hexagonal picture languages", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e8c8994e-fe12-4d69-a053-626adeaddbc2"}
{"abstract": "With the sheer amount of data stored, presented and exchanged using XML nowadays, the ability to extract knowledge from XML data sources becomes increasingly important and desirable. This paper aims to integrate the newly emerging XML technology with data mining technology, using association rule mining as a case in point. Compared with traditional association mining in the well-structured world (e.g., relational databases), mining from XML data is faced with more challenges due to the inherent flexibilities of XML in both structure and semantics. The primary challenges include 1) a more complicated hierarchical data structure; 2) an ordered data context; and 3) a much bigger data size. To tackle these challenges, in this paper, we propose an extended XML-enabled association rule framework, which is flexible and powerful enough to represent both simple and complex structured association relationships inherent in XML data.", "authors": ["L. Feng", "Tharam S. Dillon", "Hans Weigand", "Edward Y. Chang", "Werner Retschitzegger", "Olga Step\u00e1nkov\u00e1"], "n_citation": 0, "title": "An XML-enabled association rule framework", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "eb28dad9-1429-4bb8-b10b-d20222fc2ef2"}
{"abstract": "In this paper, we propose a efficient and secure point multiplication algorithm, based on double-base chains. This is achieved by taking advantage of the sparseness and the ternary nature of the so-called double-base number system (DBNS). The speed-ups are the results of fewer point additions and improved formulae for point triplings and quadruplings in both even and odd characteristic. Our algorithms can be protected against simple and differential side-channel analysis by using side-channel atomicity and classical randomization techniques. Our numerical experiments show that our approach leads to speed-ups compared to windowing methods, even with window size equal to 4, and other SCA resistant algorithms.", "authors": ["Vassil S. Dimitrov", "Laurent Imbert", "Pradeep Kumar Mishra"], "n_citation": 164, "title": "Efficient and secure elliptic curve point multiplication using double-base chains", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ee8915aa-1db0-430e-8491-d4ad9510806f"}
{"abstract": "The development of intelligent multi-agent systems involves a number of concerns, including mobility, context-awareness, reasoning and mining. Towards ubiquitous intelligence this area of research addresses the intersection between mobile agents, heterogeneous networks, and ubiquitous intelligence. This paper presents a development of hardware and software systems to address the combination of these interests as Location-Aware Service. Our architecture performs the intelligent services to meet the respective requirements. By adding autonomous mobility to the agents, the system becomes more able to dynamically localize around areas of interest and adapt to changes in the ubiquitous intelligence landscape. We also analyze some lessons learned based on our experience in using location-aware multi-agent techniques and methods.", "authors": ["Minsoo Lee", "Yong Kim", "Yoonsik Uhm", "Zion Hwang", "Gwanyeon Kim", "Sehyun Park", "Ohyoung Song"], "n_citation": 50, "title": "Location-Aware Multi-agent Based Intelligent Services in Home Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f5ff225b-7a5f-4d9c-a5b2-9c0ae2911ba3"}
{"abstract": "As XML is rapidly becoming the de-facto standard for data representation and exchange in the Internet age, there has been a lot of research on how to store and retrieve XML data in relational databases. However, even though the XML data is mostly tree-structured, the XML research community has shown little attention to the traditional RDBMS-based encoding scheme for tree data. In this paper, we investigate one of the encoding schemes, called Nested Interval, for the storage and retrieval of XML data. In particular, our approach is very robust in updating XML data, including insertion of new node. In fact, the existing RDBMS-based XML storage and indexing techniques work very poorly against XML data update because the XML data should be re-encoded from the scratch for virtually any update in XML data. In contract, Nested Interval scheme does not require re-encoding all nodes. In this respect, our approach is a viable option for storing and querying update-intensive XML application.", "authors": ["Na Gap-Joo", "Sang-Won Lee"], "n_citation": 0, "title": "A relational nested interval encoding scheme for XML data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f97a3daa-1ff3-4acf-8ffd-d335f177fff0"}
{"abstract": "With the increase of internet service providers(companies) for the rapidly growing numbers of internet users in recent years, malicious attackers has been growing too. Due to these attacks, corporate image can be impaired significantly by such damages as increditable service quality and unstable service, which can lead to fatal flaws. Among the malicious attacks, DoS(Denial-of-Service) is the most damaging and frequently reported form of internet attacks. Because DoS attacks employ IP spoofing to disguise the IP and hide the identity of the attacker's location, the correct address of attacker is not traceable only with the source IP address of packets received from damaged systems. Effective measures for the DoS attacks are not developed yet and even if defence is made for this attacks practically it is possible to repeatedly undergo attacks by the same attackers. In this point of view, in order to provide an effective countermeasure this study proposes mechanism to find out attack source by tracing the attack path using marking algorithms and then finding MAC address of attack source. In addition this study proposes technique to improve the packet arrival rate in marking algorithm and presents more effective measure with better performance to find attackers by enabling more prompt trace of the attack location.", "authors": ["Byungryong Kim", "Ki-Chang Kim"], "n_citation": 50, "title": "A Proposal of Extension of FMS-Based Mechanism to Find Attack Paths", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fb84375f-6cd7-463d-a24b-5dcf4a36872d"}
{"abstract": "Reasoning in legal and social domains appears not to be well dealt with by deductive approaches. This is because such reasoning is open-endedly defeasible, and because the various argument schemes used in these domains are often hard to construe as deductive arguments. In consequence, argumentation frameworks have proved increasingly popular for modelling disputes in such domains. In order to capture a third phenomenon of these domains, however, namely that rational disagreement is possible due to a difference in the social values of the disputants, these frameworks have been extended to relate the strengths of arguments in the dispute to the social values promoted by their acceptance. If we are to use such frameworks in computer systems we must be aware of their computational properties. While we can establish efficiently the status of an argument for a particular audience, deciding the status of an argument with respect to all potential audiences is known to be intractable. The main result of this paper is an algorithm which identifies the audiences for which some set of arguments is coherently cotenable. The usefulness of this algorithm is particularly evident in the dialectical settings in which these value based frameworks are most naturally deployed.", "authors": ["Paul E. Dunne", "Trevor J. M. Bench-Capon"], "n_citation": 0, "title": "Identifying audience preferences in legal and social domains", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "fc9cd025-d507-442a-a546-164fa10936cf"}
{"abstract": "We model Chinese pitch accent prediction as a classification problem with six C-ToBI pitch accent types, and apply conditional Maximum Entropy (ME) classification to this problem. We acquire multiple levels of linguistic knowledge from natural language processing to make well-integrated features for ME framework. Five kinds of features were used to represent various linguistic constraints including phonetic features, POS tag features, phrase break features, position features, and length features.", "authors": ["Byeongchang Kim", "Gary Geunbae Lee"], "n_citation": 50, "title": "C-TOBI-Based Pitch Accent Prediction Using Maximum-Entropy Model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "007392eb-eca7-4426-b8cc-709b3e45d266"}
{"abstract": "A plane graph is a planar graph with a fixed embedding. In a rectangular drawing of a plane graph, each vertex is drawn as a point, each edge is drawn as a horizontal or vertical line segment, and each face is drawn as a rectangle. A planar graph is said to have a rectangular drawing if at least one of its plane embeddings has a rectangular drawing. In this paper we give a linear-time algorithm to examine whether a planar graph G of the maximum degree three has a rectangular drawing or not, and to find a rectangular drawing of G if it exists.", "authors": ["Md. Saidur Rahman", "Takao Nishizeki", "Shubhashis Ghosh"], "n_citation": 50, "title": "Rectangular drawings of planar graphs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "02cca907-541d-4055-b565-c3cb4ed129ef"}
{"abstract": "The Java Data Objects specification is designed as lightweight persistence approach. Thus, JDO neither supports user authentication nor role-based authorization. Consequently, users are able to query the entire data store as well as to delete persistent objects without any restriction. The novel security approach JDOSecure was developed at the University of Mannheim to prevent unauthorized access to the data store while using the JDO API. Based on the dynamic proxy approach, JDOSecure introduces role-based permissions to JDO-based applications. In this paper we focuses on how JDOSecure enables Java Data Objects-based applications to deal with role-based permissions.", "authors": ["Matthias Merz", "Markus Aleksy"], "n_citation": 50, "title": "Using JDOSecure to introduce role-based permissions to java data objects-based applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "02f1ae22-31e4-43b3-b589-2c80139d7f69"}
{"abstract": "Utilizing Global Positioning System (GPS) technology, it is possible to find and recommend restaurants for users operating mobile devices. For recommending restaurants, Personal Digital Assistants or cellular phones only consider the location of restaurants. However, a user's background and environment information is assumed to be directly related to recommendation quality. In this paper, therefore, a recommender system using context information and a decision tree model for efficient recommendation is presented. This system considers location context, personal context, environment context, and user preference. Restaurant lists are obtained from location context, personal context, and environment context using the decision tree model. In addition, a weight value is used for reflecting user preferences. Finally, the system recommends appropriate restaurants to the mobile user. For this experiment, performance was verified using measurements such as k-fold cross-validation and Mean Absolute Error. As a result, the proposed system obtained an improvement in recommendation performance.", "authors": ["B. C. Lee", "Heung-Nam Kim", "Jin-Guk Jung", "Geun-Sik Jo"], "n_citation": 0, "title": "Location-based service with context data for a restaurant recommendation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "06c2e1a5-210c-47b0-9c78-81d836912792"}
{"abstract": "The growth and popularity of Internet applications has reinforced the need for effective information filtering techniques. The collaborative filtering approach is now a popular choice and has been implemented in many on-line systems. While many researchers have proposed and compared the performance of various collaborative filtering algorithms, one important performance measure has been omitted from the research to date - that is the robustness of the algorithm. In essence, robustness measures the power of the algorithm to make good predictions in the presence of noisy data. In this paper, we argue that robustness is an important system characteristic, and that it must be considered from the point-of-view of potential attacks that could be made on a system by malicious users. We propose a definition for system robustness, and identify system characteristics that influence robustness. Several attack strategies are described in detail, and experimental results are presented for the scenarios outlined.", "authors": ["Michael P. O'Mahony", "Neil J. Hurley", "Guenole C. M. Silvestre"], "n_citation": 91, "title": "Promoting recommendations: An attack on collaborative filtering", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0e848bb5-1cfc-41f5-a5b5-8b39caa720cc"}
{"abstract": "Currently, Web database systems are widely used to construct Web sites for their excellent capability of providing on-line information. In this paper, we analyse the workflow of a Web database system dealing with a Web page request. After forwarding the data request to database servers, Web servers must wait until all or part of query result is sent back by the database server before proceeding to next step. This is a typical serial queuing system, which is not easy to model except the service-time distribution is exponential. We have classified the different cases and given an approximate method to model a Web database system. As the service time of database servers is a primary factor in determining the input characteristics of Web servers, it is very important to investigate the relationship between database servers and Web servers.", "authors": ["Yuanling Zhu", "K. Lu"], "n_citation": 0, "title": "Performance analysis of Web Database Systems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "13b5ad63-2846-4f55-8555-295fd61e3103"}
{"abstract": "For addressing the growing problem of junk E-mail on the Internet, this paper proposes an effective E-mail classifying and cleansing method in this paper. Incidentally, E-mail messages can be modelled as semi-structured documents consisting of a set of fields with pre-defined semantics and a number of variable length free-text fields. Our proposed method deals with both fields having pre-defined semantics as well as variable length free-text fields for obtaining higher accuracy. The main contributions of this work are two-fold. First, we present a new model based on the Neural Network (NN) for classifying personal E-mails. In particular, we treat E-mail files as a particular kind of plain text files, the implication being that our feature set is relatively large (since there are thousands of different terms in different E-mail files). Second, we propose the use of Principal Component Analysis (PCA) as a preprocessor of NN to reduce the data in terms of both size as well as dimensionality so that the input data become more classifiable and faster for the convergence of the training process used in the NN model. The results of our performance evaluation demonstrate that the proposed algorithm is indeed effective in performing filtering with reasonable accuracy.", "authors": ["Bin Cui", "Anirban Mondal", "Jialie Shen", "Gao Cong", "Kian-Lee Tan"], "n_citation": 0, "title": "On effective e-mail classification via neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "19fa6ead-c17e-4b0f-aae9-b24c7a94b078"}
{"abstract": "Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.", "authors": ["Erik Wittern", "Annie T. T. Ying", "Yunhui Zheng", "Julian Dolby", "Jim Laredo"], "n_citation": 0, "references": ["0c447e07-e1d4-47a2-ba2b-6ba4d02e017c", "2ed548cc-a319-474e-b235-3991ae1a06a9", "4260263f-7be0-45d9-834b-94ed58134ad0", "53f82fd1-9708-40cd-a631-8a654161eca6", "5617ef65-e767-4409-a5a0-9239cede48a9", "652e3c4f-f451-4bf0-887b-d70fdd856909", "67c47318-fc40-42b6-bcbc-b2399acfd062", "6ba654b2-ad78-478b-b857-5f190b8a58d1", "742b8b3e-c0c3-4ed8-a740-edc696bf34bb", "74c8b507-4613-4239-bdb9-9950b20e53f3", "7c498e0f-8556-44a3-a4ee-436d8371c947", "8593c9f2-4b27-40af-a8ed-3d808ce9bd6c", "9b8a35d8-4e49-41af-8c29-da187ca5e127", "b4bb410b-87b0-4844-81b4-aeda92e2165f", "b8ff7d8e-e296-4aea-b611-b3822ddda9b3", "c81c019f-abbb-4cba-ba58-9e3fc3243b57", "cedc9594-6afc-40fe-b527-45e80d8c3b6b", "da7b7975-3003-4266-9a02-7c9014cf37e3", "f153cf20-9429-4140-8ba3-58f31a58c28b"], "title": "Statically checking web API requests in JavaScript", "venue": "international conference on software engineering", "year": 2017, "id": "1fe7f183-e6f8-493d-a394-94a05eed79a8"}
{"abstract": "In object-oriented or object-relational databases such as multimedia databases or most XML databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. However, this has been the way these databases and associated optimisation techniques like clustering have been evaluated up to now. This paper opens up research regarding this issue by proposing a dynamic object evaluation framework (DOEF) that accomplishes access pattern change by defining configurable styles of change. This preliminary prototype has been designed to be open and fully extensible. To illustrate the capabilities of DOEF, we used it to compare the performances of four state of the art dynamic clustering algorithms. The results show that DOEF is indeed effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern.", "authors": ["Zhen He", "J\u00e9r\u00f4me Darmont"], "n_citation": 0, "title": "DOEF: A dynamic object evaluation framework", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "27b91899-a2f0-4942-b8eb-f734d7a6af25"}
{"abstract": "The aim of this paper is to justify the common cryptographic practice of selecting elliptic curves using their order as the primary criterion. We can formalize this issue by asking whether the discrete log problem (DLOG) has the same difficulty for all curves over a given finite field with the same order. We prove that this is essentially true by showing polynomial time random reducibility of DLOG among such curves, assuming the Generalized Riemann Hypothesis (GRH). We do so by constructing certain expander graphs, similar to Ramanujan graphs, with elliptic curves as nodes and low degree isogenies as edges. The result is obtained from the rapid mixing of random walks on this graph. Our proof works only for curves with (nearly) the same endomorphism rings. Without-this technical restriction such a DLOG equivalence might be false; however, in practice the restriction may be moot, because all known polynomial time techniques for constructing equal order curves produce only curves with nearly equal endomorphism rings.", "authors": ["David Jao", "Stephen D. Miller", "Ramarathnam Venkatesan"], "n_citation": 50, "title": "Do all elliptic curves of the same order have the same difficulty of discrete log", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "330c4c75-66f3-479d-b9f4-351ecf05341e"}
{"abstract": "It is crucial to compute the Euclidean distance between two vectors efficiently in high-dimensional space for multimedia information retrieval. We propose an effective method for approximating the Euclidean distance between two high-dimensional vectors. For this approximation, a previous method, which simply employs norms of two vectors, has been proposed. This method, however, ignores the angle between two vectors in approximation, and thus suffers from large approximation errors. Our method introduces an additional vector called a reference vector for estimating the angle between tlie two vectors, and approximates the Euclidean distance accurately by using the estimated angle. This makes the approximation errors reduced significantly compared with the previous method. Also, we formally prove that the value approximated by our method is always smaller than the actual Euclidean distance. This implies that our method does not incur any false dismissal in multimedia information retrieval. Finally, we verify the superiority of the proposed method via performance evaluation with extensive experiments.", "authors": ["Seungdo Jeong", "Sang-Wook Kim", "Kidong Kim", "Byung-Uk Choi"], "n_citation": 0, "title": "An effective method for approximating the euclidean distance in high-dimensional space", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "38b433a2-f2b0-4dbb-8a6d-1ad02599da71"}
{"abstract": "Applying numbering schemes to simulate the structure of XML data is a promising technique for XML query processing. In this paper, we describe SKEYRUS, a system, which enables the integrated structure-keyword searches on XML data using the rUID numbering scheme. rUID has been designed to be robust in structural update and applicable to arbitrarily large XML documents. SKEYRUS accepts XPath expressions containing word-containment predicates as the input, therefore the query expressiveness is significantly extended. The structural feature and the ability to generate XPath axes of rUID are exploited in query processing. Preliminary performance results of SKEYRUS were also reported.", "authors": ["Dao Dinh Kha", "Masatoshi Yoshikawa", "Shunsuke Uemura"], "n_citation": 0, "title": "Application of rUID in processing XML queries on structure and keyword", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3c0352ba-45d8-4750-a559-23b2721a7389"}
{"authors": ["Ankur Sinha", "Pekka Malo", "Kalyanmoy Deb"], "n_citation": 0, "title": "Towards understanding bilevel multi-objective optimization with deterministic lower level decisions", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "4a895cdb-b639-4466-abae-cd6ebb88bd3f"}
{"abstract": "We show how to construct an existentially unforgeable secure signature scheme from any scheme satisfies only a weak notion of security in the standard model. This construction method combines a weakly secure signature and a one-time signature. However, key generation of the resulted fully secure signature is the same as the key generation of weak signature. Therefore the length of the public key in our fully secure signature is independent of that of the one-time signature. Our conversion from a weakly secure signature scheme to an existentially unforgeable secure signature scheme is simple, efficient and provably secure in the standard model (that is, security of the resulting scheme does not rely on the random oracle model). Our results yield a new construction of existentially unforgeable secure signature in the standard model. Furthermore, we show two efficient instantiations without random oracles converted from two previous weakly secure signature schemes.", "authors": ["Jin Li", "Yuen-Yan Chan", "Yanming Wang"], "n_citation": 0, "title": "A Generic Construction of Secure Signatures Without Random Oracles", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4c5e4608-dca4-49a7-91dd-79853ca1d2cb"}
{"abstract": "The Business Process Execution Language BPEL4WS has emerged to introduce process dimension in Web Services coordination. At the same time, a lot of needs related to business process management appeared. In this article we focus on transactional management in Web Services platforms. WS-Transaction specification had a big impact on usage of Web Services in critical situations such as financial services. This usage of transactions in web services coordination also introduced concurrency problems similar to those encountered in transactional databases world due to hard transactional constraints especially for isolation mechanisms. Today, WS-Transactions provide flexible atomicity in Web Services coordination (WS-BusinessActivity) but isolation flexibility is not provided. Isolation mechanisms used today are not really adapted to Service Oriented environments and we aim to make them more 'process friendly'. In this paper, we focus on this important part of concurrency problems and propose a new view of WS-Transactions based on Behavioural Spheres approach. This contribution suggests a reorganisation of the WS-Coordination framework adding WS-IsolationSphere for isolation management and the WS-Sphere coordination type for generalising any behaviour management in Web Services coordination.", "authors": ["Adnene Guabtni", "Fran\u00e7ois Charoy", "Claude Godart"], "n_citation": 0, "title": "Concurrency management in transactional web services coordination", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4faa9fd0-d40b-4070-ad4d-870ed67434b2"}
{"abstract": "Recently, multidimensional point indexing has generated a great deal of interest in applications where objects are usually represented through feature vectors belonging to high-dimensional spaces and are searched by similarity according to a given example. Unfortunately, although traditional data structures and access methods work well for low-dimensional spaces, they perform poorly as dimensionality increases. The application of a dimensionality reduction approach, such as the Karhunen-Love transform, is often not very effective to deal with the indexing problem, since the substantial loss of information does not allow patterns to be sufficiently discriminated in the reduced space. In this work we present a novel hierarchical data structure based on the Multispace KL transform, a generalization of the KL transform, specifically designed to cope with locally correlated data. In the MKL-tree, dimensionality reduction is performed at each node, allowing more selective features to be extracted and thus increasing the discriminant power of the index. In this work the mathematical foundations and the algorithms on which the MKL-tree is based are presented and preliminary experimental results are reported.", "authors": ["Raffaele Cappelli", "Alessandra Lumini", "Dario Maio"], "n_citation": 0, "title": "MKL-tree: A hierarchical data structure for indexing multidimensional data", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4fe5c23b-ec05-4af3-a59a-ce4e6520f67b"}
{"abstract": "Emerging applications using miniature electronic devices (e.g., tracking mobile objects using sensors) generate very large amounts of highly dynamic data that poses very high overhead on databases both in terms of processing and communication costs. A promising approach to alleviate the resulting problems is to exploit the application's tolerance to bounded error in data in order to reduce the overheads. In this paper, we consider imprecise spatial data and the correlation between the data quality and precision requirements given in user queries. We first provide an approach to answer spatial range queries over imprecise data by associating a probability value with each returned object. Then, we present a novel technique to set the data precision constraints for the data collecting process, so that a probabilistic guarantee on the uncertainty in answers to user queries could be provided. The algorithms exploit the fact that objects in two-dimensional space are distributed under certain distribution function. Experimental results are also included.", "authors": ["You Xing-bo", "Sharad Mehrotra"], "n_citation": 0, "title": "Capturing uncertainty in spatial queries over imprecise data", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5c34b183-88a3-4f57-b9bb-79bdea21cabf"}
{"abstract": "In active databases, rules are represented in the form of ECA (event-condition-action). Database events can be detected by defining triggers on the underlying application databases. Many-a-times, temporal conditions that limit the validity period of the event are associated with the ECA rule. The performance of the database can get adversely affected if such temporal constraints are checked (either at the application level or at database level) for every transaction (event) irrespective of whether that transaction (event) has occurred within the said time interval. This drawback can be avoided by optimizing the temporal constraints associated with the sub-events of a composite event based on the semantics of the composite event operators. This paper describes such an algorithm that optimizes the temporal constraints associated with (composite) events and improves the efficiency of the databases by creating and destroying triggers dynamically such that the semantics of the event is unchanged. The efficiency of the technique is validated by our experimental results.", "authors": ["Manish Bhide", "A. Gupta", "Mukul Joshi", "Mukesh K. Mohania"], "n_citation": 0, "title": "Optimal deployment of triggers for detecting events", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6650497b-211b-418c-bf2e-051803a38f8f"}
{"abstract": "In this paper we propose a robust text segmentation method in complex background. The proposed method first utilizes the K-means algorithm to decompose a detected text block into different binary image layers. Then an effective post-processing is followed to eliminate background residues in each layer. In this step we develop a group of robust constraints to characterize general text regions based on color, edge and stroke thickness. We also propose the components relation constraint (CRC) designed specifically for Chinese characters. Finally the text image layer is identified based on the periodical and symmetrical layout of text lines. The experimental results show that our method can effectively eliminate a wide range of background residues, and has a better performance than the K-means method, as well as a high speed.", "authors": ["Libo Fu", "Weiqiang Wang", "Yaowen Zhan"], "n_citation": 50, "title": "A robust text segmentation approach in complex background based on multiple constraints", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "66efdc14-cf52-45a9-ba45-6be2d40031db"}
{"abstract": "Designing, analyzing and managing complex processes are recently become crucial issues in most application contexts, such as e-commerce, business process (re-)engineering, Web/grid computing. In this paper, we propose a framework that supports the designer in the definition and in the analysis of complex processes by means of several facilities for reusing, customizing and generalizing existent process components. To this aim we tightly integrate process models with a domain ontology and an activity ontology, so providing a semantic vision of the application context and of the processes themselves. Moreover, the framework is equipped with a set of techniques providing for advanced functionalities, which can be very useful when building and analyzing process models, such as consistency checking, interactive ontology navigation, automatic (re)discovering of process models. A software architecture fully supporting our framework is also presented and discussed.", "authors": ["Gianluigi Greco", "Antonella Guzzo", "Luigi Pontieri", "Domenico Sacc\u00e0"], "n_citation": 0, "title": "An ontology-driven process modeling framework", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6c20dd9b-05ed-4ed6-b1e4-d4f9733d4656"}
{"abstract": "When defining a scheme of a web application (a web scheme) using a conceptual modelling tool, modelers successively perform design steps by extending or refining the scheme. Each design step is characterized by (i) the scheme to which the design step is applied (input scheme) and (ii) the resulting scheme (output scheme). As modelers apply similar design steps repeatedly, it would be convenient to have schema transformers that, when applied to an input scheme, generate an output scheme. In this paper, we present a way of defining schema transformers by example. A transformer comprises an input and an output template that are parameterized examples of an input and an output scheme, respectively. Therefrom executable code necessary for performing transformations can be generated automatically. A transformer is applied to an input scheme by binding elements of the scheme to parameters. For each such application, a corresponding output scheme is then generated. Our transformers can be introduced in various models/tools for web application modelling. We demonstrate this on the example of WebML.", "authors": ["Stephan Lechner", "Michael Schrefl"], "n_citation": 0, "title": "Defining Web schema transformers by example", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "73fd6a40-15e3-465e-9c9c-f8d5eadd112e"}
{"abstract": "In recent years, many emerging database applications deal with large sets of continuously moving data objects. Since no computer system can commit continuously occurring infinitesimal changes to the database, related data management techniques view a moving object's trajectory as a sequence of discretely reported spatiotemporal points. For each pair of consecutive committed trajectory points, a spatiotemporal uncertainty region representing all possible in-between trajectory points is defined. To support trajectory queries with a non-uniform probability distribution model, the query system needs to compute (interpolate) the most likely trajectories in the uncertainty regions to determine the peak points of the probability distributions. This paper proposes a generalized trajectory interpolation model using parametric trajectory representations. In addition, the paper expands and investigates three practical specializations of our proposed model using a moving object with momentum, i.e., a vehicle, as the exemplar.", "authors": ["Byunggu Yu", "Seon Ho Kim"], "n_citation": 0, "title": "Interpolating and using most likely trajectories in moving-objects databases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "756f9f02-778d-4f83-86b8-04f04a69aea1"}
{"abstract": "The Weil and Tate pairings have been used recently to build new schemes in cryptography. It is known that the Weil pairing takes longer than twice the running time of the Tate pairing. Hence it is necessary to develop more efficient implementations of the Tate pairing for the practical application of pairing based cryptosystems. In 2002, Barreto et al. and Galbraith et al. provided new algorithms for the fast computation of the Tate pairing in characteristic three. In this paper, we give a closed formula for the Tate pairing on the hyperelliptic curve y 2  = x p  - x + d in characteristic p. This result improves the implementations in [BKLS02], [GHS02] for the special case p = 3.", "authors": ["Iwan M. Duursma", "Hyang-Sook Lee"], "n_citation": 0, "title": "Tate pairing implementation for hyperelliptic curves y2 = xp - x + d", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "772bbb27-e490-4337-ad50-d0e636e858ac"}
{"abstract": "In secure video multicast systems, access control is a very important and challenging issue. A common practice is to encrypt the video content using a group key, shared only by legitimate clients. Since the clients can join and leave the multicast session frequently, the group key needs to be updated and delivered to the legitimate clients accordingly in order to provide forward/backward secrecy. Therefore, key management and distribution plays a crucial role in building a secure and scalable video multicast system. Conventional approaches usually use a dedicated secure channel independent of the video data channel, which is expensive and inefficient. In this paper, we propose a novel and scalable Media-dependent Key Management and Distribution (MKMD) solution that embeds key management messages in video data so as to save network bandwidth and enhance the security level, and most importantly, without sacrificing the quality of video content too much. We have built a prototype system of MKMD, and our extensive experimental results show that MKMD is a secure, scalable, reliable and efficient key management and distribution solution for video multicast system.", "authors": ["Hao Yin", "Xiaowen Chu", "Chuang Lin", "Feng Qiu", "Geyong Min"], "n_citation": 0, "title": "A novel key management and distribution solution for secure video multicast", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7760020e-d6fd-4272-aa86-d23ac6622d6d"}
{"abstract": "In this paper, we propose the audio watermarking algorithm based on the critical band of HAS(human auditory system) without audibly affecting the quality of the watermarked audio and implement the detecting algorithm on the BSS(broadcast synchronizing system) for testing the proposed algorithm. According to the audio quality test, the SNR(signal to noise ratio) of the watermarked audio objectively is 66dB above. In the robustness test, the proposed algorithm can detect the watermark more than 90% from various compression (MP3, AAC), A/D and D/A conversions, sampling rate conversions and especially asynchronizing attacks. The BSS automatically switches the programs between the key station and the local station in broadcasting system. The result of reliability test of implemented system by using the real broadcasting audio has no false positive error during 90 days. Because of detecting once processing per 0.5 second, we can judge that the false positive error does not nearly occur.", "authors": ["Donghwan Shin", "Jong-Weon Kim", "Jong-Uk Choi"], "n_citation": 0, "title": "Broadcast synchronizing system using audio watermark", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "78175eb2-d33e-4090-8240-8256eae966b8"}
{"abstract": "The Situation Calculus has been used by Scher1 and Levesque to represent beliefs and belief change without modal operators thanks to a predicate plays the role of an accessibility relation. Their approach has been extended by Shapiro et al. to support belief revision. In this extension plausibility levels are assigned to each situation, and the believed propositions are the propositions that are true in all the most plausible accessible situations. Their solution is quite elegant from a theoretical point of view but the definition of the plausibility assignment, for a given application domain, raises practical problems. This paper presents a new proposal that does not make use of plausibilities. The idea is to include the knowledge producing actions into the successor state axioms. In this framework each agent may have a different successor state axiom for a given fluent. Then, each agent may have his subjective view of the evolution of the world. Also, agents may know or may not know that a given action has been performed. That is, the actions are not necessarily public.", "authors": ["Robert Demolombe", "Pilar Pozos Parra"], "n_citation": 50, "title": "Belief Revision in the Situation Calculus Without Plausibility Levels", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "785db953-e035-4b3c-aa35-b93f87fbb4ce"}
{"abstract": "We introduce a method for shape similarity based retrieval in 3D object model database. The proposed method leads us to achieve effectiveness and robustness in similar 3D object search supporting both query by 3D model and query by 2D image. Our feature extraction mechanism is based on observation of human behavior in recognizing objects. Our process of extracting spatial arrangement of a 3D object by surface point distribution can be considered as using human tactile sensation without visual information. On the other hand, the process of extracting 2D features from multiple views can be considered as examining an object by moving viewpoints(camera positions). We propose shape signatures for 3D object model by measuring features of surface point and the shape distance distribution from multiple views of 3D model. Our method can be directly applied to industrial part retrieval and inspection system where different geometric representations are used.", "authors": ["Jeong-Jun Song", "Forouzan Golshani"], "n_citation": 50, "title": "3D object retrieval by shape similarity", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7d02c9ec-d8ca-47a6-89e6-9313131edabf"}
{"authors": ["Emilio F. Campana", "Matteo Diez", "Giovanni Fasano", "Daniele Peri"], "n_citation": 50, "references": ["f5cc526f-6cd4-401e-aac1-416ac15aa146"], "title": "Initial Particles Position for PSO, in Bound Constrained Optimization", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "831058e6-73ad-4b25-bee3-ba19847658e3"}
{"abstract": "In this paper, we propose an adaptive urgency and efficiency based packet scheduling (A-UEPS) algorithm that is to designed to maximize throughput of non-real-time (NRT) traffics as long as QoS requirements of real-time (RT) traffics such as the packet delay and the loss rate requirements are satisfied. In addition, the A-UEPS algorithm provides trade-off between performance objectives of RT and NRT traffics adaptively by adjusting scheduling precedence of packets of RT and NRT traffics in accordance with the time-varying traffic situations such as the offered traffic load and traffic mix. Simulation study shows that A-UEPS algorithm results in efficient and adaptive scheduling performance under various traffic situations.", "authors": ["Seungwan Ryu", "Byung-Han Ryu", "Hyun-Hwa Seo"], "n_citation": 50, "title": "Adaptive and QoS downlink multimedia packet scheduling for broadband wireless systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "83f10c24-7af7-4832-bc97-6fcc1b49a390"}
{"abstract": "Multi-Agent Systems are computational systems in which a collection of autonomous agents interact to achieve a certain task, for example to fulfil an obligation directed to the whole group, i.e., a collective obligation. Since, such a collective obligation is beyond the capacity of an individual agent, the agents have to communicate, cooperate, coordinate and negotiate with each other, to achieve the collective task: the fulfilment of the obligation. In this paper we discuss and formalise collective aspects of obligations and commitments. Collective obligations are analysed and formalised in a deontic logic framework. The notions of individual and collective commitment are defined to specify which individual has the responsibility to fulfill an 'internal' obligation as part of the collective obligation. In distributed artificial intelligence (DAI) theories of organisations, it is emphasized that `commitment  is a crucial notion to analyse a collective activity or the structure of an organisation. In this paper we give a first attempt to formalise the notion of commitment to determine which plan has to be followed to achieve a joint goal, i.e. the fulfillment of a collective obligation by using several concepts as commitment, delegation and authority-relation.", "authors": ["Lamb\u00e8r M. M. Royakkers", "F. Dignum"], "n_citation": 22, "title": "Organizations and collective obligations", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "8aeaf8d8-1203-48e7-b3e7-c570d34ae39c"}
{"abstract": "In this paper, we examine how to efficiently process top-k queries in pure P2P network. First, we propose a distributed algorithm to evaluate a top-k query in a hierarchical way. Each peer does its local top-k query, and top k results from different peers are merged hierarchically from bottom to the top (the root peer is the query initiator). Ranking and merging of results are distributed across the peers to exploit the computing resources in the network. Second, to improve performance, we maintain histograms at each peer according to the top k results returned by the peers. The histograms are used to estimate the possible upper bound scores of peers so that a query only needs to be forwarded to the most promising neighbouring peers. Our experimental study shows that the top-k query algorithm improves the query effectiveness, while the use of histograms enhances the query efficiency.", "authors": ["Yingjie He", "Yanfeng Shu", "Shan Wang", "Xiaoyong Du"], "n_citation": 3, "title": "Efficient top-k query processing in P2P network", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "913bc35b-dc58-46de-8095-56cf57eb8dae"}
{"abstract": "In relational database management systems indexes are used to accelerate specific queries. The selection of indexes is an important task when tuning a database which is performed by a database administrator or an index propagation tool which suggests a set of suitable indexes. In this paper we introduce a new index approach, called key-oriented XML index (KeyX), that uses specific XML element or attribute values as keys referencing arbitrary nodes in the XML data. KeyX is selective to specific queries avoiding efforts spent for elements which are never queried. This concept reduces memory consumption and unproductive index updates. We transfer the Index Selection Problem (ISP) to XDBMS. Applying the ISP, a workload of database operations is analyzed and a set of selective indexes that minimizes the total execution time for the workload is suggested. Because the workload is analyzed periodically and suitable indexes are created or dropped automatically our implementation of KeyX guarantees high performance over the total life time of a database.", "authors": ["Beda Christoph Hammerschmidt", "Martin Kempa", "Volker Linnemann"], "n_citation": 0, "title": "A selective key-oriented XML Index for the Index Selection Problem in XDBMS", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "961668c3-042d-4de3-b45c-0494bff85e19"}
{"abstract": "In this paper, we address the topic of monogenic curvature scale-space. Combining methods of tensor algebra, monogenic signal and quadrature filter, the monogenic curvature signal, as a novel model for intrinsically two-dimensional (i2D) structures, is derived in an algebraically extended framework. It is unified with a scale concept by employing damped spherical harmonics as basis functions. This results in a monogenic curvature scale-space. Local amplitude, phase and orientation, as independent local features, are extracted. In contrast to the Gaussian curvature scale-space, our approach has the advantage of simultaneous estimation of local phase and orientation. The main contribution is the rotationally invariant phase estimation in the scale-space, which delivers access to various phase-based applications in computer vision.", "authors": ["Di Zang", "Gerald Sommer"], "n_citation": 50, "title": "The monogenic curvature scale-space", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9aa908da-0224-4fb5-b4d4-4325ad10e813"}
{"abstract": "An important aspect of agent autonomy is the decision making capability of the agents. We discuss several issues that agents need to deliberate about in order to decide which action to perform. We assume that there is no unique (rational or universal) deliberation process and that the deliberation process can be specified in various ways. The deliberation process is investigated from two perspectives. From the agent specification point of view the deliberation process can be specified by dynamic properties such as commitment strategies, and from the agent programming point of view the deliberation process should be implemented through the deliberation cycle of the agent, which can be either fixed or determined by a deliberation programming language.", "authors": ["Mehdi Dastani", "Frank Dignum", "John-Jules Ch. Meyer"], "n_citation": 50, "title": "Autonomy and agent deliberation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9c00aa5b-9b3c-4f6d-8dcf-ba2a6e864561"}
{"abstract": "Security is generally believed to be a very important topic. However, during software development security requirements are hardly ever properly treated, least of all on the conceptual level. Security is considered as some kind of add-on which will be applied to the system after development. To fill this gap we work on the development of a conceptual security modeling method we refer to as CoSMo Conceptual Security Modeling). In this paper first a comprehensive summary of available security modeling methodologies is presented. Second, various security requirements and mechanisms which are necessary for building secure software systems are described systematically to give a clear distinction between requirements and mechanisms to enforce the security requirements. Finally, a modeling example is given to illustrate particular security requirements and mechanisms.", "authors": ["Christine Artelsmair", "Wolfgang Essmayr", "Peter Lang", "Roland Wagner", "Edgar R. Weippl"], "n_citation": 0, "title": "CoSMo: An approach towards Conceptual Security modeling", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a09f5de0-01c7-4505-a5d6-f739df075b97"}
{"abstract": "A process following a security protocol is represented by a formal proof (of a fragment of linear logic based on the multiset rewriting model), modifying the idea by Cervesato-Durgin-Lincoln-Mitchell-Scedrov [4], while the (modified) BAN logic (which was first introduced by Burrows-Abadi-Needham [2]) is used as an evaluation semantics on security-properties for processes. By this method, we can get rid of the so called idealization step in the verification procedure of the BAN framework. In particular, we classify BAN-style belief-inferences into two categories; the inferences which only require some syntactic structure of a process observed by a participant on one hand, and the inferences which require a participant's knowledge on the structure of a protocol and a certain honesty assumption. We call the latter the honesty inferences. We shall show how such honesty inferences are used in the evaluation semantics for the security verification. We also point out that the evaluation inferences on freshness of nonces/keys/messages are classified as in the first category but that some of such inferences lack the information how to evaluate due to the lack of a certain concrete time-constraint setting. We introduce a natural time-constraint setting in our process/protocol descriptions and enrich the expressive power of the freshness evaluation.", "authors": ["Koji Hasebe", "Mitsuhiro Okada"], "n_citation": 0, "title": "A logical verification method for security protocols based on linear logic and BAN logic", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "af75036f-40ee-4f0e-b1bc-9c1bc537a0af"}
{"abstract": "Many studies in automated Text Categorization focus on the performance of classifiers, with or without considering feature selection methods, but almost as a rule taking into account just one document representation. Only relatively recently did detailed studies on the impact of various document representations step into the spotlight, showing that there may be statistically significant differences in classifier performance even among variations of the classical bag-of-words model. This paper examines the relationship between the idf transform and several widely used feature selection methods, in the context of Naive Bayes and Support Vector Machines classifiers, on datasets extracted from the dmoz ontology of Web-page descriptions. The described experimental study shows that the idf transform considerably effects the distribution of classification performance over feature selection reduction rates, and offers an evaluation method which permits the discovery of relationships between different document representations and feature selection methods which is independent of absolute differences in classification performance.", "authors": ["Milo\u0161 Radovanovi\u0107", "Mirjana Ivanovi\u0107"], "n_citation": 0, "title": "Interactions between document representation and feature selection in text categorization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b1fe8b19-99a9-49da-93d4-4d94d01ec6b6"}
{"abstract": "The file attached to this record is the author's final peer reviewed version. The Publisher's final version can be found by following the DOI link.", "authors": ["Marina Jirotka", "Barbara Grimpe", "Bernd Carsten Stahl", "Grace Eden", "Mark Hartswood"], "n_citation": 0, "references": ["3b3a466f-5c5c-4a50-9c73-119ec0d3cc07", "515cc966-246b-4caa-ab19-159739d6e489", "6eed0894-c9e4-4728-b716-5ebb65a7ecd1", "b2431173-cb6a-4433-823c-962595849176", "bdfa84aa-f898-491a-9c67-a9541c6b0a05", "c61d09dd-4bab-4bf9-83a0-f463101f4f68", "e396bf7d-3f96-404c-abf1-56ba48dcf56f", "e7ae2f2a-78a5-4d27-8376-3dadf1a40444"], "title": "Responsible Research and Innovation in the Digital Age", "venue": "Communications of The ACM", "year": 2017, "id": "b205a71f-2dd1-403a-b25a-e378c8bfbde0"}
{"abstract": "This paper presents an index clustering technique called the segmented page indexing (SP-indexing) for multidimensional index structures. The design objectives of the SP-indexing are twofold: (1) to improve the range query performance of the multidimensional indexing methods and (2) to provide a compromise between optimal index clustering and excessive full index reorganization overhead. The SP-indexing uses two kinds of I/O units: pages for random disk accesses and segments for sequential accesses. The SP-indexing improves the range query performance by offering high-performance sequential disk access within a segment. Experimental results demonstrate that the SP-indexing improves the range query performance up to several times compared with the traditional page-based indexing methods with respect to the total elapsed time.", "authors": ["Guang-Ho Cha", "Yong-Ik Yoon"], "n_citation": 2, "title": "Clustered indexing technique for multidimensional index structures", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b5d45de8-e76a-4829-adb4-0255a480190a"}
{"abstract": "We develop a framework for combining configurational and statistical approaches in image retrieval. While configurations have semantic description power, the explicit representation of an image by a set of configurations lacks the vector space structure from which the statistical feature-based representations have benefitted. That makes concept learning and prediction harder. Our framework treats configurations analogously to words occurring in a document. It combines a configuration-based approach with statistical approaches to take advantage of both the semantic description power of the former, and the simple vector-space structure of the latter.", "authors": ["Huizhen Yu", "W. Eric L. Grimson"], "n_citation": 0, "title": "Combining configurational and statistical approaches in image retrieval", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "b6ed2213-7371-4ff7-9d9a-7a924777a97d"}
{"abstract": "We present a method for calculating fuzzy distances between pairs of points in an image using the A* algorithm and, furthermore, apply this method for fuzzy distance based hierarchical clustering. The method is general and can be of use in numerous applications. In our case we intend to use the clustering in an algorithm for delineation of objects corresponding to parts of proteins in 3D images. The image is defined as a fuzzy object and represented as a graph, enabling a path finding approach for distance calculations. The fuzzy distance between two adjacent points is used as edge weight and a heuristic is defined for fuzzy sets. A* is applied to the calculation of fuzzy distance between pair of points and hierarchical clustering is used to group the points. The normalised Hubert's statistic is used as validity index to determine the number of clusters. The method is tested on three 2D images; two synthetic images and one fuzzy distance transformed microscopy image of stem cells. All experiments show promising initial results.", "authors": ["Magnus Gedda", "Stina Svensson"], "n_citation": 50, "title": "Fuzzy distance based hierarchical clustering calculated using the A* algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b857c38a-7cee-47ae-8f74-a869a3cd19a0"}
{"abstract": "During Legal information systems migrations, one major problem is to handle attribute value cleaning. In the paper, we first show many data cleaning steps process on the values of the same data attributes and their derivations, and users may ignore or be puzzled by the data transforms that are defined to clean and transform the data sets, and such process can also be prone to error during process optimization. In this paper, we first define two major such problems as assignment conflict and range conflict,and giving problem definitions for such conflicts.- Then we present two separate algorithms respectively to discover and solve the conflicts.", "authors": ["Youlin Fang", "Heng Wang", "Dongqing Yang"], "n_citation": 0, "title": "Avoiding error-prone reordering optimization during legal systems migration", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ba68b88b-c8a6-4b65-b240-e054b41064b3"}
{"abstract": "In order to increase flexibility and provide up-to-date information, more and more web sites use dynamic content. This practice, however, increases server load dramatically, since each request results to code execution, which may involve processing and/or access to information repositories. In this paper we present a scheme for maintaining a server-side cache of dynamically generated pages, allowing for cache consistency maintenance, without placing heavy burdens on application programmers. We also present insights to architecture scalability and some results obtained from conducted experiments.", "authors": ["Costas Vassilakis", "Giorgos Lepouras"], "n_citation": 50, "title": "Controlled caching of dynamic WWW pages", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "bb0a1903-ce98-49cc-a019-5102d1cbcd56"}
{"abstract": "Research efforts on conventional CPU architectures over the past decade have focused primarily on performance enhancement. In contrast, the NPU (Network Processing Unit) architectures have evolved significantly in terms of functionality. The memory hierarchy of a typical network router features a Content-Addressable Memory (CAM) which provides very fast constant-time lookups over large amounts of data and facilitates a wide range of novel high-speed networking solutions such as Packet Classification, Intrusion Detection and Pattern Matching. While these networking applications span an entirely different domain than the database applications, they share a common operation of searching for a particular data entry among huge amounts of data. In this paper, we investigate how CAM-based technology can help in addressing the existing memory hierarchy bottlenecks in database operations. We present several high-speed CAM-based solutions for computationally intensive database operations. In particular, we discuss an efficient linear-time complexity CAM-based sorting algorithm and apply it to develop a fast solution for complex join operations widely used in database applications.", "authors": ["Nagender Bandi", "Divyakant Agrawal", "Amr El Abbadi"], "n_citation": 0, "title": "Fast computation of database operations using content-addressable memories", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cb38e618-9427-476f-9cb9-012ab46a9fd1"}
{"abstract": "Computing aggregates over distributed data sets constitutes an interesting class of distributed queries. Recent advances in peer-to-peer discovery of data sources and query processing techniques have made such queries feasible and potentially more frequent. The concurrent execution of multiple and often identical distributed aggregate queries can place a high burden on the data sources. This paper identifies the scalability bottlenecks that can arise in large peer-to-peer networks from the execution of large numbers of aggregate computations and proposes a solution. In our approach peers are assigned the role of aggregate computation maintainers, which leads to a substantial decrease in requests to the data sources and also avoids duplicate computation by the sites that submit identical aggregate queries. Moreover, a framework is presented that facilitates the collaboration of peers in maintaining aggregate query results. Experimental evaluation of our design demonstrates that it achieves very good performance and scales to thousands of peers.", "authors": ["Leonidas Galanis", "David J. DeWitt"], "n_citation": 0, "title": "Scalable distributed aggregate computations through collaboration", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ceae6148-3e8c-4022-9618-2d8ac4acd309"}
{"abstract": "Surface Light Fields(SLF) are light fields parameterized to the surface of a geometric model. Past research in this area has explored the use of parameterization of aquired geometric model to the base mesh, in the context of rendering from compressed representation of the SLF. We further extend this aproach to enable hardware-accelerated SLF rendering. We show that our aproach enables hardware-accelerated, scalable rendering with commodity graphics hardware.", "authors": ["Masaki Kitahara", "Shinya Shimizu", "Kazuto Kamikura", "Yashima Yoshiyuki"], "n_citation": 0, "title": "Hardware accelerated image-based rendering with compressed surface light fields and multiresolution geometry", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d2e643ee-ece3-4ed6-89df-7eb61e2f7386"}
{"abstract": "Advanced Video Coding is recently announced and widely used, although the according protection means have not been developed thoroughly. In this paper, a selective encryption scheme is constructed on Advanced Video Coding. During AVC encoding, such sensitive data as intra-prediction mode, residue data, inter-prediction mode and motion vector are partially encrypted. This encryption scheme keeps secure against brute-force attack, replacement attack or known-plaintext attack, combines encryption process with compression process with low cost, and keeps the file format unchanged with some direct operations (such as displaying, time seeking, copying, cutting, etc.) supported. These properties make it suitable for secure video transmission.", "authors": ["Shiguo Lian", "Zhongxuan Liu", "Zhen Ren", "Zhiquan Wang"], "n_citation": 65, "title": "Selective video encryption based on advanced video coding", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d6f96b03-9143-425c-91de-8c93c5a43336"}
{"abstract": "Medical guidelines are clinical behaviour recommendations used to help and support physicians in the definition of the most appropriate diagnosis and/or therapy within determinate clinical circumstances. Due to the intrinsic complexity of such guidelines, their application is not a trivial task; hence it is important to verify if health-care workers behave in a conform manner w.r.t. the intended model, and to evaluate how much their behaviour differs. In this paper we present the GPROVE framework that we are developing within a regional project to describe medical guidelines in a visual way and to automatically perform the conformance verification.", "authors": ["Federico Chesani", "Pietro De Matteis", "Paola Mello", "Marco Montali", "Sergio Storari"], "n_citation": 0, "title": "A Framework for Defining and Verifying Clinical Guidelines : A Case Study on Cancer Screening", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d7ca7de2-c08c-430e-9acb-e2f1daab856b"}
{"abstract": "News event detection is the task of discovering relevant, yet previously unreported real-life events and reporting it to users in human-readable form, while event tracking aims to automatically assign event labels to news stories when they arrive. A new method and system for performing the event detection and tracking task is proposed in this paper. The event detection and tracking method is based on subject extraction and an improved support vector machine (SVM), in which subject concepts can concisely and precisely express the meaning of a longer text. The improved SVM first prunes the negative examples, reserves and deletes a negative sample according to distance and class label, then trains the new set with SVM to obtain a classifier and maps the SVM outputs into probabilities. The experimental results with the real-world data sets indicate the proposed method is feasible and advanced.", "authors": ["Zhen Lei", "Ling-Da Wu", "Ying Zhang", "Yu-Chi Liu"], "n_citation": 0, "title": "A system for detecting and tracking internet news event", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "dad6b248-6edf-43d9-be82-20adb7d8457e"}
{"abstract": "A discrete rotation algorithm can be apprehended as a parametric map f\u03b1 from Z[i] to Z[i], whose resulting permutation looks like the map induced by an Euclidean rotation. For this kind of algorithm, to be incremental means to compute successively all the intermediate rotated copies of an image for angles in-between 0 and a destination angle. The discretized rotation consists in the composition of an Euclidean rotation with a discretization; the aim of this article is to describe an algorithm which computes incrementally a discretized rotation. The suggested method uses only integer arithmetic and does not compute any sine nor any cosine. More precisely, its design relies on the analysis of the discretized rotation as a step function: the precise description of the discontinuities turns to be the key ingredient that makes the resulting procedure optimally fast and exact. A complete description of the incremental rotation process is provided, also this result may be useful in the specification of a consistent set of definitions for discrete geometry.", "authors": ["Bertrand Nouvel", "Eric R\u00e9mila"], "n_citation": 50, "title": "Incremental and transitive discrete rotations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f0e3c569-dc5c-40b8-9f86-fd949cb038fd"}
{"abstract": "The popularity of the worldwide web and the easy availability of the digital content has brought the security issues to the forefront. NURBS is widely used in computer-aided geometry design and computer graphics for its strong representative properties. In this paper, we present a blind watermarking algorithm for NURBS surfaces. First the points sampled from the NURBS surface are watermarked in DCT domain, then the watermarked NURBS surface is obtained by fitting the watermarked points iteratively. In watermark detection stage, the sign correlation detector is used, which is a blind detector and accordingly the original NURBS surface is not required for detection. The experimental results show that the algorithm preserves the shape of the NURBS surface. The proposed algorithm is robust against attacks such as knot insertion, knot removal, repararneterization, order elevation, order reduction, additive white Gaussian noise, rotation, translation, scaling, multi-watermark attacks, etc.", "authors": ["Zhigeng Pan", "Shusen Sun", "Mingmin Zhang", "Daxing Zhang"], "n_citation": 50, "title": "Watermarking NURBS surfaces", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f190e367-b751-4895-80ac-cf79b767e682"}
{"abstract": "Video watermarking hides information (e.g. ownership, recipient information, etc) into video contents. Video watermarking research is classified into (1) extension of still image watermarking, (2) use of the temporal domain features, and (3) use of video compression formats. In this paper, we propose a watermarking scheme to resist geometric attack (rotation, scaling, translation, and mixed) for H.264 (MPEG-4 Part 10 Advanced Video Coding) compressed video contents. Our scheme is based on auto-correlation method for geometric attack, a video perceptual model for maximal watermark capacity, and watermark detection based on natural image statistics. We experiment with the standard images and video sequences and the result shows that our video watermarking scheme is robust against H.264 video compression (average PSNR = 31 dB) and geometric attacks (rotation with 0-90 degree, scaling with 75-200%, and 50%\u223c75% cropping).", "authors": ["Seong-Whan Kim", "Hyun-Sung Sung"], "n_citation": 0, "title": "Perceptually tuned auto-correlation based video watermarking using independent component analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f19c4fbd-9d37-4b6a-bcea-1276ec508b7f"}
{"abstract": "The designing of wide area networks is usually an optimization process with accurately selected optimization criterion. The most utilized criterions are the quality of service in the network (indicated by average packet delay), the capacity cost (channel capacity leasing cost) and server costs (cost of connecting servers or replicas at nodes). This paper studies the problem of designing wide area networks with taking into account those three criteria. Then, the goal is select servers replica allocation at nodes, network topology, channel capacities and flow routes in order to minimize the linear combination of average delay per packet, capacity cost and server cost. The problem is NP-complete. An exact algorithm, based on the branch and bound method is proposed. Some computational results are reported and several properties of the considered problem are formulated.", "authors": ["Marcin Markowski", "Andrzej Kasprzak"], "n_citation": 0, "title": "The Three-Criteria Servers Replication and Topology Assignment Problem in Wide Area Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f6d0f081-89e9-4dc1-af27-641fe79190bf"}
{"abstract": "Using the left. merge and communication merge from ACP, we present an equational base (i.e., a ground-complete and \u03c9-complete set of valid equations) for the fragment of CCS without restriction and relabelling. Our equational base is finite if the set of actions is finite.", "authors": ["Luca Aceto", "Wan Fokkink", "Anna Ing\u00f3lfsd\u00f3ttir", "Bas Luttik"], "n_citation": 0, "title": "A Finite Equational Base for CCS with Left Merge and Communication Merge", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fb716f49-443c-470b-9f3e-26b7d4d02933"}
{"abstract": "Multimedia Document (MMD) such as Web Page and Multimedia cyclopedias is composed of media objects of different modalities, and its integrated semantics is always expressed by the combination of all media objects in it. Since the contents in MMDs are enormous and the amount of them is increasing rapidly, effective management of MMDs is in great demand. Meanwhile, it is meaningful to provide users cross-media retrieval facilities so that users can query media objects by examples of different modalities, e.g. users may query an MMD (or an image) by submitting a audio clip and vice versa. However, there exist two challenges to achieve the above goals. First, how can we represent an MMD and fuse media objects together to achieve Cross-index and facilitate Cross-media retrieval? Second, how can we understand MMD semantics? Taking into account of the two problems, we give the definition of MMD and propose a manifold learning method to discover MMD semantics in this paper. We first construct an MMD semi-semantic graph (SSG) and then adopt Multidimensional scaling to create an MMD semantic space (MMDSS). We also propose two periods' feedbacks. The first one is used to refine SSG and the second one is adopted to introduce new MMD that is not in the MMDSS into MMDSS. Since all of the MMDs and their component media objects of different modalities lie in MMDSS, cross-media retrieval can be easily performed. Experiment results are encouraging and indicate that the performance of the proposed approach is effective.", "authors": ["Fei Wu", "Yi Yang", "Yueting Zhuang", "Yunhe Pan"], "n_citation": 0, "title": "Understanding multimedia document semantics for cross-media retrieval", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fe6956bc-c8dd-41cd-9289-986fb7638bf1"}
{"abstract": "The optimization of XML queries requires an accurate and compact structure to capture the characteristics of the underlying data. A compact structure works well when the data is uniformly distributed and has many common paths. However, more detailed information needs to be maintained when the data is skewed. This work presents a histogram-based structure to capture the distribution of skewed XML data. It builds upon a statistical method to estimate the result size of XML queries. Experiment results indicate that the proposed method leads to a more accurate estimation.", "authors": ["Hanyu Li", "Mong Li Lee", "Wynne Hsu"], "n_citation": 0, "title": "A histogram-based selectivity estimator for skewed XML data", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "01b655d8-dd0a-4d0b-9508-17d1c2953f77"}
{"abstract": "Transcoding is an important technique for reducing the bit rate or spatial resolution to meet the constrained transmission bandwidths and terminal capabilities. In this paper, we propose a spatial downscaling transcoding method to convert an MPEG-2 bitstream into an H.264/AVC bitstream. A novel feature of the proposed method is to focus on the fast mode decision that fully exploits the advantages of variable block-size motion compensation feature in H.264/AVC. In the transcoder, types and motion vectors of pre-encoded macroblocks are considered together to decide the new encoding block type. While maintaining a reasonable image quality, the proposed method significantly reduces the computational complexity and facilitates the video servers to provide multimedia service in real time for heterogeneous clients. Experimental results show that our method is very effective.", "authors": ["Bo Hu", "Peng Zhang", "Qingming Huang", "Wen Gao"], "n_citation": 0, "title": "Reducing spatial resolution for MPEG-2 to H.264/AVC transcoding", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "0d65aed8-eecf-493b-ba0e-eaefd7c3e2ef"}
{"abstract": "In this paper, we propose an approach and a framework for an a priori change impact analysis of database schemas, in federated environment. The approach is based on a model, that describes program source codes and database schemas as software components linked by meaningfully relationships. This model takes into account the software components for both centralized and CORBA-based federated database applications. We deal with the federated issue, in accordance with the Object Database Management Group specifications. The change impact analysis is done, using a Knowledge Based System, that includes impact propagation rules, in a distributed way. This is achieved by proposing a framework, that implements our model, in order to simulate the evolution of CORBA-based federated database schemas.", "authors": ["Laurent Deruelle", "M. Bouneffa", "Nordine Melab", "Henri Basson", "G. Goncalves", "Jean-Christophe Nicolas"], "n_citation": 0, "title": "A change impact analysis approach for CORBA-based federated databases", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "137b0e20-b391-4e55-a277-a5ea9d6001f3"}
{"abstract": "Unstructured peer-to-peer systems rely on strategies and data structures (Routing Indices) for the routing of requests in the network. For those requests corresponding to information retrieval queries, the emphasis can be either put on the effectiveness of the routing by privileging the relevance of the documents retrieved, or on the efficiency of the routing by privileging the response time. We propose in this paper a novel routing strategy based on adaptive Routing Indices. The Routing Indices are adaptive to the environment, i.e. network traffic, location, as well as relevance of the documents indexed, thanks to a reinforcement learning approach to their maintenance. The strategy can be used to tune the compromise between efficient and effective routing. It combines the estimation of the response time of routes with the estimation of the relevance of routes to keywords. We study performance and the tuning of the compromise offered by this novel strategy under various characteristics of the network and traffic.", "authors": ["St\u00e9phane Bressan", "Achmad Nizar Hidayanto", "Chu Yee Liau", "Zainal A. Hasibuan"], "n_citation": 0, "title": "Adaptive double Routing indices: Combining effectiveness and efficiency in P2P systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1badfa30-5ebd-4361-a460-936a8d27a2e8"}
{"abstract": "Multidimensional arrays can be used to store multidimensional data cubes. One way of storing such array onto disk is to partition the array into a set of small chunks that can fit in a disk block. Chunks are usually of the same side length and are compressed before being stored, in order to achieve high query processing speed and space utilization. This scheme is called the chunk-based cube storage scheme. Although the scheme is considered to be a position based storage scheme, the chunks cannot be retrieved fast without using indexes since they are compressed. In this paper, we propose a bitmap index for such a storage scheme. The index can be constructed with the data cube in parallel. The relative positions of the chunks are retained in the index so that the chunk retrieval can be done in constant time. We placed in an index block as many chunks as possible so that the number of index searches is minimized for OLAP operations such as slice, dice, and range queries. We evaluated the proposed index by comparing it with existing multidimensional indexes such as UB-tree and grid file in terms of time and space.", "authors": ["Yoonsun Lim", "Myung Kim"], "n_citation": 0, "title": "A bitmap index for multidimensional data cubes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1e1d5620-8acd-431f-9058-78b9d48ad930"}
{"authors": ["Elzbieta Krepska", "Nicola Bonzanni", "K.A. Feenstra", "Wan Fokkink", "Thilo Kielmann", "Henri E. Bal", "Jaap Heringa"], "n_citation": 50, "title": "Design Issues for Qualitative Modelling of Biological Cells with Petri Nets", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "1fd4c2a3-6dd3-41c6-9759-36545dbca799"}
{"abstract": "Most prior designated confirmer signature schemes either prove security in the random oracle model (ROM) or use general zero-knowledge proofs for NP statements (making them impractical). By slightly modifying the definition of designated confirmer signatures, Gold-wasser and Waisbard presented an approach in which the Confirm and ConfirmedSign protocols could be implemented without appealing to general zero-knowledge proofs for NP statements (their Disavow protocol still requires them). The Goldwasser-Waisbard approach could be instantiated using Cramer-Shoup, GMR, or Gennaro-Halevi-Rabin signatures. In this paper, we provide an alternate generic transformation to convert any signature scheme into a designated confirmer signature scheme, without adding random oracles. Our key technique involves the use of a signature on a commitment and a separate encryption of the random string used for commitment. By adding this layer of indirection, the underlying protocols in our schemes admit efficient instantiations (i.e., we can avoid appealing to general zero-knowledge proofs for NP statements) and furthermore the performance of these protocols is not tied to the choice of underlying signature scheme. We illustrate this using the Camenisch-Shoup variation on Paillier's cryptosystem and Pedersen commitments. The confirm protocol in our resulting scheme requires 10 modular exponentiations (compared to 320 for Goldwasser-Waisbard) and our disavow protocol requires 41 modular exponentiations (compared to using a general zero-knowledge proof for Goldwasser-Waisbard). Previous schemes use the encryption of a signature paradigm, and thus run into problems when trying to implement the confirm and disavow protocols efficiently.", "authors": ["Craig Gentry", "David Molnar", "Zulfikar Ramzan"], "n_citation": 0, "title": "Efficient designated confirmer signatures without random oracles or general zero-knowledge proofs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "307b4973-f249-4db7-8183-0b784e763f14"}
{"abstract": "The ever-increasing popularity of peer-to-peer (P2P) systems provides a strong motivation for designing a dependable P2P system. Dependability in P2P systems can be viewed from two different perspectives, namely system reliability (the availability of the individual peers) and system performance (data availability). This paper looks at dependability from the viewpoint of system performance and aims at enhancing the dependability of unstructured P2P systems via dynamic replication, while taking into account the disproportionately large number of 'free riders' that characterize P2P systems. Notably, the sheer size of P2P networks and the inherent heterogeneity and dynamism of the environment pose significant challenges to the improvement of dependability in P2P systems. The main contributions of our proposal are two-fold. First, we propose a dynamic data placement strategy involving data replication, the objective being to reduce the loads of the overloaded peers. Second, we present a dynamic query redirection technique which aims at reducing response times. Our performance evaluation demonstrates that our proposed technique is indeed effective in improving user response times significantly, thereby increasing the dependability of P2P systems.", "authors": ["Anirban Mondal", "Yi Lifu", "Masaru Kitsuregawa"], "n_citation": 0, "title": "On improving the performance dependability of unstructured P2P systems via replication", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3f44a260-2425-433e-84f4-f03168746a00"}
{"abstract": "Shannon entropy is a useful and important measure in information processing, for instance, data compression or randomness extraction, under the assumption-which can typically safely be made in communication theory-that a certain random experiment is independently repeated many times. In cryptography, however, where a system's working has to be proven with respect to a malicious adversary, this assumption usually translates to a restriction on the latter's knowledge or behavior and is generally not satisfied. An example is quantum key agreement, where the adversary can attack each particle sent through the quantum channel differently or even carry out coherent attacks, combining a number of particles together. In information-theoretic key agreement, the central functionalities of information reconciliation and privacy amplification have, therefore, been extensively studied in the scenario of general distributions: Partial solutions have been given, but the obtained bounds are arbitrarily far from tight, and a full analysis appeared to be rather involved to do. We show that, actually, the general case is not more difficult than the scenario of independent repetitions-in fact, given our new point of view, even simpler. When one analyzes the possible efficiency of data compression and randomness extraction in the case of independent repetitions, then Shannon entropy H is the answer. We show that H can, in these two contexts, be generalized to two very simple quantitiesH e  0  and H e \u221e, called smooth Renyi entropies-which are tight bounds for data compression (hence, information reconciliation) and randomness extraction (privacy amplification), respectively. It is shown that the two new quantities, and related notions, do not only extend Shannon entropy in the described contexts, but they also share central properties of the latter such as the chain rule as well as sub-additivity and monotonicity.", "authors": ["Renato Renner", "Stefan Wolf"], "n_citation": 0, "title": "Simple and tight bounds for information reconciliation and privacy amplification", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "44814284-180c-49db-a35d-d286db14af34"}
{"abstract": "The rescue robots developed at the International University Bremen (IUB) are semi-autonomous mobile robots providing streams of video and other essential data via wireless connections to human operated basestations, supplemented by various basic and optional behaviors on board of the robots. Due to the limitations of wireless connections and the complexity of rescue operations, the full operation of a robot can not be constantly supervised by a human operator, i.e., the robots have to be semi-autonomous. This paper describes how the main challenge of safe operation under semi-autonomous control can in general be solved. The key elements are a special software architecture and a scheduling framework that ensure Quality of Service (QoS) and Fail-Safe Guarantees (FSG) despite the unpredictable performance of standard Internet/Intranet-technologies, especially when wireless components are involved.", "authors": ["Andreas Birk", "Holger Kenn"], "n_citation": 0, "title": "A rescue robot control architecture ensuring safe semi-autonomous operation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "49f91c57-46c5-4e74-a280-d2af6d7ba7a7"}
{"authors": ["Yan Tang", "Robert Meersman", "Jan Vanthienen"], "n_citation": 0, "title": "Semantic decision tables: self-organizing and reorganizable decision tables", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "4a8bfa56-cfde-40ea-82d6-96520631e6c1"}
{"abstract": "Ambient Intelligence (AmI) is a vision of future Information Society, where people are surrounded by an electronic environment which is sensitive to their needs, personalized to their requirements, anticipatory of their behavior, and responsive to their presence. It emphasizes on greater user-friendliness, user-empowerment, and more effective service support, with an aim to make people's daily activities more convenient, thus improving the quality of human life. To make AmI real, effective data management support is indispensable. High-quality information must be available to any user, anytime, anywhere, and on any lightweight device. Beyond that, AmI also raises many new challenges related to context-awareness and natural user interaction, entailing us to re-think current database techniques. The aim of this paper is to address the impact of AmI, particularly its user-centric context-awareness requirement on data management strategies and solutions. We first provide a multidimensional view of database access context. Taking diverse contextual information into account, we then present five context-aware data management strategies, using the most fundamental database operation - context-aware query request as a case in point. We execute the proposed strategies via a two-layered infrastructure, consisting of public data manager(s) and a private data manager. Detailed steps of processing a context-aware query are also described in the paper.", "authors": ["Ling Feng", "Peter M. G. Apers", "Willem Jonker"], "n_citation": 0, "title": "Towards context-aware data management for ambient intelligence", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "52b0a15b-29de-4393-a4ce-413dbf2ed69b"}
{"abstract": "This paper studies a novel cooperative transmission scheme that allows single-antenna users to benefit from spatial diversity for the uplink of TDD-CDMA systems. In such systems, the chip-synchronous transmission is attainable and thus, using orthogonal spreading codes can completely eliminate MAI in flat Rayleigh fading channels plus AWGN and make the user-cooperation possible. The proposed cooperation scheme is applicable to any constant-envelope modulation and achieves the fullest diversity order, the low implementation complexity and the full data rate. The closed-form outage probability expression was also derived to verify its validity. A variety of numerical results reveal the cooperation significantly outperforms non-cooperative counterpart under the same transmit power constraint.", "authors": ["Ho Van Khuong", "Hyung-Yun Kong"], "n_citation": 0, "title": "Proposal of cooperative transmission for the uplink of TDD-CDMA systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "537b9a42-cd14-4ce5-9886-f59cef21538f"}
{"abstract": "Workflows have generally been accepted as a means to model and support processes in complex organizations. The fact that these processes require robustness and clear semantics has generally been observed and has lead to the combination of workflow and transaction concepts. Many variations on this combination exist, leading to many approaches to transactional workflow support. No clear classification of these approaches has been developed, however, resulting in a badly understood field. To deal with this problem, we describe a clear taxonomy of transactional workflow models, based on the relation between workflow and transaction concepts. We show that the classes in the taxonomy can directly be related to specification language and architecture types for workflow and transaction management systems. We compare the classes with respect to their characteristics and place existing approaches in the taxonomy - thus offering a basis for analysis of transactional workflow support.", "authors": ["Paul W. P. J. Grefen"], "n_citation": 0, "title": "Transactional workflows or workflow transactions", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "54445e02-00cc-412b-a0c3-ec3107e8cdef"}
{"abstract": "T-functions are a new class of primitives which have recently been introduced by Klimov and Shamir. The several concrete proposals by the authors have multiplication and squaring as core nonlinear operations. Firstly, we present time-memory trade-off algorithms to solve the problems related to multiplication and squaring. Secondly, we apply these algorithms to two of the proposals of multi-word T-functions. For the proposal based on multiplication we can recover the 128 unknown bits of the state vector in 2 40  time whereas for the proposal based on squaring the 128 unknown bits can be recovered in 2 21 time. The required amount of key stream is a few (less than five) 128-bit blocks. Experimental data from implementation suggests that our attacks work well in practice and hence such proposals are not secure enough for stand-alone usage. Finally, we suggest the use of conjugate permutations to possibly improve the security of T-functions while retaining some attractive theoretical properties.", "authors": ["Joydip Mitra", "Palash Sarkar"], "n_citation": 0, "title": "Time-memory trade-off attacks on multiplications and T-functions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "544e242d-1f5b-4895-bccb-5b46d203a254"}
{"abstract": "This paper highlights the interest of implicative statistics for classification trees. We start by showing how Gras' implication index may be defined for the rules derived from an induced decision tree. Then, we show that residuals used in the modeling of contingency tables provide interesting alternatives to Gras' index. We then consider two main usages of these indexes. The first is purely descriptive and concerns the a posteriori individual evaluation of the classification rules. The second usage, considered for instance by Zighed and Rakotomalala [15], relies upon the intensity of implication to define the conclusion in each leaf of the induced tree.", "authors": ["Gilbert Ritschard", "Djamel Abdelkader Zighed"], "n_citation": 50, "title": "Implication Strength of Classification Rules", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6fc9db4b-dfca-4c95-b27f-212e593f61fd"}
{"abstract": "This paper aims at studying the possibility of exploiting the Intelligent Agent technology in e-government for supporting the decision making activity of government agencies. Specifically, it proposes a system to assist managers of a government agency, who plan to propose a new service, to identify those citizens that could gain the highest benefit from it. The paper illustrates the proposed system and reports some experimental results.", "authors": ["Pasquale De Meo", "Giovanni Quattrone", "Domenico Ursino"], "n_citation": 0, "title": "Using Intelligent Agents in e-Government for Supporting Decision Making About Service Proposals", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7df2f121-aa20-4faa-bdda-d872b8797c4b"}
{"abstract": "This paper describes how the covariance information in MPEG video data can be incorporated into a distance measure and applies the resulting divergence measure to video content classification problems. The divergence measure is adopted into two different clustering algorithms, the Centroid Neural Network (CNN) and the Gradient Based Fuzzy c-Means (GBFCM) for MPEG video data classification problems, movie or sports. Experiments on 16 MPEG video traces show that the divergence measure with covariance information can decrease the False Alarm Rate (FAR) in classification as much as 46.6% on average.", "authors": ["Dong-Chul Park", "Chung-Nguyen Tran", "Yunsik Lee"], "n_citation": 0, "title": "Classification of MPEG video content using divergence measure with data covariance", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8b755d7a-6f3a-4f7e-909a-b43c55e8e8b1"}
{"abstract": "Micali, Rabin, and Kilian [9] recently introduced zero-knowledge sets and databases, in which a prover sets up a database by publishing a commitment, and then gives proofs about particular values. While an elegant and useful primitive, zero-knowledge databases do not offer any good way to perform updates. We explore the issue of updating zero-knowledge databases. We define and discuss transparent updates, which (1) allow holders of proofs that are still valid to update their proofs, but (2) otherwise maintain secrecy about the update. We give rigorous definitions for transparently updatable zero-knowledge databases, and give a practical construction based on the Chase et al [2] construction, assuming that verifiable random functions exist and that mercurial commitments exist, in the random oracle model. We also investigate the idea of updatable commitments, an attempt to make simple commitments transparently updatable. We define this new primitive and give a simple secure construction.", "authors": ["Moses Liskov"], "n_citation": 50, "title": "Updatable zero-knowledge databases", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8f189641-cf72-4f70-bd18-c7860cf2f605"}
{"abstract": "The approach called the Lexicographic Min-Max (LMM) optimization depends on searching for solutions minimal according to the lex-max order on a multidimensional outcome space. LMM is a refinement of the standard Min-Max optimization, but in the former, in addition to the largest outcome, we minimize also the second largest outcome (provided that the largest one remains as small as possible), minimize the third largest (provided that the two largest remain as small as possible), and so on. The necessity of point-wise ordering of outcomes within the lexicographic optimization scheme causes that the LMM problem is hard to implement. For convex problems it is possible to use iterative algorithms solving a sequence of properly defined Min-Max problems by eliminating some blocked outcomes. In general, it may not exist any blocked outcome thus disabling possibility of iterative Min-Max processing. In this paper we analyze two alternative optimization models allowing to form lexicographic sequential procedures for various nonconvex (possibly discrete) LMM problems. Both the approaches are based on sequential optimization of directly defined artificial criteria. The criteria can be introduced into the original model with some auxiliary variables and linear inequalities thus the methods are easily implementable.", "authors": ["W\u0142odzimierz Ogryczak", "Tomasz Sliwinski"], "n_citation": 0, "title": "On Direct Methods for Lexicographic Min-Max Optimization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8f40f8d4-0c93-4fae-87e6-a614d48f8782"}
{"abstract": "A new architecture of database federation called the MDSSF (Multiple Database Search Service Federation) is presented to support the procurement activities of the AEC (Architecture, Engineering and Construction) industry projects. In order to make procurement decisions, a contractor requires access to product information from several different product suppliers when constructing artefacts such as a hospital, or an office block. This product information is available from the online systems of product suppliers. However, this approach requires a contractor to visit several websites in order to find the right product which is time consuming and the product data available from different product suppliers is heterogeneous. The MDSSF architecture provides an integrated means of accessing product information from a large number of product suppliers using a single system. It brings together autonomous product suppliers to share product information with the federation users such as contractors and potential buyers using a common data model. It also creates an environment for product suppliers to compete with each other in a virtual market place based on the product information they provide to federation users. The MDSSF gives its users a Grid enabled database search mechanism for searching a large number of supplier databases in real time and protects product related sensitive data from exposure to business competitors. We describe the architecture and distinctive features of the MDSSF.", "authors": ["Jaspreet Singh Pahwa", "Pete Burnap", "W. A. Gray", "John Christopher Miles"], "n_citation": 5, "title": "MDSSF : A federated architecture for product procurement", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "932cc3a9-dfe3-42c4-a2b5-523ee15d0b5a"}
{"abstract": "The authors have implemented a complex adaptive simulation of an agent-based exchange to estimate the relative importance of attributes in a data set. This simulation uses an individual, transaction-based voting mechanism to help the system estimate the importance of each variable at the system/aggregate level. Two variations of information gain - one using entropy and one using similarity - were used to demonstrate that the resulting estimates can be computed using a smaller subset of the data and greater accommodation for missing and erroneous data than traditional methods.", "authors": ["Christopher Eichelberger", "Mirsad Hadzikadic"], "n_citation": 0, "title": "Complex Adaptive Systems : Using a Free-Market Simulation to Estimate Attribute Relevance", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "940aceff-30c0-4f05-8c98-d5392cc35987"}
{"abstract": "Data integration is a significant challenge: relevant data objects are split across multiple information sources, and often owned by different organizations. The sources represent, maintain, and export the information using a variety of formats, interfaces and semantics. This paper addresses the issue of querying distributed data in a large scale context. We present a p2p information mediation framework based on the notion of super-peers, providing a super-peer network. This makes it possible for a super-peer to reach every other peer (data source) in the system, thus realizing the concept of a integrated schema formed from all possible information sources. This is achieved by classifying data sources into domains and creating user profiles for query optimization purposes.", "authors": ["Zohra Bellahsene", "Mark Roantree"], "n_citation": 0, "title": "Querying distributed data in a super-peer based architecture", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "99c4f7c6-ca87-4c7e-ad23-c3ca22e1d0a1"}
{"abstract": "As professional sports have become big businesses all over the world, many researches with respect to sports scheduling problem have been worked over the last two decades. The traveling tournament problem (TTP) is defined as minimizing total traveling distance for all teams in the league. In this study, a mathematical model for the TTP is presented. This model is formulated using an integer programming (IP). In order to solve practical problems with large size of teams, a tabu search heuristic is suggested. Also, the concepts of alternation and intimacy were introduced for effective neighborhood search. Experiments with several instances are tested to evaluate their performances. It was shown that the proposed heuristic shows good performances with computational efficiency.", "authors": ["Jin Ho Lee", "Young Hoon Lee", "Yun Ho Lee"], "n_citation": 14, "title": "Mathematical Modeling and Tabu Search Heuristic for the Traveling Tournament Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a73fa90b-4498-431a-aa73-00fc97902571"}
{"abstract": "More accessible conferences, digital resources, and ACM SIGs will lead to greater participation by more people with disabilities.", "authors": ["Jonathan Lazar", "Elizabeth F. Churchill", "Tovi Grossman", "Gerrit C. van der Veer", "Philippe A. Palanque", "John \"Scooter\" Morris", "Jennifer Mankoff"], "n_citation": 0, "references": ["1d899b1f-299e-4bbe-8588-09faede112a2", "20d227bf-8824-4451-ab5d-3c4b823f5a95", "41d50d3c-7fc0-40b8-a294-23d521567a0b", "4664f179-a66c-48c7-a4ee-0a5d1c39c02b", "917c7244-c651-4c05-a036-4fcb52486560"], "title": "Making the field of computing more inclusive", "venue": "Communications of The ACM", "year": 2017, "id": "bc5a20e6-0a50-4de0-b4eb-d069bd082bea"}
{"abstract": "In this paper we present new ideas to extend the framework for circular drawing of networks by Six and Tollis [15] by some new concepts which makes the framework suitable for user interaction. The original approach displays each biconnected component in a circular way, and the blocktree of the graph as a tree drawn radially [9]. We introduce the concept of hicircular drawings, a hierarchical extension of the mentioned framework replacing the circles of single vertices by circles of circular or star-like structures. This concept is inspired by the works of Brandenburg on graph clustering, and the recursive concepts of series-parallel graphs, PQ- resp. SPQR-trees.", "authors": ["Michael Kaufmann", "Roland Wiese"], "n_citation": 50, "title": "Maintaining the mental map for circular drawings", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "be33dcb0-1843-40d6-87d9-18eb2f40d483"}
{"abstract": "In contrast to alpha-numerical data, multimedia data can have a wide range of quality parameters such as spatial and temporal resolution, and compression format. Users can request data with a specific quality requirement due to the needs of their applications, or the limitations of their resources. On-the-fly conversion of multimedia data (such as video transcoding) is very CPU intensive and can limit the level of concurrent access supported by the database. Storing all possible replicas, on the other hand, requires unacceptable increases in storage requirements. Although replication has been well studied, to the best of our knowledge, the problem of multiple-quality replication has not been addressed. In this paper we address the problem of multiple-quality replica selection subject to an overall storage constraint. We establish that the problem is NP-hard and provide heuristic solutions under a soft quality system model where users are willing to negotiate their quality needs. An important optimization goal under such a model is to minimize utility loss. We propose a powerful greedy algorithm to solve this optimization problem. Extensive simulations show that our algorithm finds near-optimal solutions. The algorithm is flexible in that it can be extended to deal with replica selection for multiple media objects and changes of query pattern. We also discuss an extended version of the algorithm with potentially better performance.", "authors": ["Tong Yi-cheng", "Jingfeng Yan", "Sunil Prabhakar"], "n_citation": 0, "title": "Quality-aware replication of multimedia data", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c29672ac-e738-40cb-bf50-e07809e8bdad"}
{"abstract": "Thanks to a lot of new or improved coding tools, the H.264/AVC video coding standard achieves up to twice as good compression ratios over its predecessors. Unfortunately, many of these new coding tools come with the cost of a considerable extra amount of bits needed to send motion vectors to the decoder. Especially at low bit rates, this overhead is an important drawback. In this paper, a new inter coding mode for 8x8 macroblock partitions in B slices is proposed. The mode is based on motion compensated interpolation, a commonly used technique in the world of distributed video coding. Since motion compensated interpolation allows both encoder and decoder to find the same motion vector, no bits need to be spent on coding this motion vector. Experimental results show that introducing the new coding mode can achieve a bit rate gain of up to 3.03%, especially at low bit rates.", "authors": ["Stefaan Mys", "J\u00fcrgen Slowack", "Jozef Skorupa", "Peter Lambert", "Rik Van de Walle"], "n_citation": 0, "title": "Motion compensated interpolation as a new inter coding mode for 8x8 macroblock partitions in H.264/AVC B slices", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "c3cdbcad-f7aa-4f06-9204-6689126ff29b"}
{"abstract": "This paper sets out DyQE, a dynamic query expansion information retrieval system implemented in the medical domain, aimed at exploring imprecise queries. DyQE enhances standard query and result set navigation through integration with dynamic taxonomy and broad query expansion. The query expansion rules are sourced from a fine grained evaluation of data retrieval-based query expansion. DyQE offers both a concise representation of a broad subject area, and a wide variety of interesting links to related subject areas.", "authors": ["Dennis Wollersheim", "Wenny Rahayu"], "n_citation": 0, "title": "On building a DyQE : A medical information system for exploring imprecise queries", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c8907d74-bd60-4f6a-902c-fab461b346b5"}
{"abstract": "We define the projection of a tiling as a matrix P = (p ij ) where p i1  is number of t 1,2  tiles in row i and p i2 is the number of t 2,1  tiles in row i. We give an efficient algorithm to tile a 2D-square grid with only t 1,2 , t 2,1 , t 1,1  tiles such that the projection of this tiling is the same as the given projection.", "authors": ["Masilamani Vedhanayagam", "Kamala Krithivasan"], "n_citation": 0, "title": "An efficient reconstruction of 2D-tiling with t1,2, t2,1, t1,1 tiles", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d79e4abe-7366-410b-9a6b-118155e1f590"}
{"abstract": "Two new heuristic strategies are studied based on heuristics for the linear arrangement problem and a stochastic hill-climbing method for the two-sided bipartite crossing number problem. These are compared to the standard heuristic for two-sided bipartite drawing based on iteration of the barycentre method. Our experiments show that they can efficiently find good solutions.", "authors": ["Matthew Newton", "Ondrej Sykora", "Imrich Vrto"], "n_citation": 50, "title": "Two new heuristics for two-sided bipartite graph drawing", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d94e4440-d8c0-4f80-b5b0-30017320c216"}
{"abstract": "The composition of services that are indexed in a large-scale service directory often involves many complex queries issued by the service composition algorithm to the directory. These queries may cause considerable processing effort within the directory, thus limiting scalability. In this paper we present a novel approach to increase scalability: The directory offers a compact digest that service composition clients download to solve the hard part of a composition problem locally. In this paper we compare two different digest representations, a sparse matrix and a Zero-Suppressed Reduced Ordered Binary Decision Diagram (ZDD). Our evaluation confirms that both representations are compact and shows that the ZDD enables more efficient service composition.", "authors": ["Walter Binder", "Ion Constantinescu", "Boi Faltings"], "n_citation": 0, "title": "Scalable automated service composition using a compact directory digest", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "db074a7c-7871-46f8-a95a-c2e5e1d1a3de"}
{"abstract": "In this paper, we propose a camera motion detection method that can identify pan, tilt and zoom in a video sequence. The proposed method exploits motion features based on the motion cooccurrence matrix, which is able to provide dominant motion characteristics between two images such as the size of the homogeneous motion area and the direction of motion. We show that motion cooccurrence matrices are quite different for different types of motion and can be used to effectively identify simple camera motion such as pan, tilt and zoom in video sequences. Our method does not rely on the parametric motion model and can be used to qualitatively detect camera motion. Performance of the proposed method is evaluated by experiments for a set of test sequence.", "authors": ["Hyun-Ho Jeon", "Andrea Basso", "Peter F. Driessen"], "n_citation": 2, "title": "Camera motion detection in video sequences using motion cooccurrences", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "dc6ee953-b9a7-41b9-8270-4b40331cacfa"}
{"abstract": "LBS(Location-Based Service) is generally described as an information service that provides location-based information to its mobile users. Since the conventional studies on data mining do not consider spatial and temporal aspects of data simultaneously, these techniques have limited application in studying the moving objects of LBS with respect to the spatial attributes that is changing over time. In this paper, we propose a new data mining technique and algorithms for identifying temporal patterns from series of locations of moving objects that have temporal and spatial dimensions. For this purpose, we use the spatial operation to generalize a location of moving point, applying time constraints between locations of moving objects to make valid moving sequences. Finally, we show that our technique generates temporal patterns found in frequent moving sequences.", "authors": ["Jae Du Chung", "Ok Hyun Paek", "Jun Wook Lee", "Keun Ho Ryu"], "n_citation": 50, "title": "Temporal pattern mining of moving objects for location-based service", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "df0c478c-3459-4210-baba-82adcdd40b22"}
{"abstract": "This paper is about a method to improve our everyday telephone which takes a big part of our daily life by making better changes in telephone ability. This paper is about a method to improve our everyday telephone which takes a big part of our daily life by making better changes in telephone ability. There are times when the receivers get displeased by hearing callers\u25cb usage of abusive language, dialect and impatient voices. Times like this soft-sound phone is necessary, if the receiver press the soft-sound key bottom which is attached to the phone, the receiver would be able to hear soft voice with in the range of the caller's voice. Soft-sound phone analysis the caller's voice through the phone and keep the meaning of the conversation and adjust the accent which indicates the caller's personality so the caller's voice sounds soft and generous as if the voice tone is not over the specific limit. Consequently, it is affective to change a blunt society to bright and calmed better telephonic mannered society.", "authors": ["Jong-Kuk Kim", "Myung-Jin Bae"], "n_citation": 0, "title": "On the implementation of gentle phone's function based on PSOLA algorithm", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f16db07e-fb78-4078-90ce-747b76481886"}
{"abstract": "The widespread use of XML brings new challenges for its integration into general software development processes. In particular, it is necessary to keep the consistency between different software artifacts and XML documents when evolution tasks are carried out. In this paper we present an approach to evolve XML schemas and documents conceptually modeled by means of UML class diagrams. Evolution primitives are issued on the UML class diagram and are automatically propagated down to the XML schema. The XML documents are also automatically modified to conform to the new XML schema. In this way, the consistency between the different artifacts involved is kept. This goal is achieved by using an intermediate component which reflects how the UML diagrams are translated into the XML schemas.", "authors": ["Eladio Dom\u00ednguez", "Jorge L\u00f3pez Lloret", "Angel Luis Rubio", "Mar\u00eda Antonia Zapata"], "n_citation": 0, "title": "Evolving XML schemas and documents using UML class diagrams", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f2d39cee-b53c-4ae0-949b-2fc152ef5405"}
{"abstract": "DTV data broadcasting offers broadcasters the new business model to acquire considerable revenue from digital broadcast. When the broadcasters integrate and maintain the data broadcast station system, they encounter the cost to be paid unnecessarily because there is no common interface specification among the equipments of DTV station system. This paper describes the flexible and open interface that will decrease the wasteful expenses in moving the transition to data broadcast station system.", "authors": ["Minsik Park", "Yong Ho Kim", "Jin Soo Choi", "Jin Woo Hong"], "n_citation": 0, "title": "Data broadcast metadata based on PMCP for open interface to a DTV data server", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f52a507d-8258-4c23-9621-1e1f84569cd9"}
{"abstract": "Considering influences leading to the recent U.S Supreme Court decision in a years-long case that Apple filed against Samsung over iPhone design infringement.", "authors": ["Pamela Samuelson"], "n_citation": 0, "title": "Supreme Court on design patent damages in Samsung v. Apple", "venue": "Communications of The ACM", "year": 2017, "id": "f8a998e5-b6bd-4d4c-b70c-3444bf96f5e2"}
{"abstract": "We consider the problem of layering Directed Acyclic Graphs, an NP-hard problem. We show that some useful variants of the problem are also NP-hard. We provide an Integer Linear Programming formulation of a generalization of the standard problem and discuss how a branch-and-bound algorithm could be improved upon with cutting planes. We then describe a separation algorithm for two classes of valid inequalities that we have identified - one of which is facet-defining - and discuss their efficacy.", "authors": ["Patrick Healy", "Nikola S. Nikolov"], "n_citation": 50, "title": "A branch-and-cut approach to the Directed Acyclic Graph layering problem", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fc698cda-e76b-4ac8-a352-0002430403fd"}
{"abstract": "In this paper, we performed the robust speaker identification based on the frame pruning and multivariate t-distribution respectively, and then studied on a theoretical basis for the frame pruning using the other methods. Based on the results from two methods, we showed that the robust algorithms based on the weight of frames become the theoretical basis of the frame pruning method by considering the correspondence between the weight of frame pruning and the conditional expectation of t-distribution. Both methods showed good performance when coping with the outliers occurring in a given time period, while the frame pruning method removing less reliable frames is recommended as one of good methods and, also, the multivariate t-distributions are generally used instead of Gaussian mixture models (GMM) as a robust approach for the speaker identification. In experiments, we found that the robust speaker identification has higher performance than the typical GMM algorithm. Moreover, we showed that the trend of frame likelihood using the frame pruning is similar to one of robust algorithms.", "authors": ["Younjeong Lee", "Joohun Lee", "Hernsoo Hahn"], "n_citation": 0, "title": "A study on the relation between the frame pruning and the robust speaker identification with multivariate t-distribution", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "001d7e12-cb91-4953-a11c-22ed236b92d2"}
{"abstract": "Web databases are becoming an efficient tool used to manage the web sites. In this paper we analyse the performance of a typical Web database system with different sizes of web pages and different sizes of database tables. Since a web server and a database server work simultaneously, the response time in dealing with a request to the database can not be seen simply as the web server service time plus database service time. The performance metrics and optimisation suggestions are made on the basis of the analysis of the relationship between them. Initial experiments are designed to investigate how a Web database system works and what affects its performance, in particular, the response time. We explored the different ways of sending query result files. An analysis of the initial test results and suggestions on improving the Web database system performance are presented.", "authors": ["Yi Li", "K. Lu"], "n_citation": 50, "title": "Performance issues of a Web database", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "00408613-3c3b-4236-81d2-1f898175d814"}
{"abstract": "In this paper, we are proposing a new mechanism for controlling 3D (three dimensional) avatars to create user-designed peculiar motions of avatars in real-time using general interfaces, such as a mouse, a keyboard, or a joystick. The main idea is based on the new way of interactive control that is the combined usage of keyboard and mouse simultaneously. In order to generate natural human motions of avatars, we adopted the center line concept of art drawing and some influencing physics algorithms which developed intensively in the field of biped humanoid robot research. We demonstrate that user-designed motions of avatar can be created in real-time using proposed interaction method with keyboard and mouse. Also, we show that a rich set of peculiar behaviors can be generated from a ready-made motion with motion capture data or created and stored in our system. Note that the generated peculiar motions can be more natural if we appropriately apply our center line concept and physics algorithms.", "authors": ["Dong Hoon Kim", "Mee Young Sung", "Jong-Seung Park", "Kyungkoo Jun", "Sang-Rak Lee"], "n_citation": 0, "title": "Realtime control for motion creation of 3D avatars", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "0ef543b0-0e9b-445d-93df-74d60f4b5123"}
{"abstract": "The automatic induction of classification rules from examples is an important technique used in data mining. One of the problems encountered is the overfitting of rules to training data. This paper describes a means of reducing overfitting known as J-pruning, based on the J-measure, an information theoretic means of quantifying the information content of a rule, and examines its effectiveness in the presence of noisy data for two rule induction algorithms: one where the rules are generated via the intermediate representation of a decision tree and one where rules are generated directly from examples.", "authors": ["Max Bramer"], "n_citation": 0, "title": "Using J-pruning to reduce overfitting of classification rules in noisy domains", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0f8e75fa-291f-4db1-96c8-72afe421c210"}
{"abstract": "We present two new parallel algorithms for extending the domain of a UOWHF. The first algorithm is complete binary tree based construction and has less key length expansion than Sarkar's construction which is the previously best known complete binary tree based construction. But only disadvantage is that here we need more key length expansion than that of Shoup's sequential algorithm. But it is not too large as in all practical situations we need just two more masks than Shoup's. Our second algorithm is based on non-complete l-ary tree and has the same optimal key length expansion as Shoup's which has the most efficient key length expansion known so far. Using the recent result [9], we can also prove that the key length expansion of this algorithm and Shoup's sequential algorithm are the minimum possible for any algorithms in a large class of natural domain extending algorithms. But its parallelizability performance is less efficient than complete tree based constructions. However if l is getting larger, then the parallelizability of the construction is also getting near to that of complete tree based constructions. We also give a sufficient condition for valid domain extension in sequential domain extension.", "authors": ["Wonil Lee", "Donghoon Chang", "Sangjin Lee", "Soo-Hak Sung", "Mridul Nandi"], "n_citation": 0, "title": "New parallel domain extenders for UOWHF", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "15a9fe20-2495-4210-8317-8c7d94e06972"}
{"abstract": "To ensure the success of the semantic web, structures that are implemented, usually ontologies, should maintain their validity well into the future. These ontologies can be very extensive, introducing the problem that only parts (sub-ontologies) of them are needed by applications. A number of rules were established to get from such a base ontology to a derived subontology. This paper explores these rules in a practical way, by giving a real-world scenario, and following it through from initial state to target ontology design. Each step and rule is applied to the scenario, and so the more practical side of the theoretical rules and steps is shown.", "authors": ["Carlo Wouters", "Tharam S. Dillon", "Wenny Rahayu", "Elizabeth Chang"], "n_citation": 62, "title": "A practical walkthrough of the ontology derivation rules", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "17781eed-a112-4a4c-92b3-550a83a8ed90"}
{"abstract": "Within an MPEC-21 architecture, the two key concepts are the Digital Item, representing multimedia content, and Users, interacting with this content. MPEG-21 introduced Digital Item Processing to allow content authors to describe suggested processing of their Digital Items. It standardizes ways to insert functionality into a Digital Item, as such, creating a dynamic and interactive multimedia format. Moreover, if a terminal wants to support Digital Item Processing, it needs to provide an execution environment offering basic functionality. The semantics of this functionality have been standardized, however there is significant room for interpretation. Consequently, a Digital Item author may not be aware of the actual processing when using this functionality. In this paper, a system is proposed, compliant with the Digital Item Processing specification, to give content creators full control on the processing. This allows creating advanced predictable multimedia systems in an MPEG-21 environment.", "authors": ["Chris Poppe", "Frederik De Keukelaere", "Saar De Zutter", "Sarah De Bruyne", "Wesley De Neve", "Rik Van de Walle"], "n_citation": 0, "title": "Predictable processing of multimedia content, using MPEG-21 Digital Item Processing", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "18a3fb2f-f35c-4beb-ad07-f0cfed7d7007"}
{"abstract": "This work presents a study of the nature of expertise in geology, which demands visual recognition methods to describe and interpret petroleum reservoir rocks. In an experiment using rock images we noted and analyzed how geologists with distinct levels of expertise described them. The study demonstrated that experts develop a wide variety of representations and hierarchies, which differ from those found in the domain literature. They also retain a large number of symbolic abstractions for images. These abstractions (which we call visual chunks) play an important role in guiding the inference process and integrating collections of tacit knowledge of the geological experts. We infer from our experience that the knowledge acquisition process in this domain should consider that inference and domain objects are parts of distinct ontologies. A special representation formalism, kgraphs+, is proposed as a tool to model the objects that support the inference and how they are related to the domain ontology.", "authors": ["Mara Abel", "Laura S. Mastella", "Lu\u00eds A. Lima Silva", "John A. Campbell", "Luis Fernando De Ros"], "n_citation": 0, "title": "How to model visual knowledge: A study of expertise in oil-reservoir evaluation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1d4d1c0c-ee2b-45c2-9f57-7eac8b6680f8"}
{"abstract": "Wireless sensor network consisting of a large number of small sensors with low-power can be an effective tool for the collection and integration of data in a variety of environments. Here data exchange between the sensors and base station need to be designed to conserve the limited energy resources of the sensors. Grouping the sensors into clusters has been recognized as an efficient approach for saving the energy of the sensor nodes. In this paper we propose an analytical model based on homogenous spatial Poisson process, which allows the number of clusters in a sensor network minimizing the total energy spent for data exchange. Computer simulation on various size sensor networks with the LEACH algorithm reveals that the proposed model is very accurate. We also compare the proposed model with an earlier one, and it turns out that the proposed model is more accurate.", "authors": ["Hyunsoo Kim", "Hee Yong Youn"], "n_citation": 0, "title": "Finding the Number of Clusters Minimizing Energy Consumption of Wireless Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1f60e94f-d845-49f6-805e-abb0aed20135"}
{"abstract": "In this paper, we propose a virtual aesthetic surgery (VAS) system using a deformation technique based on a radial basis function (RBF) and blending technique that combines the deformed facial component with the original face. The proposed VAS system is composed of three main steps. First, various deformation templates are matched to facial components by a multi-resolution active appearance model (MAAM), which is trained by 2D color face images. Next, the VAS system computes the degree of deformation for lattice cells on the free-form deformation (FFD) using the proposed RBF. The deformation error is compensated for by the coefficients of the mapping function, which is recursively solved by the singular value decomposition (SVD) technique using the sum of squared error (SSE) between the deformed control points and target control points on the base curves. Finally, the deformed facial component is blended with the original face using a blending ratio that is computed by the modified Euclidean distance transform. Experimental results show that the proposed deformation and blending techniques are very efficient in terms of smoothness, accuracy, and distortion.", "authors": ["Hyun Park", "Kee Wook Rim", "Young Shik Moon"], "n_citation": 0, "title": "An efficient virtual aesthetic surgery model based on 2D color photograph", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2537896e-58de-49cb-b445-626cb7095b9f"}
{"abstract": "The signature file method is a popular indexing technique used in information retrieval and databases. It excels in efficient index maintenance and lower space overhead. Different approaches for organizing signature files have been proposed, such as sequential signature files, bit-slice files, S-trees, and its different variants, as well as signature trees. In this paper, we extends the structure of signature trees by introducing multiple-bit checkings. That is, during the searching of a signature tree against a query signature s q , more than one bit in s q  will be checked each time when a node is encountered. This does not only reduce significantly the size of a signature tree, but also increases the filtering ability of the signature tree. We call such a structure a general signature tree. Experiments have been made, showing that the general signature tree uniformly outperforms the signature tree approach.", "authors": ["Yangjun Chen"], "n_citation": 50, "title": "On the general signature trees", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2a1d8751-6f25-4827-8191-76b8ebc9e32d"}
{"abstract": "The services that digital libraries provide to users can be greatly enhanced by automatically gleaning certain kinds of information from the full text of the documents they contain. This paper reviews some recent work that applies novel techniques of machine learning (broadly interpreted) to extract information from plain text, and puts it in the context of digital library applications. We describe three areas: hierarchical phrase browsing, including efficient methods for inferring a phrase hierarchy from a large corpus of text; text mining using adaptive compression techniques, giving a new approach to generic entity extraction, word segmentation, and acronym extraction; and keyphrase extraction.", "authors": ["Ian H. Witten"], "n_citation": 50, "title": "Learning structure from sequences, with applications in a digital library", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2c27551b-85f6-49ee-b805-e1100e718fc4"}
{"abstract": "In this paper we describe how to mine association rules in temporal document collections. We describe how to perform the various steps in the temporal text mining process, including data cleaning, text refinement, temporal association rule mining and rule post-processing. We also describe the Temporal Text Mining Testbench, which is a user-friendly and versatile tool for performing temporal text mining, and some results from using this tool.", "authors": ["Kjetil N\u00f8rv\u00e5g", "Trond \u00d8ivind Eriksen", "Kjell-Inge Skogstad"], "n_citation": 0, "title": "Mining Association Rules in Temporal Document Collections", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "39bfb1e2-f822-4483-a309-0cb420c8c3f7"}
{"abstract": "In this paper, we propose an adaptive MAP (Maximum A Posteriori) high-resolution image reconstruction algorithm using local statistics. In order to preserve an edge information of an original high-resolution image, a visibility function defined by local statistics of the low-resolution image is incorporated into MAP estimation process, so that the local smoothness is adaptively controlled. The weighted non-quadratic convex functional is defined to obtain the optimal solution that is as close as possible to the original high-resolution image. An iterative algorithm is utilized for obtaining the solution. The smoothing parameter is updated at each iteration step from the partially reconstructed high-resolution image, and therfore no knowledge about of the original high-resolution image is required. Experimental results demonstrate the capability of the proposed algorithm.", "authors": ["Kyung-Ho Kim", "Yoan Shin", "Min-Cheol Hong"], "n_citation": 0, "title": "Adaptive MAP high-resolution image reconstruction algorithm using local statistics", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3ce1662e-98c0-40b2-9edf-17031114bf52"}
{"abstract": "This paper presents a prototype system, which synthesizes entertainment-oriented cartoon face and translates text message to multimedia animation in mobile phone. While a digital real facial photograph and some text are imputed, a piece of exaggerated facial animation with entertainment will be shown in the phone. Three steps are used to get this entertainment effect: first is the illustration generation of the real face image, General-Scale-Edge (GSE) is adopted to take various scale of the edge into account, which can extract the feature edge on human's face efficiently. The second is the expression warping to produce a caricature. The improved feature based warping method is employed. Finally, we generate the exaggerated facial animation based on the caricature using TTVS method. In addition, we improved modified Active Shape Model to remove the background and control more feature points on the face. Experiments show the system work well with high performance on the PDA.", "authors": ["Junfa Liu", "Yiqiang Chen", "Wen Gao", "Rong Fu", "Renqin Zhou"], "n_citation": 0, "title": "Creative cartoon face synthesis system for mobile entertainment", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3cf9803d-328a-4346-9563-57781ed2d29b"}
{"abstract": "We present a novel framework for encoding images obtained by a security monitoring camera with protecting the privacy of moving objects in the images. We are motivated by the fact that although security monitoring cameras can deter crimes, they may infringe the privacy of those who and objects which are recorded by the cameras. Moving objects, whose privacy should be protected, in an input image (recorded by a monitoring camera) are encrypted and hidden in a JPEG bitstream. Therefore, a normal JPEG viewer generates a masked image, where the moving objects are unrecognizable or completely'invisible. Only a special viewer with a password can reconstruct the original recording. Data hiding is achieved by watermarking and encrypting with the advanced encryption standard (AES). We illustrate a concept of our framework and an algorithm of the encoder and the special viewer. Moreover, we show an implementation example.", "authors": ["Kenichi Yabuta", "Hitoshi Kitazawa", "Toshihisa Tanaka"], "n_citation": 0, "title": "A new concept of security camera monitoring with privacy protection by masking moving objects", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3d9e23ad-682b-4d87-90dc-01faff3c54f3"}
{"abstract": "Searching and retrieving the right information from the World-Wide Web (WWW) has always been considered of foremost importance and of considerable A.I. intensivity. Internet search technologies have been evolving over the years and will continue to do so as the WWW will continue to expand in size and increase in popularity. In a desperate attempt to restore order to the WWW after the chaos that has developed due to its heterogeneous, unstructured and uncensored nature, the eXtended Markup Language (XML) is being heralded as the successor to HTML. In this paper we investigate the evolution of Internet search technologies and present a possible and viable solution in a functional system we developed and which makes use of XML at its very core. We discuss the design issues involved as well as practical issues such as tendencies and tactics employed by some of the major players in this well-sought area.", "authors": ["Matthew Montebello", "R. Ciappara"], "n_citation": 0, "title": "Internet search technologies & XML", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "3fbd3b0f-9e04-471b-8a9c-79b3ff22f2c5"}
{"authors": ["Moshe Y. Vardi"], "n_citation": 0, "title": "ACM's open-conference principle and political reality", "venue": "Communications of The ACM", "year": 2017, "id": "40d27a82-a9ea-49fc-bfb9-3820f3acfe2a"}
{"abstract": "In order to provide communication between the deaf-dumb people and the hearing people, a two-stage system translating Turkish Sign Language into Turkish is developed by using vision based approach. Hidden Markov models are utilized to determine the global feature group in the dynamic gesture recognition stage, and k nearest neighbor algorithm is used to compare the local features in the static gesture recognition stage. The system can perform person dependent recognition of 172 isolated signs.", "authors": ["Hakan Haberdar", "Songul Albayrak"], "n_citation": 0, "title": "A Two-Stage Visual Turkish Sign Language Recognition System Based on Global and Local Features", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "44284e6e-5222-4feb-8e11-d6d09b5d60b4"}
{"abstract": "Branch indices of points on curves (introduced by Urysohn and Menger) are of basic importance in the mathematical theory of curves, defined in Euclidean space. This paper applies the concept of branch points in the 3D orthogonal grid, motivated by the need to analyze curve-like structures in digital images. These curve-like structures have been derived as 3D skeletons (by means of thinning). This paper discusses approaches of defining branch indices for voxels on 3D skeletons, where the notion of a junction will play a crucial role. We illustrate the potentials of using junctions in 3D image analysis based on a recent project of analyzing the distribution of astrocytes in human brain tissue.", "authors": ["Gisela Klette"], "n_citation": 50, "title": "Branch voxels and junctions in 3D skeletons", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "46b8baa9-0735-43bf-aa4f-dc4bf5d66ae6"}
{"abstract": "MPEG-7 is a promising standard for the description of multimedia content. A lot of applications based on MPEG-7 media descriptions have been set up. Therefore, an efficient storage solution for large amounts of MPEG-7 descriptions are certainly desirable. MPEG-7 documents are also data-centric XML documents. Due to many advantages, the relational DBMS is the best choice for storing such XML documents. However, the existing RDBMS-based XML storage solutions can not reach all the critical requirements for MPEG-7 descriptions management. In this paper, we analyse the problems when using existing RDBMS-based XML storage approaches to store MPEG-7 documents and then present a new storage approach, called SM3+ that integrates the advantages of existing XML storage models and avoid the main drawbacks from them. Its features can reach the most critical requirements for MPEG-7 documents storage and management. Performance studies are conducted and the experimental results are encouraging.", "authors": ["Y.C. Chu", "Liang-Tien Chia", "Sourav S. Bhowmick"], "n_citation": 0, "title": "SM3+ : An XML database solution for the management of MPEG-7 descriptions", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4ba30d74-c668-4792-9362-1d72bee4ce69"}
{"abstract": "The automatic video parser, a necessary tool for the development and maintenance of a video library, must accurately detect video scene changes so that the resulting video clips can be indexed in some fashion and stored in a video database. Abrupt scene changes and wipes are detected fairly well. However, dissolve changes have been often missed. In this paper, we propose a robust dissolve detection scheme based on Visual Rhythm Spectrum. The Visual Rhythm Spectrum contains distinctive patterns or visual features for many different types of video effects. The efficiency of the proposed scheme is demonstrated using a number of video clips and some performance comparisons are made with other existing approaches.", "authors": ["Seong Jun Park", "Kwang-Deok Seo", "Jae-Gon Kim", "Samuel Moon-Ho Song"], "n_citation": 0, "title": "Automatic dissolve detection scheme based on visual rhythm spectrum", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4ff8d96b-a49d-4737-b4f6-5b1f8fcfa3b5"}
{"abstract": "The complex multiplication (CM) method for genus 2 is currently the most efficient way of generating genus 2 hyperelliptic curves defined over large prime fields and suitable for cryptography. Since low class number might be seen as a potential threat, it is of interest to push the method as far as possible. We have thus designed a new algorithm for the construction of CM invariants of genus 2 curves, using 2-adic lifting of an input curve over a small finite field. This provides a numerically stable alternative to the complex analytic method in the first phase of the CM method for genus 2. As an example we compute an irreducible factor of the Igusa class polynomial system for the quartic CM field Q(i\u221a75 + 12\u221a17), whose class number is 50. We also introduce a new representation to describe the CM curves: a set of polynomials in (j 1 , j 2 , j 3 ) which vanish on the precise set of triples which are the Igusa invariants of curves whose Jacobians have CM by a prescribed field. The new representation provides a speedup in the second phase, which uses Mestre's algorithm to construct a genus 2 Jacobian of prime order over a large prime field for use in cryptography.", "authors": ["Pierrick Gaudry", "T. Houtmann", "David R. Kohel", "Christophe Ritzenthaler", "Annegret Weng"], "n_citation": 0, "title": "The 2-Adic CM Method for Genus 2 Curves with Application to Cryptography", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "511d1931-1741-47ac-8229-419226eee7c7"}
{"abstract": "Faster time to market and decreased development and maintenance Costs are goals most companies are trying to reach. Product family engineering (PFE) provides a means of achieving these goals. Product family architecture (PFA) is the key issue in family engineering. However, companies have to decide how to adopt PFE and how to develop their software PFA. This paper introduces the basic issues essential to PFA development, explores three different approaches to applying PFAs in industrial settings, and, finally, presents the evaluation results through an evaluation model of software product families.", "authors": ["Eila Niemel\u00e4", "Mari Matinlassi", "Anne Taulavuori"], "n_citation": 0, "title": "Practical evaluation of software product family architectures", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5317e9c2-4bee-42ff-9cf0-4078c2344fa8"}
{"abstract": "In practice, most conceptual schemas of information systems and databases are developed essentially from scratch. This paper deals with a new approach to that development, consisting on the refinement of a general ontology. We identify and characterize the three activities required to develop a conceptual schema from a general ontology, that we call refinement, pruning and refactoring. The focus of the paper is on the differences of the new approach with respect to the traditional one. The pruning activity may be automated. We formalize it and present a method for its realization. Besides, we identify a particular problem that appears during the refactoring activity, determining whether two types are redundant, and provide two sufficient conditions for it. We illustrate the approach with the development of a conceptual schema by refinement of the Cyc ontology. However, our results apply to any general ontology. The conceptual modeling language we have used is the UML, but we believe that our results could be applied to any similar language.", "authors": ["Jordi Conesa", "Xavier de Palol", "Antoni Oliv\u00e9"], "n_citation": 0, "title": "Building conceptual schemas by refining general ontologies", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5767edc7-0c6d-420b-ab9b-dea0dddc7452"}
{"abstract": "We propose a new concept for displaying, browsing, and retrieving web content in public spaces via a ubiquitous network. The new type of content operation, described in this paper, is semantically synchronized and based on Device Cooperative Browsing and Device Cooperative Retrieval. These are interactive methods of browsing and retrieving content related to that currently being viewed by the user. The operating mechanism is based on multiple devices connected via a peer-to-peer network. The mechanism operates cooperatively to find each user's individual interests and it maintains information about them. Multiple devices sharing common information might thus have common topics of interest listed on various user devices. When new devices are connected in this environment, they detect the required information through filtering, and obtain it from other devices in the local ubiquitous network. In addition to describing this concept, we also discuss the WebBoard, which is a successful implementation of our novel approach.", "authors": ["Yutaka Kidawara", "Koji Zettsu", "Tomoyuki Uchiyama", "Katsumi Tanaka"], "n_citation": 0, "title": "Device Cooperative web Browsing and retrieving mechanism on ubiquitous networks", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "59c21bed-58fe-4246-88c3-e4b77d6b8233"}
{"abstract": "To answer queries, many XML management systems perform structural joins, i.e., they determine all occurences of parent/child or ancestor/descendant relationships between node sets. These joins are often one of the most time-consuming phases in query evaluation, so it is desirable to reduce the size of the node sets before performing the joins. This problem has earlier been approached by using signatures built on the content of the nodes, but in this paper, we propose a novel method in which the nodes are filtered based on the structural properties of their subtrees. To achieve this, we use a schema graph which summarizes the structures of XML documents more accurately than conventional summarization methods.", "authors": ["Olli Luoma"], "n_citation": 0, "title": "A structure-based filtering method for XML management systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5a9156fd-0c66-467b-ac49-12c65032b484"}
{"abstract": "We describe a strategy for finding small modular and integer roots of multivariate polynomials using lattice-based Coppersmith techniques. Applying our strategy, we obtain new polynomial-time attacks on two RSA variants. First, we attack the Qiao-Lam scheme that uses a Chinese Remaindering decryption process with a small difference in the private exponents. Second, we attack the so-called Common Prime RSA variant, where the RSA primes are constructed in a way that circumvents the Wiener attack.", "authors": ["Ellen Jochemsz", "Alexander May"], "n_citation": 123, "title": "A Strategy for Finding Roots of Multivariate Polynomials with New Applications in Attacking RSA Variants", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "64520e3d-8bb7-438d-9088-c570c96dba64"}
{"abstract": "Proving knowledge of a signature has many interesting applications. As one of them, the Universal Designated Verifier Signature (UDVS), introduced by Steinfeld et al. in Asiacrypt 2003 aims to protect a signature holder's privacy by allowing him to convince a verifier that he holds a valid signature from the signer without revealing the signature itself. The essence of the UDVS is a transformation from a publicly verifiable signature to a designated verifier signature, which is performed by the signature holder who does not have access to the signer's secret key. However, one significant inconvenience of all the previous UDVS schemes considered in the literature is that they require the designated verifier to create a public key using the signer's public key parameter and have it certified to ensure the resulting public key is compatible with the setting that the signer provided. This restriction is unrealistic in several situations where the verifier is not willing to go through such setup process. In this paper, we resolve this problem by introducing a new type of UDVS. Different from previous approach to UDVS, our new UDVS solution, which we call. Universal Designated Verifier Signature Proof (UDVSP), employs an interactive protocol between the signature holder and the verifier while maintaining high level of efficiency. We provide a formal model and security notions for UDVSP and give two constructions based on the bilinear pairings. We prove that the first construction is secure in the random oracle model and so is the second one in the standard model.", "authors": ["Joonsang Baek", "Reihaneh Safavi-Naini", "Willy Susilo"], "n_citation": 54, "title": "Universal designated verifier signature proof (or how to efficiently prove knowledge of a signature)", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "685dd86e-61aa-4698-88a4-11b45744bd29"}
{"abstract": "It is almost a folklore-knowledge that hash-based time-stamping schemes are secure if the underlying hash function is collision-resistant but still no rigorous proofs have been published. We try to establish such proof and conclude that the existing security conditions are improper because they ignore precomputations by adversaries. After analyzing a simplistic patent filing scenario, we suggest a new security condition for time-stamping schemes that leads to a new security property of hash functions - chain-resistance. We observe that if the variety of possible shapes of hash-chains is polynomial (and the verification procedure is suitably improved), then the time-stamping scheme becomes provably secure, assuming that the underlying hash function is collision-resistant. Finally, we show that in some sense, the restrictions in the security definition are necessary -- conventional black-box techniques are unable to prove that chain-resistance follows from collision-resistance.", "authors": ["Ahto Buldas", "M\u00e4rt Saarepera"], "n_citation": 0, "title": "On provably secure time-stamping schemes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6c93da1e-36c3-423e-89df-992fc7a1f845"}
{"authors": ["Elizabeth Varki"], "n_citation": 0, "title": "Where review goes wrong", "venue": "Communications of The ACM", "year": 2017, "id": "73b8d3e8-c277-4610-829c-0db4e9f8e6c5"}
{"abstract": "We propose a novel hybrid illumination invariant feature selection scheme for face recognition, which is a combination of geometrical feature extraction and linear subspace projection. By local geometry feature enhancement technique, neighborhood histogram equalization (NHE) in our experiment, some illegible edges due to week illumination will be enhanced effectively. Then we applied classic linear subspace projection methods, such as Principle Component Analysis (PCA), subspace Fisher Linear Discriminant (FLD), and Direct Fisher Linear Discriminant (DFLD), on these face images to decrease training samples' dimension as well as diminish the effect of noise introduced at the first step. Our methods are evaluated on an elaborate selected subset (with large illumination variation) of YaleB database. Experiments on this data set show that the NHE+DFLD yields the best performance. By using only 3-dimensional features (the original face images are 256 x 256), error rate can be decreased from 0.73 (by DFLD only) to 0.07.", "authors": ["Yazhou Liu", "Hongxun Yao", "Wen Gao", "Debin Zhao"], "n_citation": 0, "title": "Illumination invariant feature selection for face recognition", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "83f708d6-e3dc-4646-bf37-3c816a488f10"}
{"abstract": "This paper presents simulations of long-term co-evolution in simple artificial ecosystems composed of different plant species. Artificial plants and plant communities, evolving in a 3D environment, are based on a multi-agent model. This multi-agent approach allows to define communication processes existing at plant and organ levels. A model of a particular chemical interaction between plants is developed in this paper: the allellopathy. This intraspecific and interspecific phenomenon is used in nature both in competition and in cooperation in plant community development. This paper focuses particularly on the emergence of competition and cooperation during long time periods. Several results obtained in this paper are close to natural observations.", "authors": ["Claude Lattaud"], "n_citation": 0, "title": "Co-evolution in artificial ecosystems: Competition and cooperation using allellopathy", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "87eba222-909d-4600-a2ce-ba9f53c77955"}
{"abstract": "This paper presents the advantage that knowledge-intensive activities, such as Scientific Conference Management, can take by the exploitation of expert components in the key tasks. Typically, in this domain the task of scheduling the activities and resources or the assignment of reviewers to papers is performed manually leading therefore to time-consuming procedures with high degree of inconsistency due to many parameters and constraints to be considered. The proposed systems, evaluated on real conference datasets, show good results compared to manual scheduling and assignment, in terms of both accuracy and reduction of runtime.", "authors": ["Marenglen Biba", "Stefano Ferilli", "Nicola Di Mauro", "Teresa Maria Altomare Basile"], "n_citation": 0, "title": "Intelligent Methodologies for Scientific Conference Management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8d9bc3d7-a535-4dc2-b7f7-a765c1c110ba"}
{"abstract": "We present the transformation of the Coremetrics analytic application software engineering approaches from a traditional Java 2 Enterprise Edition (J2EE) environment to a software product line approach. Particular emphasis is placed on the motivation for a product line approach and the software engineering challenges faced particularly as an application service provider. We also discuss our definitions of product line variation and binding, and how we provide tool support using the open source Integrated Development Environment Eclipse. We also provide our observations about the software product line approach, how our work and findings relate to other work in this area, and lastly, how configuration management plays an integral and complementary role in our approach.", "authors": ["James Snyder", "Harry Lai", "Shirish Reddy", "Jimmy Wan"], "n_citation": 0, "title": "Software product line support in Coremetrics OA2004", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "919f3d68-730f-4f04-ad0b-26b835afee7c"}
{"abstract": "In this paper, we show how to use the three dimensional topological map in order to compute efficiently topological features on objects contained in a 3D image. These features are useful for example in image processing to control operations or in computer vision to characterize objects. Topological map is a combinatorial model which represents both topological and geometrical information of a three dimensional labeled image. This model can be computed incrementally by using only two basic operations: the removal and the fictive edge shifting. In this work, we show that Euler characteristic can be computed incrementally during the topological map construction. This involves an efficient algorithm and open interesting perspectives for other features.", "authors": ["Guillaume Damiand", "Samuel Peltier", "Laurent Fuchs", "Pascal Lienhardt"], "n_citation": 50, "title": "Topological map : An efficient tool to compute incrementally topological features on 3D images", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "985c2623-5892-4d77-8da7-f92c00175719"}
{"abstract": "XML query processing is one of the most active areas of database research. Although the main focus of past research has been the processing of structural XML queries, there are growing demands for a full-text search for XML documents. In this paper, we propose XICS (XML Indices for Content and Structural search), novel indices built on a B + -tree, for the fast processing of queries that involve structural and full-text searches of XML documents. To represent the structural information of XML trees, each node in the XML tree is labeled with an identifier. The identifier contains an integer number representing the path information from the root node. XICS consist of two types of indices, the COB-tree (COntent B + -tree) and the STB-tree (Structure B + -tree). The search keys of the COB-tree are a pair of text fragments in the XML document and the identifiers of the leaf nodes that contain the text, whereas the search keys of the STB-tree are the node identifiers. By using a node identifier in the search keys, we can retrieve only the entries that match the path information in the query. Our experimental results show the efficiency of XICS in query processing.", "authors": ["Toshiyuki Shimizu", "Masatoshi Yoshikawa"], "n_citation": 50, "title": "Full-text and structural XML indexing on B+-tree", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a158042d-5502-4663-8f83-6b3b42165af9"}
{"abstract": "Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions.", "authors": ["He Jiang", "Xiaochen Li", "Zijiang Yang", "Jifeng Xuan"], "n_citation": 0, "references": ["0895c22d-37c5-4c8f-9202-a32ebd2cb0c0", "0cdaf8c0-b032-4efd-86ab-60a89c1c65f4", "15a5a75d-bb62-485a-9dcf-5b499fbeb96e", "17821d3a-3ed8-4981-9152-2ef1f82bce09", "23bb57fa-5285-430f-ade8-969e65bd7db8", "38a089f4-13b4-40b1-9cb7-7cb7ce7ee8ee", "41428b11-270e-435b-8b65-7554d9aa76d7", "4deb16ac-7d54-4a0f-a66a-6d957c7b9832", "58b5ddb8-d607-44ea-851d-4c60bd691e5b", "58f3e4e0-59de-4f38-b9b2-05236036219c", "5ce1e987-ed5f-4c2f-b289-46ad2d24ca3c", "5fbb67df-ccd4-47c1-8bbd-c6dd7acce5ea", "674080d0-ef73-4d77-91db-20cf6a0d99fa", "6773fb79-1f63-41e4-9080-e8eaa175c0a2", "69a6ecb2-4af3-4b65-8dc8-157c11e20568", "70c19e51-9d17-4be0-a920-b47630b765d4", "7b5bce5b-5386-4d65-9d35-9c48c1edab07", "7bcff1f8-d53a-46db-b024-5f8080e77c75", "7e149918-691f-4fb9-88e5-df3bb89e587a", "8026f56a-a93e-4933-8ead-c9aa9e3f0498", "806c546c-5406-4a8b-a9b3-39aafc3f7f33", "82d13732-1c21-4352-8f69-be184553a7f6", "8bd64682-8e62-4368-9867-048004af85d7", "91142915-2f52-459e-842f-6f588eb1f63a", "931a1518-10de-4a85-8bc1-88ac483f5fa3", "9dbd0793-227e-4d0e-8dbb-7682ff9b6376", "a6296b5e-a9e6-4e0c-a99c-027697f33964", "a9ea2415-9095-4044-a17a-02881db193ad", "b8774530-26af-495e-b0d9-168bedf38a1f", "bb9a3524-56be-4892-a1a5-284888b2b847", "be23df9d-eee9-4db4-8e88-55c3b9dd0481", "d37c6508-58e5-4218-a813-637858457f95", "d70fb3e4-1e5f-4ab3-900d-34ecb9efc841", "da90e33a-074d-4006-9d66-7830f96462a4", "f4f043b0-785a-468d-be04-28d7857446e8", "fb4cb85c-cd3c-4aa0-baff-fff44e500c24"], "title": "What causes my test alarm?: automatic cause analysis for test alarms in system and integration testing", "venue": "international conference on software engineering", "year": 2017, "id": "aca91169-9785-4247-9a72-3cd6fa4b1608"}
{"abstract": "The number of edge crossings is a commonly accepted measure to judge the readability of graph drawings. In this paper we present a new algorithm for high quality multi-layer straight-line crossing minimization. The proposed method uses a local optimization technique where subsets of nodes and edges are processed exactly. The algorithm uses optimization on a window applied in a manner, similar to those used in the area of formal verification of logic circuits. In contrast to most existing heuristics, more than two layers are considered simultaneously. The algorithm tries to reduce the total number of crossings based on an initial placement of the nodes and can thus also be used in a post-processing step. Experiments are given to demonstrate the efficacy of the proposed technique on benchmarks from the area of circuit design.", "authors": ["Thomas Eschbach", "Wolfgang G\u00fcnther", "Rolf Drechsler", "Bernd Becker"], "n_citation": 0, "title": "Crossing reduction by windows optimization", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b1c2ad17-27b2-4f60-ba72-aa6df94ad497"}
{"authors": ["Federica Zanca", "Guozhi Zhang", "Nicholas Marshall", "Emmy Shaheen", "Elena Salvagnini", "Guy Marchal", "Hilde Bosmans"], "n_citation": 0, "title": "Software Framework for Simulating Clusters of Microcalcifications in Digital Mammography", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "bc351c79-725c-4ebc-a471-4caa887da57d"}
{"authors": ["Tomer Ashur", "Orr Dunkelman", "Nimrod Talmon"], "n_citation": 0, "title": "Breaching the Privacy of Israel's Paper Ballot Voting System", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "bcb3cfbf-e27b-432d-84a7-871121f6f3bc"}
{"abstract": "This paper presents a new interest point descriptors representation method based on empirical mode decomposition (EMD) and independent components analysis (ICA). The proposed algorithm first finds the characteristic scale and the location of the interest points using Harris-Laplacian interest point detector. We then apply the Hilbert transform to each component and get the amplitude and the instantaneous frequency as the feature vectors. Then independent components analysis is used to model the image subspace and reduces the dimension of the feature vectors. The aim of this algorithm is to find a meaningful image subspace and more compact descriptors. Combination the proposed descriptors with an effective interest point detector, the proposed algorithm has a more accurate matching rate besides the robustness towards image deformations.", "authors": ["Dongfeng Han", "Wenhui Li", "Xiaosuo Lu", "Yi Wang", "Ming Li"], "n_citation": 0, "title": "Representation Interest Point Using Empirical Mode Decomposition and Independent Components Analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c1ae8699-c384-4e09-a309-9a05a40055db"}
{"abstract": "This paper presents an extension to a novel Web transaction management protocol, previously defined for a failure-free environment, such that it provides reliable recovery from failure in e-commerce applications. This protocol manages complex Web transactions upon multiple distributed Web services, and overcome limitations of two-phase commit protocols by applying correctness criteria based upon semantic atomicity. Further, it supports enhanced transaction resilience through the use of compensating and alternative transactions. The protocol has been prototyped in a CORBA environment. An evaluation carried out on this prototype shows that the new recovery mechanism minimises the logging cost, increases fault tolerance, and permits independent recovery of autonomous systems.", "authors": ["Muhammad Younas", "Barry Eaglestone"], "n_citation": 0, "title": "Ensuring recovery for sacred Web transactions in the e-commerce applications", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c8bab00a-ac96-4213-b0c6-51d83b9ddf01"}
{"abstract": "In three quasi-experimental studies, we investigated the effects of placing a Teachable Agent (TA) from a math game in a digital summative test. We hypothesized that the TA would affect test performance, even without actual \"teachability\", by social influence on the test situation. In Study 1 (N=47), students did a pretest, played the math game for seven weeks, and did a posttest either with or without the TA. In Study 2 (N=62), students did not play the game but were introduced to a TA directly in the posttest. In Study 3 (N=165), the game included a social chat with the TA, and the posttest offered a choice of more difficult questions. Results showed significant effects of the TA on choice and performance on conceptual math problems, though not on overall test scores. We conclude that experience with a TA can influence performance beyond interaction and informative feedback.", "authors": ["Bj\u00f6rn Sj\u00f6d\u00e9n", "Agneta Gulz"], "n_citation": 0, "title": "From Learning Companions to Testing Companions Experience with a Teachable Agent Motivates Students' Performance on Summative Tests", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "c912b727-60d0-4537-b203-b4fcd77aea0b"}
{"abstract": "A fair network payment protocol plays an important role in electronic commerce. The fairness concept in payments can be illustrated as that two parties (e.g. customers and merchants) exchange the electronic items (e.g. electronic money and goods) with each other in a fair manner that no one can gain advantage over the other even if there are malicious actions during exchanging process. In the previous works of fair payments, the buyer is usually required to sign a purchase message which can be traced by everyone. The information about where the buyer spent the money and what he purchased would easily be revealed by this way. This paper employs two techniques of off-line untraceable cash and designated confirmer signatures to construct a new fair payment protocol, in which the untraceability (or privacy) property can be achieved. A Restrictive Confirmation Signature Scheme (RCSS) will be introduced and used in our protocol to prevent the interested persons except the off-line TTP (Trusted Third Party) from tracing the buyer's spending behavior.", "authors": ["Chih-Hung Wang"], "n_citation": 0, "title": "Untraceable fair network payment protocols with off-line TTP", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "cc4928ca-2d58-4c17-9ffd-541e7646f549"}
{"abstract": "In this paper an efficient token-update scheme based on nonce is proposed. This scheme provides an enhancement, resolving some problems with regard to Lee's scheme, which cannot defend against and replay and impersonation attacks. Accordingly, an analysis and comparison with Lee's and other schemes, demonstrate that the current paper avoids replay and impersonation attacks, providing mutual authentication, and also results in a lower computation cost than the original scheme.", "authors": ["Wenbo Shi", "Hyeong Seon Yoo"], "n_citation": 0, "title": "Efficient Nonce-Based Authentication Scheme Using Token-Update", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cdf9ce74-df13-42f6-a810-13ef0ab450d8"}
{"abstract": "As their field of application has evolved and matured, the importance of verifying knowledge-based systems is now widely recognized. Nevertheless, some problems have remained. In this paper, we address the poor scalability to larger systems of the computation methods commonly applied to rule-chain anomaly checking. To tackle this problem, we introduce a novel anomaly checking method based on binary decision diagrams (BDDs), a technique emanating mainly from the hardware design community. In addition, we present empirical evidence of its computational efficiency, especially on rule bases with a deeper inference space.", "authors": ["Christophe Mues", "Jan Vanthienen"], "n_citation": 0, "title": "Efficient rule base verification using binary decision diagrams", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d2f5ce0d-af23-4049-a08e-0c600ce707d7"}
{"abstract": "While Spread Activation has shown its effectiveness in solving the problem of cold start and sparsity in collaborative recommendation, it will suffer a decay of performance (over activation) as the dataset grows denser. In this paper, we first introduce the concepts of Rating Similarity Matrix (RSM) and Rating Similarity Aggregation (RSA), based on which we then extend the existing spreading activation scheme to deal with both the binary (transaction) and the numeric ratings. After that, an iterative algorithm is proposed to learn RSM parameters from the observed ratings, which makes it automatically adaptive to the user similarity shown through their ratings on different items. Thus the similarity calculations tend to be more reasonable and effective. Finally, we test our method on the EachMovie dataset, the most typical benchmark for collaborative recommendation and show that our method succeeds in relieving the effect of over activation and outperforms the existing algorithms on both the sparse and dense dataset.", "authors": ["Peng Han", "Bo Xie", "Fan Yang", "Ruimin Shen"], "n_citation": 0, "title": "An adaptive spreading activation scheme for performing more effective collaborative recommendation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d60fae04-97a5-45b4-a79b-f32d9c5051fa"}
{"abstract": "Complexity science is characterised by computational irreducibility, chaotic dynamics, combinatorial explosion, co-evolution, and multilevel lattice hierarchical structure. One of its main predictive tools is computer-generated distributions of possible future system states. This assumes that the system can be represented inside computers. Robot soccer provides an excellent laboratory subject for complexity science, and we seek a lattice hierarchical vocabulary to provide coherent symbolic representations for reasoning about robot soccer systems at appropriate levels. There is a difference between constructs being human-supplied and being abstracted autonomously. The former are implicitly lattice-hierarchically structured. We argue that making the lattice hierarchy explicit is necessary for autonomous systems to abstract their own constructs. The ideas are illustrated using data taken from the RoboCup simulation competition.", "authors": ["Jeffrey Johnson", "Blaine A. Price"], "n_citation": 0, "title": "Complexity science and representation in robot soccer", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d9d77893-d21d-495d-b5fa-d1a790af2410"}
{"abstract": "A system for multimedia retrieval and summarization in a ubiquitous environment is presented. Hierarchical clustering of data from pressure-based floor sensors is followed by video handover to retrieve video sequences showing the movement of each person in the environment. Audio handover is implemented to dub these sequences. Several methods for extracting key frames from the video sequences were implemented and evaluated by experiments. An adaptive spatio-temporal sampling algorithm based on the rate of footsteps yielded the best performance. The measured accuracy of key frame extraction within a difference of 3 seconds is approximately 80%, The system consists of a graphical user interface that can be used to retrieve video summaries interactively using simple queries.", "authors": ["Gamhewage C. de Silva", "Tadashi Yamasaki", "K. Aizawa"], "n_citation": 0, "title": "Multimedia retrieval from a large number of sources in a ubiquitous environment", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "db1edde9-ece1-4990-955c-51203ed00548"}
{"abstract": "In Wyner-Ziv (WZ) video coding, low-complexity encoding is achieved by generating the prediction signal only at the decoder. An accurate model of the correlation between the original frame and its prediction is necessary for efficient coding. Firstly, we propose an improvement for the pixel-domain correlation estimation. In transform-domain WZ video coding current models estimate the necessary correlation parameters directly in the transform domain. We propose an alternative approach, where an accurate estimation in the pixel domain is followed by a novel method of transforming the pixel-domain estimation into the transform domain. The experimental results show that our model leads to average bit-rate gain of 3.5-8%.", "authors": ["Jozef Skorupa", "J\u00fcrgen Slowack", "Stefaan Mys", "Peter Lambert", "Rik Van de Walle"], "n_citation": 0, "title": "Accurate correlation modeling for transform-domain Wyner-Ziv video coding", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "df5e71f7-3f7d-4d92-915c-4b544c1290d0"}
{"authors": ["Huining Qiu", "Duc-Son Pham", "Svetha Venkatesh", "Wanquan Liu", "Jian-Huang Lai"], "n_citation": 0, "title": "A fast extension for sparse representation on robust face recognition", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "e65c4757-275f-45fd-98d5-487014219ded"}
{"abstract": "As an extension of multi-party computation (MPC), we propose the concept of secure parallel multi-party computation which is to securely compute multi-functions against an adversary with multistructures. Precisely, there are m functions f i ,..., f m  and m adversary structures A 1 ,..., A m , where f i  is required to be securely computed against an A i -adversary. We give a general construction to build a parallel multi-party computation protocol from any linear multi-secret sharing scheme (LMSSS), provided that the access structures of the LMSSS allow MPC at all. When computing complicated functions, our protocol has more advantage in communication complexity than the direct sum method which actually executes a MPC protocol for each function. The paper also provides an efficient and generic construction to obtain from any LMSSS a multiplicative LMSSS for the same multi-access structure.", "authors": ["Zhifang Zhang", "Mulan Liu", "Liangliang Xiao"], "n_citation": 0, "title": "Parallel multi-party computation from linear multi-secret sharing schemes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e8105e89-b812-480d-9576-a9c7fc89f66b"}
{"abstract": "The algorithm of Walker [5] is widely used for drawing trees of unbounded degree, and it is widely assumed to run in linear time, as the author claims in his article. But the presented algorithm clearly needs quadratic runtime. We explain the reasons for that and present a revised algorithm that creates the same layouts in linear time.", "authors": ["Christoph Buchheim", "Michael J\u00fcnger", "Sebastian Leipert"], "n_citation": 0, "title": "Improving Walker's algorithm to run in linear time", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e84ac9e9-8a9f-422e-8d3b-f6f96861ce71"}
{"abstract": "Defuzzification of fuzzy spatial sets by feature distance minimization, recently proposed as an alternative to crisp segmentation, is studied further. Fully utilizing information available in a fuzzy (discrete) representation of a continuous shape, we present an improved defuzzification method, such that the crisp discrete representation of a fuzzy set is generated at an increased spatial resolution, compared to the resolution of the fuzzy set. The correspondence between a fuzzy and a crisp set is established through a distance between their representations based on selected features, where the different resolutions of the images to cornpare are taken into account. The performance of the method is tested on both synthetic and real images.", "authors": ["Joakim Lindblad", "Natasa Sladoje"], "n_citation": 50, "title": "Feature based defuzzification at increased spatial resolution", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ea084c6c-f4ac-413f-9c7a-2b846ce81134"}
{"abstract": "Two important constraints of association rule mining algorithm are support and confidence. However, such constraints-based algorithms generally produce a large number of redundant rules. In many cases, if not all, number of redundant rules is larger than number of essential rules, consequently the novel intention behind association rule mining becomes vague. To retain the goal of association rule mining, we present several methods to eliminate redundant rules and to produce small number of rules from any given frequent or frequent closed itemsets generated. The experimental evaluation shows that the proposed methods eliminate significant number of redundant rules.", "authors": ["Mafruz Zaman Ashrafi", "David Taniar", "Kate A. Smith"], "n_citation": 0, "title": "A new approach of eliminating redundant association rules", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f33356f4-99bf-4290-8591-920350704b68"}
{"abstract": "This paper proposes a new blocking artifact reduction algorithm using an adaptive filter based on classifying the block boundary area. Generally, block-based coding, such as JPEG and MPEG, introduces blocking and ringing artifacts to an image, where the blocking artifact consists of grid noise, staircase noise, and comer outliers. In the proposed method, a ID low-pass filter reduces staircase noise and comer outliers. Next, the block boundaries are divided into two classes based on the gradient of the pixel intensity in the boundary region. For each class, an adaptive filter is applied so that the grid noise is reduced in the block boundary regions. Thereafter, for those blocks with an edge component, the ringing artifact is removed by applying an adaptive filter around the edge. Finally, high frequency components are added to those block boundaries where the natural characteristics have been lost due to the adaptive filter. The computer simulation results confirmed a better performance by the proposed method in both the subjective and objective image qualities.", "authors": ["Jung-Youp Suk", "Gunwoo Lee", "Kuhn-Il Lee"], "n_citation": 0, "title": "Effective blocking artifact reduction using classification of block boundary area", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "046fc207-307c-41cf-8f56-7b401d34e47a"}
{"abstract": "We present a graph drawing algorithm that was developed as an extension of the hierarchical layout method of Sugiyama [4]. The main features of the algorithm are that the edges are orthogonal and that each edge has at most two bends.", "authors": ["Danil E. Baburin"], "n_citation": 0, "title": "Some modifications of Sugiyama approach", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0545c146-82c2-427b-a606-289e0bce1eb3"}
{"abstract": "This paper presents an approach for integrating perceptual signal features (i.e. color and texture) and semantic information within a coupled architecture for image indexing and retrieval. It relies on an expressive knowledge representation formalism handling high-level image descriptions and a full-text query framework. It consequently brings the level of image retrieval closer to users' needs by translating low-level signal features to high-level conceptual data and integrate them with semantic characterization within index and query structures. Experiments on a corpus of 2500 photographs validate our approach by considering recall-precision indicators over a set of 46 full-text queries coupling high-level semantic and signal features.", "authors": ["Mohammed Belkhatir", "Philippe Mulhem", "Yves Chiaramella"], "n_citation": 0, "title": "A full-text framework for the image retrieval signal/semantic integration", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1256de88-f52f-4603-9fa1-0a10831d5a15"}
{"abstract": "Consistency enforcement aims at systematically modifying a database program such that the result is consistent with respect to a specified set of integrity constraints. This modification may be done at compile-time or at run-time. The commonly known run-time approach uses rule triggering systems (RTSs). It has been shown that these systems cannot solve the problem in general. As an alternative greatest consistent specializations (GCSs) have been studied. This approach requires the modified program specification to be a maximal consistent diminution of the original one with respect to some partial order. The chosen order is operational specialization. On this basis it is possible to derive a commutativity result and a compositionality result. The first one enables step-by-step enforcement for sets of constraints. The second one reduces the problem to providing the GCSs just for basic operations, whereas for complex programs the GCS can be easily determined. The approach turns out to be well-founded since the GCS for such complex programs is effectively computable if we require loops to be bounded. Despite its theoretical merits the GCS approach is still too coarse. This leads to the problem of modifying the chosen specialization order and to relax the requirement that the result should be unique. One idea is to exploit the fact that operational specialization is equivalent to the preservation of a set of transition invariants. In this case a reasonable order arises from a slight modification of this set, in which case we talk of a maximal consistent effect preserve (MCE). However, a strict theory of MCEs is still outstanding.", "authors": ["Sebastian Link"], "n_citation": 0, "title": "Consistency enforcement in databases", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "18570e0f-391d-4566-8f15-814c36096acb"}
{"abstract": "A transaction model defines the behaviour, constraints, integrity, inter-relationships, and robustness of database transactions. Such models are generally evaluated indirectly, often by experiments on a database monitor that implements the model, or by workload simulation. In this paper, we propose a novel method of comparing transaction models based on functions of architectural- and isolation-work. Using these functions, we show the complexity of ten transaction models and discuss the relationship between them. We conclude that our architectural- and isolation-work functions can be used to reason about transaction models and as one measure for selecting the model appropriate to specific applications.", "authors": ["Andrew G. Fry", "Hugh E. Williams"], "n_citation": 0, "title": "Comparing the overhead requirements of database transaction models", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1e8074fb-7eb1-4423-9340-5e9abfaff020"}
{"abstract": "Translating ontologies from a source language towards a target one is a required process both for the conception of a new ontology from existing ones (reuse) and for common use of an ontology by different knowledge based systems. The usual approach for translation is based upon the existence of a translator for each pair of languages from and to which the ontology must be translated. Therefore, several translating tools are necessary. We propose a translation approach which employs a unique translator no matter what source and target languages are involved. It relies on two principal components. The first one is a meta-language called M\u2015Kif which extends the Kif language with the concept of meta-relation to describe representation ontologies. The second one is a pivot representation which unifies different styles of representations. The translation tool is an interpretation program of the meta-relation definitions specified in the source and target representation ontologies.", "authors": ["Houria Mihoubi", "Ana Simonet", "Michel Simonet"], "n_citation": 0, "title": "An ontology driven approach to ontology translation", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "33da754d-be08-426a-9203-801151e9a73e"}
{"authors": ["Marcin Kowalski", "Dominik \u015al\u0119zak", "Graham Toppin", "Arkadiusz Wojna"], "n_citation": 0, "title": "Injecting Domain Knowledge into RDBMS - Compression of Alphanumeric Data Attributes", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "34cbc628-12f0-4e77-b53a-1d184d276093"}
{"abstract": "Recent work on continuous queries has focused on processing queries in very large, mobile environments. In this paper, we propose a system leveraging the computing capacities of mobile devices for continuous range query processing. In our design, continuous range queries are mainly processed on the mobile device side, which is able to achieve real-time updates with minimum server load. Our work distinguish itself from previous work with several important contributions. First, we introduce a distributed server infrastructure to partition the entire service region into a set of service zones and cooperatively handle requests of continuous range queries. This feature improves the robustness and flexibility of the system by adapting to a time-varying set of servers. Second, we propose a novel query indexing structure, which records the difference of the query distribution on a grid model. This approach significantly reduce the size and complexity of the index so that in-memory indexing can be achieved on mobile objects with constrained memory size. We report on the rigorous evaluation of our design, which shows substantial improvement in the efficiency of continuous range query processing in mobile environments.", "authors": ["Haojun Wang", "Roger Zimmermann", "Wei-Shinn Ku"], "n_citation": 0, "title": "Distributed continuous range query processing on moving objects", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "40af7fbf-eae3-4289-825e-2d56b1ea3cf9"}
{"abstract": "The paper proposed a new environment matting algorithm to model the appearance of transparent object under different background. We used the frequency response to compute the relationship between the area of foreground object and background image. Moreover, the Kaczmarz method was applied to obtain more accurate weight matrices. In the experiments, we demonstrated that the algorithm can effectively improve the quality of compositing picture and has higher PSNR than the previous method. Besides, we also showed that the proposed algorithm can maintain the quality even the high threshold is set to reduce the computation time.", "authors": ["I-Cheng Chang", "Tian-Lin Yang", "Chung-Ling Huang"], "n_citation": 0, "title": "Environment matting of transparent objects based on frequency-domain analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "422e7d0f-bc61-4907-bddb-25d4d8468e6d"}
{"abstract": "A main challenge in wrapping web data is to make wrappers robust w.r.t. variations in HTML sources, reducing human effort as much as possible. In this paper we develop a new approach to speed up the specification of robust wrappers, allowing the wrapper designer to not care about detailed definition of extraction rules. The key-idea is to enable a schema-based wrapping system to automatically generalize an original wrapper w.r.t. a set of example HTML documents. To accomplish this objective, we propose to exploit the notions of extraction rule and wrapper subsumption for computing a most general wrapper which still shares the extraction schema with the original wrapper, while maximizes the generalization of extraction rules w.r.t. the set of example documents.", "authors": ["Bettina Fazzinga", "Sergio Flesca", "Andrea Tagarelli"], "n_citation": 50, "title": "Learning robust web wrappers", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "53159db5-acf5-4b8f-b482-72736ce5171f"}
{"abstract": "We present a novel personalization engine that provides individualized access to Web contents/services by means of data mining techniques. It associates adaptive content delivery and navigation support with form filling, a functionality that catches the typical interaction of a user with a Web service, in order to automatically fill in its form fields at successive accesses from that visitor. Our proposal was developed within the framework of the ITB@NK system to the purpose of effectively improving users' Web experience in the context of Internet Banking. This study focuses on its software architecture and formally investigates the underlying personalization process.", "authors": ["Eugenio Cesario", "Francesco Folino", "Riccardo Ortale"], "n_citation": 0, "title": "Putting enhanced hypermedia personalization into practice via Web mining", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5650c23d-96eb-4ad4-8b69-4481ef03f1da"}
{"abstract": "Blind signatures play a central role in applications such as e-cash and e-voting systems. The notion of partially blind signature is a more applicable variant such that the part of the message contains some common information pre-agreed by the signer and the signature requester in an unblinded form. In this paper, we propose two efficient partially blind signatures with provable security in the random oracle model. The former is based on witness indistinguishable (WI) signatures. Compared with the state-of-the-art construction due to Abe and Fujisaki [1], our scheme is 25% more efficient while enjoys the same level of security. The latter is a partially blind Schnorr signature without relying on witness indistinguishability. It enjoys the same level of security and efficiency as the underlying blind signature.", "authors": ["Qianhong Wu", "Willy Susilo", "Yi Mu", "Fanguo Zhang"], "n_citation": 0, "title": "Efficient Partially Blind Signatures with Provable Security", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "59891ab0-e169-4900-9655-e16cdd033927"}
{"abstract": "The eXtensible Markup Language (XML) is well accepted in many different application areas. As a consequence, there is an increasing need for storing XML documents persistently. As soon as many users and applications work concurrently on the same collection of XML documents - i.e. an XML base - isolating accesses and modifications of different transactions becomes an important issue. We discuss two different timestamp-based protocols for synchronizing access on XML document collections. These core protocols synchronize structure traversals and modifications. Further, we extend the protocols to handle node contents and IDREF jumps, so that they can be integrated into a native XML base management System (XBMS).", "authors": ["Sven Helmer", "Carl-Christian Kanne", "Guido Moerkotte"], "n_citation": 0, "title": "Timestamp-based protocols for synchronizing access on XML documents", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5aa29451-2f32-4121-9335-39130d07181c"}
{"abstract": "Pattern matching is one of critical parts of Network Intrusion Prevention Systems (NIPS). Pattern matching hardware for NIPS should find a matching pattern at wire speed. However, that alone is not good enough. First, pattern matching hardware should be able to generate sufficient pattern match information including the pattern index number and the location of the match found at wire speed. Second, it should support pattern grouping to reduce unnecessary pattern matches. Third, it should show constant worst-case performance even if the number of patterns is increased. Finally it should be able to update patterns in a few minutes or seconds without stopping its operations. We modify Shift-OR hardware accelerator and propose a system architectures to meet the above requirement. Using Xilinx FPGA simulation, we show the new system scaled well to achieve a high speed over 10Gbps and satisfies all of the above requirements.", "authors": ["Dae Y. Kim", "Sunil Kim", "Lynn Choi", "Hyogon Kim"], "n_citation": 50, "title": "A high-throughput system architecture for deep packet filtering in network intrusion prevention", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5e8713f3-dc20-451f-8a7a-ee636dde0d6c"}
{"abstract": "Understanding what construction strategy has a chance to be a good hash function is extremely important nowadays. In TCC'04, Maurer et al. [13] introduced the notion of indifferentiability as a generalization of the concept of the indistinguishability of two systems. In Crypto'2005, Coron et al. [5] suggested to employ indifferentiability in generic analysis of hash functions and started by suggesting four constructions which enable eliminating all possible generic attacks against iterative hash functions. In this paper we continue this initial suggestion and we give a formal proof of indifferentiability and indifferentiable attack for prefix-free MD hash functions (for single block length (SBL) hash and also some double block length (DBL) constructions) in the random oracle model and in the ideal cipher model. In particular, we observe that there are sixteen PGV hash functions (with prefix-free padding) which are indifferentiable from random oracle model in the ideal cipher model.", "authors": ["Donghoon Chang", "Sangjin Lee", "Mridul Nandi", "Moti Yung"], "n_citation": 63, "title": "Indifferentiable Security Analysis of Popular Hash Functions with Prefix-Free Padding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5fdba10b-8358-4bfc-848e-3b4004e0d760"}
{"abstract": "Streaming XML documents has many emerging applications. However, in this paper, we show that the restrictions imposed by data streaming are too restrictive for processing twig queries - the core operation for XML query processing. Previous proposed algorithm TwigStack is an optimal algorithm for processing twig queries with only descendent edges over streams of nodes. The cause of the suboptimality of the TwigStack algorithm is the structurally recursions appearing in XML documents. We show that without relaxing the data streaming model, it is not possible to develop an optimal holistic algorithm for twig queries. Also the computation of the twig queries is not memory bounded. This motivates us to study two variations of the data streaming model: (1) offline sorting is allowed and the algorithm is allowed to select the correct nodes to be streamed and (2) multiple scans on the data streams are allowed. We show the lower bounds of the two variations.", "authors": ["Byron Choi", "Malika Mahoui", "Derick Wood"], "n_citation": 0, "title": "On the optimality of holistic algorithms for twig queries", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "6372998a-23df-4ea6-9514-e72e8ce27a59"}
{"authors": ["Elena Andreeva", "Andrey Bogdanov", "Atul Luykx", "Bart Mennink", "Elmar Tischhauser", "Kan Yasuda"], "n_citation": 0, "title": "Parallelizable and Authenticated Online Ciphers", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "6a0c32ca-cafe-447c-8d4e-0707e31a6b7e"}
{"abstract": "In relational database systems a combination of privileges and views is employed to limit a user's access and to hide non-public data. The data privacy problem is to decide whether the views leak information about the underlying database instance. Or, to put it more formally, the question is whether there are certain answers of a database query with respect to the given view instance. In order to answer the problem of provable date privacy, we will make use of query answering techniques for data exchange. We also investigate the impact of database dependencies on the privacy problem. An example about health care statistics in Switzerland shows that we also have to consider dependencies which are inherent in the semantics of the data.", "authors": ["Kilian Stoffel", "Thomas Studer"], "n_citation": 0, "title": "Provable data privacy", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6efb56d7-4408-4b1d-a9e6-9ac4f698bc88"}
{"abstract": "Despite the rapid growth of wallpaper image downloading service in the mobile contents market, users experience high levels of frustration in searching for desired images, due to the absence of intelligent searching aid. Although Content Based Image Retrieval is the most widely used technique for image retrieval in the PC-based system, its application in the mobile Web environment poses one major problem of not being able to satisfy its initial query requirement because of the limitations in user interfaces of the mobile application software. We propose a new approach, so called a CF-fronted CBIR, where Collaborative Filtering (CF) technique automatically generates a list of candidate images that can be used as an initial query in Content Based Image Retrieval (CBIR) by utilizing relevance information captured during Relevance Feedback. The results of the experiment using a PC-based prototype system verified that the proposed approach not only successfully satisfies the initial query requirement of CBIR in the mobile Web environment but also outperforms the current search process.", "authors": ["Deok Hwan Kim", "Chan Young Kim", "Yoon Ho Cho"], "n_citation": 0, "title": "Automatic generation of the initial query set for CBIR on the mobile web", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "72b7bcdf-1bdd-4f79-8c0e-a72189b5d8df"}
{"abstract": "We show that graph-theoretic thickness and geometric thickness are not asymptotically equivalent: for every t, there exists a graph with thickness three and geometric thickness > t.", "authors": ["David Eppstein"], "n_citation": 0, "title": "Separating thickness from geometric thickness", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "743c9b1e-b992-4bba-a052-c714dafcc899"}
{"abstract": "A new stochastic algorithm for determination of a global minimum of a real valued continuous function defined on K, a compact set of R, having an unique global minimizer in K is introduced and studied, a context discussion is presented and implementations are used to compare the performance of the algorithm with other algorithms. The algorithm may be thought to belong to the random search class but although we use Gaussian distributions, the mean is changed at each step to be the intermediate minimum found at the preceding step and the standard deviations, on the diagonal of the covariance matrix, are halved from one step to the next. The convergence proof is simple relying on the fact that the sequence of intermediate random minima is an uniformly integrable conditional Gaussian martingale.", "authors": ["Manuel L. Esqu\u00edvel"], "n_citation": 0, "title": "A Conditional Gaussian Martingale Algorithm for Global Optimization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7c105476-3459-43ed-9ec5-190b6e0b0185"}
{"abstract": "Efficient storage and retrieval of trajectory indexes has become an essential requirement for moving objects databases. The existing 3DR-tree is known to be an effective trajectory index structure for processing trajectory and time slice queries. Efficient processing of trajectory queries requires parallel processing based on indexes and parallel access methods for the trajectory index. Several heuristic methods have been developed to decluster R-tree nodes of spatial data over multiple disks to obtain high performance for disk accesses. However, trajectory data is different from two-dimensional spatial data because of peculiarities of the temporal dimension and the connectivity of the trajectory. In this paper, we propose a declustering policy based on spatio-temporal trajectory proximity. Extensive experiments show that our STP scheme is better than other declustering schemes by about 20%.", "authors": ["Youngduk Seo", "Bonghee Hong"], "n_citation": 0, "title": "Declustering of trajectories for indexing of moving objects databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7d8ee56f-cc13-4dcd-ba35-c2543baf3169"}
{"abstract": "In this paper, we propose a method for multi-term query expansions based on WordNet. In our approach, Hypernym/Hyponymy and Synonym relations in WordNet is used as the basic expansion rules. Then we use WordNet Lexical Chains and WordNet semantic similarity to assign terms in the same query into different groups with respect to their semantic similarities. For each group, we expand the highest terms in the WordNet hierarchies with Hypernym and Synonym, the lowest terms with Hyponym and Synonym, and all other terms with only Synonym. Furthermore, we use collection related term semantic network to remove the low-frequency and unusual words in the expansions. And our experiment reveals that our solution for query expansion can improve the query performance dramatically.", "authors": ["Zhiguo Gong", "Chan Wa Cheang", "Leong Hou U"], "n_citation": 14, "title": "Multi-term web query expansion using wordNet", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "81788922-a0bb-4a3a-8b35-0896beb7e958"}
{"abstract": "Based on recent studies, the most common queries in Web searches involve one or two keywords. While most Web search engines perform very well for a single-keyword query, their precisions is not as good for queries involving two or more keywords. Search results often contain a large number of pages that are only weakly relevant to either of the keywords. One solution is to focus on the proximity of keywords in the search results. Filtering keywords by semantic relationships could also be used. We developed a method to improve the precison of Web retrieval based on the semantic relationships between and proximity of keywords for two-keyword queries. We have implemented a system that re-ranks Web search results based on three measures: first-appearance term distance, minimum term distance, and local appearance density. Furthermore, the system enables the user to assign weights to the new rank and original ranks so that the result can be presented in order of the combined rank. We built a prototype user interface in which the user can dynamically change the weights on two different ranks. The result of the experiment showed that our method improves the precision of Web search results for two-keyword queries.", "authors": ["Chi Tian", "Taro Tezuka", "Satoshi Oyama", "Keishi Tajima", "Katsumi Tanaka"], "n_citation": 0, "title": "Improving web retrieval precision based on semantic relationships and proximity of query keywords", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "83afb695-7697-4c2a-95d3-e57d3faa1b9b"}
{"abstract": "We expose paraconsistent logic with regard to its potential to contribute to the foundations of databases. We do so from a historical perspective, starting at the ancient inception and arriving at the contemporary use of logic as a computational device. We show that an understanding of the logic foundations of databases in terms of paraconsistency is adequate. It avoids absurd connotations of the ex contradictione quodlibet principle, which in fact never applies in databases. We interpret datalog, its origins and some of its extensions by negation and abduction, in terms of paraconsistency. We propose a procedural definition of paraconsistency and show that many well-known query answering procedures comply with it.", "authors": ["Hendrik Decker"], "n_citation": 50, "title": "Historical and computational aspects of paraconsistency in view of the logic foundation of databases", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "87a0b454-c811-4fd5-8a6b-4847d78710a9"}
{"abstract": "In [1] a method for inducing the effects of actions was introduced which provides a solution to the frame problem in induction. The method relied on well-known monotonic methods of ILP making it as efficient as induction of Horn Logic Programs. That proposal is not intended for the induction of the ramifications of the effects of actions (indirect effects) thus providing domain descriptions with the so-called ramification problem. In this work we introduce the induction of such ramification rules describing effects directly from other effects without mentioning the action. A framework based on causality in action formalisms is used to induce causal ramification rules. The method is shown sound and complete while efficient as the induction of action rules.", "authors": ["Ram\u00f3n P. Otero"], "n_citation": 50, "title": "Induction of the indirect effects of actions by monotonic methods", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "88a57c4c-8c59-464f-994f-d8ceef715afb"}
{"abstract": "The OAEP construction is already 10 years old and well-established in many practical applications. But after some doubts about its actual security level, four years ago, the first efficient and provably IND-CCA1 secure encryption padding was formally and fully proven to achieve the expected IND-CCA2 security level, when used with any trapdoor permutation. Even if it requires the partial-domain one-wayness of the permutation, for the main application (with the RSA permutation family) this intractability assumption is equivalent to the classical (full-domain) one-wayness, but at the cost of an extra quadratic-time reduction. The security proof which was already not very tight to the RSA problem is thus much worse. However, the practical optimality of the OAEP construction is two-fold, hence its attractivity: from the efficiency point of view because of two extra hashings only, and from the length point of view since the ciphertext has a minimal bit-length (the encoding of an image by the permutation.) But the bandwidth (or the ratio ciphertext/plaintext) is not optimal because of the randomness (required by the semantic security) and the redundancy (required by the plaintext-awareness, the sole way known to provide efficient CCA2 schemes.) At last Asiacrypt '03, the latter intuition had been broken by exhibiting the first IND-CCA2 secure encryption schemes without redundancy, and namely without achieving plaintext-awareness, while in the random-oracle model: the OAEP 3-round construction. But this result achieved only similar practical properties as the original OAEP construction: the security relies on the partial-domain one-wayness, and needs a trapdoor permutation, which limits the application to RSA, with still a quite bad reduction. This paper improves this result: first we show the OAEP 3-round actually relies on the (full-domain) one-wayness of the permutation (which improves the reduction), then we extend the application to a larger class of encryption primitives (including ElGamal, Paillier, etc.) The extended security result is still in the random-oracle model, and in a relaxed CCA2 model (which lies between the original one and the replayable CCA scenario).", "authors": ["Duong Hieu Phan", "David Pointcheval"], "n_citation": 0, "title": "OAEP 3-round: A generic and secure asymmetric encryption padding", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "88fab3da-625c-4b09-a631-d6440afff52d"}
{"authors": ["Ling Zhuang", "Huafeng Liu", "Wei Chen", "Hujun Bao", "Pengcheng Shi"], "n_citation": 0, "title": "Simultaneous segmentation and motion recovery in 3D cardiac image analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "98d0c682-80d9-4877-816f-4b8adaf46cdc"}
{"abstract": "This paper concerns the modelling of fuzzy information in geographic databases. In the past, a theoretical model for fuzzy regions has been presented, along with various operations useful in a geographic database: union, intersection, topology, bounding rectangle, etc. and feasible models for implementation based on this theoretical model. Now, the attention is directed at some of the problems that can occur when determining numerical properties of fuzzy regions: what type of result is expected (and desired), and how does this impact the definitions of the operations. As an example, the definition of the surface area of a fuzzy set is studied in more detail.", "authors": ["J\u00f6rg Verstraete", "Axel Hallez"], "n_citation": 0, "title": "Numerical properties of fuzzy regions: Surface area", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "99b4addd-47a3-4e82-b31b-5948254a2016"}
{"abstract": "We propose a Zooming Cross-Media concept that uses zooming to achieve both changes in the level of detail and transitions between media, for contents containing varied media. Examples are text, images, video, and sound. As part of the concept, we propose a zooming description language (ZDL) based on XML. Unlike existing zooming interfaces, ZDL codes the zooming operation and behavior on the content side. Because ZDL adopts XML coding, we can locate zooming as the third interface in the Web document environment after scrolling and anchor clicking. The zooming operation and behavior is independently coded from the content structure in ZDL. With ZDL, it is possible to (1) control the zooming of each zoom object making up the contents, (2) control the degree of zooming by introducing a zoom rate parameter, and (3) relate objects mutually and specify zooming propagation between related objects.", "authors": ["Tadashi Araki", "Hisashi Miyamori", "Mitsuru Minakuchi", "Ai Kato", "Zoran Stejic", "Yasushi Ogawa", "Katsumi Tanaka"], "n_citation": 0, "title": "Zooming cross-media : A zooming description language coding LOD control and media transition", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9c6b0a63-6878-4b4e-b6b6-9dda05d89656"}
{"authors": ["I. A. C. P. Esp\u00edrito Santo", "Edite Manuela da G. P. Fernandes"], "n_citation": 50, "title": "Heuristic pattern search for bound constrained minimax problems", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "a3f14668-8502-4f0a-a8c6-c007333a6d6f"}
{"abstract": "The LOGAN-H system is a bottom up ILP system for learning multi-clause and multi-predicate function free Horn expressions in the framework of learning from interpretations. The paper introduces a new implementation of the same base algorithm which gives several orders of magnitude speedup as well as extending the capabilities of the system. New tools include several fast engines for subsumption tests, handling real valued features, and pruning. We also discuss using data from the standard ILP setting in our framework, which in some cases allows for further speedup. The efficacy of the system is demonstrated on several ILP datasets.", "authors": ["Marta Arias", "Roni Khardon"], "n_citation": 0, "title": "Bottom-up ILP using large refinement steps", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ae4e0da2-09f7-4e27-b71d-98ad655f2257"}
{"abstract": "In this paper, we propose a new scheme for digital watermarking of three-dimensional (3-D) triangle meshes. In order to insert and extract watermark signals, we generate triangle strips by traversing the 3-D mesh model. We split the strips into two groups. For the first groups we embed the watermark signal into vertex positions of the 3-D model in the DCT domain, and for the second group, we embed the watermark in the vertex positions in the spherical coordinates. We use space-time coding for watermark data before embedding in the mesh. The information of the two generated bitstream of space-time coder are embedded in DCT and spherical coordinate of the mesh. At the receiver, the space-time decoder combines the information of each bitstream to reconstruct the watermark data. In both cases, we apply masking operation for the watermark signal according to the variation of the host data along the traversed strips so that we can change the embedding strength adaptively, and reduce the visibility distortion of the data embedding. We test robustness of the watermarking scheme by embedding a random binary sequence and applying different attacks, such as additive random noise, compression by MPEG-4 SNHC, and affine transform.", "authors": ["Mohsen Ashourian", "Keyvan Mohebbi"], "n_citation": 0, "title": "Using space-time coding for watermarking of three-dimensional triangle mesh", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b21b3af6-749d-4b62-884e-2268bf36f9ad"}
{"abstract": "Information on the Web like HTML documents with images, video, and sound is a collection of heterogeneous data. HTML documents are semistructured in nature. Semistructured data are used to describe those structures which are less rigid or regular than those data found in standard database systems. This study presents a novel means of using Patricia Tree [14] to index semistructured data. This index is used by transferring the query into a regular expression and querying the regular expression over the Patricia Tree. The highlight of this approach is supporting query on content and structure simultaneously, while also supporting fast query time on long path and regular expressions.", "authors": ["Li-Cheng Wu", "Jorng Tzong Horng", "Baw Jhiung Liu", "Changyu Wang", "Gwo Dong Chen"], "n_citation": 0, "title": "Indexing semistructured data using PATRICIA Tree", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "b24f1f8a-3061-4ea7-8411-257545c011ae"}
{"abstract": "Radio frequency identification systems based on low-cost computing devices is the new plaything that every company would like to adopt. The biggest challenge for RFID technology is to provide benefits without threatening the privacy of consumers. Using cryptographic primitives to thwart RFID security problems is an approach which has been explored for several years. In this paper, we introduce a new security problem called as Tag Killing which aims to wipe out the functioning of the system, e.g., denial of service attacks. We analyze several well-known RFID protocols which are considered as good solutions with Tag Killing adversary model and we show that most of them have weaknesses and are vulnerable to it.", "authors": ["Dong-Guk Han", "Tsuyoshi Takagi", "Ho Won Kim", "Kyo Il Chung"], "n_citation": 0, "title": "New Security Problem in RFID Systems Tag Killing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b3742e6b-4c7c-42cb-8a62-7d6e2a0dfd3e"}
{"abstract": "We present a novel approach to the aesthetic drawing of undirected graphs. The method has two phases: first embed the graph in a very high dimension and then project it into the 2-D plane using principal components analysis. Running time is linear in the graph size, and experiments we have carried out show the ability of the method to draw graphs of 10 5  nodes in few seconds. The new method appears to have several advantages over classical methods, including a significantly better running time, a useful inherent capability to exhibit the graph in various dimensions, and an effective means for interactive exploration of large graphs.", "authors": ["David Harel", "Yehuda Koren"], "n_citation": 0, "title": "Graph drawing by high-dimensional embedding", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c35167ef-f9aa-4ab5-8458-3376db83a150"}
{"abstract": "In this paper, we develop inventory models for the three level supply chain (one supplier, one warehouse, and one retailer) and consider the problem of determining the optimal integer multiple n of time interval, time interval between successive setups and orders in the coordinated inventory model. We consider three types of individual models (independent model, retailer's point of view model, and supplier's point of view model). The focus of this model is minimization of the coordinated total relevant cost, and then we apply the compensation policy for the benefits and losses to our coordinated inventory model. The optimal solution procedure for the developed model is derived and the effects of the compensation policy on the optimal results are studied with the help of numerical examples.", "authors": ["Jeong Hun Lee", "Il Kyeong Moon"], "n_citation": 50, "title": "Coordinated Inventory Models with Compensation Policy in a Three Level Supply Chain", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c63e69ad-e16e-492f-b8d2-decf98e2282b"}
{"abstract": "To support necessary requirements and flexibility to the buyers of different goods; advanced and efficient internet-based Electronic Commerce services must be designed and developed. In addition to the traditional user requirements, the developed system must properly address efficiency issues, among which the data catalogues, information classification, short response time for on-line requests, high system performance, and high data transfer rates must be considered. The MegaStore 1  system described in this paper, aims at the design and set-up of the necessary database structure and platform architecture for advanced e-commerce applications. The proposed system architecture best suits the e-commerce application, by separating the public information from the private information and supporting the large data sets that need to be securely kept at predefined Internet sites. The design of the innovative architecture and technology for the distributed MegaStore application although applied to music CD industry, is general enough to be applicable to other complex application environments, especially to e-commerce applications.", "authors": ["Ammar Benabdelkader", "Hamideh Afsarmanesh", "Louis O. Hertzberger"], "n_citation": 0, "title": "Megastore : Advanced internet-based E-commerce service for music industry", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "c9e801dc-7fd8-475a-a3af-233ed0704752"}
{"abstract": "Given sample data and background knowledge encoded in the form of logic programs, a predictive Inductive Logic Programming (ILP) system attempts to find a set of rules (or clauses) for predicting classification labels in the data. Most present-day systems for this purpose rely on some variant of a generate-and-test procedure that repeatedly examines a set of potential candidates (termed here as the hypothesis space). On each iteration a search procedure is employed to find the best clause. The worst-case time-complexity of such systems depends critically on: (1) the size of the hypothesis spaces examined; and (2) the cost of estimating the goodness of a clause. To date, attempts to improve the efficiency of such ILP systems have concentrated either on examining fewer clauses within a given hypothesis space, or on efficient means of estimating the goodness of clauses. The principal means of restricting the size of the hypothesis space itself has been through the use of language and search constraints. Given such constraints, this paper is concerned with investigating the use of a dimensionality reduction method to reduce further the size of the hypothesis space. Specifically, for a particular kind of ILP system, clauses in the search space are represented as points in a high-dimension space. Using a sample of points from this geometric space, feature selection is used to discard dimensions of little or no (statistical) relevance. The resulting lower dimensional space translates directly, in the worst-case, to a smaller hypothesis space. We evaluate this approach on one controlled domain (graphs) and two real-life datasets concerning problems from biochemistry (mutagenesis and carcinogenesis). In each case, we obtain unbiased estimates of the size of the hypothesis space before and after feature selection; and compare the the results obtained using a complete search of the two spaces.", "authors": ["Ashwin Srinivasan", "Ravi Kothari"], "n_citation": 0, "title": "A study of applying dimensionality reduction to restrict the size of a hypothesis space", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d0c5cf81-b703-4ccd-965a-ccdd8f95bc24"}
{"abstract": "New quantization method based on magnitude-sign split scheme for bandwidth scalable wideband speech codec is proposed. In the high-band codec, the signal is band-pass filtered and each band is transformed independently into DCT domain. The DCT coefficients are split into magnitude and sign, and each is quantized separately based on its unique characteristics. In addition, the quantized gain parameter in the low-band codec is utilized in the high-band co' dec for an enhanced performance. The 19.8kbps bandwidth scalable wideband codec consisting of G.729E for low-band and the proposed codec for high-band is developed, and it is confirmed that the proposed codec has better subjective performance than 24kbps G.722.1.", "authors": ["Ji-Hyuk You", "Chul-Man Park", "Jung-Il Lee", "Chang-Beom Ahn", "Oh Seoung-Jun", "Hochong Park"], "n_citation": 0, "title": "Magnitude-sign split quantization for bandwidth scalable wideband speech codec", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d6b97a70-0221-40a8-a14e-46ac6becd454"}
{"abstract": "In this paper, we propose a Wireless Class Based Flexible Queueing (WCBFQ) which is an improved packet scheduling with QoS management based on wireless networks using OFDMA-TDD with adaptive modulation. Under time-varying channel condition, scheduler tries to guarantee required throughput demand for real-time traffic by allocating more channel time to the users in poor channel condition. On the other hand, for non-real time traffic, the scheduler gives lower priority to those experiencing poor channel conditions thereby favoring those having better channel for increased overall system throughput. Simulations are performed to demonstrate the effectiveness of the proposed scheme.", "authors": ["Ryong Oh", "Se-Jin Kim", "Hyong-Woo Lee", "Choong-Ho Cho"], "n_citation": 50, "title": "Downlink packet scheduling based on channel condition for multimedia services of mobile users in OFDMA-TDD", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "de2b7699-d7d4-455a-bd7e-0d1bd7372723"}
{"abstract": "Nearest neighbor search has a wide variety of applications. Unfortunately, the majority of search methods do not scale well with dimensionality. Recent efforts have been focused on finding better approximate solutions that improve the locality of data using dimensionality reduction. However, it is possible to preserve the locality of data and find exact nearest neighbors in high dimensions without dimensionality reduction. This paper introduces a novel high-performance technique to find exact \u03ba-nearest neighbors in both low and high dimensional spaces. It relies on a new method for data-sensitive space partitioning based on explicit data clustering, which is introduced in the paper for the first time. This organization supports effective reduction of the search space before accessing secondary storage. Costly Euclidean distance calculations are reduced through efficient processing of a lightweight memory-based filter. The algorithm outperforms sequential scan and the VA-File in high-dimensional situations. Moreover, the results with dynamic loading of data show that the technique works well on dynamic datasets as well.", "authors": ["Sachin Kulkarni", "Ratko Orlandic"], "n_citation": 50, "title": "High-dimensional similarity search using data-sensitive space partitioning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e1c75b80-2f1a-43d0-9575-8d8bbbdaec4f"}
{"abstract": "This study investigates the combination of four different classification methods for text categorization through experimental comparisons. These methods include the Support Vector Machine, kNN (nearest neighbours), kNN model-based approach (kNNM), and Rocchio methods. We first review these learning methods and the method for combining the classifiers, and then present some experimental results on a benchmark data collection of 20-newsgroup with an emphasis of average group performance - looking at the effectiveness of combining multiple classifiers on each category. In an attempt to see why the combination of the best and the second best classifiers can achieve better performance, we propose an empirical measure called closeness as a basis of our experiments. Based on our empirical study, we verify the hypothesis that when a classifier has the high closeness to the best classifier, their combination can achieve the better performance.", "authors": ["Yaxin Bi", "David A. Bell", "Hui Wang", "Gongde Guo", "Werner Dubitzky"], "n_citation": 0, "title": "Classification decision combination for text categorization: An experimental study", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e1df644b-2382-4eed-a2c3-1586fc9fea28"}
{"abstract": "Evaluating queries over massive amounts of data is a major challenge in the big data era. Modern massively parallel systems, such as, Spark, organize query answering as a sequence of rounds each consisting of a distinct communication phase followed by a computation phase. The communication phase redistributes data over the available servers, while in the subsequent computation phase each server performs the actual computation on its local data. There is a growing interest in single-round algorithms for evaluating multiway joins where data is first reshuffled over the servers and then evaluated in a parallel but communication-free way. As the amount of communication induced by a reshuffling of the data is a dominating cost in such systems, we introduce a framework for reasoning about data partitioning to detect when we can avoid the data reshuffling step. Specifically, we formalize the decision problems parallel-correctness and transfer of parallel-correctness, provide semantical characterizations, and obtain tight complexity bounds.", "authors": ["Tom J. Ameloot", "Gaetano Geck", "Bas Ketsman", "Frank Neven", "Thomas Schwentick"], "n_citation": 0, "references": ["0b2b5b0c-74d1-486b-88ae-7280fcd09c27", "10bc5abf-4f64-4a90-8a98-5d0b535b56b7", "1431b8bc-30e4-4da0-b8e7-110529fea498", "15bd2cf8-9ae4-43aa-91e0-15156d8e8cfe", "203fcd80-2b9b-4da1-9633-8a1a3d166a06", "25ef5976-b916-4b65-a6b3-f3a1e03f5f8a", "295effa7-f364-4883-a8d8-86c6c8b952b3", "2d53d27a-3cd1-422e-8676-dc1e094210f9", "3061a657-1e98-4936-93e7-d2265311163a", "3c967383-64df-4d0c-a74a-1fbc1d2ef8bf", "56a5b3ee-354a-403f-99bf-be7467144dcc", "5e1cad30-6c4f-4917-aa9b-6e33fba035a6", "657f3755-a526-4d8c-96f9-1aacb5e952ea", "8b729754-b172-4f22-a390-481bf1f88e60", "911fe583-b750-4378-86b4-2a5723a0b0d7", "955c7a46-7067-482b-9d35-897121ffd375", "9a493f6e-356c-4b89-b025-43e4d5258c6a", "9ba11a2e-b321-4823-aa62-f0088193852d", "a79d58b5-c04e-49cc-a68c-76475063eda7", "e13ec6c2-f328-46be-b499-dd9bd56c469b"], "title": "Reasoning on data partitioning for single-round multi-join evaluation in massively parallel systems", "venue": "Communications of The ACM", "year": 2017, "id": "f06977d1-db5b-4081-a1ef-6d146c3ac06c"}
{"abstract": "Inductive Logic Programming (ILP) systems have had noteworthy successes in extracting comprehensible and accurate models for data drawn from a number of scientific and engineering domains. These results suggest that ILP methods could enhance the model-construction capabilities of software tools being developed for the emerging discipline of knowledge discovery from databases. One significant concern in the use of ILP for this purpose is that of efficiency. The performance of modern ILP systems is principally affected by two issues: (1) they often have to search through very large numbers of possible rules (usually in the form of definite clauses); (2) they have to score each rule on the data (usually in the form of ground facts) to estimate goodness. Stochastic and greedy approaches have been proposed to alleviate the complexity arising from each of these issues. While these techniques can result in order-of-magnitude improvements in the worst-case search complexity of an ILP system, they do so at the expense of exactness. As this may be unacceptable in some situations, we examine two methods that result in admissible transformations of clauses examined in a search. While the methods do not alter the size of the search space (that is, the number of clauses examined), they can alleviate the theorem-proving effort required to estimate goodness. The first transformation simply involves eliminating literals using a weak test for redundancy. The second involves partitioning the set of literals within a clause into groups that can be executed independently of each other. The efficacy of these transformations are evaluated empirically on a number of well-known ILP datasets. The results suggest that for problems that require the use of highly non-determinate predicates, the transformations can provide significant gains as the complexity of clauses sought increases.", "authors": ["V. Santos Costa", "A. Srinivasan", "Rui Camacho"], "n_citation": 0, "title": "A note on two simple transformations for improving the efficiency of an ILP system", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "f938978c-be72-4d34-84e2-1cfbcdd66227"}
{"abstract": "In this paper we propose a method for using ethnographic field data to substantiate agent-based models for socially-oriented systems. We use the agent paradigm because the ability to represent organisations, individuals, and interactions is ideal for modelling socio-technical systems. We present the results of in-situ use of a domestic application created to encourage engagement between grandparents and grandchildren separated by distance. In such domains, it is essential to consider abstract and complex quality requirements such as showing presence and sharing fun. The success of such domestic technologies is based on the meaningful realisation of these difficult-to-define quality goals. Our method addresses the need to adequately inform these quality goals with field data. We substantiate the quality goals with field data collected by introducing an application into the home of three families. The field data adds an understanding of what sharing fun means when \"filled\" with concrete activities. The quality goals served as a template to explore and represent the rich field data, while the field data helped to formulate the requirements for a more complex and refined technology. This paper's contribution is twofold. First, we extend the understanding of agent-oriented concepts by applying them to household interactions. Second, we make a methodological contribution by establishing a new method for informing quality goals with field data.", "authors": ["Sonja Pedell", "Tim Miller", "Leon Sterling", "Frank Vetere", "Stephen D. Howard"], "n_citation": 50, "title": "Substantiating agent-based quality goals for understanding socio-technical systems", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "fd7b9825-3a24-47e0-8f55-83e7b646c1d1"}
{"abstract": "Formal Concept Analysis is an unsupervised learning technique for conceptual clustering. We introduce the notion of iceberg concept lattices and show their use in Knowledge Discovery in Databases (KDD). Iceberg lattices are designed for analyzing very large databases. In particular they serve as a condensed representation of frequent patterns as known from association rule mining. In order to show the interplay between Formal Concept Analysis and association rule mining, we discuss the algorithm TITANIC. We show that iceberg concept lattices are a starting point for computing condensed sets of association rules without loss of information, and are a visualization method for the resulting rules.", "authors": ["Gerd Stumme"], "n_citation": 94, "title": "Efficient data mining based on Formal Concept Analysis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ff3cd472-2a71-44ab-8620-c291217f8032"}
{"authors": ["Lei Chen", "Xiang Lian"], "n_citation": 0, "title": "Query processing over uncertain and probabilistic databases", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "0353d8c7-9ac5-4c36-ad81-2accb7cb4089"}
{"abstract": "There are two types of formalization for induction in logic. In descriptive induction, induced hypotheses describe rules with respect to observations with all predicates minimized. In explanatory induction, on the other hand, hypotheses abductively account for observations without any minimization principle. Both inductive methods have strength and weakness, which are complementary to each other. In this work, we unify these two logical approaches. In the proposed framework, not all predicates are minimized but minimality conditions can be flexibly determined as a circumscription policy. Constructing appropriate policies, we can intentionally minimize models of an augmented axiom set. As a result, induced hypotheses can have both conservativeness and explainability, which have been considered incompatible with each other in the literature. We also give two procedures to compute inductive hypotheses in the proposed framework.", "authors": ["Katsumi Inoue", "H. Saito"], "n_citation": 0, "title": "Circumscription policies for induction", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "13f1a338-9dd8-4238-8ad3-76b48e074b20"}
{"abstract": "Recently, there has been significant work in the integration of probabilistic reasoning with first order logic representations. Learning algorithms for these models have been developed and they all considered modifications in the entire structure. In a previous work we argued that when the theory is approximately correct the use of techniques from theory revision to just modify the structure in places that failed in classification can be a more adequate choice. To score these modifications and choose the best one the log likelihood was used. However, this function was shown not to be well-suited in the propositional Bayesian classification task and instead the conditional log likelihood should be used. In the present paper, we extend this revision system showing the necessity of using specialization operators even when there are no negative examples. Moreover, the results of a theory modified only in places that are responsible for the misclassification of some examples are compared with the one that was modified in the entire structure using three databases and considering four probabilistic score functions, including conditional log likelihood.", "authors": ["Aline Paes", "Kate Revoredo", "Gerson Zaverucha", "V\u00edtor Santos Costa"], "n_citation": 0, "title": "Probabilistic first-order theory revision from examples", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2e42a465-fc64-4603-80cd-63f3540b0eda"}
{"authors": ["Annalisa Bossi", "Chiara Meo Maria"], "n_citation": 0, "title": "Theoretical Foundations and Semantics of Logic Programming", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "3024d8ed-00cc-48d9-9e9a-89d5425a1cb5"}
{"abstract": "This paper tackles the problem that methods for propositionalization and feature construction in first-order logic to date construct features in a rather unspecific way. That is, they do not construct features on demand, but rather in advance and without detecting the need for a representation change. Even if structural features are required, current methods do not construct these features in a goal-directed fashion. In previous work, we presented a method that creates structural features in a class-sensitive manner: We queried the molecular feature miner (MOLFEA) for features (linear molecular fragments) with a minimum frequency in the positive examples and a maximum frequency in the negative examples, such that they are, statistically significant, over-represented in the positives and under-represented in the negatives. In the present paper, we go one step further. We construct structural features in order to discriminate between those examples from different classes that are particularly problematic to classify. In order to avoid overfitting, this is done in a boosting framework. We are alternating AdaBoost re-weighting episodes and feature construction episodes in order to construct structural features on demand. In a feature construction episode, we are querying for features with a minimum cumulative weight in the positives and a maximum cumulative weight in the negatives, where the weights stem from the previous AdaBoost iteration. In summary, we propose to construct structural features on demand by a combination of AdaBoost and an extension of MOLFEA to handle weighted learning instances.", "authors": ["Stefan Kramer"], "n_citation": 50, "title": "Demand-driven construction of structural features in ILP", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "341d4f75-3faf-442c-976d-68dc3f96829f"}
{"authors": ["Michael Gygli", "Helmut Grabner", "Hayko Riemenschneider", "Luc Van Gool"], "n_citation": 0, "title": "Creating summaries from user videos", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "40b56e0f-a955-4c2c-9493-2abe675f95ef"}
{"abstract": "Similarity of graphs with labeled vertices and edges is naturally defined in terms of maximal common subgraphs. To avoid computation overload, a parameterized technique for approximation of graphs and their similarity is used. A lattice-based method of binarizing labeled graphs that respects the similarity operation on graph sets is proposed. This method allows one to compute graph similarity by means of algorithms for computing closed sets. Results of several computer experiments in predicting biological activity of chemical compounds that employ the proposed technique testify in favour of graph approximations as compared to complete graph representations: gaining in efficiency one (almost) does not lose in accuracy.", "authors": ["Sergei O. Kuznetsov", "Mikhail Vasil'evich Samokhin"], "n_citation": 0, "title": "Learning closed sets of labeled graphs for chemical applications", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4eb795f1-f857-4703-9887-5ce723f3d0da"}
{"authors": ["N. de Vries", "Ahmed Shamsul Arefin", "Luke Mathieson", "Benjamin Lucas", "Pablo Moscato"], "n_citation": 0, "title": "Relative Neighborhood Graphs Uncover the Dynamics of Social Media Engagement", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "502b18a1-8822-446d-9430-9a4221264649"}
{"abstract": "Clustering is a fundamental task in Spatial Data Mining where data consists of observations for a site (e.g. areal units) descriptive of one or more (spatial) primary units, possibly of different type, collected within the same site boundary. The goal is to group structured objects, i.e. data collected at different sites, such that data inside each cluster models the continuity of socio-economic or geographic environment, while separate clusters model variation over the space. Continuity is evaluated according to the spatial organization arising in data, namely discrete spatial structure, expressing the (spatial) relations between separate sites implicitly defined by their geometrical representation and positioning. Data collected within sites that are (transitively) connected in the discrete spatial structure are clustered together according to the similarity on multi-relational descriptions representing their internal structure. CORSO is a novel spatial data mining method that resorts to a multi-relational approach to learn relational spatial data and exploits the concept of neighborhood to capture relational constraints embedded in the discrete spatial structure. Relational data are expressed in a first-order formalism and similarity among structured objects is computed as degree of matching with respect to a common generalization. The application to real-world spatial data is reported.", "authors": ["Donato Malerba", "Annalisa Appice", "Antonio Varlaro", "Antonietta Lanza"], "n_citation": 0, "title": "Spatial clustering of structured objects", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5f4ad1f0-3b96-47f1-9ea2-94ab663f5716"}
{"abstract": "This paper studies equivalence issues in inductive logic programming. A background theory B 1  is inductively equivalent to another background theory B 2  if Bi and B 2  induce the same hypotheses for any given set of examples. Inductive equivalence is useful to compare inductive capabilities among agents having different background theories. Moreover, it provides conditions for optimizing background theories through appropriate program transformations. In this paper, we consider three different classes of background theories: clausal theories, Horn logic programs, and nonmonotonic extended logic programs. We show that logical equivalence is the necessary and sufficient condition for inductive equivalence in clausal theories and Horn logic programs. In nonmonotonic extended logic programs, on the other hand, strong equivalence is necessary and sufficient for inductive equivalence in general. Interestingly, however, we observe that several existing induction algorithms require weaker conditions of equivalence under restricted problem settings. We also discuss connection to equivalence in abductive logic and conclude that the notion of strong equivalence is useful to characterize equivalence of non-deductive reasoning.", "authors": ["Chiaki Sakama", "Katsumi Inoue"], "n_citation": 0, "title": "Inductive equivalence of logic programs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6b1e4451-7b4a-4c3a-beb0-a981a8b5eb48"}
{"abstract": "In recent years, many have suggested to apply encryption in the domain of software protection against malicious hosts. However, little information seems to be available on the implementation aspects or cost of the different schemes. This paper tries to fill the gap by presenting our experience with several encryption techniques: bulk encryption, an ondemand decryption scheme, and a combination of both techniques. Our scheme offers maximal protection against both static and dynamic code analysis and tampering. We validate our techniques by applying them on several benchmark programs of the CPU2006 Test Suite. And finally, we propose a heuristic which trades off security versus performance, resulting in a decrease of the runtime overhead.", "authors": ["Jan Cappaert", "Bart Preneel", "Bertrand Anckaert", "Matias Madou", "Koen De Bosschere"], "n_citation": 50, "title": "Towards Tamper Resistant Code Encryption: Practice and Experience", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "72f0872c-618e-469c-ac6e-47442733e974"}
{"abstract": "Many distance-based methods in machine learning are able to identify similar cases or prototypes from which decisions can be made. The explanation given is usually based on expressions such as because case a is similar to case b. However, a more general or meaningful pattern, such as because case a has properties x and y (as b has) is usually more difficult to find. Even in this case, the connection of this pattern with the original distance-based method is generally unclear, or even inconsistent. In this paper, we study the connection between the concept of distance (or similarity) and the concept of generalisation. More precisely, we define several conditions which, in our view, a sensible distance-based generalisation must have. From that, we are able to tell whether a generalisation operator for a pattern representation language is consistent with the metric space defined by the underlying distance. We show that there are pattern languages and generalisation operators which comply with these properties for typical data types: nominal, numerical, sets and lists. We also show the relationship between the well-known concepts of lgg and distances between terms, and the definition of generalisation presented in this paper.", "authors": ["Vicent Estruch", "C\u00e9sar Ferri", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Mar\u00eda Jos\u00e9 Ram\u00edrez-Quintana"], "n_citation": 50, "title": "Distance based generalisation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7d25472d-dff5-4e47-84f5-74010983633c"}
{"abstract": "We describe a framework for generating agent programs that model expert task performance in complex dynamic domains, using expert behavior observations and goal annotations as the primary source. We map the problem of learning an agent program on to multiple learning problems that can be represented in a supervised concept learning setting. The acquired procedural knowledge is partitioned into a hierarchy of goals and it is represented with first order rules. Using an inductive logic programming (ILP) learning component allows us to use structured goal annotations, structured background knowledge and structured behavior observations. We have developed an efficient mechanism for storing and retrieving structured behavior data. We have tested our system using artificially created examples and behavior observation traces generated by AI agents. We evaluate the learned rules by comparing them to hand-coded rules.", "authors": ["Tolga K\u00f6nik", "John E. Laird"], "n_citation": 50, "title": "Learning goal hierarchies from structured observations and expert annotations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "92a35aac-b2b4-4cdb-97aa-74a64a9e0a3b"}
{"abstract": "Hierarchical reinforcement learning has been proposed as a solution to the problem of scaling up reinforcement learning. The RL-TOPs Hierarchical Reinforcement Learning System is an implementation of this proposal which structures an agent's sensors and actions into various levels of representation and control. Disparity between levels of representation means actions can be misused by the planning algorithm in the system. This paper reports on how ILP was used to bridge these representation gaps and shows empirically how this improved the system's performance. Also discussed are some of the problems encountered when using an ILP system in what is inherently a noisy and incremental domain.", "authors": ["Mark D. Reid", "Malcolm Ryan"], "n_citation": 0, "title": "Using ILP to improve planning in Hierarchical Reinforcement Learning", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "9584a72e-4fea-4c20-9f03-5ec7ab92317e"}
{"authors": ["M.D. den Burger", "Ceriel J. H. Jacobs", "Thilo Kielmann", "Andre Merzky", "Ole Weidner", "Hartmut Kaiser", "Vu1012417", "Faculteit der Exacte Wetenschappen"], "n_citation": 0, "title": "What Is the Price of Simplicity? A Cross-platform Evaluation of the SAGA API", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "9964e8fb-8f15-4047-bbe2-dd147945108c"}
{"authors": ["Konstantinos Pliakos", "Celine Vens"], "n_citation": 0, "title": "Feature Induction and Network Mining with Clustering Tree Ensembles", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "9dbb01d6-2606-4e2d-9015-2232a9ebc50f"}
{"abstract": "Since learning with Inductive Logic Programming (ILP) can be regarded as the search problem through the hypotheses space, it is essential to reduce the search space in order to improve the efficiency. In the propositional learning framework, an efficient admissible search algorithm called OPUS (Optimized Pruning for Unordered Search) has been developed. OPUS employed the effective pruning techniques for unordered search and succeeded in improving the efficiency. In this paper, we propose an application of OPUS to an ILP system Progol. However, because of the difference of representation language, it is not applicable to ILP directly. We make the conditions clear under which the pruning techniques in OPUS can be applied in the framework of Progol. In addition, we propose a new pruning criterion, which can be regarded as inclusive pruning. Experiments are conducted to assess the effectiveness of the proposed algorithms. The results show that the proposed algorithms reduce the number of candidate hypotheses to be evaluated as well as the computational time for a certain class of problems.", "authors": ["Tomonobu Ozaki", "Koichi Furukawa"], "n_citation": 50, "title": "Application of Pruning techniques for propositional learning to progol", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "a77db824-2ebc-4bc6-965a-4c9520e1bf6f"}
{"abstract": "The identification of evolutionary related (homologous) proteins is a key problem in molecular biology. Here we present a inductive logic programming based method, Homology Induction (HI), which acts as a filter for existing sequence similarity searches to improve their performance in the detection of remote protein homologies. HI performs a PSI-BLAST search to generate positive, negative, and uncertain examples, and collects descriptions of these examples. It then learns rules to discriminate the positive and negative examples. The rules are used to filter the uncertain examples in the twilight zone. HI uses a multitable database of 51,430,710 pre-fabricated facts from a variety of biological sources, and the inductive logic programming system Aleph to induce rules. Hi was tested on an independent set of protein sequences with equal or less than 40 per cent sequence similarity (PDB40D). ROC analysis is performed showing that HI can significantly improve existing similarity searches. The method is automated and can be used via a web/mail interface.", "authors": ["Andreas Karwath", "Ross D. King"], "n_citation": 50, "title": "An automated ILP server in the field of bioinformatics", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "b03bedc6-027f-43e9-905e-77a92dfec80f"}
{"abstract": "Considering the difficulties inherent in the manual construction of natural language parsers, we have designed and implemented our system GRIND which is capable of learning a sequence of context--dependent parsing actions from an arbitrary corpus containing labelled parse trees. To achieve this, GRIND combines two established methods of machine learning: transformation-based learning (TBL) and inductive logic programming (ILP). Being trained and tested on corpus SUSANNE, GRIND reaches the accuracy of 96 % and the recall of 68 %.", "authors": ["Miloslav Nepil"], "n_citation": 0, "title": "Learning to parse from a treebank: Combining TBL and ILP", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "ba498c8a-8710-4dd1-a208-e1b1dd9c02f9"}
{"authors": ["Yusuke Nakano", "Nobuhiro Inuzuka", "\u30ce\u30d6\u30d2\u30ed \u30a4\u30cc\u30c5\u30ab", "\u4fe1\u535a \u72ac\u585a"], "n_citation": 0, "title": "Multi-Relational Pattern Mining Based-on Combination of Properties with Preserving Their Structure in Examples", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "d7cf1b03-7d4a-4531-bd8b-456e4691b707"}
{"abstract": "We propose to use ILP techniques to learn sets of temporally constrained events called chronicles that a monitoring tool will use to detect pathological situations. ICL, a system providing a declarative bias language, was used for the experiments on learning cardiac arrhythmias. We show how to obtain properties, such as compactness, robustness or readability, by varying the learning bias.", "authors": ["Ren\u00e9 Quiniou", "Marie-Odile Cordier", "Guy Carrault", "Feng Wang"], "n_citation": 0, "title": "Application of ILP to cardiac arrhythmia characterization for chronicle recognition", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "fd8dc656-b8df-4e12-8ea9-84100436b31c"}
{"abstract": "In 2001 Microsoft shipped the first public version of its Common Language Runtime (CLR) and the associated object-oriented.NET Framework. This Framework was designed for use by multiple languages through adherence to a Common Language Specification (CLS). The CLR, the CLS, and the basic level of the.NET Framework are all part of International Standard ISO/IEC 23271. Over 20 programming languages have been implemented on top of the CLR, all providing access to the same.Net Framework, and over 20,000,000 copies have been downloaded since its initial release. As a commercial software vendor, Microsoft is deeply concerned with evolving this system. Innovation is required to address new needs, new ideas, and new applications. But backwards compatibility is equally important to give existing customers the confidence that they can build on a stable base even as it evolves over time. This is a hard problem in general, it is made harder by the common use of virtual methods and public state, and harder still by a desire to make the programming model simple. This talk will describe the architectural ramifications of combining ease-of-use with system evolution and modularity. These ramifications extend widely throughout the system infrastructure, ranging from the underlying binding mechanism of the virtual machine, through program language syntax extensions, and into the programming environment.", "authors": ["Jim Miller"], "n_citation": 0, "title": "Evolving a Multi-language Object-Oriented Framework: Lessons from .NET", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "0cb4fd60-149c-4a1b-b25e-7ebde6701344"}
{"abstract": "Successful application of Machine Learning to certain real-world situations sometimes requires to take into account relations among objects. Inductive Logic Programming, being based on First-Order Logic as a representation language, provides a suitable learning framework to be adopted in these cases. However, the intrinsic complexity of this framework, added to the complexity of the specific application context, often requires pure induction to be supported by various kinds of meta-information on the domain itself and/or on its representation in order to prune the search space of all possible definitions. Indeed, avoiding the exploration of paths that do not lead to any correct solution can greatly reduce computational times, and hence becomes a critical issue for the performance of the whole learning process. In the current practice, providing such information is often in charge of the human expert. It is also a difficult and error-prone activity, in which mistakes are highly probable because of a number of factors. This makes it desirable to develop procedures that can automatically generate such information starting from the same observations that are input to the learning process. This paper focuses on a specific kind of meta-information: the types used in the description language and their related domains. Indeed, many learning systems known in the literature are able to exploit (and sometimes require) such a kind of knowledge to improve their performance. An algorithm is proposed to automatically identify types from observations, and detailed examples of its behaviour are given. An evaluation of its performance in domains with different characteristics is reported, and its robustness with respect to incomplete observations is studied.", "authors": ["Stefano Ferilli", "Floriana Esposito", "Teresa Maria Altomare Basile", "Nicola Di Mauro"], "n_citation": 0, "title": "Automatic induction of First-Order Logic descriptors type domains from observations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "106f65a9-7fe4-42bf-92c4-77e973f11c04"}
{"abstract": "We propose a new approach to Inductive Logic Programming that systematically exploits caching and offers a number of advantages over current systems. It avoids redundant computation, is more amenable to the use of set-oriented generation and evaluation of hypotheses, and allows relational DBMS technology to be more easily applied to ILP systems. Further, our approach opens up new avenues such as probabilistically scoring rules during search and the generation of probabilistic rules. As a first example of the benefits of our ILP framework, we propose a scheme for defining the hypothesis search space through Inverse Entailment using multiple example seeds.", "authors": ["H\u00e9ctor Corrada Bravo", "David C. Page", "Raghu Ramakrishnan", "Jude W. Shavlik", "V\u00edtor Santos Costa"], "n_citation": 0, "title": "A framework for set-oriented computation in inductive logic programming and its application in generalizing inverse entailment", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1a4b9b3c-6e22-4d6c-8dd5-12d5919ac480"}
{"abstract": "This paper is concerned with how to classify examples that are not covered by any rule in an unordered hypothesis. Instead of assigning the majority class to the uncovered examples, which is the standard method, a novel method is presented that minimally generalises the rules to include the uncovered examples. The new method, called Rule Stretching, has been evaluated on several domains (using the inductive logic programming system Virtual Predict for induction of the base hypothesis). The results show a significant improvement over the standard method.", "authors": ["Martin Eineborg", "Henrik Bostr\u00f6m"], "n_citation": 50, "title": "Classifying uncovered examples by rule stretching", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "34e02faa-ad6a-4c5c-b3c7-dbc4d8187170"}
{"abstract": "Stringent power and performance constraints, coupled with detailed knowledge of the target applications of a processor, allows for application-specific processor optimizations. It has been shown that application-specific reconfigurable hash functions eliminate a large number of cache conflict misses. These hash functions minimize conflicts by modifying the mapping of cache blocks to cache sets.#R##N#This paper describes an algorithm to compute optimal XOR-functions, a particular type of hash functions based on XORs. Using this algorithm, we set an upper bound on the conflict reduction achievable with XOR-functions. We show that XOR-functions perform better than other reconfigurable hash functions studied in the literature such as bit-selecting functions.#R##N#The XOR-functions are optimal for one particular execution of a program. However, we show that optimal XOR-functions are less sensitive, to the characteristics of the execution than optimal bit-selecting hash functions. This again underlines that XOR-functions are the best known hash functions to implement reconfigurable hash functions.", "authors": ["Hans Vandierendonck", "Koen De Bosschere"], "n_citation": 50, "title": "Constructing optimal XOR-functions to minimize cache conflict misses", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "37825b0e-4b91-4056-9f08-885a19272666"}
{"abstract": "The problem of overfitting (focusing closely on examples at the loss of generalization power) is encountered in all supervised machine learning schemes. This study is dedicated to explore some aspects of overfitting in the particular case of genetic programming. After recalling the causes usually invoked to explain overfitting such as hypothesis complexity or noisy learning examples, we test and compare the resistance to overfitting on three variants of genetic programming algorithms (basic GP, sizefair crossover GP and GP with boosting) on two bench-marks, a symbolic regression and a classification problem. We propose guidelines based on these results to help reduce overfitting with genetic programming.", "authors": ["Gr\u00e9gory Paris", "Denis Robilliard", "Cyril Fonlupt"], "n_citation": 0, "title": "Exploring overfitting in genetic programming", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "523a2486-67aa-4ad8-9c8a-0e01b4bd0440"}
{"abstract": "In this paper we survey work being conducted at Imperial College on the use of machine teaming to build Systems Biology models of the effects of toxins on biochemical pathways. Several distinct, and complementary modelling techniques are being explored. Firstly, work is being conducted on applying Support-Vector ILP (SVILP) as an accurate means of screening high-toxicity molecules. Secondly, Bayes' networks have been machine-learned to provide causal maps of the effects of toxins on the network of metabolic reactions within cells. The data were derived from a study on the effects of hydrazine toxicity in rats. Although the resultant network can be partly explained in terms of existing KEGG (Kyoto Encyclopedia of Genes and Genomes) pathway descriptions, several of the strong dependencies in the Bayes' network involve metabolite pairs with high separation in KEGG. Thirdly, in a complementary study KEGG pathways are being used as background knowledge for explaining the same data using a model constructed using Abductive ILP, a logic-based machine learning technique. With a binary prediction model (up/down regulation) cross validation results show that even with a restricted number of observed metabolites high predictive accuracy (80-90%) is achieved on unseen metabolite concentrations. Further increases in accuracy are achieved by allowing discovery of general rules from additional literature data on hydrazine inhibition. Ongoing work is aimed at formulating probabilistic logic models which combine the learned Bayes' network and ILP models.", "authors": ["Stephen Muggleton"], "n_citation": 0, "title": "Machine learning for systems biology", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "604e0370-0448-40bb-bdbd-3600f701244e"}
{"abstract": "Recent work on representing action and change has introduced high-level action languages which describe the effects of actions as causal laws in a declarative way. In this paper, we propose an algorithm to induce the effects of actions from an incomplete domain description and observations after executing action sequences, all of which are represented in the action language A. Our induction algorithm generates effect propositions in A based on regular inference, i.e., an algorithm to learn finite automata. As opposed to previous work on learning automata from scratch, we are concerned with explanatory induction which accounts for observations from background knowledge together with induced hypotheses. Compared with previous approaches in ILP, an observation input to our induction algorithm is not restricted to a narrative but can be any fact observed after executing a sequence of actions. As a result, induction of causal laws can be formally characterized within action languages.", "authors": ["Katsumi Inoue", "Hideyuki Bando", "Hidetomo Nabeshima"], "n_citation": 50, "title": "Inducing causal laws by regular inference", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "790e9a20-25cd-41e3-af2d-1a66c90ca4b7"}
{"authors": ["Emma Hart", "Francisco C. Santos", "Hugues Bersini"], "n_citation": 50, "title": "Topological constraints in the evolution of idiotypic networks", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "7d63fd96-8baa-41be-b561-8f6643eddc3b"}
{"authors": ["Henrik Schoenau-Fog", "Luis Emilio Bruni", "Faysal Fuad Khalil", "Jawid Faizi"], "n_citation": 50, "title": "First Person Victim: Developing a 3D Interactive Dramatic Experience", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "8dc5a934-c364-4756-bf32-197a18b45d54"}
{"abstract": "In this chapter, we provide some first thoughts on, and preliminary answers to the question how intelligent software agents could take most advantage of the potential of quantum computation and communication, once practical quantum computers become available in foreseeable future. In particular, we discuss the question whether the adoption of quantum computational and communication means will affect the autonomy of individual and systems of agents. We show that the ability of quantum computing agents to perform certain computational tasks more efficient than classically computing agents is at the cost of limited self-autonomy, due to non-local effects of quantum entanglement.", "authors": ["Matthias Klusch"], "n_citation": 0, "title": "Toward quantum computational agents", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c9cceb26-31a7-48cc-a8ec-eb4b1c3fdc5a"}
{"abstract": "Online learning algorithms such as Winnow have received much attention in Machine Learning. Their performance degrades only logarithmically with the input dimension, making them useful in large spaces such as relational theories. However, online first-order learners are intrinsically limited by a computational barrier: even in the finite, function-free case, the number of possible features grows exponentially with the number of first-order atoms generated from the vocabulary. To circumvent this issue, we exploit the paradigm of closure-based learning which allows the learner to focus on the features that lie in the closure space generated from the examples which have lead to a mistake. Based on this idea, we develop an online algorithm for learning theories formed by disjunctions of existentially quantified conjunctions of atoms. In this setting, we show that the number of mistakes depends only logarithmically on the number of features. Furthermore, the computational cost is essentially bounded by the size of the closure lattice.", "authors": ["Fr\u00e9d\u00e9ric Koriche"], "n_citation": 0, "title": "Online closure-based learning of relational theories", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d4db6492-3114-437f-965c-1e4bcf711bbd"}
{"authors": ["Stefan Dziembowski", "Sebastian Faust"], "n_citation": 0, "title": "Leakage-Resilient Cryptography from the Inner-Product Extractor.", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "f2cd02fe-da7d-4ad0-89cc-6e419554a533"}
{"abstract": "First-order theory refinement using neural networks is still an open problem. Towards a solution to this problem, we use inductive logic programming techniques to introduce FOCA, a First-Order extension of the Cascade ARTMAP system. To present such a first-order extension of Cascade ARTMAP, we: a) modify the network structure to handle first-order objects; b) define first-order versions of the main functions that guide all Cascade ARTMAP dynamics, the choice and match functions; c) define a first-order version of the propositional learning algorithm to approximate Plotkin's least general generalization. Preliminary results indicate that our initial goal of learning logic programs using neural networks can be achieved.", "authors": ["Rodrigo Basilio", "Gerson Zaverucha", "Valmir Carneiro Barbosa"], "n_citation": 0, "title": "Learning logic programs with neural networks", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "feaac842-815e-4c3b-9252-2dd9e0722c0d"}
{"abstract": "We consider the problem of scheduling jobs on a single machine with preemption, when the server is allowed to reject jobs at some penalty. We consider minimizing two objectives: total flow time and total job-idle time (the idle time of a job is the flow time minus the processing time). We give 2-competitive online algorithms for the two objectives and extend some of our results to the case of weighted flow time and machines with varying speeds. We also give a resource augmentation result for the case of arbitrary penalties achieving a competitive ratio of O(1/\u2208 (log W+logC) 2 ) using a (1+\u2208) speed processor. Finally, we present a number of lower bounds for both the case of uniform and arbitrary penalties.", "authors": ["Nikhil Bansal", "Avrim Blum", "Shuchi Chawla", "Kedar Dhamdhere"], "n_citation": 0, "title": "Scheduling for flow-time with admission control", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "04b328a5-c339-498b-af43-38dc359c6590"}
{"abstract": "In the cake cutting problem, n > 2 players want to cut a cake into n pieces so that every player gets a 'fair' share of the cake by his own measure. We prove the following result: For every e > 0, there exists a cake division scheme for n players that uses at most cen cuts, and in which each player can enforce to get a share of at least (1-e)/n of the cake according to his own private measure.", "authors": ["Gerhard J. Woeginger"], "n_citation": 50, "title": "An approximation scheme for cake division with a linear number of cuts", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "05988d44-0049-4ce0-8663-a47536d6fc17"}
{"abstract": "An abstract NP-hard covering problem is presented and fixed-parameter tractable algorithms for this problem are described. The running times of the algorithms are expressed in terms of three parameters: n, the number of elements to be covered, k, the number of sets allowed in the covering, and d, the combinatorial dimension of the problem. The first algorithm is deterministic and has running time O'(k dk n). The second algorithm is also deterministic and has running time O'(k d(k+1)  + n d+1 ). The third is a Monte-Carlo algorithm that runs in time O'(k d(k+1) +c 2 dk [(d+1)/2][(d+1)/2] n log n) time and is correct with probability 1-n -c . Here, the O' notation hides factors that are polynomial in d. These algorithms lead to fixed-parameter tractable algorithms for many geometric and non-geometric covering problems.", "authors": ["Stefan Langerman", "Pat Morin"], "n_citation": 0, "title": "Covering things with things", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0ed6bbd4-dab0-4921-823f-7a08d15ee1b0"}
{"abstract": "In this paper we propose algorithms for solving a variety of geometric optimization problems on a stream of points in R 2  or R 3 . These problems include various extent measures (e.g. diameter, width, smallest enclosing disk), collision detection (penetration depth and distance between polytopes), and shape fitting (minimum width annulus, circle/line fitting). The main contribution of this paper is a unified approach to solving all of the above problems efficiently using modern graphics hardware. All the above problems can be approximated using a constant number of passes over the data stream. Our algorithms are easily implemented, and our empirical study demonstrates that the running times of our programs are comparable to the best implementations for the above problems. Another significant property of our results is that although the best known implementations for the above problems are quite different from each other, our algorithms all draw upon the same set of tools, making their implementation significantly easier.", "authors": ["Pankaj K. Agarwal", "Shankar Krishnan", "Nabil H. Mustafa", "Suresh Venkatasubramanian"], "n_citation": 0, "title": "Streaming geometric optimization using graphics hardware", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1c79b8a6-dcc9-46d3-9d5d-df6a839c2681"}
{"abstract": "In on-line dial-a-ride problems, servers are traveling in some metric space to serve requests for rides which are presented over time. Each ride is characterized by two points in the metric space, a source, the starting point of the ride, and a destination, the end point of the ride. Usually it is assumed that at the release of such a request complete information about the ride is known. We diverge from this by assuming that at the release of such a ride only information about the source is given. At visiting the source, the information about the destination will be made available to the servers. For many practical problems, our model is closer to reality. However, we feel that the lack of information is often a choice, rather than inherent to the problem: additional information can be obtained, but this requires investments in information systems. In this paper we give mathematical evidence that for the problem under study it pays to invest.", "authors": ["Maarten Lipmann", "Xiaosong Lu", "Willem de Paepe", "Ren\u00e9 Sitters", "Leen Stougie"], "n_citation": 0, "title": "On-line dial-a-ride problems under a restricted information model", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "453563d2-0817-4a33-a846-65215e062340"}
{"abstract": "We prove that in a certain cake cutting model, every fair cake division protocol for n players must use \u03a9(n log n) cuts in the worst case. Up to a small constant factor, our lower bound matches a corresponding upper bound in the same model by Even & Paz from 1984.", "authors": ["Jir\u00ed Sgall", "Gerhard J. Woeginger"], "n_citation": 0, "title": "A lower bound for cake cutting", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "488706b0-1c41-4297-b2d4-00f0e5ed577c"}
{"authors": ["Krzysztof Diks", "Piotr Sankowski"], "n_citation": 0, "title": "Dynamic Plane Transitive Closure", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "5708b612-fbc7-4203-b572-32a2473f4689"}
{"abstract": "Given the positions of n sites in a radio network we consider the problem of finding routes between any pair of sites that minimize energy consumption and do not use more than some constant number k of hops. Known exact algorithms for this problem required \u03a9(n log n) per query pair (p, q). In this paper we relax the exactness requirement and only compute approximate (1+\u2208) solutions which allows us to guarantee constant query time using linear space and O(n log n) preprocessing time. The dependence on e is polynomial in 1/\u2208. One tool we employ might be of independent interest: For any pair of points (p,q) \u2208 P \u2282 Z 2  we can report in constant time the cluster pair (A, B) representing (p, q) in a well-separated pair decomposition of P.", "authors": ["Stefan Funke", "Domagoj Matijevic", "Peter Sanders"], "n_citation": 0, "title": "Approximating energy efficient paths in wireless multi-hop networks", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "5f68c065-8fee-40cc-ac58-4485a6656b59"}
{"abstract": "Used for topology control in ad-hoc wireless networks, Power Assignment is a family of problems, each defined by a certain connectivity constraint (such as strong connectivity) The input consists of a directed complete weighted graph G = (V, c). The power of a vertex u in a directed spanning subgraph H is given by p H (u) = max uv \u2208 E(H)  c(uv). The power of H is given by p(H) = \u03a3 u \u2208 V  p H (u), Power Assignment seeks to minimize p(H) while H satisfies the given connectivity constraint. We present asymptotically optimal O(logn)-approximation algorithms for three Power Assignment problems: Min-Power Strong Connectivity, Min-Power Symmetric Connectivity (the undirected graph having an edge uv iff H has both uv and vu must be connected) and Min-Power Broadcast (the input also has r \u2208 V, and H must be a r-rooted outgoing spanning arborescence). For Min-Power Symmetric Connectivity in the Euclidean with efficiency case (when c(u,v) = \u2225u,v\u2225 k /e(u), where \u2225u,v\u2225 is the Euclidean distance, k is a constant between 2 and 5, and e(u) is the transmission efficiency of node u), we present a simple constant-factor approximation algorithm. For all three problems we give exact dynamic programming algorithms in the Euclidean with efficiency case when the nodes lie on a line. In Network Lifetime, each node u has an initial battery supply b(u), and the objective is to assign each directed subgraph H satisfying the connectivity constraint a real variable \u03b1(H) > 0 with the objective of maximizing \u03a3 H \u03b1(H) subject to \u03a3 H p T (u)\u03b1(H) \u2264 b(u) for each node u E V. We are the first to study Network Lifetime and give approximation algorithms based on the PTAS for packing linear programs of Garg and Konemann. The approximation ratio for each case of Network Lifetime is equal to the approximation ratio of the corresponding Power Assignment problem with non-uniform transmission efficiency.", "authors": ["Gruia Calinescu", "Sanjiv Kapoor", "Alexander Olshevsky", "Alexander Zelikovsky"], "n_citation": 0, "title": "Network Lifetime and Power Assignment in ad hoc wireless networks", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "75e27a1b-b318-4d4d-8387-00108e044a2d"}
{"abstract": "The stable marriage problem has recently been studied in its general setting, where both ties and incomplete lists are allowed. It is NP-hard to find a stable matching of maximum size, while any stable matching is a maximal matching and thus trivially a factor two approximation. In this paper, we give the first nontrivial result for approximation of factor less than two. Our algorithm achieves an approximation ratio of 2/(1+L -2 ) for instances in which only men have ties of length at most L. When both men and women are allowed to have ties, we show a ratio of 13/7(  1.1052).", "authors": ["Magn\u00fas M. Halld\u00f3rsson", "Kazuo Iwama", "Shuichi Miyazaki", "Hiroki Yanagisawa"], "n_citation": 0, "title": "Improved approximation of the stable marriage problem", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "8980877f-b333-4aeb-8a7a-160c56525289"}
{"abstract": "Emergency Preparedness is one of the most appealing classes of applications for context-aware wireless sensor networks (WSN). In such environments, contexts can be captured and interpreted in the WSN application layer to help preventing, fighting, rescuing and checking against fire, explosions, leaking of toxic gases etc. In this paper, we show a Wireless Actor and Sensor Network (WASN) that can be used to interpret simple and complex contexts. We present an efficient technique that make use of actors to aggregate sensor events, eliminate ambiguities and redundancy and realize context interpretations. Each actor of the WASN is configured with rules that are determined by the application in the network configuration phase. Context information is exchanged among actors and sink(s) of the WASN through the publish/subscribe paradigm based on specific topics. We present a set of simulation of experiments to evaluate the performance of our protocols for wireless sensor networks.", "authors": ["Azzedine Boukerche", "Regina Borges de Araujo", "Femando H. S. Silva"], "n_citation": 50, "title": "A Context Interpretation Based Wireless Sensor Network for the Emergency Preparedness Class of Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a7c252ed-5897-4e30-b26e-638b37a6224b"}
{"authors": ["Vinton G. Cerf"], "n_citation": 0, "title": "A genetic theory of the Silicon Valley phenomenon", "venue": "Communications of The ACM", "year": 2017, "id": "ab55c749-5f0e-4548-b65d-d030ed7c0a11"}
{"abstract": "Microsecond-scale I/O means tension between performance and productivity that will need new latency-mitigating ideas, including in hardware.", "authors": ["Luiz Andr\u00e9 Barroso", "Mike Marty", "David Patterson", "Parthasarathy Ranganathan"], "n_citation": 0, "references": ["205029f5-6b88-4861-bdf9-f07f2c77792d", "240293ef-c638-42fc-84f0-adfcb0154303", "8670529b-23af-4175-a855-f536b4caa939", "8e41ddcb-5f5e-4c95-8f8f-07865f8e36ef", "d7dfd275-9d80-43a5-9a55-6fa9ea7d0926", "e3cdee6a-b72e-4d63-85c5-6754711be9a8"], "title": "Attack of the killer microseconds", "venue": "Communications of The ACM", "year": 2017, "id": "cf9cef48-5838-4d28-a5da-81b4bb722ec4"}
{"abstract": "We consider a router on the Internet analyzing the statistical properties of a TCP/IP packet stream. A fundamental difficulty with measuring traffic behavior on the Internet is that there is simply too much data to be recorded for later analysis, on the order of gigabytes a second. As a result, network routers can collect only relatively few statistics about the data. The central problem addressed here is to use the limited memory of routers to determine essential features of the network traffic stream. A particularly difficult and representative subproblem is to determine the top k categories to which the most packets belong, for a desired value of k and for a given notion of categorization such as the destination IP address. We present an algorithm that deterministically finds (in particular) all categories having a frequency above 1/(m+1) using m counters, which we prove is best possible in the worst case. We also present a sampling-based algorithm for the case that packet categories follow an arbitrary distribution, but their order over time is permuted uniformly at random. Under this model, our algorithm identifies flows above a frequency threshold of roughly 1/\u221anm with high probability, where m is the number of counters and n is the number of packets observed. This guarantee is not far off from the ideal of identifying all flows (probability 1/n), and we prove that it is best possible up to a logarithmic factor. We show that the algorithm ranks the identified flows according to frequency within any desired constant factor of accuracy.", "authors": ["Erik D. Demaine", "Alejandro L\u00f3pez-Ortiz", "J. Ian Munro"], "n_citation": 504, "title": "Frequency estimation of Internet packet streams with limited space", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d2f2ae86-8579-4244-a9b5-7f59a34b5683"}
{"abstract": "Determining whether an ellipsoid contains the intersection of many concentric ellipsoids is an NP-hard problem. In this paper, we study various convex relaxations of this problem, namely two semidefinite relaxations and a second-order cone relaxation. We establish some links between these relaxations and perform extensive numerical testings to verify their exactness, their computational load and their stability. As an application of this problem, we study an issue emerging from an aircraft wing design problem: how can we simplify the description of a feasible loads region?", "authors": ["David Adjiashvili"], "n_citation": 0, "title": "Removing Redundant Quadratic Constraints", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "f01a763d-be16-4ca4-966e-f9f65fc2091e"}
{"abstract": "Intelligent, interactive and pervasively accessible tools for providing information about elements of a language are crucial in learning a language, especially in an advanced secondary language learning setting and learning for linguistic exploration. The paper describes a prototype implementation of a tool that provides intelligent, active and interactive tools for helping linguistics students inquire and learn about lexical and syntactic properties of words and phrases in Turkish text. The tool called LINGBROWSER uses extensive finite state language processing technology to provide instantaneous information about morphological, segmental, pronunciation properties about the words in any real text. Additional resources also provide access to semantic properties of (root) words.", "authors": ["Kemal Oflazer", "Mehmet Dincer Erbas", "M\u00fcge Erdogmus"], "n_citation": 0, "title": "Using finite state technology in a tool for linguistic exploration", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f1f5edc5-4606-4d28-9952-e1ebb35ad357"}
{"abstract": "Search games are attractive for their correspondence with classical width parameters. For instance, the invisible search number (a.k.a. node search number) of a graph is equal to its pathwidth plus 1, and the visible search number of a graph is equal to its treewidth plus 1. The connected variants of these games ask for search strategies that are connected, i.e., at every step of the strategy, the searched part of the graph induces a connected subgraph. We focus on monotone search strategies, i.e., strategies for which every node is searched exactly once. It is known that the monotone connected visible search number of an n-node graph is at most O(log n) times its visible search number. First, we prove that this logarithmic bound is tight. Precisely, we prove that there is an infinite family of graphs for which the ratio monotone connected visible search number over visible search number is \u03a9(log n). Second, we prove that, as opposed to the non-connected variant of visible graph searching, recontamination helps for connected visible search. Precisely, we describe an infinite family of graphs for which any monotone connected visible search strategy for any graph in this family requires strictly more searchers than the connected visible search number of the graph.", "authors": ["Pierre Fraigniaud", "Nicolas Nisse"], "n_citation": 0, "title": "Monotony properties of connected visible graph searching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "28b4146f-a05b-4a5e-aa96-17d8a35af65f"}
{"abstract": "Given a flight schedule, which consists of a set of flights with specified departure and arrival times, a set of aircraft types and a set of restrictions, the airline fleet assignment problem (FAP) is to determine which aircraft type should fly each flight. As the FAP is only one step in a sequence of several optimization problems, important restrictions of later steps should also be considered in the FAP. This paper shows how one type of these restrictions, connection dependent ground times, can be added to the fleet assignment problem and presents three optimization methods that can solve real-world problem instances with more than 6000 legs within minutes.", "authors": ["Sven Grothklags"], "n_citation": 0, "title": "Fleet assignment with connection dependent ground times", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "3cf6639f-824d-4159-9b0e-87c37ce48c37"}
{"abstract": "We introduce a general model for time-expanded versions of packing problems with a variety of applications. Our notion for time-expanded packings, which we introduce in two natural variations, requires elements to be part of the solution for several consecutive time steps. Despite the fact that the time-expanded counterparts of most combinatorial optimization problems become computationally hard to solve, we present strong approximation algorithms for general dependence systems and matroids, respectively, depending on the considered variant. More precisely, for both notions of time-expanded packings that we introduce, the approximation guarantees we obtain are at most a small constant-factor worse than the best approximation algorithms for the underlying problem in its non-time-expanded version.", "authors": ["David Adjiashvili"], "n_citation": 0, "title": "Time-Expanded Packings", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "451c7918-2f3f-4ff1-906b-702d48165a61"}
{"authors": ["Sinh Hoa Nguyen", "Hung Son Nguyen"], "n_citation": 0, "title": "Hierarchical Rough Classifiers", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "4e577cb8-0d94-4b9e-b903-13cbdc63af3b"}
{"abstract": "The present paper deals with communication in random geometric topologies; in particular, we model modern wireless ad hoc networks by random geometric topologies. The paper has two goals: the first is to implement the network power control mechanism extended by several wireless engineering features and to use the model to assess communicational properties of two typical random geometric topologies known from the literature in real-life conditions. The second goal is to suggest a modification of the LowDegree algorithm (one of the two studied topologies), which preserves excellent power requirements of the model, but matches the performance of the standard UnitDisk algorithm in terms of higher total network throughput.", "authors": ["Ludek Kucera", "Stepan Kucera"], "n_citation": 0, "title": "Wireless Communication in Random Geometric Topologies", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7802f1c9-338e-438c-bde6-b608e58a19f2"}
{"abstract": "In this paper we present a new approach to uniform sampling and approximate counting. The presented method is called multisampling and is a generalization of the importance sampling technique. It has the same advantage as importance sampling, it is unbiased, but in contrary to it's prototype it is also an almost uniform sampler. The approach seams to be as universal as Markov Chain Monte Carlo approach, but simpler. Here we report very promising test results of using multisampling to the following problems: counting matchings in graphs, counting colorings of graphs, counting independent sets in graphs, counting solutions to knapsack problem, counting elements in graph matroids and computing the partition function of the Ising model.", "authors": ["Piotr Sankowski"], "n_citation": 0, "title": "Multisampling: A new approach to uniform sampling and approximate counting", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7b1c08e6-7315-4fa2-9002-8e16ef9495e4"}
{"abstract": "This paper describes compact storage models for gazetteers using state-of-the-art finite-state technology. In particular, we compare the standard method based on numbered indexing automata associated with an auxiliary storage device, against a pure finite-state representation, the latter being superior in terms of space and time complexity, when applied to real-world test data. Further, we pinpoint some pros and cons for both approaches and provide results of empirical experiments, which form handy guidelines for selecting a suitable data structure for implementing a gazetteer.", "authors": ["Jakub Piskorski"], "n_citation": 0, "title": "On compact storage models for gazetteers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "886a4282-ab2b-4f15-9b91-cb0ab099bf71"}
{"abstract": "We introduce an innovative decomposition technique which reduces a multi-dimensional searching problem to a sequence of one-dimensional problems, each one easily manageable in optimal timer space complexity using traditional searching strategies. The reduction has no additional storage requirement and the time complexity to reconstruct the result of the original multi-dimensional query is linear in the dimension. More precisely, we show how to preprocess a set of S subset of or equal to INd of multi-dimensional objects into a data structure requiring O(mlogn) space, where m = \\S\\ and n is the maximum number of different values for each coordinate. The obtained data structure is implicit, i.e. does not use pointers, and is able to answer the exact match query in 7(d - 1) steps. Additionally the model of computation required for querying the data structure is very simple; the only arithmetic operation needed is the addition and no shift operation is used. The technique introduced, overcoming the multi-dimensional bottleneck, can be also applied to non traditional models of computation as external memory, distributed, and hierarchical environments. Additionally, we will show how the proposed technique permits the effective realizability of the well known perfect hashing techniques on real data. The algorithms for building the data structure are easy to implement and run in polynomial time.", "authors": ["Enrico Nardelli", "Maurizio Talamo", "Paola Vocca"], "n_citation": 50, "title": "Efficient searching for multi-dimensional data made simple", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "96b7a27c-a20a-49ab-b927-ac0f0e2f9442"}
{"abstract": "We consider the problem of laying out a tree or trie in a hierarchical memory, where the tree/trie has a fixed parent/child structure. The goal is to minimize the expected number of block transfers performed during a search operation, subject to a given probability distribution on the leaves. This problem was previously considered by Gil and Itai, who show optimal but high-complexity algorithms when the block-transfer size is known. We propose a simple greedy algorithm that is within an additive constant strictly less than 1 of optimal. We also present a relaxed greedy algorithm that permits more flexibility in the layout while decreasing performance (increasing the expected number of block transfers) by only a constant factor. Finally, we extend this latter algorithm to the cache-oblivious setting in which the block-transfer size is unknown to the algorithm; in particular this extension solves the problem for a multilevel memory hierarchy. The query performance of the cache-oblivious layout is within a constant factor of the query performance of the optimal layout with known block size.", "authors": ["Michael A. Bender", "Erik D. Demaine", "Martin Farach-Colton"], "n_citation": 0, "title": "Efficient tree layout in a multilevel memory hierarchy", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a18e2fc4-4dae-41af-b0b2-bd43a4722801"}
{"abstract": "Text search engines return a set of k documents ranked by similarity to a query. Typically, documents and queries are drawn from natural language text, which can readily be partitioned into words, allowing optimizations of data structures and algorithms for ranking. However, in many new search domains (DNA, multimedia, OCR texts, Far East languages) there is often no obvious definition of words and traditional indexing approaches are not so easily adapted, or break down entirely. We present two new algorithms for ranking documents against a query without making any assumptions on the structure of the underlying text. We build on existing theoretical techniques, which we have implemented and compared empirically with new approaches introduced in this paper. Our best approach is significantly faster than existing methods in RAM, and is even three times faster than a state-of-the-art inverted file implementation for English text when word queries are issued.", "authors": ["J Culpepper", "Gonzalo Navarro", "Simon J. Puglisi", "Andrew Turpin"], "n_citation": 0, "title": "Top-K ranked document search in general text databases", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "acfff823-0600-4e98-9079-0825be03b9d9"}
{"abstract": "A radio labeling of a graph G is an assignment of pairwise distinct, positive integer labels to the vertices of G such that labels of adjacent vertices differ by at least 2. The radio labeling problem (RL) consists in determining a radio labeling that minimizes the maximum label that is used (the so-called span of the labeling). RL is a well-studied problem, mainly motivated by frequency assignment problems in which transmitters are not allowed to operate on the same frequency channel. We consider the special case where some of the transmitters have pre-assigned operating frequency channels. This leads to the natural variants P-RL(l) and P-RL(*) of RL with I pre-assigned labels and an arbitrary number of pre-assigned labels, respectively. We establish a number of combinatorial, algorithmical, and complexity-theoretical results for these variants of radio labeling. In particular, we investigate a simple upper bound on the minimum span, yielding a linear time approximation algorithm with a constant additive error bound for P-RL(*) restricted to graphs with girth > 5. We consider the complexity of P-RL(l) and P-RL(*) for several cases in which RL is known to be polynomially solvable. On the negative side, we prove that P-RL(*) is NP-hard for cographs and for k-colorable graphs where a k-coloring is given (k > 3). On the positive side, we derive polynomial time algorithms solving P-RL(*) and P-RL(l) for graphs with bounded maximum degree, and for solving P-RL(l) for k-colorable graphs where a k-coloring is given.", "authors": ["Hans L. Bodlaender", "Hajo Broersma", "Fedor V. Fomin", "Artem V. Pyatkin", "Gerhard J. Woeginger"], "n_citation": 50, "title": "Radio labeling with pre-assigned frequencies", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "bc00c296-2255-46be-947c-fec47b8adbc8"}
{"abstract": "We address a fundamental string problem arising from analysis of biomolecular sequences. The input consists of two integers L and U and a sequence S of n number pairs (a i , w i ) with w i  > 0. Let segment S(i, j) of S be the consecutive subsequence of S starting index i to index j. The density of S(i, j) is d(i, j) = (a i  + a i+1  +... + a j )/(w i  + w i+1  + ...+w j ). The maximum-density segment problem is to find a maximum-density segment over all segments of S with L \u2264 w i +w i+1 +...+w j  < U. The best previously known algorithm for the problem, due to Goldwasser, Kao, and Lu, runs in O(nlog(U - L + 1)) time. In the present paper, we solve the problem in O(n) time. Our approach bypasses the complicated right-skew decomposition, introduced by Lin, Jiang, and Chao. As a result, our algorithm has the capability to process the input sequence in an online manner, which is an important feature for dealing with genome-scale sequences. Moreover, for an input sequence S representable in O(k) space, we also show how to exploit the sparsity of S and solve the maximum-density segment problem for S in O(k) time.", "authors": ["Kai-Min Chung", "Hsueh-I Lu"], "n_citation": 0, "title": "An optimal algorithm for the maximum-density segment problem", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c028d414-6b0e-4aad-ad30-f71c563a1c90"}
{"authors": ["Tetsuo Asano", "Wolfgang Mulzer", "Yajun Wang"], "n_citation": 50, "title": "Constant-Work-Space Algorithm for a Shortest Path in a Simple Polygon", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "c4bbde3a-a949-4c27-ad50-557207e80a12"}
{"authors": ["Dennis E. Shasha"], "n_citation": 0, "title": "Stacking the deck", "venue": "Communications of The ACM", "year": 2017, "id": "d0e9a4fd-d314-4454-82de-d67d55c99d5b"}
{"abstract": "An exact algorithm to solve the Steiner tree problem for uniform orientation metrics in the plane is presented. The algorithm is based on the two-phase model, consisting of full Steiner tree (FST) generation and concatenation, which has proven to be very successful for the rectilinear and Euclidean Steiner tree problems. By applying a powerful canonical form for the FSTs, the set of optimal solutions is reduced considerably. Computational results both for randomly generated problem instances and VLSI design instances are provided. The new algorithm solves most problem instances with 100 terminals in seconds, and problem instances with up to 10000 terminals have been solved to optimality.", "authors": ["Benny K. Nielsen", "Pawel Winter", "Martin Zachariasen"], "n_citation": 0, "title": "An exact algorithm for the uniformly-oriented Steiner tree problem", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f407a48e-7caf-4e8d-a13a-e21b8958927f"}
{"abstract": "We discuss problems inherent in domain specific tagging (biomedical domain) and their relevance to tagging issues in general. We present a novel approach to this problem which we call tagging with delayed disambiguation (TDD). This approach uses a modified, statistically-driven lexicon together with a small set of morphological, heuristic, and chunking rules which are implemented using finite state machinery. They make use of both delayed disambiguation and the concept of tag underspecification as an ordered sequence of tags.", "authors": ["Jos\u00e9 M. Casta\u00f1o", "James Pustejovsky"], "n_citation": 0, "title": "Tagging with delayed disambiguation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f4bcc0bf-9669-4aa3-87aa-efa08c97370e"}
{"abstract": "We present a simple new (randomized) algorithm for computing minimum spanning trees that is more than two times faster than the best previously known algorithms (for dense, difficult inputs). It is of conceptual interest that the algorithm uses the property that the heaviest edge in a cycle can be discarded. Previously this has only been exploited in asymptotically optimal algorithms that are considered impractical. An additional advantage is that the algorithm can greatly profit from pipelined memory access. Hence, an implementation on a vector machine is up to 10 times faster than previous algorithms. We outline additional refinements for MSTs of implicitly defined graphs and the use of the central data structure for querying the heaviest edge between two nodes in the MST. The latter result is also interesting for sparse graphs.", "authors": ["Irit Katriell", "Peter Sanders", "Jesper Larsson Tr\u00e4ff"], "n_citation": 0, "title": "A practical minimum spanning tree algorithm using the cycle property", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "fb6061d9-a790-4954-900e-af7196c369e7"}
{"abstract": "We describe a data structure for three-dimensional Nef complexes, algorithms for boolean operations on them, and our implementation of data structure and algorithms. Nef polyhedra were introduced by W. Nef in his seminal 1978 book on polyhedra. They are the closure of half-spaces under boolean operations and can represent non-manifold situations, open and closed boundaries, and mixed dimensional complexes. Our focus lies on the generality of the data structure, the completeness of the algorithms, and the exactness and efficiency of the implementation. In particular, all degeneracies are handled.", "authors": ["Miguel Granados", "Peter Hachenberger", "Susan Hert", "Lutz Kettner", "Kurt Mehlhorn", "Michael Seel"], "n_citation": 0, "title": "Boolean operations on 3D selective Nef complexes: Data structure, algorithms, and implementation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1a10b784-dce5-43ff-b96c-691bdf364c39"}
{"abstract": "In this paper some earlier defined local transformations between eulerian trails are generalized to transformations between decompositions of graphs into (possibly more) closed subtrails. For any graph G with a forbidden partition system F, we give an efficient algorithm which transforms any F-compatible decomposition of G into closed subtrails to another one, and at the same time it preserves F-compatibility and does not increase the number of subtrails by more than one. From this, several earlier results for eulerian trails easily follow. These results are embedded into the rich spectrum of results of theory of eulerian graphs and their applications. We further apply this statement to digraphs and discuss the time complexity of enumeration of all F-compatible decompositions (resp. of all F-compatible eulerian trails) in both graphs and digraphs.", "authors": ["Jana Maxov\u00e1", "Jaroslav Nesetril"], "n_citation": 0, "title": "Complexity of compatible decompositions of eulerian graphs and their transformations", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1c3d3183-5698-40d4-a266-74cb25b20011"}
{"abstract": "This work gives new insight into two well-known approximation algorithms for the uncapacitated facility location problem: the primal-dual algorithm of Jain & Vazirani, and an algorithm of Mettu & Plaxton. Our main result answers positively a question posed by Jain & Vazirani of whether their algorithm can be modified to attain a desired continuity property. This yields an upper bound of 3 on the integrality gap of the natural LP relaxation of the k-median problem, but our approach does not yield a polynomial time algorithm with this guarantee. We also give a new simple proof of the performance guarantee of the Mettu-Plaxton algorithm using LP duality, which suggests a minor modification of the algorithm that makes it Lagrangian-multiplier preserving.", "authors": ["Aaron Archer", "Ranjithkumar Rajagopalan", "David B. Shmoys"], "n_citation": 0, "title": "Lagrangian relaxation for the k-median problem: New insights and continuity properties", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2cc4e43a-6981-498d-8e82-11760929682d"}
{"abstract": "Conformational polymorphs are identical molecules that crystallize in different spatial formations. Understanding the amount of difference between the polymorphs might aid drug design as there is a widespread assumption that there exists a direct connection between the conformations in the crystallized form of the molecule and the conformations in the solvent. We define a measure of similarity between conformational polymorphs and present an algorithm to compute it. For this end we weave together in a novel way our graph isomorphism method and substructure matching. We tested our algorithm on conformational polymorphs from the Cambridge Structural Database. Our experiments show that our method is very efficient in practice and has already yielded an important insight on the polymorphs stored in the data base.", "authors": ["Angela Enoshl", "Klara Kedem", "Joel Bernstein"], "n_citation": 50, "title": "Determining similarity of conformational polymorphs", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3adb3734-8180-4017-b418-0e3bcaefb182"}
{"authors": ["Jun Liu", "Xue-Cheng Tai", "Shing Yu Leung"], "n_citation": 0, "title": "A generic convexification and graph cut method for multiphase image segmentation", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "3eb55e3b-8da4-4632-96f9-c5310359dc3c"}
{"abstract": "Motivated by applications in road traffic control, we study flows in networks featuring special characteristics. In contrast to classical static flow problems, time plays a decisive role. Firstly, there are transit times on the arcs of the network which specify the amount of time it takes for flow to travel through a particular arc; more precisely, flow values on arcs may change over time. Secondly, the transit time of an arc varies with the current amount of flow using this arc. Especially the latter feature is crucial for various real-life applications of flows over time; yet, it dramatically increases the degree of difficulty of the resulting optimization problems. Most problems dealing with flows over time and constant transit times can be translated to static flow problems in time-expanded networks. We develop an alternative time-expanded network with flow-dependent transit times to which the whole algorithmic toolbox developed for static flows can be applied. Although this approach does not entirely capture the behavior of flows over time with flow-dependent transit times, we present approximation results which provide evidence of its surprising quality.", "authors": ["Ekkehard K\u00f6hler", "Katharina Langkau", "Martin Skutella"], "n_citation": 93, "title": "Time-expanded graphs for flow-dependent transit times", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3f73ad85-b9f0-425a-96e4-e6f923dcfc90"}
{"abstract": "Register file design is one of the critical issues facing designers of out-of-order processors. Scaling up its size and number of ports with issue width and instruction window size is difficult in terms of both performance and power consumption. Two types of register file architectures have been proposed in the past: a future logical file and a centralized physical file. The centralized register file does not scale well but allows fast branch mis-prediction recovery. The Future File scales well, but requires reservation stations and has slow mis-prediction recovery. This paper proposes a register file architecture that combines the best features of both approaches. The new register file has the large size of the centralized file and its ability to quickly recover from branch misprediction. It has the advantage of the future file in that it is accessed in the front end allowing about 1/3rd of the source operands that are ready when an instruction enters the window to be read immediately. The remaining operands come from bypass logic / instruction queues and do not require register file access. The new architecture does require reservation stations for operand storage and it investigates two approaches in terms of power-efficiency. Another advantage of the new architecture is that banking is much easier to use in this case as compared to the centralized register file. Banking further improves the scalability of the new architecture. A technique for early release of short-lived registers called writehack filtering is used in combination with banking to further improve the new architecture. The use of a large front-end register file results in significant power savings and a slight IPC degradation (less than 1%). Overall, the resulting energy-delay product is lower than in previous proposals.", "authors": ["Miquel Peric\u00e0s", "Rub\u00e9n Gonz\u00e1lez", "Adri\u00e1n Cristal", "Alexander V. Veidenbaum", "Mateo Valero"], "n_citation": 0, "title": "An optimized front-end physical register file with banking and writeback filtering", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4780ff45-ae52-40e7-9d57-225b6e9bef64"}
{"abstract": "Given a ring of size n and a set K of traffic demands, the ring loading problem with demand splitting (RLPW) is to determine a routing to minimize the maximum load on the edges. In the problem, a demand between two nodes can be split into two flows and then be routed along the ring in different directions. If the two flows obtained by splitting a demand are restricted to integers, this restricted version is called the ring loading problem with integer demand splitting (RLPWI). In this paper, efficient algorithms are proposed for the RLPW and the RLPWI. Both the proposed algorithms require O(|K| + ts) time, where t s  is the time for sorting |K| nodes. If |K| > n\u2208 for some small constant e > 0, integer sort can be applied and thus t s  = O(|K|); otherwise, t s  = O(|K| log |K|). The proposed algorithms improve the previous upper bounds from O(n|K|) for both problems.", "authors": ["Biing-Feng Wang", "Yong-Hsian Hsieh", "Li-Pu Yeh"], "n_citation": 0, "title": "Efficient algorithms for the ring loading problem with demand splitting", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "48e04ee3-ddbc-49ed-85ba-55c7ec5cb0f3"}
{"abstract": "Ad hoc sensor networks consist of large number of wireless sensors that communicate with each other in the absence of a fixed infrastructure. Fast self-reconfiguration and power efficiency are very important property on any sensor network management. The clustering problem consists in partitioning network nodes into groups called clusters, thus giving at the network a hierarchical organization. Clustering increases the scalability and the energy efficiency of communication among the sensors. A self-stabilizing algorithm, regardless of the initial system state, converges to a set of states that satisfy the problem specification without external intervention. Due to this property, self-stabilizing algorithms are adapted highly dynamic networks. In this paper we present a Self-stabilizing Clustering Algorithm for Ad hoc sensor network. Our algorithm adapts faster than other algorithms to topology changes.", "authors": ["Colette Johnen", "Le Huy Nguyen"], "n_citation": 50, "title": "Self-stabilizing Weight-Based Clustering Algorithm for Ad Hoc Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4bac8484-a296-415b-a9db-13dac068110d"}
{"abstract": "This framework for developing pre-service teachers' knowledge does not necessarily depend on computers or other educational technology.", "authors": ["Aman Yadav", "Chris Stephenson", "Hai Hong"], "n_citation": 0, "references": ["12d16ded-79ed-405b-953d-fe5ffefc2687", "33827cd9-00b0-4814-b4b7-d1666f825777", "4382c9d1-0301-48db-9ad1-196833427763", "45ef8d88-57b1-449b-8daf-10a5d81f6d8c", "51c7e02e-f5ed-431a-8cf5-f761f266d4be", "69b625b9-ebc5-4b60-b385-8a07945f5de9", "6a21b18f-e47a-4c17-b438-4831dcf163c6", "792adbfb-21fd-4660-8493-f5b4441cef0f", "9c3dc564-7211-448d-8f20-715fd292c7cb", "ef406687-4b62-47bf-920b-01f4ca5540af"], "title": "Computational thinking for teacher education", "venue": "Communications of The ACM", "year": 2017, "id": "5984be1a-7754-49d7-b24f-607e67c48433"}
{"authors": ["John H. Drake", "Ender \u00d6zcan"], "n_citation": 50, "title": "An Improved Choice Function Heuristic Selection for Cross Domain Heuristic Search", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "5c21ffcc-54f9-4fef-8a96-92da8f44c857"}
{"abstract": "This paper proposes a computer-assisted system for the surgical treatment of exophthalmia. This treatment is classically characterized by a de-compression of the orbit, by the mean of an orbital walls osteotomy. The plan-ning of this osteotomy consists in defining the size and the location of the de-compression hole. A biomechanical model of the orbital soft tissues and its in-teractions with the walls are provided here, in order to help surgeons in the definition of the osteotomy planning. The model is defined by a generic Finite Element poro-elastic mesh of the orbit. This generic model is automatically adapted to the morphologies of four patients, extracted from TDM exams. Four different FE models are then generated and used to simulate osteotomies in the maxillary or ethmoid sinuses regions. Heterogeneous results are observed, with different backwards movements of the ocular globe according to the size and/or the location of the hole.", "authors": ["Vincent Luboz", "Annaig Pedrono", "F. Boutault", "Pascal Swider", "Yohan Payan"], "n_citation": 0, "title": "Simulation of the Exophthalmia Reduction using a Finite Element Model of the Orbital Soft Tissues", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6fb03173-b630-4919-b64e-613fecb044e0"}
{"abstract": "We explore general strategies for finite-state syllabification and describe a specific implementation of a wide-coverage syllabifier for English, as well as outline methods to implement differing ideas encountered in the phonological literature about the English syllable. The syllable is a central phonological unit to which many allophonic variations are sensitive. How a word is syllabified is a non-trivial problem and reliable methods are useful in computational systems that deal with non-orthographic representations of language, for instance phonological research, text-to-speech systems, and speech recognition. The construction strategies for producing syllabifying transducers outlined here are not theory-specific and should be applicable to generalizations made within most phonological frameworks.", "authors": ["Mans Hulden"], "n_citation": 0, "title": "Finite-state syllabification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ca50f63-db83-4a3f-91a6-f93f0faedb77"}
{"authors": ["Lauri Karttunen"], "n_citation": 0, "title": "Finnish optimality-theoretic prosody", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a01f2ba2-9a45-4cda-a00d-f8fbc0dbdff0"}
{"abstract": "We report on a project with a German car manufacturer. The task is to compute (approximate) solutions to a specific large-scale packing problem. Given a polyhedral model of a car trunk, the aim is to pack as many identical boxes of size 4 x 2 x 1 units as possible into the interior of the trunk. This measure is important for car manufacturers, because it is a standard in the European Union. First, we prove that a natural formal variant of this problem is NP-complete. Further, we use a combination of integer linear programming techniques and heuristics that exploit the geometric structure to attack this problem. Our experiments show that for all considered instances, we can get very close to the optimal solution in reasonable time.", "authors": ["Friedrich Eisenbrand", "Stefan Funke", "Joachim Reichel", "Elmar Sch\u00f6mer"], "n_citation": 0, "title": "Packing a trunk", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "aaa87d01-4633-464e-8bfa-b87ee319955e"}
{"abstract": "Mapping out the challenges and strategies for the widespread adoption of service computing.", "authors": ["Athman Bouguettaya", "Munindar P. Singh", "Michael N. Huhns", "Quan Z. Sheng", "Hai Dong", "Qi Yu", "Azadeh Ghari Neiat", "Sajib Mistry", "Boualem Benatallah", "Brahim Medjahed", "Mourad Ouzzani", "Fabio Casati", "Xumin Liu", "Hongbing Wang", "Dimitrios Georgakopoulos", "Liang Chen", "Surya Nepal", "Zaki Malik", "Abdelkarim Erradi", "Yan Wang", "M. Brian Blake", "Schahram Dustdar", "Frank Leymann", "Mike P. Papazoglou"], "n_citation": 0, "references": ["036d64b3-1ddf-44b6-8d91-8cb3c1a848c3", "05413217-22dc-4ad1-9a4a-c3b117764d7d", "0697277c-f339-48cd-b805-b6c8ce59c211", "17087703-c484-4f06-b507-7e7c949428b9", "2c4e2e27-8ac8-4e68-9e61-b7e0d3574d10", "33da7827-3b37-41e7-99f2-02893f402435", "3406cda4-8c1b-4fba-a5c9-cb538825eb16", "48343283-6bb7-4370-9f59-82888521c8b6", "56d7aa55-1c09-4cfb-b19a-493cc7e86f4d", "60702d92-c7e8-4342-80d5-f579a50e20e1", "6e221de8-24af-42c0-a7d5-bda7509b583e", "6ef2b150-1723-47cb-bff1-6ceafa71c755", "7420ffd8-1d88-46a3-aa58-f414be5ae4c7", "7c5bce13-5ba6-4b48-b9e9-9f186c2ad8cb", "93140d6e-2e06-467c-9146-1820f8b43eb0", "a806ec25-b5bc-4b82-a130-92431607feb2", "a848db5b-35b2-42ff-b304-f04bee9e8330", "a8b3d9a1-4ff2-4204-9fc0-b04e5e688eee", "c025e2e5-bf09-44b6-bba5-79d1b214bdcc", "d8a18a2a-1ce6-4046-8ea2-92632c7eaa47", "fd54603c-3112-4374-a4a4-df5231c4c255"], "title": "A service computing manifesto: the next 10 years", "venue": "Communications of The ACM", "year": 2017, "id": "ae0dd9dc-5c15-4356-a8d0-d498c643a180"}
{"authors": ["Tao Lin", "Fangzhou Guo", "Yingcai Wu", "Biao Zhu", "Fan Zhang", "Huamin Qu", "Wei Chen"], "n_citation": 0, "title": "TieVis: Visual analytics of evolution of interpersonal ties", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "ae8917d2-52bb-4efa-9159-6d641c85c7e4"}
{"authors": ["Burak G\u00f6vem", "Kimmo J\u00e4rvinen", "Ingrid Verbauwhede", "Nele Mentens"], "n_citation": 0, "title": "A Fast and Compact FPGA Implementation of Elliptic Curve Cryptography using Lambda Coordinates", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "b3884d7d-d960-412f-82cb-d067e7c04f85"}
{"abstract": "FSCQ is the first file system with a machine-checkable proof that its implementation meets a specification, even in the presence of fail-stop crashes. FSCQ provably avoids bugs that have plagued previous file systems, such as performing disk writes without sufficient barriers or forgetting to zero out directory blocks. If a crash happens at an inopportune time, these bugs can lead to data loss. FSCQ's theorems prove that, under any sequence of crashes followed by reboots, FSCQ will recover its state correctly without losing data.   To state FSCQ's theorems, this paper introduces the Crash Hoare logic (CHL), which extends traditional Hoare logic with a crash condition, a recovery procedure, and logical address spaces for specifying disk states at different abstraction levels. CHL also reduces the proof effort for developers through proof automation. Using CHL, we developed, specified, and proved the correctness of the FSCQ file system. Although FSCQ's design is relatively simple, experiments with FSCQ as a user-level file system show that it is sufficient to run Unix applications with usable performance. FSCQ's specifications and proofs required significantly more work than the implementation, but the work was manageable even for a small team of a few researchers.", "authors": ["Tej Chajed", "Haogang Chen", "Adam Chlipala", "M. Frans Kaashoek", "Nickolai Zeldovich", "Daniel Ziegler"], "n_citation": 0, "references": ["0b3fba17-84b8-4ab2-b4dd-0318b36ac0f2", "0bf5f455-4c3b-4336-af8a-1024da5a1eff", "0cb27455-a4ee-4aa7-a55a-ef983ded50b3", "1e4e4627-0f96-4119-94c4-5b850890f47d", "25b46064-0d14-4675-a8e7-ab20f1199e54", "2ec8757c-a088-44f2-b1f6-9e8b26a0053e", "3442385f-8b67-4ecf-b971-be398b52aab3", "35c8c06c-2ad0-46b4-9e92-2d684f3abd94", "3e47b0a4-0d88-4012-a288-aaec4172d8d3", "5c15704b-f03e-416d-98bf-88d701f37560", "6673d885-7eee-449d-afe0-a8e3b0245ff7", "6e47785e-680a-469e-9e43-35b70d73316e", "7365093b-1e46-4f2b-a376-bf13e98318c4", "87e5353c-806c-4ee7-b942-a629cb1f85eb", "8c4ba77c-3fa3-4eb4-9318-3c2b37a90dbb", "a9b82916-c0e1-4abb-8e8c-eb32a10440c6", "e4840beb-88a3-41ae-b25d-ed284fcc7e6f"], "title": "Certifying a file system using crash hoare logic: correctness in the presence of crashes", "venue": "Communications of The ACM", "year": 2017, "id": "ba0e0905-6a40-4ba4-b2d1-100b370e5522"}
{"authors": ["Peyman Afshani", "Timothy M. Chan", "Konstantinos Tsakalidis"], "n_citation": 0, "title": "Deterministic Rectangle Enclosure and Offline Dominance Reporting on the RAM", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "bf0973de-fd60-48e4-a3ed-8d9c726bbbe1"}
{"abstract": "The  Communications  Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of  Communications , we'll publish selected posts or excerpts.   twitter  Follow us on Twitter at http://twitter.com/blogCACM  http://cacm.acm.org/blogs/blog-cacm   John Arquilla considers how we should interpret the alleged Russian cyberattack on the U.S. Presidential election; Mark Guzdial describes the potential benefits of a 'computing lab.'", "authors": ["John Arquilla", "Mark Guzdial"], "n_citation": 0, "title": "Crafting a national cyberdefense, and preparing to support computational literacy", "venue": "Communications of The ACM", "year": 2017, "id": "dd223bf8-38f3-4a8f-ac2a-2843255690d1"}
{"abstract": "We investigate the problem to round a given [0,1]-valued matrix to a 0,1 matrix such that the rounding error with respect to 2 x 2 boxes is small. Such roundings yield good solutions for the digital halftoning problem as shown by Asano et al. (SODA 2002). We present a randomized algorithm computing roundings with expected error at most 0.6287 per box, improving the 0.75 non-constructive bound of Asano et al. Our algorithm is the first one solving this problem fast enough for practical application, namely in linear time. Of a broader interest might be our rounding scheme, which is a modification of randomized rounding. Instead of independently rounding the variables (expected error 0.82944 per box in the worst case), we impose a number of suitable dependencies. Experimental results show that roundings obtained by our approach look much less grainy than by independent randomized rounding, and only slightly more grainy than by error diffusion. On the other hand, the latter algorithm (like all known deterministic algorithms) tends to produce unwanted structures, a problem that randomized algorithms like ours are unlikely to encounter.", "authors": ["Benjamin Doerr", "Henning Schnieder"], "n_citation": 0, "title": "Non-independent randomized rounding and an application to digital halftoning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ed5e47a6-17dd-4711-9447-d378c3c5df14"}
{"abstract": "It is shown that the optimum of an integer program in fixed dimension, which is defined by a fixed number of constraints, can be computed with O(s) basic arithmetic operations, where s is the binary encoding length of the input. This improves on the quadratic running time of previous algorithms which are based on Lenstra's algorithm and binary search. It follows that an integer program in fixed dimension, which is defined by m constraints, each of binary encoding length at most s, can be solved with an expected number of O(m+log(m) s) arithmetic operations using Clarkson's random sampling algorithm.", "authors": ["Friedrich Eisenbrand"], "n_citation": 0, "title": "Fast integer programming in fixed dimension", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f07e4179-7736-400f-ad6d-c1bfc32eadc2"}
{"abstract": "We consider the problem of finding, for a given n point set P in the plane and an integer k < n, the smallest circle enclosing at least k points of P. We present a randomized algorithm that computes in O(nk) expected time such a circle, improving over all previously known algorithms. Since this problem is believed to require \u03a9(nk) time, we present a linear time \u03b4-approximation algorithm that outputs a circle that contains at least k points of P, and of radius less than (1 + \u03b4)r opt (P,k), where r opt (P, k) is the radius of the minimal disk containing at least k points of P. The expected running time of this approximation algorithm is O(n + n . min(1/k\u03b4 3  log 2  1/\u03b4,k)).", "authors": ["Sariel Har-Peled", "Soham Mazumdar"], "n_citation": 0, "title": "Fast algorithms for computing the smallest k-enclosing disc", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "fa8f3185-b5a1-4542-ae35-c1f66d4d8a56"}
{"abstract": "The paper presents two algorithms involving shooting in three dimensions. We first present a new algorithm for performing ray shooting amid several special classes of n triangles in three dimensions. We show how to implement this technique to obtain improved query time for a set of fat triangles, and for a set of triangles stabbed by a common line. In both cases our technique requires near-linear preprocessing and storage, and answers a query in about n 2/3  time. This improves the best known result of close to n 3/4 query time for general triangles. The second algorithm handles stone-throwing amid arbitrary triangles in 3-space, where the curves along which we shoot are vertical parabolas, which are trajectories of stones thrown under gravity. We present an algorithm that answers stone-throwing queries in about n 3/4  time, using near linear storage and preprocessing. As far as we know, this is the first nontrivial solution of this problem. Several extensions of both algorithms are also presented.", "authors": ["Micha Sharir", "Hayim Shaul"], "n_citation": 0, "title": "Ray shooting and stone throwing", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "fd1e0643-d6c3-49e7-87ad-f7ec3eb92d74"}
{"authors": ["Hui Zhang", "Yuhao Song", "Zhuo Chen", "Ji Cai", "Ke Lu"], "n_citation": 50, "title": "Chinese shadow puppetry with an interactive interface using the kinect sensor", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "083a1c88-a8ac-4aee-88a3-7cf194020bee"}
{"abstract": "Compounding is a very productive process in German to form complex nouns and adjectives which represent about 7% of the words of a newspaper text. Unlike English, German compounds do not contain spaces or other word boundaries, and the automatic analysis is often ambiguous. A (non-weighted) finite-state morphological analyzer provides all potential segmentations for a compound without any filtering or prioritization of the results. The paper presents an experiment in analyzing German compounds with the Xerox Weighted Finite-State Compiler (wfsc). The model is based on weights for compound segments and gives priority (a) to compounds with the minimal number of segments and (b) to compound segments with the highest frequency in a training list. The results with this rather simple model will show the advantage of using weighted finite-state transducers over simple FSTs.", "authors": ["Anne Schiller"], "n_citation": 0, "title": "German compound analysis with wfsc", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0f9d976c-e70b-437e-b4f2-fb5334d31c25"}
{"abstract": "We consider a network providing Differentiated Services (Diffserv) which allow Internet service providers (ISP) to offer different levels of Quality of Service (QoS) to different traffic streams. We study FIFO buffering algorithms, where packets must be transmitted in the order they arrive. The buffer space is limited, and packets are lost if the buffer is full. Each packet has an intrinsic value, and the goal is to maximize the total value of transmitted packets. Our main contribution is an algorithm for arbitrary packet values that for the first time achieves a competitive ratio better than 2, namely 2 - e for a constant e > 0.", "authors": ["Alexander Kesselman", "Yishay Mansour", "Rob van Stee"], "n_citation": 0, "title": "Improved competitive guarantees for QoS buffering", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "14c86514-1157-499a-b0c0-1a55eb01ffc0"}
{"abstract": "We present a new finger search tree with O(1) worst-case update time and O(log log d) expected search time with high probability in the Random Access Machine (RAM) model of computation for a large class of input distributions. The parameter d represents the number of elements (distance) between the search element and an element pointed to by a finger, in a finger search tree that stores n elements. For the need of the analysis we model the updates by a balls and bins combinatorial game that is interesting in its own right as it involves insertions and deletions of balls according to an unknown distribution.", "authors": ["Alexis C. Kaporis", "Christos Makris", "Spyros Sioutas", "Athanasios K. Tsakalidis", "Kostas Tsichlas", "Christos D. Zaroliagis"], "n_citation": 0, "title": "Improved bounds for finger search on a ram", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1e2b1583-26f6-42dd-afc8-dd2772b85dd6"}
{"abstract": "We describe a semantic wiki system with an underlying controlled natural language grammar implemented in Grammatical Framework (GF). The grammar restricts the wiki content to a well-defined subset of Attempto Controlled English (ACE), and facilitates a precise bidirectional automatic translation between ACE and language fragments of a number of other natural languages, making the wiki content accessible multilingually. Additionally, our approach allows for automatic translation into the Web Ontology Language (OWL), which enables automatic reasoning over the wiki content. The developed wiki environment thus allows users to build, query and view OWL knowledge bases via a user-friendly multilingual natural language interface. As a further feature, the underlying multilingual grammar is integrated into the wiki and can be collaboratively edited to extend the vocabulary of the wiki or even customize its sentence structures. This work demonstrates the combination of the existing technologies of Attempto Controlled English and Grammatical Framework, and is implemented as an extension of the existing semantic wiki engine AceWiki.", "authors": ["Kaarel Kaljurand"], "n_citation": 0, "title": "A Multilingual Semantic Wiki Based on Attempto Controlled English and Grammatical Framework", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "28da9b00-1ea4-409e-9d66-02b44350c174"}
{"abstract": "We study a scheduling problem in which jobs have locations. For example, consider a repairman that is supposed to visit customers at their homes. Each customer is given a time window during which the repairman is allowed to arrive. The goal is to find a schedule that visits as many homes as possible. We refer to this problem as the Prize-Collecting Traveling Salesman Problem with time windows (TW-TSP). We consider two versions of TW-TSP. In the first version, jobs are located on a line, have release times and deadlines but no processing times. A geometric interpretation of the problem is used that generalizes the Erdos-Szekeres Theorem. We present an O(log n) approximation algorithm for this case, where n denotes the number of jobs. This algorithm can be extended to deal with non-unit job profits. The second version deals with a general case of asymmetric distances between locations. We define a density parameter that, loosely speaking, bounds the number of zig-zags between locations within a time window. We present a dynamic programming algorithm that finds a tour that visits at least OPT/density locations during their time windows. This algorithm can be extended to deal with non-unit job profits and processing times.", "authors": ["Reuven Bar-Yehuda", "Guy Even", "Shimon (Moni) Shahar"], "n_citation": 0, "title": "On approximating a geometric prize-collecting traveling Salesman Problem with time windows: Extended abstract", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2c3a3099-5d90-4db3-9d18-f78ed6edb4c6"}
{"authors": ["Jonathan Ullman"], "n_citation": 0, "title": "Technical Perspective: Building a safety net for data reuse", "venue": "Communications of The ACM", "year": 2017, "id": "417b8912-0b9e-4471-9cc6-3ac5cc0fa16c"}
{"authors": ["Joris Maervoet", "Pascal Brackman", "Katja Verbeeck", "Patrick De Causmaecker", "Greet Vanden Berghe"], "n_citation": 0, "title": "Tour Suggestion for Outdoor Activities", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "546791a7-e519-4c6b-907b-eae45d43e32c"}
{"abstract": "Some fundamental aspects of restricted availability and accessibility for multi-rate traffic are clarified. In classical teletraffic models all connections have the same bandwidth demand. In service-integrated systems each service has individual bandwidth requirement. For example in CDMA systems we may experience blocking (outage) even if the nominal capacity of a cell has not been fully used, because the actual capacity depends on interference from neighboring cells. Therefore, calls will experience individual accessibility. This paper presents a new theory and model for this problem. The model preserves the reversibility and insensitivity and generalizes the basic knowledge of classical teletraffic models. The theory is illustrated by a WCDMA traffic model.", "authors": ["Villy B. Iversen"], "n_citation": 0, "title": "Modelling restricted accessibility for wireless multi-service systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "64087a75-7c03-4cbf-b0b3-2221e52e0fc6"}
{"abstract": "A joint effort between the Chinese Academy of Sciences and the Hong Kong University of Science and Technology, the BLOSSOMS sensor network project aims to identify research issues at all levels from practical applications down to the design of sensor nodes. In this project, a heterogeneous sensor array including different types of application-dependent sensors as well as monitoring sensors and intruding sensors are being developed. Application-dependent power-aware communication protocols are also being studied for communications among sensor nodes. An ontology-based middleware is built to relieve the burden of application developers from collecting, classifying and processing messy sensing contexts. This project will also develop a set of tools allowing researchers to model, simulate/emulate, analyze, and monitor various functions of sensor networks.", "authors": ["Wen Gao", "Lionel M. Ni", "Zhiwei Xu"], "n_citation": 0, "title": "BLOSSOMS: A CAS/HKUST joint project to build lightweight optimized sensor systems on a massive scale", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6dba2e92-6e46-48eb-912e-644847e1caa6"}
{"abstract": "Recent years in cancer research were characterized by both accumulation of data and growing awareness of its overwhelming complexity. Consortia like The Cancer Genome Atlas [1] generated large collections of tumor samples, recording presence or absence of genomic alterations, such as somatic point mutations, amplifications, or deletions of genes. One of the basic tasks in the analysis of tumor genomic data is to elucidate sets of genes involved in a common oncogenic pathway. A de novo approach to this task is to search for mutually exclusive patterns in cancer genomic data [2, 3, 4, 5], where these alterations tend not to occur together in the same patient. Such patterns are commonly evaluated and ranked by their coverage and impurity. Coverage is defined as the number of patient samples in which at least one alteration occurred, while impurity refers to non-exclusive, additional alterations that violate strict mutual exclusivity. Mutually exclusive patterns have frequently been observed in cancer data, and were associated with functional pathways [6].", "authors": ["Ewa Szczurek"], "n_citation": 50, "title": "Modeling Mutual Exclusivity of Cancer Mutations", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "76b30810-c784-4510-b5d8-3101b6b34877"}
{"abstract": "Query optimization problems for expensive predicates have received much attention in the database community. In these situations, the output to the database query is a set of tuples that obey certain conditions, where the conditions may be expensive to evaluate computationally. In the simplest case when the query looks for the set of tuples that simultaneously satisfy two expensive conditions on the tuples and these can be checked in two different distributed processors, the problem reduces to one of ordering the condition evaluations at each processor to minimize the time to output all the tuples that are answers to the query. We improve upon a previously known deterministic 3-approximation for this problem: In the case when the times to evaluate all conditions at both processors are identical, we give a 2-approximation; In the case of non-uniform evaluation times, we present a -approximation that uses randomization. While it was known earlier that no deterministic algorithm (even with exponential running time) can achieve a performance ratio better than 2, we show a corresponding lower bound of for any randomized algorithm.", "authors": ["Eduardo Sany Laber", "Ojas Parekh", "R. Ravi"], "n_citation": 50, "title": "Randomized approximation algorithms for query optimization problems on two processors", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "93fadadd-4747-4f2c-b391-a985db239186"}
{"abstract": "A kinetic dictionary is a data structure for storing a set S of continuously moving points on the real line, such that at any time we can quickly determine for a given query point q whether q \u2208 S. We study trade-offs between the worst-case query time in a kinetic dictionary and the total cost of maintaining it during the motions of the points.", "authors": ["Mark de Berg"], "n_citation": 0, "title": "Kinetic dictionaries: How to shoot a moving target", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "a9337558-a341-4189-a61a-ff8aa258158a"}
{"abstract": "Let L = L 1  \u00d7 ... x L n  be the product of n lattices, each of which has a bounded width. Given a subset A \u2286 L, we show that the problem of extending a given partial list of maximal independent elements of A in L can be solved in quasi-polynomial time. This result implies, in particular, that the problem of generating all minimal infrequent elements for a database with semi-lattice attributes, and the problem of generating all maximal boxes that contain at most a specified number of points from a given n-dimensional point set, can both be solved in incremental quasi-polynomial time.", "authors": ["Khaled M. Elbassioni"], "n_citation": 0, "title": "An algorithm for dualization in products of lattices and its applications", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "da421a43-d42d-46f4-919c-e21d2dc57e36"}
{"authors": ["Sunil Arya", "G.D. Da Fonseca", "David M. Mount"], "n_citation": 0, "title": "A unified approach to approximate proximity searching", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "dbc2ef4d-6e63-4de2-9d36-11e84869cb57"}
{"abstract": "We present a (seemingly first) framework for an automated generation of exact search tree algorithms for NP-hard problems. The purpose of our approach is two-fold-rapid development and improved upper bounds. Many search tree algorithms for various problems in the literature are based on complicated case distinctions. Our approach may lead to a much simpler process of developing and analyzing these algorithms. Moreover, using the sheer computing power of machines it may also lead to improved upper bounds on search tree sizes (i.e., faster exact solving algorithms) in comparison with previously developed hand-made search trees.", "authors": ["Jens Gramm", "Jiong Guo", "Falk H\u00fcffner", "Rolf Niedermeier"], "n_citation": 0, "title": "Automated generation of search tree algorithms for graph modification problems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "edfd56bb-992f-4069-abca-459eab9fcfee"}
{"authors": ["Carolyn Whitnall", "M E Oswald"], "n_citation": 0, "title": "A Comprehensive Evaluation of Mutual Information Analysis Using a Fair Evaluation Framework", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "f7283654-667c-4d00-9445-aef8959b2811"}
{"abstract": "Given a wireless Ad-Hoc network modeled as a unit disk graph U in the plane, we present a localized distributed algorithm that constructs a bounded degree planar power spanner of U with a bounded stretch factor. More specifically, for an integer parameter k > 8 and a power exponent constant p \u2208 [2,5], our algorithm constructs a planar power spanner for the network of degree bounded by k + 5 and a stretch factor bounded by 1 + 2 P  sin P  (\u03c0/k). This significantly improves the previous best results in the literature by Song et al..", "authors": ["Iyad A. Kanj", "Ljubomir Perkovic"], "n_citation": 0, "title": "Improved Stretch Factor for Bounded-Degree Planar Power Spanners of Wireless Ad-Hoc Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d7e9b52-81a4-4416-86cb-472b8a518b3d"}
{"abstract": "In this work we present a protocol for securing communication trees in sensor networks. Communication trees can be used in a number of ways; to broadcast commands, to aggregate information or to route messages in the network. Broadcast trees are necessary for broadcasting information such as commands of maintenance packets from the base station to all sensors in the network. Aggregation trees are used in a complementary way to aggregate information collected by individual nodes so that meaningful summaries are presented to the base station, thus saving energy from unnecessary retransmissions. Alternatively, trees are constructed by many routing protocols, hence the security of these trees is important to prevent against many routing attacks. In this work we demonstrate how to establish such trees in a secure and authenticated way. Our protocol is simple and efficient and furthermore enables changes to the tree structure due to failure of nodes or addition of new ones. Finally, it is resistant to a host of attacks that can be applied to sensor networks.", "authors": ["Tassos D. Dimitriou"], "n_citation": 0, "title": "Securing Communication Trees in Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6d8e8fea-0dcd-4157-9315-53dd7a4ecb48"}
{"abstract": "We study the problem of maintaining a dynamic ordered set subject to insertions, deletions, and traversals of k consecutive elements. This problem is trivially solved on a RAM and on a simple two-level memory hierarchy. We explore this traversal problem on more realistic memory models: the cache-oblivious model, which applies to unknown and multi-level memory hierarchies, and sequential-access models, where sequential block transfers are less expensive than random block transfers.", "authors": ["Michael A. Bender", "Richard Cole", "Erik D. Demaine", "Martin Farach-Colton"], "n_citation": 0, "title": "Scanning and traversing: Maintaining data for traversals in a memory hierarchy", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "91b21605-ff9b-42ee-b2f9-f46b8d1b4443"}
{"authors": ["Gernot Heiser"], "n_citation": 0, "title": "Technical Perspective: Proving file systems meet expectations", "venue": "Communications of The ACM", "year": 2017, "id": "ad0254b8-5181-4586-96e9-a814f521ea79"}
{"abstract": "Let G = (V, E) be an undirected graph, with three numbers do(e) > d 1 (e) > d 2 (e) >0 for each edge e \u2208 E. A solution is a subset U C V and d i (e) represents the cost contributed to the solution by the edge e if exactly i of its endpoints are in the solution. The cost of including a vertex v in the solution is c(v). A solution has cost that is equal to the sum of the vertex costs and the edge costs. The minimum generalized vertex cover problem is to compute a minimum cost set of vertices. We study the complexity of the problem when the costs do(e) = 1, d 1 (e) = a and d 2 (e) = 0 \u2200e \u2208 E and c(v) = \u03b2 \u2200 v  \u2208 V for all possible values of a and \u03b2. We also provide a pair of 2-approximation algorithms for the general case.", "authors": ["Refael Hassin", "Asaf Levin"], "n_citation": 0, "title": "The minimum generalized vertex cover problem", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c17b667c-6d05-4799-88fb-ec65b254c763"}
{"abstract": "This paper deals with Finite Element (FE) modeling of human body structures. More specifically, it focuses on the FE mesh generation process, which is a long and tedious task in the case of irregular and non-homogeneous structures. Whereas for regular and symmetrical bodies (like in the architectural domain for example), some automatic mesh generators have been developed, no robust system is provided for living structures, which are, by definition, non-homogeneous, irregular and patient-specific. This paper proposes a new algorithm, called the mesh-matching (M-M) algorithm, that automatically generates patient-specific 3D meshes for FE models of structures with complex geometry. It assumes that the shape which is studied is sufficiently close to a known \u201cstandard\u201d model for which a mesh has been already generated by an expert. The algorithm proposes then to use a registration method, in order to infer the \u201cstandard\u201d finite element mesh to the data. The M-M algorithm is tested and initially validated on five human femurs.", "authors": ["B. Couteau", "Yohan Payan", "Stephane Lavallee", "Marie-Christine Hobatho"], "n_citation": 50, "title": "The Mesh-Matching algorithm: a new automatic 3D mesh generator for Finite Element analysis", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "d664ff3a-eeb4-464e-8445-28a22938e85f"}
{"authors": ["Helmut Schmid"], "n_citation": 50, "title": "A programming language for finite state transducers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e065543b-21eb-408d-bf2a-10bf0bea77a6"}
{"abstract": "In this paper we present a dynamic algorithm for the construction of the additively weighted Voronoi diagram of a set of weighted points in the plane. The novelty in our approach is that we use the dual of the additively weighted Voronoi diagram to represent it. This permits us to perform both insertions and deletions of sites easily. Given a set B of n sites, among which h sites have a non-empty cell, our algorithm constructs the additively weighted Voronoi diagram of B in O(nT(h) + h log h) expected time, where T(k) is the time to locate the nearest neighbor of a query site within a set of k sites. Deletions can be performed for all sites whether or not their cell is empty. The space requirements for the presented algorithm is O(n). Our algorithm is simple to implement and experimental results suggest an O(n log h) behavior.", "authors": ["Menelaos I. Karavelas", "Mariette Yvinec"], "n_citation": 0, "title": "Dynamic additively weighted Voronoi diagrams in 2D", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e0ec7ca9-b696-49e2-a9ce-f09df72111b9"}
{"abstract": "We consider a branch-and-bound algorithm for maximum clique problems. We introduce cost based filtering techniques for the so-called candidate set (i.e. a set of nodes that can possibly extend the clique in the current choice point). Additionally, we present a taxonomy of upper bounds for maximum clique. Analytical results show that our cost based filtering is in a sense as tight as most of these well-known bounds for the maximum clique problem. Experiments demonstrate that the combination of cost based filtering and vertex coloring bounds outperforms the old approach as well as approaches that only apply either of these techniques. Furthermore, the new algorithm is competitive with other recent algorithms for maximum clique.", "authors": ["Torsten Fahle"], "n_citation": 0, "title": "Simple and fast: Improving a branch-and-bound algorithm for maximum clique", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fbf34cf2-72cb-47f4-8a97-e14c14399065"}
{"abstract": "We give an algorithm to minimize the total completion time on-line on a single machine, using restarts, with a competitive ratio of 3/2. The optimal competitive ratio without using restarts is 2 for deterministic algorithms and e/(e - 1) \u2243 1.582 for randomized algorithms. This is the first restarting algorithm to minimize the total completion time that is proved to be better than an algorithm that does not restart.", "authors": ["Rob van Stee", "Han La Poutr\u00e9"], "n_citation": 0, "title": "Minimizing the total completion time on-line on a single machine, using restarts", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fc7e3c23-8c28-489c-94f5-4ab480ac5e40"}
{"abstract": "Widely used positioning systems like GPS are not a valid solution in large networks with small size, low cost sensors, due both to their size and their cost. Thus, new solutions for localization awareness are emerging, commonly based on the existence of a few references spread into the network. We propose a localization algorithm to reduce the number of transmitting nodes. The algorithm relies on self selecting nodes for location information disclosure. Each node makes a decision based on its proximity to the nodes in the area covered only by two of the references used for its own localization. We analyze different aspects of the location awareness propagation problem: communication overhead, redundant transmissions, network coverage.", "authors": ["Pierre Leone", "Luminita Moraru", "Olivier Powell", "Jos\u00e9 D. P. Rolim"], "n_citation": 50, "title": "Localization Algorithm for Wireless Ad-Hoc Sensor Networks with Traffic Overhead Minimization by Emission Inhibition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ffad7413-9ba3-46d8-b021-f743677c87d3"}
{"abstract": "Ambient calculus is an abstract model of the basic features of distribution and mobility of the computing and computation. The central notion of ambient calculus is that of a mobile ambient, which is a bounded place where a computation happens. It was shown that a P system with symbol-objects with membrane dissolution can be expressed in ambient calculus. We want to do here the converse work: to express ambient calculus in membrane computing. In this paper we present the first part of this work: we show that the Ethernet Network (local electronic computer network) can be expressed in terms of P systems with symbol-objects.", "authors": ["Vladimir Rogozhin", "Elena Boian"], "n_citation": 0, "title": "Simulation of mobile ambients by P systems. Part 1", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "00c50270-78f0-4a8f-831c-199fd72cf8fe"}
{"abstract": "We present efficient and accurate approximation algorithms for computing the premium price of Asian options. First, we modify an algorithm developed by Aingworth et al. in SODA 2000 for pricing the Europian-Asian option and improve its accuracy (both theoretically and practically) by transforming it into a randomized algorithm. Then, we present a new option named Saving-Asian option, whose merit is in the middle of European-Asian and American-Asian options, and show that our method works for its pricing.", "authors": ["Kenichiro Ohta", "Kunihiko Sadakane", "Akiyoshi Shioura", "Takeshi Tokuyama"], "n_citation": 0, "title": "A fast, accurate and simple method for pricing European-Asian and Saving-Asian options", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0de9b081-887e-4943-a185-053cc35d8743"}
{"abstract": "Identifying palindromes in sequences has been an interesting line of research in combinatorics on words and also in computational biology, after the discovery of the relation of palindromes in the DNA sequence with the HIV virus. Efficient algorithms for the factorization of sequences into palindromes and maximal palindromes have been devised in recent years. We extend these studies by allowing gaps in decompositions and errors in palindromes, and also imposing a lower bound to the length of acceptable palindromes. #R##N#We first present an algorithm for obtaining a palindromic decomposition of a string of length n with the minimal total gap length in time O(n log n * g) and space O(n g), where g is the number of allowed gaps in the decomposition. We then consider a decomposition of the string in maximal \\delta-palindromes (i.e. palindromes with \\delta errors under the edit or Hamming distance) and g allowed gaps. We present an algorithm to obtain such a decomposition with the minimal total gap length in time O(n (g + \\delta)) and space O(n g).", "authors": ["Micha\u0142 Adamczyk", "Mai Alzamel", "Panagiotis Charalampopoulos", "Costas S. Iliopoulos", "Jakub Radoszewski"], "n_citation": 0, "references": ["0ad7c6e5-045d-4ed2-b9cb-5c4749c36796", "29ea3c4e-a8e5-4439-8eba-ec2248c370de", "33834a31-132a-409b-b646-c8918a85afb0", "49d1a3a9-ad7a-4a30-a16a-5c094281e466", "52d9dbbf-239b-4769-83a6-dd4a90fbadc3", "562b7401-fd0f-43b4-9cdf-fbb9e0b1acae", "72a1527e-4e61-49a1-b8b5-1b6465ab5306", "8b5f9a4d-7cca-4cef-9926-b275b761b894", "8c1e1ef9-61bb-4d3d-8adf-dd7cc054d74a", "eb9eba04-7220-4676-91d6-c4d38120facf", "f2eb0d47-2e33-4866-b86a-bb8b37fe5ac5"], "title": "Palindromic Decompositions with Gaps and Errors", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "0f66c94e-48a1-415c-92b9-c4a9939829e8"}
{"abstract": "Garbage collection by copying is becoming more and more popular for Prolog. Copying requires a marking phase in order to be safe: safeness means that the to-space is guaranteed not to overflow. However, some systems use a copying garbage collector without marking prior to copying, and instead postpone the copying of potentially unsafe cells. Such systems only collect small portions of the heap and it is not clear whether postponing works while collecting the whole heap. Moreover, it is shown here that postponing does not solve the problem in a fundamental way. Since marking takes time, it is worth studying the tradeoffs involved. These observations have prompted the experimentation with a series of garbage collectors based on copying without marking and without postponing. In particular, variants were implemented that are named dangerous, optimistic and cautious copying which exhibit various degrees of unsafeness. Versions of each have been implemented based on recursive copying as in most implementations of copy-tern/2 and on the Cheney algorithm. Performance on benchmarks suggests that large performance gains can be obtained by skipping the marking phase, that dangerous copying is still relatively safe but can be costly, and that the additional effort of cautious copying over optimistic copying is not worth it. The optimistic collectors based on recursive copying perform best and slightly better than the ones based on Cheney. Cache performance measurements back up the benchmark results.", "authors": ["Bart Demoen", "Phuong-Lan Nguyen", "Ruben Vandeginste"], "n_citation": 0, "title": "Copying garbage collection for the WAM: To mark or not to mark?", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "14a581b4-e653-4896-bf72-415b70c3d6e8"}
{"abstract": "This paper proposes modelling the semantics of natural-language calendar expressions as extended regular expressions (XREs). The approach covers expressions ranging from plain dates and times of the day to more complex ones, such as the second Tuesday following Easter. Expressions denoting disconnected periods of time are also covered. The paper presents an underlying string-based temporal model, sample calendar expressions with their XR.E representations, and possible applications in temporal reasoning and natural-language generation.", "authors": ["Jyrki Niemi", "Lauri Carlson"], "n_citation": 0, "title": "Modelling the semantics of calendar expressions as extended regular expressions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2a1f7fdf-648b-4641-b4ec-cc96a8af9c69"}
{"abstract": "We investigate algorithmic questions and structural problems concerning graph families defined by 'edge-counts'. Motivated by recent developments in the unique realization problem of graphs, we give an efficient algorithm to compute the rigid, redundantly rigid, M-connected, and globally rigid components of a graph. Our algorithm is based on (and also extends and simplifies) the idea of Hendrickson and Jacobs, as it uses orientations as the main algorithmic tool. We also consider families of bipartite graphs which occur in parallel drawings and scene analysis. We verify a conjecture of Whiteley by showing that 2d-connected bipartite graphs are d-tight. We give a new algorithm for finding a maximal d-sharp subgraph. We also answer a question of Imai and show that finding a maximum size d-sharp subgraph is NP-hard.", "authors": ["Alex R. Berg", "Tibor Jord\u00e1n"], "n_citation": 0, "title": "Algorithms for graph rigidity and scene analysis", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "664ae88d-514d-435a-8ffe-d9d2b8c8dfd3"}
{"abstract": "Behavior has only recently received the attention of structural computing research. Thus, the exact relation of behavior to structure is to a large extent unspecified. Current structural computing research still views behavior as something that is orthogonal to structure: something that has to be added on top of it. In this paper, we attempt to create a framework to discuss the notions of structure, function and behavior. We argue that a structure exhibits behavior, behaviors effects function, and function enables purpose. Moreover, we point out propagation as an inherent behavioral capability of relationships. This discussion aims at providing the ground to recognize the variant and invariant behaviors found in structures.", "authors": ["Michalis Vaitis", "Manolis Tzagarakis", "Konstantinos Grivas", "Eleftherios Chrysochoos"], "n_citation": 0, "title": "Some notes on behavior in structural computing", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "68057c08-64f9-491d-8ae9-281a99c94c32"}
{"authors": ["Silvio Lattanzi", "Stefano Leonardi"], "n_citation": 0, "references": ["137f765b-6442-4d9d-bf6b-4b5d9a7e724a"], "title": "Efficient Computation of the Weighted Clustering Coefficient", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "6f669d96-23a4-49d2-b26c-63ad9deb1230"}
{"abstract": "This paper presents a dynamic algorithm for the construction of the Euclidean Voronoi diagram of a set of convex objects in the plane. We consider first the Voronoi diagram of smooth convex objects forming pseudo-circles set. A pseudo-circles set is a set of bounded objects such that the boundaries of any two objects intersect at most twice. Our algorithm is a randomized dynamic algorithm. It does not use a conflict graph or any sophisticated data structure to perform conflict detection. This feature allows us to handle deletions in a relatively easy way. In the case where objects do not intersect, the randomized complexity of an insertion or deletion can be shown to be respectively O(log 2 n) and O(log 3 n). Our algorithm can easily be adapted to the case of pseudo-circles sets formed by piecewise smooth convex objects. Finally, given any set of convex objects in the plane, we show how to compute the restriction of the Voronoi diagram in the complement of the objects' union.", "authors": ["Menelaos I. Karavelas", "Mariette Yvinec"], "n_citation": 0, "title": "The Voronoi diagram of planar convex objects", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "8810ca9a-2288-44b4-8f68-fe6767724d74"}
{"abstract": "The problems of gossiping and broadcasting have been widely studied. The basic gossip problem is defined as follows: there are n individuals, with each individual having an item of gossip. The goal is to communicate each item of gossip to every other individual. Communication typically proceeds in rounds, with the objective of minimizing the number of rounds. One popular model, called the telephone call model, allows for communication to take place on any chosen matching between the individuals in each round. Each individual may send (receive) a single item of gossip in a round to (from) another individual. In the broadcasting problem, one individual wishes to broadcast an item of gossip to everyone else. In this paper, we study generalizations of gossiping and broadcasting. The basic extensions are: (a) each item of gossip needs to be broadcast to a specified subset of individuals and (b) several items of gossip may be known to a single individual. We study several problems in this framework that generalize gossiping and broadcasting. Our study of these generalizations was motivated by the problem of managing data on storage devices, typically a set of parallel disks. For initial data distribution, or for creating an initial data layout we may need to distribute data from a single server or from a collection of sources.", "authors": ["Samir Khuller", "Yoo-Ah Kim", "Yung-Chun Justin Wan"], "n_citation": 0, "title": "On generalized gossiping and broadcasting: (Extended abstract)", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "8b755c2b-771f-4e41-aa6c-4a00f6573b29"}
{"abstract": "We present new geometric approximation and exact algorithms for the density-based data clustering problem in d-dimensional space R d  (for any constant integer d > 2). Previously known algorithms for this problem are efficient only for uniformly-distributed points. However, these algorithms all run in \u0398(n 2 ) time in the worst case, where n is the number of input points. Our approximation algorithm based on the e-fuzzy distance function takes O(n log n) time for any given fixed value e > 0, and our exact algorithms take sub-quadratic time. The running times and output quality of our algorithms do not depend on any particular data distribution. We believe that our fast approximation algorithm is of considerable practical importance, while our sub-quadratic exact algorithms are more of theoretical interest. We implemented our approximation algorithm and the experimental results show that our approximation algorithm is efficient on arbitrary input point sets.", "authors": ["Danny Z. Chenl", "Michiel H. M. Smid", "Bin Xu"], "n_citation": 0, "title": "Geometric algorithms for density-based data clustering", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9477fb40-b2e3-483d-a303-72b5a91774be"}
{"abstract": "We propose pivotal synchronization languages (PSLs) that represent alignments of parallel processes. PSLs are closely related to synchronization languages [10], but the strings in PSLs are partitioned into sequences of pivots. In the partitioned representation, each pivot gathers and aligns simultaneous process boundaries (starts and terminations). The paper demonstrates that PSLs (and new join operators) provide a unified framework for implementing some independent formalisms. In particular, we show that at least two existing formalisms, generalized synchronization expressions [10] and interleave-disjunction-lock expressions [8] have PSL-based counterparts. Furthermore, we sketch tentatively a new formalism that adapts the ideas of the operator of generalized restriction [11] to PSLs. All this suggests that the union of these formalisms might be implementable.", "authors": ["Anssi Mikael Yli-Jyr\u00e4", "Jyrki Niemi"], "n_citation": 0, "title": "Pivotal synchronization languages : A framework for alignments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "978801f7-091c-429b-979f-8a1ed80c8626"}
{"abstract": "We describe and analyze empirically an implementation of some generalizations of Dijkstra's algorithm for shortest paths in graphs. The implementation formed a part of the TRANSIMS project at the Los Alamos National Laboratory. Besides offering the first implementation of the shortest path algorithm with regular language constraints, our code also solves problems with time-dependent edge delays in a quite general first-in-first-out model. We describe some details of our implementation and then analyze the behavior of the algorithm on real but extremely large transportation networks. Even though the questions we consider in our experiments are fundamental and natural, it appears that they have not been carefully examined before. A methodological contribution of the present work is the use of formal statistical methods to analyze the behaviour of our algorithms. Although the statistical methods employed are simple, they provide a possibly novel approach to the experimental analysis of algorithms. Our results provide evidence for our claims of efficiency of the algorithms described in a very practical setting.", "authors": ["Chris Barrett", "Keith R. Bisset", "Riko Jacob", "Goran Konjevod", "Madhav V. Marathe"], "n_citation": 51, "title": "Classical and contemporary shortest path problems in road networks: Implementation and experimental analysis of the TRANSIMS router", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a58515ea-5ceb-47b8-aba6-b4621926eee1"}
{"authors": ["Piotr Dembinski", "Agata Janowska", "Pawe\u0142 Janowski", "Wojciech Penczek", "Agata P\u00f3\u0142rola", "Maciej Szreter", "Bozena Wozna", "Andrzej Zbrzezny"], "n_citation": 74, "title": "Verics: A Tool for Verifying Timed Automata and Estelle Specifications", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "ac4dcf47-74cc-4b73-95d5-7aa40a86c073"}
{"abstract": "Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. Yet the practice of data analysis is an intrinsically interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused.   In this work, we initiate a principled study of how to guarantee the validity of statistical inference in adaptive data analysis. We demonstrate new approaches for addressing the challenges of adaptivity that are based on techniques developed in privacy-preserving data analysis.    As an application of our techniques we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced adaptively by a learning algorithm operating on a training set.", "authors": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "n_citation": 50, "references": ["17055d15-c111-48ca-b5b4-9685b671e0aa", "254a151f-40f3-4af5-84b7-96a1312e958e", "2a4df8c4-87d6-485c-beef-5fd21a78bcbd", "3abbef92-fcf5-4ed1-ba10-1262e929c8cd", "5b26d855-551c-47cc-be7b-0bb83b54efd7", "6047f55b-9e81-476c-bedf-bf7195a84a03", "7c3addb9-6b68-48e6-a378-edbf90f4a6ef", "92031bfe-7728-41ae-a9f9-f323b522ef7f", "b72d6743-4e1d-42fc-8cc3-ef32a94d0a05", "c13c3d13-d47d-437a-8edb-d0a8fac7e4bb", "c585699c-927c-4cb5-8518-d496b58bbe5d", "dc667ef3-91f0-400e-b405-b0844931aec5", "dd38b522-cbfd-4111-897d-17c3bc38386e", "dec16f9d-2a5a-45d9-8a61-7d1bbdc6fabf", "fcaecd0d-369f-4eb6-9e3b-3c14bdbf623f"], "title": "Guilt-free data reuse", "venue": "Communications of The ACM", "year": 2017, "id": "cee3a658-86b0-4dc9-bbee-94c9df8a0ffe"}
{"abstract": "In the Universal Facility Location problem we are given a set of demand points and a set of facilities. The goal is to assign the demands to facilities in such a way that the sum of service and facility costs is minimized. The service cost is proportional to the distance each unit of demand has to travel to its assigned facility, whereas the facility cost of each facility i depends on the amount of demand assigned to that facility and is given by a cost function f i (.). We present a (7.88 + e)-approximation algorithm for the Universal Facility Location problem based on local search, under the assumption that the cost functions f i  are nondecreasing. The algorithm chooses local improvement steps by solving a knapsack-like subproblem using dynamic programming. This is the first constant-factor approximation algorithm for this problem. Our algorithm also slightly improves the best known approximation ratio for the capacitated facility location problem with non-uniform hard capacities.", "authors": ["Mohammad Mahdian", "Martin P\u00e1l"], "n_citation": 0, "title": "Universal Facility Location", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d1d12189-f3ab-44ae-ae2c-a1e59f1adfe4"}
{"abstract": "We prove that various geometric covering problems, related to the Travelling Salesman Problem cannot be efficiently approximated to within any constant factor unless P = NP. This includes the Group-Travelling Salesman Problem (TSP with Neighborhoods) in the Euclidean plane, the Group-Steiner-Tree in the Euclidean plane and the Minimum Watchman Tour and the Minimum Watchman Path in 3-D. Some inapproximability factors are also shown for special cases of the above problems, where the size of the sets is bounded. Group-TSP and Group-Steiner-Tree where each neighbourhood is connected are also considered. It is shown that approximating these variants to within any constant factor smaller than 2, is NP-hard.", "authors": ["Shmuel Safra", "Oded Schwartz"], "n_citation": 0, "title": "On the complexity of approximating TSP with neighborhoods and related problems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "17612660-6592-4192-9c28-714127b0ea60"}
{"abstract": "In this paper we consider the problem of designing a mechanism for double auctions where bidders each bid to buy or sell one unit of a single commodity. We assume that each bidder's utility value for the item is private to them and we focus on truthful mechanisms, ones were the bidders' optimal strategy is to bid their true utility. The profit of the auctioneer is the difference between the total payments from buyers and total to the sellers. We aim to maximize this profit. We extend the competitive analysis framework of basic auctions [9] and give an upper bound on the profit of any truthful double auction. We then reduce the competitive double auction problem to basic auctions by showing that any competitive basic auction can be converted into a competitive double auction with a competitive ratio of twice that of the basic auction. In addition, we show that better competitive ratios can be obtained by directly adapting basic auction techniques to the double auction problem. This result provides insight into the design of profit maximizing mechanisms in general.", "authors": ["Kaustubh Deshmukh", "Andrew V. Goldberg", "Jason D. Hartline", "Anna R. Karlin"], "n_citation": 68, "title": "Truthful and competitive double auctions", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1dcffb79-f550-4bfa-bd88-344ecb1b6891"}
{"abstract": "Due to their small form factor and modest energy budget, individual sensors are not expected to be GPS-enabled. Moreover, in most applications, exact geographic location is not necessary, and all that the individual sensors need is a coarse-grain location awareness. The task of acquiring such a coarse-grain location awareness is referred to as training. In this paper, a scalable energy-efficient training protocol is proposed for massively-deployed sensor networks, where sensors are initially anonymous and unaware of their location. The training protocol is lightweight and simple to implement; it is based on an intuitive coordinate system imposed onto the deployment area which partitions the anonymous sensors into clusters where data can be gathered from the environment and synthesized under local control.", "authors": ["Alan A. Bertossi", "Stephan Olariu", "Maria Cristina Pinotti"], "n_citation": 0, "title": "Efficient Training of Sensor Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "222952a6-daa8-4e66-bbe2-1eda69af11ac"}
{"abstract": "Given a set P of n points in R d  and an integer k > 1, let w* denote the minimum value so that P can be covered by k cylinders of radius at most w*. We describe an algorithm that, given P and an e > 0, computes k cylinders of radius at most (1 + e)w* that cover P. The running time of the algorithm is O(n log n), with the constant of proportionality depending on k, d, and e. We first show that there exists a small certificate Q C P, whose size does not depend on n, such that for any k-cylinders that cover Q, an expansion of these cylinders by a factor of (1 + e) covers P. We then use a well-known scheme based on sampling and iterated re-weighting for computing the cylinders.", "authors": ["Pankaj K. Agarwal", "Cecilia M. Procopiuc", "Kasturi R. Varadarajan"], "n_citation": 0, "title": "Approximation algorithms for k-line center", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "428f628b-acf9-4fb4-b260-3d61712f36a3"}
{"abstract": "This paper explores the feasibility of implementing a model for an open domain, automated question and answering framework that leverages Wikipedia\u2019s knowledgebase. While Wikipedia implicitly comprises answers to common questions, the disambiguation of natural language and the difficulty of developing an information retrieval process that produces answers with specificity present pertinent challenges. However, observational analysis suggests that it is possible to discount the syntactical and lexical structure of a sentence in contexts where questions contain a specific target entity (words that identify a person, location or organisation) and that correspondingly query a property related to it. To investigate this, we implemented an algorithmic process that extracted the target entity from the question using CRF based named entity recognition (NER) and utilised all remaining words as  potential  properties. Using DBPedia, an ontological database of Wikipedia\u2019s knowledge, we searched for the closest matching property that would produce an answer by applying standardised string matching algorithms including the Levenshtein distance, similar text and Dice\u2019s coefficient. Our experimental results illustrate that using Wikipedia as a knowledgebase produces high precision for questions that contain a singular unambiguous entity as the subject, but lowered accuracy for questions where the entity exists as part of the object.", "authors": ["Saleem Ameen", "Hyunsuk Chung", "Soyeon Caren Han", "Byeong Ho Kang"], "n_citation": 0, "references": ["7d5b37e4-1285-426d-a5f1-bbbfda32b662", "b670b484-f609-4605-be88-d1d08577ae3c", "b68745bb-0e31-4e9f-84db-58ed855fbda8", "b96bfdb9-637d-4b9e-96fd-aef604464011", "da49c35a-07ab-4742-9a14-08505f75ed68", "e2d29662-606d-4d06-b579-0ef0640e2dd2", "eeaed464-4ab2-4884-b60c-327e989d50c8", "f5a47a74-c49b-479e-aea2-f14b8532b6a8"], "title": "Open-domain question answering framework using Wikipedia", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "492d4144-40b1-4fc6-bb70-64dc5023748e"}
{"abstract": "Given an undirected edge-weighted graph and a depot node, postman problems are generally concerned with traversing the edges of the graph (starting and ending at the depot node) while minimizing the distance traveled. For the Min-Max k-Chinese Postman Problem (MM k-CPP) we have k > 1 postmen and want to minimize the longest of the k tours. We present two new heuristics and improvement procedures for the MM k-CPP. Furthermore, we give three new lower bounds in order to assess the quality of the heuristics. Extensive computational results show that our algorithms outperform the heuristic of Frederickson et al. [12].", "authors": ["Dino Ahr", "Gerhard Reinelt"], "n_citation": 0, "title": "New heuristics and lower bounds for the Min-Max k-Chinese Postman Problem", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6294aa0f-1494-4ff9-aafe-6e2a33ca18a8"}
{"abstract": "We study a generalized coloring and routing problem for interval and circular graphs that is motivated by design of optical line systems. In this problem we are interested in finding a coloring and routing of demands of minimum total cost where the total cost is obtained by accumulating the cost incurred at certain links in the graph. The colors are partitioned in sets and the sets themselves are ordered so that colors in higher sets cost more. The cost of a link in a coloring is equal to the cost of the most expensive set such that a demand going through the link is colored with a color in this set. We study different versions of the problem and characterize their complexity by presenting tight upper and lower bounds. For the interval graph we show that the most general problem is hard to approximate to within \u221as and we complement this result with a O(\u221as)-approximation algorithm for the problem. Here s is proportional to the number of color sets. For the circular graph problem we show that most versions of the problem are hard to approximate to any bounded ratio and we present a 2(1+ e) approximation scheme for a special version of the problem.", "authors": ["Mansoor Alicherry", "Randeep Bhatia"], "n_citation": 0, "title": "Line system design and a generalized coloring problem", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "65aff0fc-207b-41ec-80e5-ad78b1b2b03f"}
{"abstract": "TAGH is a system for automatic recognition of German word forms. It is based on a stem lexicon with allomorphs and a concatenative mechanism for inflection and word formation. Weighted FSA and a cost function are used in order to determine the correct segmentation of complex forms: the correct segmentation for a given compound is supposed to be the one with the least cost. TAGH is based on a large stem lexicon of almost 80.000 stems that was compiled within 5 years on the basis of large newspaper corpora and literary texts. The number of analyzable word forms is increased considerably by more than 1000 different rules for derivational and compositional word formation. The recognition rate of TAGH is more than 99% for modern newspaper text and approximately 98.5% for literary texts.", "authors": ["Alexander Geyken", "Thomas Hanneforth"], "n_citation": 0, "title": "TAGH : A complete morphology for german based on weighted finite state automata", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6eb9095b-fd48-4d4e-b730-5f2fb92b2e5b"}
{"authors": ["Ponomarenko Alexander", "Yury Malkov", "Vladimir Krylov", "Logvinov Andrey"], "n_citation": 50, "title": "Scalable Distributed Algorithm for Approximate Nearest Neighbor Search Problem in High Dimensional General Metric Spaces", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "70e963d2-8db2-400a-ab83-df0bd6378fb9"}
{"abstract": "Analysis of genomes evolving by inversions leads to a general combinatorial problem of Sorting by Reversals, MIN-SBR, the problem of sorting a permutation by a minimum number of reversals. Following a series of preliminary results, Hannenhalli and Pevzner developed the first exact polynomial time algorithm for the problem of sorting signed permutations by reversals, and a polynomial time algorithm for a special case of unsigned permutations. The best known approximation algorithm for MIN-SBR, due to Christie, gives a performance ratio of 1.5. In this paper, by exploiting the polynomial time algorithm for sorting signed permutations and by developing a new approximation algorithm for maximum cycle decomposition of breakpoint graphs, we design a new 1.375-algorithm for the MIN-SBR problem.", "authors": ["Piotr Berman", "Sridhar Hannenhalli", "Marek Karpinski"], "n_citation": 0, "title": "1.375-Approximation algorithm for sorting by reversals", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "731816a2-2cfc-456e-ae60-95fd5e096428"}
{"abstract": "We investigate a curious problem from additive number theory: Given two positive integers S and Q, does there exist a sequence of positive integers that add up to S and whose squares add up to Q? We show that this problem can be solved in time polynomially bounded in the logarithms of S and Q. As a consequence, also the following question can be answered in polynomial time: For given numbers n and m, do there exist n lines in the Euclidean plane with exactly m points of intersection?", "authors": ["Gerhard J. Woeginger"], "n_citation": 0, "title": "Seventeen lines and one-hundred-and-one points", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7596b750-955c-4914-9004-d0f8ada786be"}
{"authors": ["David Vivas Balcones", "David Fern\u00e1ndez Llorca", "Miguel \u00c1ngel Sotelo", "Miguel Gavil\u00e1n", "Sergio Alvarez", "Ignacio Parra", "Manuel Oca\u00f1a"], "n_citation": 50, "title": "Real-Time Vision-Based Vehicle Detection for Rear-End Collision Mitigation Systems", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "7ae3f12f-56cd-428f-823e-49bdece966ec"}
{"authors": ["Jakob Rehof", "Pawel Urzyczyn"], "n_citation": 50, "title": "Finite Combinatory Logic with Intersection Types", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "7bf982f8-fa42-4fbc-b3f4-9d5d8d47ffc6"}
{"abstract": "In broadcast scheduling multiple users requesting the same information can be satisfied with one single broadcast. In this paper we study preemptive on-demand broadcast scheduling with deadlines on a single broadcast channel. We will show that the upper bound results in traditional real-time scheduling does not hold under broadcast scheduling model. We present two easy to implement online algorithms BCast and its variant BCast2. Under the assumption the requests are approximately of equal length (say k), we show that BCast is O(k) competitive. We establish that this bound is tight by showing that every online algorithm is \u03a9(k) competitive even if all requests are of same length k. We then consider the case where the laxity of each request is proportional to its length. We show that BCast is constant competitive if all requests are approximately of equal length. We then establish that BCast2 is constant competitive for requests with arbitrary length. We also believe that a combinatorial lemma that we use to derive the bounds can be useful in other scheduling system where the deadlines are often changing (or advanced).", "authors": ["Bala Kalyanasundaram", "Mahe Velauthapillai"], "n_citation": 0, "title": "On-demand broadcasting under deadline", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7fba51ee-a897-4418-801b-95b39a1792df"}
{"abstract": "This paper introduces a new Finite Element biomechanical model of the human face, which has been developed to be integrated into a simulator for plastic and maxillo-facial surgery. The idea is to be able to predict, from an aesthetic and functional point of view, the deformations of a patient face, resulting from repositioning of the maxillary and mandibular bone structures. This work will complete the simulator for bone-repositioning diagnosis that has been developed by the laboratory. After a description of our research project context, each step of the modeling is precisely described: the continuous and elastic structure of the skin tissues, the orthotropic muscular fibers and their insertions points, and the functional model of force generation. First results of face deformations due to muscles activations are presented. They are qualitatively compared to the functional studies provided by the literature on face muscles roles and actions.", "authors": ["Matthieu Chabanas", "Yohan Payan"], "n_citation": 50, "title": "A 3D Finite Element model of the face for simulation in plastic and maxillo-facial surgery", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "a1b0d213-2f9e-432f-a97d-5027babb89db"}
{"abstract": "Action systems are a framework for reasoning about discrete reactive systems. Back, Petre and Porres have extended these action systems to continuous action systems, which can be. used to model hybrid systems. In this paper we define a refinement relation, and develop practical data refinement rules for continuous action systems. The meaning of continuous action systems is expressed in terms of a mapping from continuous action systems to action systems. First, we present a new mapping from continuous act ion systems to action systems, such that Back's definition of trace refinement is correct with respect to it. Second, we present a stream semantics that is compatible with the trace semantics, but is preferable to it because it is more general. Although action system trace refinement rules are applicable to continuous action systems with a stream semantics, they are not complete. Finally, we introduce a new data refinement rule that is valid with respect to the stream semantics and can be used to prove refinements that are not possible in the trace semantics, and we analyse the completeness of our new rule in conjunction with the existing trace refinement rules.", "authors": ["Larissa Meinicke", "Ian J. Hayes"], "n_citation": 50, "title": "Continuous action system refinement", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bc216171-02ea-48a3-a0af-fad4e419d956"}
{"abstract": "This paper reports on research into and development of portable hardware that will enable users in the field to send images, and associated positional data from a PDA to a server for processing. The central aim is to provide navigational and informational services to an urban mobile user based on building recognition. The paper begins by describing the hardware before presenting research into server-side building recognition methods that operate by comparing user-supplied images with images generated by an existing 3d virtual model.", "authors": ["Wanji Mail", "G. Dodds", "Chris Tweed"], "n_citation": 0, "title": "A PDA-based system for recognizing buildings from user-supplied images", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ca624d49-2380-4e3f-8804-7726645db5a8"}
{"abstract": "Computer-Assisted Translation (CAT) is an alternative approach to Machine Translation, that integrates human expertise into the automatic translation process. In this framework, a human translator interacts with a translation system that dynamically offers a list of translations that best completes the part of the sentence already translated. Stochastic finite-state transducer technology is proposed to support this CAT system. The system was assessed on two real tasks of different complexity in several languages.", "authors": ["Jorge Civera", "Juan Miguel Vilar", "Elsa Cubel", "Antonio L. Lagarda", "Sergio Barrachina", "Francisco Casacuberta", "Enrique Vidal"], "n_citation": 0, "title": "A novel approach to computer-assisted translation based on finite-state transducers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cef35a97-202a-488e-8e0d-9a1da695b3d5"}
{"abstract": "We study the computational complexity of relay placement in energy-constrained wireless sensor networks. The goal is to optimise balanced data gathering, where the utility function is a weighted sum of the minimum and average amounts of data collected from each sensor node. We define a number of classes of simplified relay placement problems, including a planar problem with a simple cost model for radio communication. We prove that all of these problem classes are NP-hard, and that in some cases even finding approximate solutions is NP-hard.", "authors": ["Jukka Suomela"], "n_citation": 0, "title": "Computational complexity of relay placement in sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "defbe572-e76d-49ea-b8d2-f07920f589e9"}
{"abstract": "Our research on coaching refers to one autonomous agent providing advice to another autonomous agent about how to act. In past work, we dealt with advice-receiving agents with fixed strategies, and we now consider agents which are learning. Further, we consider agents which have various limitations, with the hypothesis that if the coach adapts its advice to those limitations, more effective learning will result. In this work, we systematically explore the effect of various limitations upon the effectiveness of the coach's advice. We state the two learning problems faced by the coach and the coached agents, and empirically study these problems in a predator-prey environment. The coach has access to optimal policies for the environment, and advises the predator on which actions to take. We experiment with limitations on the predator agent's actions, the bandwidth between the coach and agent, and the memory size of the agent. We analyze the results which show that coaching can improve agent performance in the face of all these limitations.", "authors": ["Patrick Riley", "Manuela M. Veloso"], "n_citation": 50, "title": "Coaching advice and adaptation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e5a4f861-3bd6-43f6-b69c-a4727d69b971"}
{"abstract": "A chunking algorithm with a Markov model is extended to accept bracketing constraints. The extended algorithm is implemented by modifying a state-of-the-art Japanese dependency parser. Then the effect of bracketing constraints in preventing parsing errors is evaluated. A method for improving the parser's accuracy is proposed. That method adds brackets according to a set of optimal brackets obtained from a training corpus. Although the method's coverage is limited, the F-measure for the sentences to which the method adds brackets is improved by about 7%.", "authors": ["Takashi Miyata", "K\u00f4iti Hasida"], "n_citation": 0, "title": "Error-driven learning with bracketing constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ed8583b8-ba47-487c-a378-765ca170f132"}
{"abstract": "Finite state transducers can be automatically learnt from bilingual corpus, and they can be easily integrated in an automatic speech recognition system for speech translation applications. In this work we explore the possibility of using k-testable language models to generate translations models. We report speech translation results for one easy and well known task, Eu Trans (Spanish-English), and for other similar task, Euskal Turista (Spanish-Basque). Euskal Turista has proved to be a quite difficult task because of the distance between the languages involved.", "authors": ["Alicia P\u00e9rez", "Francisco Casacuberta", "In\u00e9s Torres", "V\u00edctor G. Guijarrubia"], "n_citation": 0, "title": "Finite state transducers based on k-TSS grammars for speech translation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f5219d7d-1dcc-41a1-be8d-1ab35f4f4184"}
{"abstract": "We propose algorithms for maintaining two variants of kd-trees of a set of moving points in the plane. A pseudo kd-tree allows the number of points stored in the two children to differ by a constant factor. An overlapping kd-tree allows the bounding boxes of two children to overlap. We show that both of them support range search operations in O(n 1/2+ \u2208) time, where \u2208 only depends on the approximation precision. As the points move, we use event-based kinetic data structures to update the tree when necessary. Both trees undergo only a quadratic number of events, which is optimal, and the update cost for each event is only polylogarithmic. To maintain the pseudo kd-tree, we develop algorithms for computing an approximate median level of a line arrangement, which itself is of great interest. We show that the computation of the approximate median level of a set of lines or line segments can be done in an online fashion smoothly, i.e., there are no expensive updates for any events. For practical consideration, we study the case in which there are speed-limit restrictions or smooth trajectory requirements. The maintenance of the pseudo kd-tree, as a consequence of the approximate median algorithm, can also adapt to those restrictions.", "authors": ["Pankaj K. Agarwal", "Jie Gao", "Leonidas J. Guibas"], "n_citation": 0, "title": "Kinetic medians and kd-trees", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0494b2eb-66d9-4ed9-88ee-f2a77ab7e683"}
{"abstract": "We propose an algorithm for coordinating access to a shared broadcast channel in an ad hoc network of unknown size n. We reduce the runtime necessary to self-organize access to the channel over the previous algorithm of Cai, Lu and Wang. The runtime of this algorithm is O(n), our work is to improve the constant factors involved. Apart from experimental evidence of algorithm quality, we provide a rigorous probabilistic analysis of its behavior.", "authors": ["Jacek Cicho\u0144", "Miros\u0142aw Kuty\u0142owski", "Marcin Zawada"], "n_citation": 0, "title": "Adaptive Initialization Algorithm for Ad Hoc Radio Networks with Carrier Sensing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "054efe30-e655-4710-a56e-e1a5675a611c"}
{"authors": ["Philippe Smets"], "n_citation": 71, "title": "Belief functions versus probability functions", "venue": "Lecture Notes in Computer Science", "year": 1988, "id": "12e5e4bc-fe34-481f-8de9-d3bba563ba92"}
{"abstract": "Given a biconnected graph G = (V, E) with edge {s, t} E E, an st-ordering is an ordering v 1 ,..., v n  of V such that s = v 1 , t = v n , and every other vertex has both a higher-numbered and a lower-numbered neighbor. Previous linear-time st-ordering algorithms are based on a preprocessing step in which depth-first search is used to compute lowpoints. The actual ordering is determined only in a second pass over the graph. We present a new, incremental algorithm that does not require lowpoint information and, throughout a single depth-first traversal, maintains an st-ordering of the biconnected component of {s, t} in the traversed subgraph.", "authors": ["Ulrik Brandes"], "n_citation": 0, "title": "Eager st-ordering", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1c81706a-c59d-434f-8f06-47a0d03d10b0"}
{"authors": ["Miros\u0142aw Kowaluk", "Andrzej Lingas"], "n_citation": 0, "title": "Unique Lowest Common Ancestors in Dags Are Almost as Easy as Matrix Multiplication", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "40ba399c-e0e6-4826-a88a-645f84b060fc"}
{"abstract": "The achromatic number of a graph is the largest number of colors needed to legally color the vertices of the graph so that adjacent vertices get different colors and for every pair of distinct colors c 1 ,c 2  there exists at least one edge whose endpoints are colored by c 1 , c 2 . We give a greedy O(n 4/5 ) ratio approximation for the problem of finding the achromatic number of a bipartite graph with n vertices. The previous best known ratio was n log log n/ log n [12]. We also establish the first non-constant hardness of approximation ratio for the achromatic number problem; in particular, this hardness result also gives the first such result for bipartite graphs. We show that unless NP has a randomized quasi-polynomial algorithm, it is not possible to approximate achromatic number on bipartite graph within a factor of (ln n) 1/4- \u2208. The methods used for proving the hardness result build upon the combination of one-round, two-provers techniques and zero-knowledge techniques inspired by Feige et.al. [6].", "authors": ["Guy Kortsarz", "Sunil M. Shende"], "n_citation": 0, "title": "Approximating the achromatic number problem on bipartite graphs", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "47e274af-ea81-4baf-a39f-a6b408c920d2"}
{"abstract": "We prove that sorting 13 elements requires 34 comparisons. This solves a long-standing problem posed by D. E. Knuth in his famous book The Art of Computer Programming, Volume 3, Sorting and Searching. The result is due to an efficient implementation of an algorithm for counting linear extensions of a given partial order. We present also some useful heuristics which allowed us to decrease the running time of the implementation.", "authors": ["Marcin Peczarski"], "n_citation": 0, "title": "Sorting 13 Elements Requires 34 Comparisons", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "60b7dee9-6d9b-4991-a94e-7f24d86b3f75"}
{"abstract": "LB-triang, an algorithm for computing minimal triangulations of graphs, was presented by Berry in 1999 [1], and it gave a new characterization of minimal triangulations. The time complexity was conjectured to be O(nm), but this has remained unproven until our result. In this paper we present and prove an O(nm) time implementation of LB-triang, and we call the resulting algorithm LB-treedec. The data structure used to achieve this time bound is tree decomposition. We also report from practical runtime tests on randomly generated graphs which indicate that the expected behavior is even better than the proven bound.", "authors": ["Pinar Heggernes", "Yngve Villanger"], "n_citation": 50, "title": "Efficient implementation of a minimal triangulation algorithm", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "614eaa78-d25b-4e9c-a898-7d71eb2b1e6e"}
{"authors": ["Valerie Barr"], "n_citation": 0, "title": "Gender diversity in computing: are we making any progress?", "venue": "Communications of The ACM", "year": 2017, "id": "6c1ada51-8ac5-4355-a881-ac4e5cea370b"}
{"abstract": "We consider the problem of minimizing the number of wavelength interchangers in the design of wide-sense nonblocking cross-connects for wavelength division multiplexed (WDM) optical networks. The problem is modeled as a graph theoretic problem that we call dynamic edge coloring. In dynamic edge coloring the nodes of a graph are fixed but edges appear and disappear, and must be colored at the time of appearance without assigning the same color to adjacent edges. For wide-sense nonblocking WDM cross-connects with k input and k output fibers, it is straightforward to show that 2k-1 wavelength interchangers are always sufficient. We show that there is a constant c > 0 such that if there are at least ck 2  wavelengths then 2k-1 wavelength interchangers are also necessary. This improves previous exponential bounds. When there are only 2 or 3 wavelengths available, we show that far fewer than 2k-1 wavelength interchangers are needed. However we also prove that for any e > 0 and k > 1/2e, if the number of wavelengths is at least 1/e 2  then 2(1-e)k wavelength interchangers are needed.", "authors": ["Penny E. Haxell", "April Rasala", "Gordon T. Wilfong", "Peter Winkler"], "n_citation": 50, "title": "Wide-sense nonblocking WDM cross-connects", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7ab7905e-9456-446d-80cf-70843e8b2dcc"}
{"authors": ["Pankaj K. Agarwal", "Danny Z. Chen", "Shashidhara K. Ganjugunte", "Ewa Misio\u0142ek", "Micha Sharir", "Kai Tang"], "n_citation": 0, "title": "Stabbing convex polygons with a segment or a polygon", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "9b4e7586-ba96-436d-b744-bdb1623b9c42"}
{"abstract": "A number of fields, including genome rearrangements and interconnection network design, are concerned with sorting permutations in \"as few moves as possible\", using a given set of allowed operations. These often act on just one or two segments of the permutation, e.g. by reversing one segment or exchanging two segments. The cycle graph of the permutation to sort is a fundamental tool in the theory of genome rearrangements. In this paper, we present an algebraic reinterpretation of the cycle graph as an even permutation, and show how to reformulate our sorting problems in terms of particular factorisations of the latter permutation. Using our framework, we recover known results in a simple and unified way, and obtain a new lower bound on the prefix transposition distance (where a prefix transposition displaces the initial segment of a permutation), which is shown to outperform previous results. Moreover, we use our approach to improve the best known lower bound on the prefix transposition diameter from 2n/3 to [3n+1/4]. \u00a9 2008 Springer-Verlag Berlin Heidelberg.", "authors": ["Anthony Labarre"], "n_citation": 50, "title": "Edit distances and factorisations of even permutations", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "c08b2168-b8a2-4a6e-8e2a-2564e71b7770"}
{"abstract": "I present a new method of longest match pattern matching based on weighted finite state automata. Contrary to the approach of Karttunen [9] we do not need expensive complementation operations to construct the pattern matching transducer.", "authors": ["Thomas Hanneforth"], "n_citation": 0, "title": "Longest-match pattern matching with weighted finite state automata", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cd548e92-f94b-41ea-8a82-9ad36b767c88"}
{"authors": ["Angel Llamazares", "Eduardo J. Molinos", "Manuel Oca\u00f1a", "Luis Miguel Bergasa", "Noelia Hern\u00e1ndez", "Fernando Herranz"], "n_citation": 50, "title": "3D Map Building Using a 2D Laser Scanner", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "d504d80e-bec0-43f4-b4b8-bf0c0a9cd9e7"}
{"abstract": "Given a graph and pairs s i t i  of terminals, the edge-disjoint paths problem is to determine whether there exist s i t i paths that do not share any edges. We consider this problem on acyclic digraphs. It is known to be NP-complete and solvable in time n O(k)  where k is the number of paths. It has been a long-standing open question whether it is fixed-parameter tractable in k. We resolve this question in the negative: we show that the problem is W[1]-hard. In fact it remains W[1]-hard even if the demand graph consists of two sets of parallel edges. On a positive side, we give an O(m + k! n) algorithm for the special case when G is acyclic and G + H is Eulerian, where H is the demand graph. We generalize this result (1) to the case when G + H is nearly Eulerian, (2) to an analogous special case of the unsplittable flow problem. Finally, we consider a related NP-complete routing problem when only the first edge of each path cannot be shared, and prove that it is fixed-parameter tractable on directed graphs.", "authors": ["Aleksandrs Slivkins"], "n_citation": 0, "title": "Parameterized tractability of edge-disjoint paths on directed acyclic graphs", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e5e52e6a-4475-457c-9b09-a178a3e772b0"}
{"abstract": "With over 700 entries, this is the most comprehensive bibliography of the machine learning systems introduced by John Holland.", "authors": ["Tim Kovacs"], "n_citation": 0, "title": "The 2003 learning classifier systems bibliography", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "4a0aa33f-b0f4-45aa-953f-8159a453c0d4"}
{"abstract": "We introduce covrel, an adaptive software testing approach based on the combined use of operational profile and coverage spectrum, with the ultimate goal of improving the delivered reliability of the program under test. Operational profile-based testing is a black-box technique that selects test cases having the largest impact on failure probability in operation; as such, it is considered well suited when reliability is a major concern. Program spectrum is a characterization of a program's behavior in terms of the code entities (e.g., branches, statements, functions) that are covered as the program executes. The driving idea of covrel is to complement operational profile information with white-box coverage measures based on count spectra, so as to dynamically select the most effective test cases for reliability improvement. In particular, we bias operational profile-based test selection towards those entities covered less frequently. We assess the approach by experiments with 18 versions from 4 subjects commonly used in software testing research, comparing results with traditional operational and coverage testing. Results show that exploiting operational and coverage data in a combined adaptive way actually pays in terms of reliability improvement, with covrel overcoming conventional operational testing in more than 80% of the cases.", "authors": ["Antonia Bertolino", "Breno Miranda", "Roberto Pietrantuono", "Stefano Russo"], "n_citation": 0, "references": ["00f74881-dd85-4eb8-8d80-663f8ecccef1", "02b00093-2b2f-46bc-8ceb-c6865196b12b", "039c16b7-d16c-4928-842c-5c031966d940", "0464afb5-9ba6-40ee-9242-a06516e7d207", "07e871cb-89f8-4ae9-afa7-64acbb25cb8e", "14267265-46eb-4192-9c0a-eff0ad70fc0e", "20734eb0-ccef-43e6-9499-53880a309716", "20c1899d-8c33-4291-98fa-8ed2c54d0f64", "22fc1b74-f122-46b8-a795-8d361d2db914", "24a579d3-c45c-4cce-b83c-b134b4abfc91", "2ecd5d03-99fa-4407-ad9e-565c4e2982d3", "31c9e8e6-4495-4dc5-994c-bacc1050ab06", "3439d1b6-2280-400f-a9bc-c2075511dea7", "39e9307a-7725-4156-99d5-d09c02a33aad", "3a1e2c92-50d6-4356-b96b-115113a34fcc", "4a1e0ca0-605c-4ad9-891e-303ed0a11d98", "606ce1ad-ca8c-4bd7-9ced-f3a8fd12c2d9", "6955a99b-479b-4790-8efa-7841628bf127", "69f369c6-98ba-4ce0-8bde-d952a65875a5", "7b1aa892-d7d5-4557-a3c9-94159c9d2891", "822bf1c1-1f89-46c4-9b7b-b3ea4fce4fb7", "8253e62d-ce6f-4e11-b484-1d5cd1cf02ff", "873c3b36-6b4a-4c98-8707-6d3009ce15c2", "90ae8084-01b4-43fc-9fe5-0a4ffdafa37e", "b1a2b257-17e0-4ddf-90b9-cf39cdc9d775", "d71186ea-076b-4388-be1f-f71eaefc963d", "e9d81875-89f0-44fe-a390-72e2590fbc46", "ea69dddb-df25-47e8-88a4-df84a7bab603"], "title": "Adaptive coverage and operational profile-based testing for reliability improvement", "venue": "international conference on software engineering", "year": 2017, "id": "1dad6d61-1e94-4d55-98ae-fc2f2f05c2b0"}
{"authors": ["Marek Cygan", "\u0141ukasz Kowalik", "Marcin Mucha", "Marcin Pilipczuk", "Piotr Sankowski"], "n_citation": 0, "title": "Fast Approximation in Subspaces by Doubling Metric Decomposition", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "f316bd86-e691-4ed8-947a-ef4c49bcacbe"}
{"abstract": "We study routing problems in networks that require guaranteed reliability against multiple correlated link failures. We consider two different routing objectives: The first ensures \u201clocal reliability,\u201d i.e., the goal is to route so that each connection in the network is as reliable as possible. The second ensures \u201cglobal reliability,\u201d i.e., the goal is to route so that as few as possible connections are affected by any possible failure. We exhibit a trade-off between the two objectives and resolve their complexity and approximability for several classes of networks. Furthermore, we propose approximation algorithms and heuristics. We perform experiments to evaluate the heuristics against optimal solutions that are obtained using an integer linear programming solver. We also investigate up to what degree the routing trade-offs occur in randomly generated instances.", "authors": ["Stamatis Stefanakos"], "n_citation": 0, "references": ["0205b56c-bf70-4491-9984-15e5f6ae5674", "0653edf7-1e18-4bcd-8e6e-af11d1ce93ce", "0a56edb8-bb88-4131-afde-7747a78796ba", "23926133-51c6-4c6a-a4d8-cd53cb218d18", "4283029b-b2fa-423e-9fb4-197f18cba8a9", "4dc14a4e-6fa4-4015-b321-fe11ab227fc6", "6daea404-7640-4e9a-a26e-b4812b92f11e", "763dfe06-fdf3-4be0-9e6b-46fb779f5088", "a4212fe4-f769-42d4-a8c6-e543540818db", "b127084b-b45a-4ef5-9de8-e28c080c9b5c", "ca1d3032-e288-42f1-9228-f95da0535c0c", "cc7ed4e6-efd0-4283-9c56-8765a58f81a3", "d50a0dd0-e524-4b3a-a0ea-edeea9a34d05", "e8ba19cc-3a55-40c5-ba5b-81d0733ed8c7", "ffcaad21-0688-435d-8bb0-aa8a9b253430"], "title": "Reliable routings in networks with generalized link failure events", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a1104ae9-e13d-48b1-bf00-a8fd4c0d60f1"}
{"abstract": "This paper presents experimental results of our model-driven approach to agent based simulation. According to this, the development process for agent based simulation should focus on modeling rather than implementation (i.e., programming on some concrete agent simulation platform). This requires the use of transformation tools from simulation models to implementation code. Describing social phenomena with a visual, high-level modeling language and implementing the simulation in this way should facilitate the use of toolkits by experts in social sciences without a deep knowledge of programming concerns. This simulation approach is supported here by the INGENIAS Development Kit (IDK), which provides a model editor for multi-agent systems, and code generation support. To validate the transformation mechanism, we have modeled a concrete social system with the INGENIAS agent-oriented modeling language, and generated two independent implementations for two different platforms (Repast and Mason simulation toolkits). This experimentation shows the feasibility of the model driven implementation approach and has enabled the study of facilities provided by simulation toolkits that can have impact on the transformation process, in particular, the scheduling techniques. Also, comparing the simulation results of the case study with the original work and between implementations has led us to discover biases introduced by simulation mechanisms that can be found in most simulation platforms.", "authors": ["Candelaria E. Sansores", "Juan Pav\u00f3n"], "n_citation": 50, "title": "Agent based simulation for social systems : From modeling to implementation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "43fcba8e-7695-4379-bf92-5e19f7f85c82"}
{"authors": ["Zhaowei Miao", "Ke Fu", "Qi Fei", "Fan Wang"], "n_citation": 0, "title": "Meta-heuristic algorithm for the transshipment problem with fixed transportation schedules", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "8469ca3c-1fc8-4bae-8130-fc2541b7cd96"}
{"abstract": "Wilson's XCS has rapidly become the most popular classifier system of all time, and is a major focus of current research. XCS's primary distinguishing feature is that it bases rule fitness on the accuracy with which rules predict reward, rather than the magnitude of the reward predicted (as traditional, strength-based systems do). XCS is a complex system and differs from other systems in a number of ways. In order to isolate the source of XCS's adaptive power, and, in particular to study the difference between strength and accuracy-based fitness, we introduce a system called Strength-Based XCS (SB-XCS), which is as similar to the accuracy-based XCS as we could make it, apart from being strength-based. This work provides a specification of SB-XCS and initial results for it and XCS on the 6 multiplexer and woods2 tasks. It then analyses the solutions found by the two systems and finds that each prefers a particular type of solution. A sequel paper provides further analysis.", "authors": ["Tim Kovacs"], "n_citation": 0, "title": "XCS's strength-based twin: Part II", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "da9b20ca-404c-4f5b-bfb5-6b8f7a891ec9"}
{"authors": ["Amanda Whitbrook", "Qinggang Meng", "Paul Wai Hing Chung"], "n_citation": 0, "references": ["1d75c168-8cbc-4a6c-b5be-c6d5677bd8cd", "3ee2f21a-c048-42a8-a478-a1fe456e2d7a", "44865ff1-f997-4c6d-ad34-0266def0d0a6", "4e31af81-0f1a-45f2-9ad2-a124e75b0c9f", "66b4d3a5-0ed1-4a5b-a843-3b3c95cff641", "6ff38722-9236-4211-a875-2e700a9d6a3c", "840486c6-787b-44ee-9fb8-c37e17cfba2b", "856e6970-af29-491a-b7bc-ef97d9cc7d8c", "88e7a36a-d13d-483b-98ba-f7744439873c", "8cc8d5d3-d9b9-4135-b497-5c56216b74fc", "926f0bc0-7973-4d89-8702-519b35988e1b", "9f6ba833-23ba-4cdc-b488-47c978b843d7", "a4541d58-b65d-4342-b1aa-1e422093421a", "ce913870-1c74-4428-a946-e258f07242f1", "dadc91fa-b064-4272-b9e7-81c93ee473b2", "e2ed2056-414c-4d5e-ac6b-ebee9400404f", "e35da921-d993-4f9d-9c77-843019dc7753"], "title": "A robust, distributed task allocation algorithm for time-critical, multi agent systems operating in uncertain environments", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "276390f6-2308-46fe-bd48-91c811cb761b"}
{"authors": ["Gabor Aranyi", "Marc Cavazza", "Fred Charles"], "n_citation": 0, "title": "Using fNIRS for Prefrontal-Asymmetry Neurofeedback: Methods and Challenges", "venue": "Lecture Notes in Computer Science", "year": 2015, "id": "37bf7574-f988-4d2d-b81f-67b46c545ba6"}
{"authors": ["Weiming Xiang", "Taylor T. Johnson"], "n_citation": 0, "title": "On Reachable Set Estimation for Discrete-Time Switched Linear Systems under Arbitrary Switching", "venue": "advances in computing and communications", "year": 2017, "id": "91173cf2-e1a2-4c1a-9a54-d8151abc0f1d"}
{"abstract": "A significant property of temporal data is their richness of semantics. Although several temporal data models and query languages have been designed specifically to handle the temporal data, users must still deal with much of the implicit temporal information, which can be automatically derived from the stored data in certain situations. We propose a multidatabase architecture where an appropriate formalization of the intended semantics is associated with each temporal relation and temporal database. This allows a temporal mediator to access the databases to retrieve implicit information in terms of time granularities different from those used to store data. We also describe how the temporal mediator can provide a user interface to the multidatabase system allowing temporal queries in terms of arbitrary granularities and involving relations in different TDBMS.", "authors": ["Claudio Bettini", "Xiaoyang Sean Wang", "Sushil Jajodia"], "n_citation": 0, "title": "An architecture for supporting interoperability among temporal databases", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "cdb330e0-ab2f-4838-911f-812042659959"}
{"abstract": "Length and area regularization are commonplace for inverse problems today. It has however turned out to be much more difficult to incorporate a curvature prior. In this paper we propose several improvements to a recently proposed framework based on global optimization. We identify and solve an issue with extraneous arcs in the original formulation by introducing region consistency constraints. The mesh geometry is analyzed both from a theoretical and experimental viewpoint and hexagonal meshes are shown to be superior. We demonstrate that adaptively generated meshes significantly improve the performance. Our final contribution is that we generalize the framework to handle mean curvature regularization for 3D surface completion and segmentation. (Less)", "authors": ["Petter Strandmark", "Fredrik Kahl"], "n_citation": 50, "title": "Curvature Regularization for Curves and Surfaces in a Global Optimization Framework", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "42c4dbb6-9f6c-436c-8997-e744b14c21f7"}
{"abstract": "Local feature detection and description have gained a lot of interest in recent years since photometric descriptors computed for interest regions have proven to be very successful in many applications. In this paper, we propose a novel interest region descriptor which combines the strengths of the well-known SIFT descriptor and the LBP texture operator. It is called the center-symmetric local binary pattern (CS-LBP) descriptor. This new descriptor has several advantages such as tolerance to illumination changes, robustness on flat image areas, and computational efficiency. We evaluate our descriptor using a recently presented test protocol. Experimental results show that the CS-LBP descriptor outperforms the SIFT descriptor for most. of the test cases, especially for images with severe illumination variations.", "authors": ["Marko Heikkil\u00e4", "Matti Pietik\u00e4inen", "Cordelia Schmid"], "n_citation": 183, "title": "Description of interest regions with center-symmetric local binary patterns", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b0bb3049-334a-4f3e-b86b-16f7d5df7723"}
{"authors": ["Ivan Blecic", "Arnaldo Cecchini", "Tanja Congiu", "Giovanna Fancello", "Giuseppe A. Trunfio"], "n_citation": 0, "title": "Walkability Explorer: An Evaluation and Design Support Tool for Walkability", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "11b7bfed-6d84-4e91-844d-b56eaa8937b2"}
{"abstract": "To perform higher-order matching, we need to decide the /3?7-equivalence on A-terms. The first way to do it is to use simply typed A-calculus and this is the usual framework where higher-order matching is performed. Another approach consists in deciding a restricted equivalence based on finite superdevelopments. We consider higher-order matching modulo this equivalence over untyped A-terms for which we propose a terminating, sound and complete matching algorithm. This is in particular of interest since all second-order \u03b2-matches are matches modulo superdevelopments. We further propose a restriction to second-order matching that gives exactly all second-order matches.", "authors": ["Germain Faure"], "n_citation": 0, "title": "Matching modulo superdevelopments application to second-order matching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0b088728-af8b-4aba-9af1-f9914c71ae40"}
{"abstract": "With the advent of hardware technologies, high-performance parallel computers and commodity clusters are becoming affordable. However, complexity of parallel application development remains one of the major obstacles towards the mainstream adoption of parallel computing. As one of the solution techniques, researchers are actively investigating the pattern-based approaches to parallel programming. As re-usable components, patterns are intended to ease the design and development phases of a parallel applications. While using patterns, a developer supplies the application specific code-components whereas the underlying environment generates most of the code for parallelization. PAS (Parallel Architectural Skeleton) is one such pattern-based parallel programming model and tool, which defines the architectural aspects of parallel computational patterns. Like many other pattern-based models and tools, the PAS model was hampered by its lack of extensibility, i.e., lacking of support for the systematic addition of new skeletons to an existing skeleton repository. Lack of extensibility significantly reduces the flexibility and hence the usability of a particular approach. SuperPAS is an extension of PAS that defines a model for systematically designing and implementing PAS skeletons by a skeleton designer. The newly implemented skeletons can subsequently be used by an application developer, SuperPAS model is realized through a Skeleton Description Language (SDL), which assists both a skeleton designer and an application developer. The paper discusses the SuperPAS model through examples that use the SDL. The paper also discusses some of the recent usability and performance studies, which demonstrate that SuperPAS is a practical and usable parallel programming model and tool.", "authors": ["Mohammad Mursalin Akon", "Dhrubajyoti Goswami", "Hon Fung Li"], "n_citation": 0, "title": "A model for designing and implementing parallel applications using extensible architectural skeletons", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "00a072ea-fe2b-41c7-9863-a30020df1a93"}
{"abstract": "The next-generation wireless networks are evolving toward an IP-based network that can provide various multimedia services seamlessly. To establish such a wireless mobile Internet, the registration domain supporting fast handoff is integrated with the DiffServ mechanism. In this paper, a Distributed-request-based CDMA DiffServ call admission control (CAC) scheme is proposed for the evolving mobile Internet. Numerical examples show that the forced termination ratio of handoff calls is guaranteed to be much less than the blocking ratio of new calls for a seamless fast-handoff while proposed scheme provides quality of service guarantee for each service class efficiently.", "authors": ["Kyeong Hurl", "Hyung-Kun Park", "Yeonwoo Lee", "Seon Wook Kim", "Doo Seop Eom"], "n_citation": 0, "references": ["39248b9a-7ad7-429a-8908-e2a78d3f3c1d", "d7eb99c8-f750-4474-8fbc-1443157cf068", "f4d1bedd-d046-42f3-ae6b-c8cad7ddd955"], "title": "A Distributed-Request-Based DiffServ CAC for Seamless Fast-Handoff in Mobile Internet.", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "09e0e88b-4558-4019-9626-9fabe22f2b5e"}
{"authors": ["Ren Zhang", "Bart Preneel"], "n_citation": 0, "title": "Publish or Perish: A Backward-Compatible Defense against Selfish Mining in Bitcoin", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "0a5596b5-bbdd-4a6f-938d-0bec295f7f01"}
{"authors": ["Michel Abdalla", "Alexander W. Dent", "John Malone-Lee", "Gregory Neven", "Phan D.H.", "Nigel P. Smart"], "n_citation": 50, "title": "Identity-based traitor tracing", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "0b289e60-a6f2-49b0-b1bb-91ebfceb9ddb"}
{"authors": ["Catherine C. Marshall", "Frank M. Shipman"], "n_citation": 0, "references": ["5518c8f7-d115-4432-bfa9-428fa43d09d7", "81704b49-df10-4b26-a905-eb5ef297e7dc", "8ec14976-009b-468a-be72-24cfcb00d0e3", "a9b1557f-a90e-46f4-b9e9-2076ae981ad8", "b08307a2-8fdc-4b75-8b5d-f7dfd8f49b8a", "bf175a68-f6e4-4cd7-98aa-ed85445eec88", "cf78e448-d7e7-43a8-b1cf-41fbeb0007ff", "d6a75c41-8d73-4d46-8f05-218f4243c1a9", "da0d6735-a27a-4f25-98aa-2dc5784c02a7", "daf3b652-4e1d-4c52-9099-b90a5f56be30", "e6f0af72-a34a-4003-ad41-ed3e8874ead1", "f4136db4-93d2-4ed5-88ef-9bba9750a569", "f81f0548-fa80-4979-b783-17a714816e9d"], "title": "Who owns the social web", "venue": "Communications of The ACM", "year": 2017, "id": "0f4357d2-4f3d-4151-af61-fcaa3c2d626d"}
{"abstract": "Agent technology is often claimed to be the most natural approach for automating e-commerce business processes. Despite these claims, up till now, the most successful e-commerce systems are still based on humans to make the most important decisions in various stages of an e-commerce transaction. Consequently, it is difficult to find successful actually implemented and working large-scale agent-based e-commerce applications to confirm agents superiority. Here, we discuss an abstract e-commerce environment that allows agents of different types to interact with each other and operate with an overarching goal of supporting an. e-commerce transaction. A prototype system that implements this vision using JADE agent platform is also described. Finally, we report, on experiments with the implemented system skeleton.", "authors": ["Costin Badica", "Maria Ganzha", "Marcin Paprzycki", "Amalia Pirvanescu"], "n_citation": 50, "title": "Experimenting with a multi-agent e-commerce environment", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "156c4756-8f75-48c0-ab0c-49d2be05e61a"}
{"authors": ["Michael Kai Petersen", "Andrius Butkus"], "n_citation": 50, "title": "Modeling moods in BBC programs based on emotional context", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "1687c4ee-a211-451a-b27f-7dcb65353d45"}
{"abstract": "Computer systems usually rely on hardware counters and software instrumentation to acquire performance information about the cache access behavior. These approaches either provide only limited data or are restricted in their applicability. This paper introduces a novel approach based on a hardware cache monitoring facility that exhibits both the details of traditional software mechanisms and the low-overhead of hardware counters. More specially, the cache monitor can be combined with any location of the memory hierarchy and present a detailed view of the complete memory access behavior of applications. The monitoring concept has been verified using a multiprocessor simulator. Initial experimental results show its feasibility in terms of hardware design and functionality with respect to providing comprehensive performance data.", "authors": ["Jie Tao", "J\u00fcrgen Jeitner", "Carsten Trinitis", "Wolfgang Karl", "Josef Weidendorfer"], "n_citation": 0, "title": "Comprehensive cache inspection with hardware monitors", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "19e408c5-4bb5-4545-8d2a-41be7907cee8"}
{"abstract": "We present a method to convert (i) an operational semantics for a given machine language, and (ii) an off-the-shelf theorem prover, into a high assurance verification condition generator (VCG). Given a program annotated with assertions at cutpoints, we show how to use the theorem prover directly on the operational semantics to generate verification conditions analogous to those produced by a custom-built VCG. Thus no separate VCG is necessary, and the theorem prover can be employed both to generate and to discharge the verification conditions. The method handles both partial and total correctness. It is also compositional in that the correctness of a subroutine needs to be proved once, rather than at each call site. The method has been used to verify several machine-level programs using the ACL2 theorem prover.", "authors": ["John Matthews", "J. Strother Moore", "Sandip Ray", "Daron Vroon"], "n_citation": 0, "title": "Verification condition generation via theorem proving", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "24b8bd6a-db7e-4d62-b622-7440f0ec2cb7"}
{"abstract": "Much of the information on the web is indeed dynamic content provided through linkups with databases. However, due to heterogeneity of databases, it is difficult to provide an integrated information retrieval. Meanwhile, information on web databases can be easily provided to users through web interfaces. In analyzing web interfaces, therefore, an information integration system can integrate web databases without concerning the database structures. This paper presents a solution to semantic heterogeneity through the analysis of web interfaces and the building of semantic networks.", "authors": ["Jeong-Oog Lee", "Myeong-Cheol Ko", "Jin-Soo Kim", "Chang-Joo Moon", "Young-Gab Kim", "Hoh Peter In"], "n_citation": 0, "references": ["7d3726d8-21e5-4321-833d-dfde277c3693", "ab077515-f5ea-4028-b567-9ff9a13355be", "d906df7b-8345-4b0c-85a8-5cedc5f54f5a"], "title": "Analyzing Web Interfaces of Databases for Retrieving Web Information.", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2d3b1bc7-6afd-437d-85a1-35a5bd091c58"}
{"authors": ["Alin Drimus", "Arne Bilberg"], "n_citation": 50, "title": "Novel Approaches for Bio-inspired Mechano-Sensors", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "8703e490-211c-4aaf-8713-c191d653709e"}
{"authors": ["Baptiste Allain", "Mingxing Hu", "Laurence Lovat", "Richard Cook", "Tom Vercauteren", "Sebastien Ourselin", "David J. Hawkes"], "n_citation": 0, "title": "A System for Biopsy Site Re-targeting with Uncertainty in Gastroenterology and Oropharyngeal Examinations", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "2e238d82-5dfb-4b1b-b19f-62a0fa1757d6"}
{"authors": ["Leo Porter", "Cynthia Bailey Lee", "Beth Simon", "Mark Guzdial"], "n_citation": 0, "references": ["a0c0a241-1992-4fe0-a8c5-bd8c5f2e79a3", "fe34174c-3b75-4848-8ba0-5ef899272ef9"], "title": "Preparing tomorrow's faculty to address challenges in teaching computer science", "venue": "Communications of The ACM", "year": 2017, "id": "3a3ab4a0-8d28-4f98-8d14-a4de19dff007"}
{"authors": ["Bo Brinkman", "Catherine Flick", "Don Gotterbarn", "Keith W. Miller", "Kate Vazansky", "Marty J. Wolf"], "n_citation": 0, "title": "Listening to professional voices: draft 2 of the ACM code of ethics and professional conduct", "venue": "Communications of The ACM", "year": 2017, "id": "4335eb13-fdcd-4347-8a2b-5a794326f63c"}
{"abstract": "The k-set agreement problem is a generalization of the consensus problem: each process proposes a value, and each non-faulty process has to decide a value such that a decided value is a proposed value, and no more than k different values are decided. This paper presents a surprisingly simple protocol that solves the k-set agreement problem in synchronous systems prone to up to t < n processes can crash (where n is the total number of processes). The proposed protocol is the first early stopping k-set agreement protocol that does not impose a constraint on t. It allows the processes to decide and stop by min([f/k] + 2, [t/k] + 1) rounds where f is the number of actual crashes (0 < f < t). In addition to its conceptual simplicity, the protocol has an additional noteworthy feature, namely, it is particularly efficient in common case scenarios. This comes from the fact that it is based on a mechanism that allows the processes to take into account the actual pattern of failures and not only their number, thereby allowing the processes to decide in much less than [f/k] + 2 rounds in a lot of cases.", "authors": ["Philippe Ra\u00efpin Parv\u00e9dy", "Michel Raynal", "Corentin Travers"], "n_citation": 50, "title": "Early-stopping k-set agreement in synchronous systems prone to any number of process crashes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "43c24d68-8c9c-4846-bb78-8204eed83612"}
{"authors": ["Ce Zhang", "Christopher R\u00e9", "Michael J. Cafarella", "Christopher De Sa", "Alexander Ratner", "Jaeho Shin", "Feiran Wang", "Sen Wu"], "n_citation": 0, "references": ["04472ec2-ccc3-44bb-a718-277cb4855719", "0a50832c-f69b-4bdd-9a63-e49c5f678e8c", "0ebe7b5d-f8e2-4cc1-b4d5-c193b7a3f5a6", "126b1db6-8a45-4f11-a723-fd9310253fd9", "145e57eb-d121-435e-888e-066601384cd5", "2814987d-e0c0-4238-905c-1162ed7533fd", "2bc0a8d0-0dbc-433d-969d-5c1de6c517ed", "315271c0-1245-4e78-ba2c-3190768468bb", "40cd7f71-233e-4e38-bc91-601c7b5ee66b", "42511ba1-be88-47f3-8345-45cf67a0aa4e", "4aea48d0-ad0a-49d0-8d8d-254da7f88289", "52981273-36dc-429f-a12d-bada6edee02e", "52adac81-4d20-42c8-aa39-3695781454ef", "59586f05-5e5e-4eda-bb88-87e5aebba347", "62b64804-bee2-493c-a73e-f7798f7b2d8d", "6c17009e-415e-403e-b55e-23f547de5ef6", "6d734580-24cc-4532-9f1b-b4ad99835a58", "7030d6a2-aed0-4079-9bb7-a656ee3e6f78", "719a5559-b6be-48d2-8375-7387992f06f1", "7bc70a6c-221f-48a2-beb5-111e15133c8b", "7d4cc25b-e8e5-454f-b8ad-03f69cc5ccf3", "8437b22c-59d5-4107-b733-5e99d3045457", "869007f7-ac7f-49dd-9d24-1859396fd4db", "8e39ac03-ab94-4c28-a1cc-b4ec715e7c02", "923d1a0a-a46a-4528-b459-8a79c53434ca", "975ca2bd-6cc7-4fc3-b94f-4324045d7eda", "9eee7c7d-c63a-4d06-9e03-f67e83c69d16", "a2f53748-4caa-4026-b2e1-d499296da5ce", "a445d289-4be6-4564-b6f2-a71642724f8e", "a92f2d85-8736-4bb2-b448-0f969adfd0c2", "c72e787b-0a21-41c2-846f-525bf8765b1b", "cbf7acc5-98bb-485a-bf20-5e58af3e29fc", "e06f0623-bd0b-410a-9495-95cfcd101157", "e4218bc9-fb47-457e-b500-4d921251502d", "ecaf51e5-d073-41f5-a04c-b5ff79b56223", "ed53b45b-f2a2-449f-ad40-5995e7d0847b", "f0366074-a9a9-4328-8339-34404fad6c06", "f75ac4eb-b1aa-400c-ad9b-88fbcc02ce15", "fc8d6144-11cb-4150-bbbc-1ff48d3fd156", "fc969b34-ce5f-40f7-a915-ac1889c6e4a1", "fd3d707a-c237-4924-b5c3-037e49e1fcb8"], "title": "DeepDive: declarative knowledge base construction", "venue": "Communications of The ACM", "year": 2017, "id": "517286ce-f03d-43e4-a592-d2a72d05fd85"}
{"abstract": "The goal of our investigation is to find automatically the best rule for a cell in the cellular automata model. The cells are either of type OBSTACLE, EMPTY or CREATURE. Only CREATURE can move around in the cell space and can perform one of the four actions: if the path to the next cell is blocked: turn left or right, if the path is free: move ahead and simultaneously turn left or right. The task of the creature is to cross all empty cells with a minimum number of steps. The behavior was modeled using a variable state machine represented by a state table. Input to the state table is the neighbor's state in front of its moving direction. The goal is to find the absolutely best rule in the set of all possible rules. The search space grows exponentially with the number of states. As simulation, testing and evaluating the quality are very time consuming in software, the migration of the problem to a parallel hardware platform is a promising solution. In order to reduce the computation time, the search procedure was (1) implemented in hardware and (2) solutions which are equivalent under state permutations were not generated and (3) solutions which show or expect bad or trivial behavior were excluded as soon as possible in a preselection phase. Exactly six different five-state algorithms could be detected, which allow to cross all empty cells for all the given initial configurations. We described this model in Verilog HDL and in AHDL. A hardware synthesizing tool transforms the description into a configuration file which was loaded into a field programmable gate array (FPGA). Hardware implementation offers a significant speed up of many thousands compared to software.", "authors": ["Mathias Halbach", "Rolf Hoffmann"], "n_citation": 0, "title": "Optimal behavior of a moving creature in the cellular automata model", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5f4da7e6-1995-484a-a6c0-263c55d7912a"}
{"authors": ["Moshe Y. Vardi"], "n_citation": 0, "title": "Cyber insecurity and cyber libertarianism", "venue": "Communications of The ACM", "year": 2017, "id": "60698aa1-157a-48e5-9300-221751379431"}
{"authors": ["Vinton G. Cerf"], "n_citation": 0, "title": "Can liberty survive the digital age", "venue": "Communications of The ACM", "year": 2017, "id": "63154ca0-6a5f-46d2-8c5c-4475a8f3418f"}
{"abstract": "Generalized resources are defined as multisets of Petri net vertices. Here places represent material resources (designated by tokens residing in these places). Transitions correspond to activity resources represented by transition firings. Two generalized resources are called similar if in any Petri net marking one resource can be replaced by another without changing the observable system's behaviour (modulo bisimulation). In this paper we study some basic properties of generalized resource similarity and prove that, being undecidable, generalized resource similarity is finitely based, and thus can be finitely described. We show also, that similarity of generalized resources allows to express some substantial properties of systems modelled by Petri nets.", "authors": ["V. A. Bashkin", "Irina A. Lomazova"], "n_citation": 0, "title": "Similarity of generalized resources in Petri nets", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "63d2aca7-8ce2-4043-9bdb-e9b0efe00484"}
{"authors": ["Woodrow Hartzog", "Ira S. Rubinstein"], "n_citation": 0, "references": ["869f6b48-8d38-4d55-96d2-3e0cc26e9436"], "title": "The anonymization debate should be about risk, not perfection", "venue": "Communications of The ACM", "year": 2017, "id": "64e62219-345d-4c02-a6fc-dc67aacd5374"}
{"authors": ["David Batchelor"], "n_citation": 50, "references": [], "title": "Beyond 'Star Trek'", "venue": "Communications of The ACM", "year": 2017, "id": "72842f27-2a2f-42ea-9eb0-eefe7d9def3b"}
{"abstract": "The neural network approach for parallel construction of adaptive mesh on two-dimensional area is proposed. The approach is based on unsupervised learning algorithm for Kohonen's Self Organizing Map and enables to obtain an adaptive mesh being isomorphic to a rectangular uniform one. A parallel algorithm for the construction of those meshes based on master-slave programming model is presented. The main feature of the obtained mesh is that their decomposition into subdomains required for parallel simulation on this mesh is reduced to partitioning of a rectangular array of nodes. The way of partitioning may be defined based on parallel simulations on the mesh. The efficiency of the parallel realization of the proposed algorithm is about 90%.", "authors": ["Olga Nechaeva"], "n_citation": 50, "title": "Neural network approach for parallel construction of adaptive meshes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "78f0a29d-cb92-4256-b802-02b376d6e9b3"}
{"abstract": "The hotspot seriously degrades the performance of a parallel algorithm but there have not been many methods proposed for this problem. Without modification of mesh topology a reasonable method is fully utilizing all the links of the hotspot node. A new routing method that incorporates both minimal routes and non-minimal routes was proposed and approved with the hotspot traffic patterns. In particular the routing method decide on misrouting without the congestion detection. The routing method requires only little addition of hardware and it is relatively simple.", "authors": ["Minhwan Ok", "Myong-Soon Park"], "n_citation": 0, "title": "Minimizing hotspot delay by fully utilizing the link bandwidth on 2D mesh with virtual cut-through switching", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7e947ca1-587a-48ae-a461-583b3023a867"}
{"abstract": "In this paper  1 , a new formulation is proposed for the problem of constructing parallel abstract programs of a required length in parallel computing systems. The conditions of a planning problem are represented as a system of Boolean equations (constraints), whose solutions determine the possible plans for activating the program modules. Specifications of modules are stored in the knowledgebase of the planner. Constraint on number of processors and time delays at execution of modules are taken into account.", "authors": ["G. A. Oparin", "Alexei P. Novopashin"], "n_citation": 0, "title": "Planning of parallel abstract programs as boolean satisfiability", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8055c5e4-25b7-4758-a369-4d586785bdbf"}
{"authors": ["Bertrand Meyer"], "n_citation": 0, "references": ["789273e0-a009-47eb-85c4-c0e812cfa377"], "title": "Ending null pointer crashes", "venue": "Communications of The ACM", "year": 2017, "id": "813b107e-81dd-4ecf-ad7f-e6273dc15603"}
{"abstract": "This paper studies an emerging technology in network service market: smart routing technology. We focus on the economics issues related to the adoption of this new technology, such as contracting, bandwidth allocation and cost-benefit analysis. This study shows the advantage of deploying smart routers assuming efficient contract and usage, and quantifies the extra benefit they can generate. Interestingly, we find smart routing technology can also potentially help ISPs in todays sluggish service market: smart router users tend to contract more capacities and ISPs can raise their service prices without losing customers.", "authors": ["Rui Dai", "Dale O. Stahl", "Andrew B. Whinston"], "n_citation": 0, "references": ["31fa95c1-db50-4b0b-8a61-8dd73a1ef0d4"], "title": "The Economics of Smart Routing and Quality of Service.", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "819f034a-2d3b-401a-a41e-b2eead4af06b"}
{"authors": ["Alon Y. Halevy"], "n_citation": 0, "title": "Technical Perspective: Building knowledge bases from messy data", "venue": "Communications of The ACM", "year": 2017, "id": "8dbe2ff9-ae98-4055-b64a-004d83d1fa66"}
{"abstract": "Deduction modulo is a theoretical framework designed to introduce computational steps in deductive systems. This approach is well suited to automated theorem proving and a tableau method for first-order classical deduction modulo has been developed. We reformulate this method and give an (almost constructive) semantic completeness proof. This new proof allows us to extend the completeness theorem to several classes of rewrite systems used for computations in deduction modulo. We are then able to build a counter-model when a proof fails for these systems.", "authors": ["Richard Bonichon", "Olivier Hermant"], "n_citation": 0, "title": "A semantic completeness proof for TaMeD", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "96fe25b7-f1ce-4693-9048-3fa2f5f2ffef"}
{"abstract": "A class of fine-grained (FG) parallel models and algorithms is defined as a generalization of Cellular Automata (CA). It comprises all CA-modifications, in which two main CA-properties (locality and parallelism of intercell interaction) are preserved, no constraint being imposed on state alphabets and transition functions. A set of methods for composing a complex FG-algorithm out of a number of simple ones is proposed. To make compatible FG-algorithms with different alphabets, a number of algebraic operations on cellular arrays are introduced. The set of proposed composition methods has a two-level structure: the lower level comprises composition of cell transition functions, while the higher level deals with global operators on cellular arrays. For each type of proposed methods an example is given and the domain of application is determined.", "authors": ["Olga L. Bandman"], "n_citation": 50, "title": "Composing fine-grained parallel algorithms for spatial dynamics simulation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9bd1bce2-b51b-488c-9b42-9a2a52e62006"}
{"abstract": "Inference of phylogenetic trees comprising hundreds or even thousands of organisms based on the Maximum Likelihood (ML) method is computationally extremely intensive. In order to accelerate computations we implemented RAxML-OMP, an efficient OpenMP-parallelization for Symmetric Multi-Processing machines (SMPs) based on the sequential program RAxML-V (Randomized Axelerated Maximum Likelihood). RAxML-V is a program for inference of evolutionary trees based upon the ML method and incorporates several advanced search algorithms like fast hill-climbing and simulated annealing. We assess performance of RAxML-OMP on the widely used Intel Xeon, Intel Itanium, and AMD Opteron architectures. RAxML-OMP scales particularly well on the AMD Opteron architecture and achieves even super-linear speedups for large datasets (with a length > 5.000 base pairs) due to improved cache-efficiency and data locality. RAxML-OMP is freely available as open source code.", "authors": ["Alexandros Stamatakis", "Michael Ott", "Thomas Ludwig"], "n_citation": 171, "title": "RAxML-OMP : An efficient program for phylogenetic inference on SMPs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a16e51f0-9149-4601-8463-e50c4525f87b"}
{"authors": ["Robert Granger", "Philipp Jovanovic", "Bart Mennink", "Samuel Neves"], "n_citation": 0, "title": "Improved Masking for Tweakable Blockciphers with Applications to Authenticated Encryption", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "a8731998-42be-42ab-9a20-d5a9868ac03f"}
{"authors": ["Geoffrey Mainland", "Roman Leshchinskiy", "Simon L. Peyton Jones"], "n_citation": 50, "references": ["06c9dbcf-4cc8-4457-a2e0-b5aedcb0d65b", "07749ca3-7fff-4074-a20b-c143641e33aa", "4a2e2e1f-13e5-4435-9d61-dc5a02b6449e", "5f227056-ed54-4f20-aa5a-df4ce6725b6d", "7ccba7ef-b7a5-46da-bae3-70649a139230", "a8b79ea5-1d9a-4989-ba73-c3199bc90ab5", "b86b00ac-96bf-4aee-9294-54206897fecb", "d459709a-42db-4d7c-bc84-ea772e9beea8", "d45d18b5-c0cc-46b9-a815-45d0292217de", "d778f216-81b6-4ced-a2b6-3b1c4fac146b", "e3f78b7e-3aa3-4540-9ffa-e6a8f8d7b44f", "ea46ef1d-9fba-4182-871a-fbfe1b1fe09c", "f4c59d87-6746-4617-a02b-e190b0f26a1f"], "title": "Exploiting vector instructions with generalized stream fusion", "venue": "Communications of The ACM", "year": 2017, "id": "b1a7d7a1-238f-4716-9e47-e72c73b99b1f"}
{"abstract": "In this paper we show how the concepts of answer set programming and fuzzy logic can be succesfully combined into the single framework of fuzzy answer set programming (FASP). The framework offers the best of both worlds: from the answer set semantics, it inherits the truly declarative non-monotonic reasoning capabilities while, on the other hand, the notions from fuzzy logic in the framework allow it to step away from the sharp principles used in classical logic, e.g., that something is either completely true or completely false. As fuzzy logic gives the user great flexibility regarding the choice for the interpretation of the notions of negation, conjunction, disjunction and implication, the FASP framework is highly configurable and can, e.g., be tailored to any specific area of application. Finally, the presented framework turns out to be a proper extension of classical answer set programming, as we show, in contrast to other proposals in the literature, that there are only minor restrictions one has to demand on the fuzzy operations used, in order to be able to retrieve the classical semantics using FASP.", "authors": ["Davy Van Nieuwenborgh", "Martine De Cock", "Dirk Venneir"], "n_citation": 0, "title": "Fuzzy answer set programming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0047d7d5-e447-41c9-8368-c96df86ca3fa"}
{"abstract": "This paper proposes a method for generating classifiers from large datasets by building a committee of simple base classifiers using a standard boosting algorithm. It permits the processing of large datasets even if the underlying base learning algorithm cannot efficiently do so. The basic idea is to split incoming data into chunks and build a committee based on classifiers built from these individual chunks. Our method extends earlier work by introducing a method for adaptively pruning the committee. This is essential when applying the algorithm in practice because it dramatically reduces the algorithm's running time and memory consumption. It also makes it possible to efficiently race committees corresponding to different chunk sizes. This is important because our empirical results show that the accuracy of the resulting committee can vary significantly with the chunk size. They also show that pruning is indeed crucial to make the method practical for large datasets in terms of running time and memory requirements. Surprisingly, the results demonstrate that pruning can also improve accuracy.", "authors": ["Eibe Frank", "Geoffrey Holmes", "Richard Brendon Kirkby", "Mark A. Hall"], "n_citation": 0, "title": "Racing committees for large datasets", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0051014c-3bd6-409e-8173-986b281246fe"}
{"abstract": "The aim of this paper is to present some methods of handling discontinuity (and freer word order in general) within a medium-level grammatical framework. A context-free formalism and the backbone set of rules for verbal phrases are presented as the background for this paper. The main result consists in showing how discontinuous infinitive phrases and discontinuous noun phrases (interrogative phrases included) can be theoretically covered within the introduced formalism and similar grammatical frameworks. The second result reported in this paper is the cost-effectiveness analysis of introducing discontinuity rules into a medium-level grammatical framework: it turns out that attempting to cover some types of discontinuity may be unprofitable within a given grammatical framework. Although only examples from the Polish language are discussed, the described solutions are likely to be relevant for other languages with similar word order properties.", "authors": ["Filip Grali\u0144ski"], "n_citation": 0, "title": "Some methods of describing discontinuity in polish and their cost-effectiveness", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0075bf7d-9791-4296-9216-95cbf493d5af"}
{"abstract": "The two main challenges typically associated with mining data streams are concept drift and data contamination. To address these challenges, we seek learning techniques and models that are robust to noise and can adapt to changes in timely fashion. In this paper, we approach the stream-mining problem using a statistical estimation framework, and propose a discriminative model for fast mining of noisy data streams. We build an ensemble of classifiers to achieve adaptation by weighting classifiers in a way that maximizes the likelihood of the data. We further employ robust statistical techniques to alleviate the problem of noise sensitivity. Experimental results on both synthetic and real-life data sets demonstrate the effectiveness of this new discriminative model.", "authors": ["Fang Chu", "Yizhou Wang", "Carlo Zaniolo"], "n_citation": 0, "title": "Mining noisy data streams via a discriminative model", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "00cac2d9-fec1-400a-8c95-6442fad3e0c0"}
{"abstract": "Since electromagnetic waves are strongly attenuated inside the water, the satellite based global positioning system (GPS) cannot be used by submarine robots except at the surface of the water. This paper shows that the localization problem in deep water can often be cast into a continuous constraints satisfaction problem where interval constraints propagation algorithms are particularly efficient. The efficiency of the resulting propagation methods is illustrated on the localization of a submarine robot, named Redermor. The experiments have been collected by the GESMA (Groupe d'Etude Sous-Marine de l'Atlantique) in the Douarnenez bay, in Brittany.", "authors": ["Luc Jaulin"], "n_citation": 0, "title": "Localization of an underwater robot using interval constraint propagation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "00ecaae4-9b52-461c-b4e0-19b4f2435e4a"}
{"abstract": "The main purpose of this paper is to describe techniques for the numerical solution of a Sturm-Liouville equation (in its Schrodinger form) by employing a Magnus expansion. With a suitable method to approximate the highly oscillatory integrals which appear in the Magnus series, high order schemes can be constructed. A method of order ten is presented. Even when the solution is highly-oscillatory, the scheme can accurately integrate the problem using stepsizes typically much larger than the solution \"wavelength\". This makes the method well suited to be applied in a shooting process to locate the eigenvalues of a boundary value problem.", "authors": ["Veerle Ledoux", "Marnix Van Daele", "Guido Vanden Berghe"], "n_citation": 0, "title": "Using a (higher-order) Magnus method to solve the Sturm-Liouville problem", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "013dc40f-51df-4bf5-af9f-c95b14a7244d"}
{"authors": ["Marcello M. Bonsangue", "Dave Clarke", "Alexandra Silva"], "n_citation": 55, "title": "Automata for context-dependent connectors", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "015962f8-5d7d-4c70-bcd6-07098f0ba02e"}
{"abstract": "This paper describes use of negative examples in training the HVS semantic model. We present a novel initialization of the lexical model using negative examples extracted automatically from a semantic corpus as well as description of an algorithm for extraction these examples. We evaluated the use of negative examples on a closed domain human-human train timetable dialogue corpus. We significantly improved the standard PARSEVAL scores of the baseline system. The labeled F-measure (LF) was increased from 45.4% to 49.1%.", "authors": ["Filip Jurc\u00edcek", "Jan \u0160vec", "J. Zahradil", "L. Jel\u00ednek"], "n_citation": 0, "title": "Use of negative examples in training the HVS semantic model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "016115a1-c733-434a-8db2-d09af3e13775"}
{"abstract": "Many propagation and search algorithms have been developed for constraint satisfaction problems (CSPs). In a standard CSP all variables are existentially quantified. The CSP formalism can be extended to allow universally quantified variables, in which case the complexity of the basic reasoning tasks rises from NP-complete to PSPACE-complete. Such problems have, so far, been studied mainly in the context of quantified Boolean formulae. Little work has been done on problems with discrete non-Boolean domains. We attempt to fill this gap by extending propagation and search algorithms from standard CSPs to the quantified case. We also show how the notion of value interchangeability can be exploited to break symmetries and speed up search by orders of magnitude. Finally, we test experimentally the algorithms and methods proposed.", "authors": ["Nikos Mamoulis", "Kostas Stergiou"], "n_citation": 0, "title": "Algorithms for quantified constraint satisfaction problems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "02306605-562f-4863-9ea8-2e56616e2557"}
{"abstract": "Prediction is one of the techniques that has been applied to Augmentative and Alternative Communication to help people enhance the quality and quantity of the composed text in a time unit. Most of the literature has focused on word prediction methods that may easily be applied to non-inflected languages. However, for inflected languages, other approaches that mainly distinguish roots and suffixes may enhance the results (in terms of keystroke savings and hit ratio) of predictive systems. In this paper, we present the approaches we have applied to the Basque language (highly inflected) and the results they achieve with a particular text (that was not used while creating the initial lexicons the systems use for prediction). Starting from this evaluation, one of the presented approaches is suggested as being the best.", "authors": ["Nestor Garay-Vitoria", "Julio Abascal", "Luis Gardeazabal"], "n_citation": 0, "title": "Evaluation of prediction methods applied to an inflected language", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0298ad91-455d-4711-af08-f0900fa694f0"}
{"abstract": "Visual language refers to the idea that communication occurs through visual symbols, as opposed to verbal symbols or words. Contrast to a sentence construction in spoken language with a linear ordering of words, a visual language has a simultaneous structure with a parallel temporal and spatial configuration. Inspired by Deikto [51, we propose a two-dimensional string or sentence construction of visual expressions, i.e. spatial arrangements of symbols, which represent concepts. A proof of concept communication interface has been developed, which enables users to create visual messages to represent concepts or ideas in their mind. By the employment of ontology, the interface constructs both the syntax and semantics of a 2D visual string using a Lexicalized Tree Adjoining Grammar (LTAG) into (natural language) text. This approach captures elegantly the interaction between pragmatic and syntactic descriptions in a 2D sentence, and the inferential interactions between multiple possible meanings generated by the sentence. From our user test results, we conclude that our developed visual language interface could serve as a communication mediator.", "authors": ["Siska Fitrianie", "L\u00e9on J. M. Rothkrantz"], "n_citation": 50, "title": "Two-dimensional visual language grammar", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "029ecbf8-c5ff-455b-9faf-a40077ecbe56"}
{"abstract": "This paper contributes to the identification, formalisation and analysis of desirable properties of agent models in general and of the KGP model in particular. This model is specified in computational logic, and consequently lends itself well to formal analysis. We formalise three notions of welfare, in terms of goal achievement, progress, and reactive awareness, and we prove results related to these notions for KGP agents. These results broadly demonstrate the coherence of some of the design decisions in the KGP model, the need for some of its components for effectiveness in goal achievement, the extent to which the welfare of KGP agents can be shown to improve during their life-time, and the awareness of the agents of their reactions to changes in the environment.", "authors": ["Fariba Sadri", "Francesca Toni"], "n_citation": 0, "title": "A formal analysis of KGP agents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "02a98db4-55e8-48d9-a953-a9167aee68d3"}
{"abstract": "Using a process modeling-based approach to address privacy and related legal issues arising in an RFID-based augmented reality shopping situation.", "authors": ["Brian Subirana", "Malcolm Bain"], "n_citation": 50, "references": ["37944de8-ca5d-466f-a84b-f5e5149e5a2b", "3c902b93-bba9-4507-9ae7-5a1ead249402", "48e9f19f-a7eb-4ce9-85b6-1b34452881f1", "7b30e170-4de9-4dbf-91f5-48dc84d6bd6a", "e4545a28-d060-4321-897b-d3fe0ff9c510"], "title": "Legal programming", "venue": "Communications of The ACM", "year": 2006, "id": "02b1be76-c305-4e4a-906a-1f58b1340379"}
{"abstract": "In the same way as the static Semantic Web deals with data model and language heterogeneity and semantics that lead to RDF and OWL, there is language heterogeneity and the need for a semantical account concerning Web dynamics. Thus, generic rule markup has to bridge these discrepancies, i.e., allow for composition of component languages, retaining their distinguished semantics and making them accessible e.g. for reasoning about rules. In this paper we analyze the basic concepts for a general language for evolution and reactivity in the Semantic Web. We propose an ontology based on the paradigm of Event-Condition-Action (ECA) rules including an XML markup. In this framework, different languages for events (including languages for composite events), conditions (queries and tests) and actions (including complex actions) can be composed to define high-level rules for describing behavior in the Semantic Web.", "authors": ["Wolfgang May", "Jos\u00e9 J\u00falio Alferes", "Ricardo Amador"], "n_citation": 0, "title": "Active rules in the Semantic Web : Dealing with language heterogeneity", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "02f4f4fc-c7e0-43a9-9c2c-2c1bd3e4d673"}
{"authors": ["Jer-Yu Hsu", "Yan-Zu Wu", "Xuan-Yi Lin", "Yeh-Ching Chung"], "n_citation": 0, "title": "SCRF \u2013 A Hybrid Register File Architecture", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "02fae60e-b01a-47a9-852d-45fd7eaa33c8"}
{"abstract": "We present a statistically-hiding commitment scheme allowing commitment to arbitrary size integers, based on any (Abelian) group with certain properties, most importantly, that it is hard for the committee to compute its order. We also give efficient zero-knowledge protocols for proving knowledge of the contents of commitments and for verifying multiplicative relations over the integers on committed values. The scheme can be seen as a generalization, with a slight modification, of the earlier scheme of Fujisaki and Okamoto [14]. The reasons we revisit the earlier scheme and give some modification to it are as follows: - The earlier scheme [14] has some gaps in the proof of soundness of the associated protocols, one of which presents a non-trivial problem which, to the best of our knowledge, has remained open until now. We fill all the gaps here using additional ideas including minor modification of the form of a commitment. - Although related works such as [8, 3, 10, 4] do not suffer from the main problem we solve here, the reason for this is that they use commitments with a single base (i.e., of form c = g s  mod n). Such commitments, however, cannot satisfy the standard hiding property for commitments, and hence protocols using them cannot in general be (honest-verifier) zero-knowledge nor witness indistinguishable. - In a computationally convincing proof of knowledge where the prover produces the common input (which is the type of protocol we look at here), one cannot completely exclude the possibility that a prover manages to produce a common input on which he can cheat easily. This means that the standard definition of proofs of knowledge cannot be satisfied. Therefore we introduce a new definition for computationally convincing proofs of knowledge, designed to handle the case where the common input is chosen by the (possibly cheating) prover. - Our results apply to any group with suitable properties. In particular, they apply to a much larger class of RSA moduli than the safe prime products proposed in [14] - Potential examples include RSA moduli, class groups and, with a slight modification, even non-Abelian groups. Our scheme can replace the earlier one in various other constructions, such as the efficient interval proofs of Boudot [4] and the efficient proofs for the product of two safe primes proposed by Camenisch and Michels [9].", "authors": ["Ivan Damg\u00e5rd", "Eiichiro Fujisaki"], "n_citation": 284, "title": "A statistically-hiding integer commitment scheme based on groups with hidden order", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "038919b7-ac0a-4de7-b8f0-6a861a991388"}
{"abstract": "A formal prosody model is proposed together with its application in a text-to-speech system. The model is based on a generative grammar of abstract prosodic functionally involved units. This grammar creates for each sentence a structure of immediate prosodic constituents in the form of a tree. Each prosodic word of a sentence is assigned with a description vector by a description function and this vector is used by a realization function to create appropriate intonation for the prosodic word. Parameters of the model are automatically set up using real speech data from a prosody corpus, which is also described.", "authors": ["Jan Romportl", "Jindrich Matousek", "Daniel Tihelka"], "n_citation": 0, "title": "Advanced prosody modelling", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "03bad900-983f-4e91-b991-0a51295eafd5"}
{"abstract": "Naturalness of synthetic speech highly depends on appropriate modelling of prosodic aspects. Mostly, three prosody components are modelled: segmental duration, pitch contour and intensity. In this study, we present our work on modelling segmental duration in Turkish using machine-learning algorithms, especially Classification and Regression Trees. The models predict phone durations based on attributes such as current, preceding and following phones' identities, stress, part-of-speech, word length in number of syllables, and position of word in utterance extracted from a speech corpus. Obtained models predict segment durations better than mean duration approximations (\u223c0.77 Correlation Coefficient, and 20.4 ms Root-Mean Squared Error). In order to improve prediction performance further, attributes used to develop segmental duration are optimized by means of Sequential Forward Selection method. As a result of Sequential Forward Selection method, phone identity, neighboring phone identities, lexical stress, syllable type, part-of-speech, phrase break information, and location of word in the phrase constitute optimum attribute set for phoneme duration modelling.", "authors": ["\u00d6zlem \u00d6zt\u00fcrk", "Tolga Ciloglu"], "n_citation": 50, "title": "Segmental duration modelling in turkish", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "040de644-b69d-4de4-9c18-f73f36430209"}
{"authors": ["Milan Leg\u00e1t", "Daniel Tihelka", "J. Matou\u0161ek"], "n_citation": 0, "title": "Pitch Marks at Peaks or Valleys", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "045108a8-3a45-4d59-9702-cf8f204be4f8"}
{"abstract": "Over the last fifteen years, database management systems (DBMSs) have been enhanced by the addition of rule-based programming to obtain active DBMSs. One of the greatest challenges in this area is to formally account for all the aspects of active behavior using a uniform formalism. In this paper, we formalize active relational databases within the framework of the situation calculus by uniformly accounting for them using theories embodying non-Markovian control in the situation calculus. We call these theories active relational theories and use them to capture the dynamics of active databases. Transaction processing and rule execution is modelled as a theorem proving task using active relational theories as background axioms. We show that major components of an ADBMS may be given a clear semantics using active relational theories.", "authors": ["Iluju Kiringa", "Raymond Reiter"], "n_citation": 0, "title": "A unifying semantics for active databases using non-Markovian theories of actions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "046bce51-35c7-49da-862e-261a5e262341"}
{"abstract": "In this paper we present a prototype of a tool that demonstrates how existing limitations in ensuring an agent's compliance to an argumentation-based dialogue protocol can be overcome. We also present the implementation of compliance enforcement components for a deliberation dialogue protocol, and an application that enables two human participants to engage in an efficiently moderated dialogue, where all inappropriate utterances attempted by an agent are blocked and prevented from inclusion within the dialogue.", "authors": ["Daniel Bryant", "Paul Krause", "Sotiris Moschoyiannis"], "n_citation": 0, "title": "A tool to facilitate agent deliberation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "05280aeb-437e-4707-a5dd-f8bcd7808ed9"}
{"authors": ["Luc Vandeurzen", "Marc Gyssens", "Dirk Van Gucht"], "n_citation": 0, "title": "On Query Languages for Linear Queries Definable with Polynomial Constraints", "venue": "Lecture Notes in Computer Science", "year": 1996, "id": "057367ed-45bd-4646-ae85-2ec166b4bc9c"}
{"abstract": "A full-text index is a data structure built over a text string T[1,n]. The most basic functionality provided is (a) counting how many times a pattern string P[1,m] appears in T and (b) locating all those occ positions. There exist several indexes that solve (a) in O(m) time and (b) in O(occ) time. In this paper we propose two new queries, (c) counting how many times P[1,m] appears in T[l,r] and (d) locating all those occ i,r  positions. These can be solved using (a) and (b) but this requires O(occ) time. We present two solutions to (c) and (d) in this paper. The first is an index that requires O(n log n) bits of space and answers (c) in O(m + log n) time and (d) in O(logn) time per occurrence (that is, O(ccc l,r  log n) time overall). A variant of the first solution answers (c) in O(m + log log n) time and (d) in constant time per occurrence, but requires O(n log 1+ \u2208 n) bits of space for any constant e > 0. The second solution requires O(nm log \u03c3) bits of space, solving (c) in O(m [log \u03c3 / log log n]) time and (d) in O(m [log \u03c3/ log log n]) time per occurrence, where a is the alphabet size. This second structure takes less space when the text is compressible. Our solutions can be seen as a generalization of rank and select dictionaries, which allow computing how many times a given character c appears in a prefix T[1,i] and also locate the i-th occurrence of c in T. Our solution to (c) extends character rank queries to substring rank queries, and our solution to (d) extends character select to substring select queries. As a byproduct, we show how rank queries can be used to implement fractional cascading in little space, so as to obtain an alternative implementation of a well-known two-dimensional range search data structure by Chazelle. We also show how Grossi et al.'s wavelet trees are suitable for two-dimensional range searching, and their connection with Chazelle's data structure.", "authors": ["Veli M\u00e4kinen", "Gonzalo Navarro"], "n_citation": 84, "title": "Position-restricted substring searching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "059f30fc-1a2e-412f-b849-94f66384a524"}
{"abstract": "This paper presents a new optimization metaheuristic called ID Walk (Intensification/Diversification Walk) that offers advantages for combining simplicity with effectiveness. In addition to the number S of moves, ID Walk uses only one parameter Max which is the maximum number of candidate neighbors studied in every move. This candidate list strategy manages the Max candidates so as to obtain a good tradeoff between intensification and diversification. A procedure has also been designed to tune the parameters automatically. We made experiments on several hard combinatorial optimization problems, and ID Walk compares favorably with correspondingly simple instances of leading metaheuristics, notably tabu search, simulated annealing and Metropolis. Thus, among algorithmic variants that are designed to be easy to program and implement, ID Walk has the potential to become an interesting alternative to such recognized approaches. Our automatic tuning tool has also allowed us to compare several variants of ID Walk and tabu search to analyze which devices (parameters) have the greatest impact on the computation time. A surprising result shows that the specific diversification mechanism embedded in ID Walk is very significant, which motivates examination of additional instances in this new class of dynamic candidate list strategies.", "authors": ["Bertrand Neveu", "Gilles Trombettoni", "Fred Glover"], "n_citation": 0, "title": "ID Walk: A candidate list strategy with a simple diversification device", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0611a943-17d8-452f-9215-4df72dd1e640"}
{"abstract": "Online feature selection using Bayes error rate is proposed to address visual tracking problem, where the appearances of the tracked object and scene background change during tracking. Given likelihood functions of the object and background with respect to a feature, Bayes error rate is a natural way to evaluate discriminating power of the feature. From previous frame, object and background pixels are sampled to estimate likelihood functions of each color feature in the form of histogram. Then, all features are ranked according to their Bayes error rate. And the top N features with the smallest Bayes error rate are selected to generate a weight map for current frame, where mean shift is employed to find the local mode, and hence, the location of the object. Experimental results on real image sequences demonstrate the effectiveness of the proposed approach.", "authors": ["Dawei Liang", "Qingming Huang", "Wen Gao", "Hongxun Yao"], "n_citation": 50, "title": "Online Selection of Discriminative Features Using Bayes Error Rate for Visual Tracking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0632f1ac-0e98-4b99-8915-ec87287dcb4f"}
{"abstract": "Dynamic Constraint Satisfaction Problems play a very important role in modeling and solving real-life problems where the set of constraints is changing. The paper addresses a problem of maintaining arc consistency after removing a constraint from the constraint model. A new dynamic arc consistency algorithm is proposed that improves the practical time efficiency of the existing AC|DC algorithm by using additional data-structures. The algorithm achieves real time efficiency close to the so far fastest DynAC-6 algorithm while keeping the memory consumption low.", "authors": ["Pavel Surynek", "Roman Bart\u00e1k"], "n_citation": 0, "title": "A new algorithm for maintaining arc consistency after constraint retraction", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0660c25e-69fb-48a4-ae77-9b40dc0e33d7"}
{"abstract": "A number of pitfalls of empirical scheduling research are illustrated using real experimental data. These pitfalls, in general, serve to slow the progress of scheduling research by obsfucating results, blurring comparisons among scheduling algorithms and algorithm components, and complicating validation of work in the literature. In particular, we look at difficulties brought about by viewing algorithms in a monolithic fashion, by concentrating on CPU time as the only evaluation criteria, by failing to prepare for gathering of a variety of search statistics at the time of experimental design, by concentrating on benchmarks to the exclusion of other sources of experimental problems, and, more broadly, by a preoccupation with optimization of makespan as the sole goal of scheduling algorithms.", "authors": ["John C. Beck", "Andrew J. Davenport", "Mark S. Fox"], "n_citation": 0, "title": "Five pitfalls of empirical scheduling research", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "06781ad5-d38d-4da2-b55b-cf046e506d60"}
{"abstract": "Traditionally, constraint satisfaction has been applied in closed-world scenarios, where all choices and constraints are known from the beginning and fixed. With the Internet, many of the traditional CSP applications in resource allocation, scheduling and planning pose themselves in open-world settings, where choices and constraints are to be discovered from different servers in a network. We examine how such a distributed setting affects changes the assumptions underlying most CSP algorithms, and show how solvers can be augmented with an information gathering component that allows open-world constraint satisfaction. We report on experiments that show strong performance of such methods over others where gathering information and solving the CSP are separated.", "authors": ["Boi Faltings", "Santiago Macho-Gonzalez"], "n_citation": 75, "title": "Open constraint satisfaction", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "06905b3d-f71d-471d-94df-4bec6dafb7ce"}
{"authors": ["Orr Dunkelman", "Nathan Keller"], "n_citation": 0, "title": "A New Attack on the LEX Stream Cipher", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "06d15ce2-566b-482e-b080-e0ce038ec0da"}
{"abstract": "Two sets of linguistic features are developed: The first one to estimate if a single step in a dialogue between a human being and a machine is successful or not. The second set to classify dialogues as a whole. The features are based on Part-of-Speech-Labels (POS), word statistics and properties of turns and dialogues. Experiments were carried out on the SympaFly corpus, data from a real application in the flight booking domain. A single dialogue step could be classified with an accuracy of 83% (class-wise averaged recognition rate). The recognition rate for whole dialogues was 85%.", "authors": ["Stefan Steidl", "Christian Hacker", "Christine Ruff", "Anton Batliner", "Elmar N\u00f6th", "J\u00fcrgen Haas"], "n_citation": 0, "title": "Looking at the last two turns, I'd say this dialogue is doomed: Measuring dialogue success", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "073021ec-03a2-4b52-b32c-726fa1f8c8f5"}
{"abstract": "In this paper, we propose an associative parallel algorithm for updating a minimum spanning tree when a vertex and all its incident edges are deleted from the underlying graph. This algorithm is represented as the corresponding procedure implemented on a model of associative parallel systems of the SIMD type with vertical data processing (the STAR-machine). We justify the correctness of this procedure and evaluate its time complexity.", "authors": ["Anna Nepomniaschaya"], "n_citation": 50, "title": "Associative parallel algorithm for dynamic reconstruction of a minimum spanning tree after deletion of a vertex", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "dbf89838-c490-4aca-89e3-f7fb85bb7efb"}
{"authors": ["Jan Camenisch", "Markulf Kohlweiss", "Alfredo Rial Duran", "Caroline Sheedy"], "n_citation": 50, "title": "Blind and Anonymous Identity-Based Encryption and Authorised Private Searches on Public Key Encrypted Data", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "dc994f80-0206-4cc3-99c4-f25227420e1f"}
{"abstract": "In this paper we present a program for simulation of the two-component protoplanetary disc evolution. The model includes gas dynamics and collisionless solid body dynamics. Multigrid method for solution of Poisson equation in cylindrical coordinate system is described. An essentially parallel Poisson equation solver is constructed by means of applying Fast Fourier Transform along the angular direction. Processor workload rearrangement is performed in order to increase the speedup. The results of computational experiments are given to demonstrate the physical validity of the program.", "authors": ["Alexey V. Snytnikov", "Vitaly Vshivkov"], "n_citation": 50, "title": "A multigrid parallel program for protoplanetary disc simulation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f72f8de2-8f47-45f3-8f5f-322ac9b00512"}
{"authors": ["Dusan Bozilov", "Beg\u00fcl Bilgin", "Hac\u0131 Ali Sahin"], "n_citation": 0, "title": "A Note on 5-bit Quadratic Permutations' Classification", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "f96e1212-dd67-4432-aa93-26876ca38032"}
{"abstract": "In this paper, we present a novel approach of online virtual disk migration with performance guarantees, which is important for storage maintenance tasks. Our approach can be applied to moving virtual disk and exchanging virtual disks. It identifies the surplus I/O resource of storage pools after satisfying performance requirement of virtual disks with EPYFQ scheduling algorithm, and gives high priority of using these I/O resource to migration tasks. Thus, the performance of virtual disks is guaranteed during migration, and the migration is completed in the shortest possible time. Moreover, our approach divides migration task into multiple storage transactions, which can protect the consistency of the data in the migrated virtual disks when application I/O and migration I/O execute concurrently. We implement our approach into E-DM, a kernel module of Linux, and evaluate it. The result shows that the IOPS of virtual disks is decreased not more than 3% during migration.", "authors": ["Yong Feng", "Yanyuan Zhang", "Rui-yong Jia", "Xiao Zhang"], "n_citation": 0, "title": "Online virtual disk migration with performance guarantees in a shared storage environment", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fbafd2a3-2800-40a4-ad25-6474bb676f90"}
{"abstract": "Program slicing is a well-known technique that has been widely used for debugging in the context of imperative programming. Debugging is a particularly difficult task within lazy declarative programming. In particular, there exist very few approaches to program slicing in this context. In this paper, we describe a slicing tool for first-order lazy functional logic languages. We also illustrate its usefulness by means of an example.", "authors": ["Claudio Ochoa", "Josep Silva", "Germ\u00e1n Vidal"], "n_citation": 50, "title": "A slicing tool for lazy functional logic programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "087f2872-5230-4779-8a88-8e82ee835752"}
{"abstract": "A method of shadow removal from sole uncalibrated outdoor image is proposed. Existing approaches usually decompose the image into albedo and illumination images, in this paper, based on the mechanism of shadow generation, the occlusion factor is introduced, and the illumination image is further decomposed as the linear combination of solar irradiance and ambient irradiance images. The involved irradiance are achieved from the user-supplied hints. The shadow matte are evaluated by the anisotropic diffusion of posterior probability. Experiments show that our method could simultaneously extract the detailed shadow matte and recover the texture beneath the shadow.", "authors": ["Zhenlong Du", "Xueying Qin", "Wei Hua", "Hujun Bao"], "n_citation": 50, "title": "Shadow Removal in Sole Outdoor Image", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "08cfe64d-59d8-4666-b072-62e549dffb81"}
{"authors": ["George Baciu", "Brian Pw Lee"], "n_citation": 50, "title": "An interface for synthesizing 3D multibody structures", "venue": "Lecture Notes in Computer Science", "year": 1995, "id": "092efb26-71c1-4781-8b49-9f9a6b775c4f"}
{"abstract": "In this paper we consider privacy problems with anonymized transaction databases, i.e., transaction databases where the items are renamed in order to hide sensitive information. In particular, we show how an anonymized transaction database can be deanonymized using non-anonymized frequent itemsets. We describe how the problem can be formulated as an integer programming task, study the computational complexity of the problem, discuss how the computations could be done more efficiently in practice and experimentally examine the feasibility of the proposed approach.", "authors": ["Taneli Mielik\u00e4inen"], "n_citation": 0, "title": "Privacy problems with anonymized transaction databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "097fd0e4-7ed0-48ef-a20b-9dbf00057cba"}
{"abstract": "Recently, methods from provable security, that had been developped for the last twenty years within the research community, have been extensively used to support emerging standards. This in turn has led researchers as well as practitioners to raise some concerns about this methodology. Should provable security be restricted to the standard computational model or can it rely on the so-called random oracle model? In the latter case, what is the practical meaning of security estimates obtained using this model? Also, the fact that proofs themselves need time to be validated through public discussion was somehow overlooked. Building on two case studies, we discuss these concerns. One example covers the public key encryption formatting scheme OAEP originally proposed in [3]. The other comes from the area of signature schemes and is related to the security proof of ESIGN [43]. Both examples show that provable security is more subtle than it at first appears.", "authors": ["Jacques Stern"], "n_citation": 0, "title": "Why provable security matters", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "0aa0ff71-ac67-4cd5-80ab-26e5f4d9dd7b"}
{"abstract": "A temporal reasoning problem can often be naturally characterized as a collection of constraints with associated local preferences for times that make up the admissible values for those constraints. Globally preferred solutions to such problems emerge as a result of well-defined operations that compose and order temporal assignments. The overall objective of this work is a characterization of different notions of global temporal preference within a temporal constraint reasoning framework, and the identification of tractable sub-classes of temporal reasoning problems incorporating these notions. This paper extends previous results by refining the class of useful notions of global temporal preference that are associated with problems that admit of tractable solution techniques. This paper also resolves the hitherto unanswered question of whether the solutions that are globally preferred from a utilitarian criterion for global preference can be found tractably. A technique is described for identifying and representing the entire set of utilitarian-optimal solutions to a temporal problem with preferences.", "authors": ["Paul H. Morris", "Robert A. Morris", "Lina Khatib", "Sailesh Ramakrishnan", "Andrew G. Bachmann"], "n_citation": 0, "title": "Strategies for global optimization of temporal preferences", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0b0ec536-3668-4609-a73e-aae3e4f92622"}
{"abstract": "In this paper we study four algorithms for computing the frequency of a given partial order. Here the frequency of a partial order is the number of standard labellings respecting that partial order. The first two algorithms count by enumerating all solutions to a CSP. However, large numbers of solutions to csPs soon make algorithms based on enumeration infeasible. The third and fourth algorithm, to a degree, overcome this problem. They avoid repeatedly solving problems with certain kinds of isomorphic solutions. A prototype implementation of the fourth algorithm was significantly more efficient than an enumeration based counting implementation using OPL.", "authors": ["Marc R. C. Van Dongen"], "n_citation": 0, "title": "Computing the frequency of partial orders", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0b1b9d09-44fe-498b-8eb5-981e24a0c0aa"}
{"abstract": "Based on the incremental and decremental learning strategies, an adaptive support vector machine learning algorithm (ASVM) is presented for large classification problems in this paper. In the proposed algorithm, the incremental and decremental procedures are performed alternatively, and a small scale working set, which can cover most of the information in the training set and overcome the drawback of losing the sparseness in least squares support vector machine (LS-SVM), can be formed adaptively. The classifier can be constructed by using this working set. In general, the number of the elements in the working set is much smaller than that in the training set. Therefore the proposed algorithm can be used not only to train the data sets quickly but also to test them effectively with losing little accuracy. In order to examine the training speed and the generalization performance of the proposed algorithm, we apply both ASVM and LS-SVM to seven UCI datasets and a benchmark problem. Experimental results show that the novel algorithm is very faster than LS-SVM and loses little accuracy in solving large classification problems.", "authors": ["Shu Yu", "Xiaowei Yang", "Zhifeng Hao", "Yanchun Liang"], "n_citation": 0, "title": "An Adaptive Support Vector Machine Learning Algorithm for Large Classification Problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0b9261e1-bc2d-4c8d-aba8-26ffcef462ae"}
{"abstract": "The present study investigates a geometrical method for optimizing the kernel function of a support vector machine. The method is an improvement of the one proposed in [4, 5]. It consists of using prior knowledge obtained from conventional SVM training to conformally rescale the initial kernel function, so that the separation between two classes of data is effectively enlarged. It turns out that the new algorithm works efficiently, has few free parameters, consumes very low computational cost, and overcomes the susceptibility of the original method.", "authors": ["Peter M. Williams", "Sheng Li", "Jianfeng Feng", "Si Wu"], "n_citation": 0, "title": "Scaling the kernel function to improve performance of the support vector machine", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "0bc5a10c-50b9-4eb4-80da-642f3f6736f2"}
{"abstract": "We attempt to quantify the significance of increasing the number of neurons in the hidden layer of a feedforward neural network architecture using the singular value decomposition (SVD). Through this, we extend some well-known properties of the SVD in evaluating the generalizability of single hidden layer feedforward networks (SLFNs) with respect to the number of hidden neurons. The generalization capability of the SLFN is measured by the degree of linear independency of the patterns in hidden layer space, which can be indirectly quantified from the singular values obtained from the SVD, in a post-learning step.", "authors": ["Eu Jin Teoh", "Clieng Xiang", "Kay Clien Tan"], "n_citation": 0, "title": "Estimating the Number of Hidden Neurons in a Feedforward Network Using the Singular Value Decomposition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0c2848f9-a6f4-4dd2-8dcb-6b009ab18fa6"}
{"abstract": "We consider the human body as a well-orchestrated system of interacting swarms. Utilizing swarm intelligence techniques, we present our latest virtual simulation and experimentation environment, IMMS:VIGO::3D, to explore key aspects of the human immune system. Immune system cells and related entities (viruses, bacteria, cytokines) are represented as virtual agents inside 3-dimensional, decentralized and compartmentalized environments that represent primary and secondary lymphoid organs as well as vascular and lymphatic vessels. Specific immune system responses emerge as by-products from collective interactions among the involved simulated 'agents' and their environment. We demonstrate simulation results for clonal selection and primary and secondary collective responses after viral infection, as well as the key response patterns encountered during bacterial infection. We see this simulation environment as an essential step towards a hierarchical whole-body simulation of the immune system, both for educational and research purposes.", "authors": ["Christian Jacob", "Scott Steil", "Karel P. Bergmann"], "n_citation": 50, "title": "The swarming body : Simulating the decentralized defenses of immunity", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0cacbd01-7454-46ca-b465-68d4c19fd1c3"}
{"abstract": "The fast growth of Internet is creating a society where the demand on information storage, organization, access, and analysis services is continuously growing. This constantly increases the number of inexperienced users that need to access databases in a simple way. Together with the emergence of voice interfaces, such a situation foretells a promising future for database querying systems using natural language interfaces. We describe the architecture of a relational database querying system using a natural language (Spanish) interface, giving a brief explanation of the implementation of each of the constituent modules: lexical parser, syntax checker, and semantic analyzer.", "authors": ["Rodolfo A. Pazos Rangel", "Alexander F. Gelbukh", "Javier Gonzalez Barbosa", "Erika Alarc\u00f3n Ruiz", "Alejandro Mendoza Mej\u00eda", "A. Patricia Dom\u00ednguez S\u00e1nchez"], "n_citation": 50, "title": "Spanish natural language interface for a relational database querying system", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0cc9bae7-035a-4eb6-812e-d2447bddba57"}
{"abstract": "The four most important issues in content-based image retrieval (CBIR) are how to extract features from an image, how to represent these features, how to search the images similar to the query image based on these features as fast as we can and how to perform relevance feedback. This paper mainly concerns the third problem. The traditional features such as color, shape and texture are extracted offline from all images in the database to compose a feature database, each element being a feature vector. The linear scaling to unit variance normalization method is used to equalize each dimension of the feature vector. A fast search method named equal-average K nearest neighbor search (EKNNS) is then used to find the first K nearest neighbors of the query feature vector as soon as possible based on the squared Euclidean distortion measure. Experimental results show that the proposed retrieval method can largely speed up the retrieval process, especially for large database and high feature vector dimension.", "authors": ["Liu Zhe-ming", "Hans Burkhardt", "Sebastian Boehmer"], "n_citation": 0, "title": "Fast Content-Based Image Retrieval Based on Equal-Average K-Nearest-Neighbor Search Schemes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0cfc4917-05e7-42ad-a4e4-f994f4a558b7"}
{"abstract": "A central problem in distributed computing and telecommunications is the establishment of common knowledge between two computing entities. An immediate use of such common knowledge is in the initiation of a secure communication session between two entities since the two entities may use this common knowledge in order to produce a secret key for use with some symmetric cipher. The dynamic establishment of shared information (e.g. secret key) between two entities is particularly important in networks with no predetermined structure such as wireless mobile ad-hoc networks. In such networks, nodes establish and terminate communication sessions dynamically with other nodes which may have never been encountered before in order to somehow exchange information which will enable them to subsequently communicate in a secure manner. In this paper we give and theoretically analyze a protocol that enables two entities initially possessing a string each to securely eliminate inconsistent bit positions, obtaining strings with a larger percentage of similarities. This can help the nodes establish a shared set of bits and use it as a key with some shared key encryption scheme.", "authors": ["Effie Makri", "Yannis C. Stamatiou"], "n_citation": 0, "title": "Distributively increasing the percentage of similarities between strings with applications to key agreement", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0d4a2bef-741a-4de9-8c76-284471b6c8d6"}
{"abstract": "This paper presents a novel solution for efficient representation of multi-channel spatial audio signals. Unlike other spatial audio coding techniques, the solution inherently requires no additional side information to recover the surround sound panorama from a two-channel downmix. For a typical five-channel case, only a stereo downmix signal is required for the decoder to reconstruct the full five-channel audio signal. In addition to the bandwidth saved by transmitting no side information, the technique has significant advantages in terms of computational complexity.", "authors": ["Bin Cheng", "Christian Ritz", "Ian Bumett"], "n_citation": 0, "title": "Squeezing the Auditory Space : A New Approach to Multi-channel Audio Coding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0d7bfeac-4448-4b6b-bbca-c4c7225a8220"}
{"authors": ["Tatiana Tommasi", "Elisabetta La Torre", "Barbara Caputo"], "n_citation": 50, "title": "Melanoma recognition using representative and discriminative kernel classifiers", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0dc503f1-6117-43b6-a34f-8f5b100b6081"}
{"abstract": "This paper describes a text-document-filtering software tool TEA (TExt Analyzer), which was originally developed for physicians to support selections of large numbers of unstructured medical text documents obtained from available Internet services. TEA learns interesting and relevant documents for individual users basically by the naive Bayes algorithm. Moreover, TEA provides a number of additional functions that can improve its classification accuracy, allow more specific document selection for individual users, and enable users to work with dictionaries generated from analyzed documents. The learning process of TEA is based on a set of labeled positive and negative examples of text documents, which obtain their labels from users interested in documents of certain, usually very specific topics.", "authors": ["Jan Zizka", "Ale\u0161 Bourek"], "n_citation": 0, "title": "Filtering of large numbers of unstructured text documents by the developed tool TEA", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "0e1d92d2-e774-4237-98fd-8ee6c5d5deb6"}
{"abstract": "In this paper, we investigate a large class of periodic Cohen-Grossberg neural networks and prove that under some mild conditions, there is a unique almost-periodic solution, which is globally stable. As special examples, we derive many results given in literature.", "authors": ["Tianping Chen", "Lili Wang", "Changlei Ren"], "n_citation": 50, "title": "Existence and Global Stability Analysis of Almost Periodic Solutions for Cohen-Grossberg Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0f410e5c-be60-45c9-9f09-ba795cbc82f5"}
{"abstract": "Symmetry in constraint problems can be exploited to greatly improve search performance. A form of symmetry that has been the subject of considerable research is value interchangeability. Automatically detecting full interchangeability is thought to be intractable, so research has focused on either discovery of local interchangeability or programmer knowledge of full interchangeability. This paper shows that full dynamic substitutability can be broken in a CSP by reformulating it as a SAT problem. No analysis is necessary, space requirements are modest, solutions are collected into Cartesian products, and unit propagation enforces forward checking on the CSP. In experiments on unsatisfiable problems, better results are obtained than with standard SAT encodings.", "authors": ["Steven David Prestwich"], "n_citation": 0, "title": "Full dynamic substitutability by SAT encoding", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "0fa8131f-c821-4d7f-a3c7-df14607815e0"}
{"abstract": "The Tied Mixture Hidden Markov Model (TMHMM) is an important approach to reduce the number of free parameters in speech recognition. However, this model suffers from degradation in recognition accuracy due to its Gaussian Probability Density Function (GPDF) clustering error. This paper proposes a clustering algorithm called a Heterogeneous Centroid Neural Network (HCNN) for use in TMHMMs. The algorithm utilizes a Centroid Neural Network (CNN) to cluster acoustic feature vectors in the TMHMM. The HCNN uses a heterogeneous distance measure to allocate more code vectors in the heterogeneous areas where probability densities of different states overlap each other. When applied to an isolated Korean digit word recognition problem, the HCNN reduces the error rate by 9.39% over CNN clustering. and 14.63% over the traditional K-means clustering.", "authors": ["Dong-Chul Park", "Duc-Hoai Nguyen", "Song-Jae Lee", "Yunsik Lee"], "n_citation": 0, "title": "Heterogeneous Centroid Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "0fdc1c1f-cbae-4257-b80e-7aab2d1bd332"}
{"abstract": "In this paper we present first experimental results with a novel audio coding technique based on approximating Hilbert envelopes of relatively long segments of audio signal in critical-band-sized sub-bands by autoregressive model. We exploit the generalized autocorrelation linear predictive technique that allows for a better control of fitting the peaks and troughs of the envelope in the sub-band. Despite introducing longer algorithmic delay, improved coding efficiency is achieved. Since the described technique does not directly model short-term spectral envelopes of the signal, it is suitable not only for coding speech but also for coding of other audio signals.", "authors": ["Petr Motlick", "Hynek Hermansky", "Harinath Garudadri", "Naveen Srinivasamurthy"], "n_citation": 50, "title": "Speech coding based on spectral dynamics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1051c431-f68a-43ff-870c-d7a35d72ffd4"}
{"abstract": "A distributed concurrent search algorithm for distributed constraint satisfaction problems (DisCSPs) is presented. Concurrent search algorithms are composed of multiple search processes (SPs) that operate concurrently and scan non-intersecting parts of the global search space. Search processes are generated dynamically, started by the initializing agent, and by any number of agents during search. In the proposed, ConcDB, algorithm, all search processes perform dynamic backtracking (DB). As a consequence of DB, a search space scanned by one search process can be found unsolvable by a different search process. This enhances the efficiency of the ConcDB algorithm. Concurrent search is an asynchronous distributed algorithm and is shown to be faster than asynchronous backtracking (ABT). The network load of ConcDB is also much lower than that of ABT.", "authors": ["Roie Zivan", "Amnon Meisels"], "n_citation": 0, "title": "Concurrent dynamic backtracking for distributed CSPs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "10d15dce-8ab2-4d32-94bb-edb9d6c4198e"}
{"abstract": "Mobile ad hoc networks (MANET) are dynamic networks formed on-the-fly as mobile nodes move in and out of each others' transmission ranges. In general, the mobile ad hoc networking model makes no assumption that nodes know their own locations. However, recent research shows that location-awareness can be beneficial to fundamental tasks such as routing and energy-conservation. On the other hand, the cost and limited energy resources associated with common, low-cost mobile nodes prohibits them from carrying relatively expensive and power-hungry location-sensing devices such as GPS. This paper proposes a mechanism that allows non-GPS-equipped nodes in the network to derive their approximated locations from a limited number of GPS-equipped nodes. In our method, all nodes periodically broadcast their estimated location, in term of a compressed particle filter distribution. Non-CPS nodes estimate the distance to their neighbors by measuring the received signal strength of incoming messages. A particle filter is then used to estimate the approximated location from the sequence of distance estimates. Simulation studies show that our solution is capable of producing good estimates equal or better than the existing localization methods such as APS-Euclidean.", "authors": ["Rui Huang", "Gergely V. Z\u00e1ruba"], "n_citation": 50, "title": "Location tracking in mobile ad hoc networks using particle filters", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "110be506-9c08-44c0-bc44-835559737822"}
{"abstract": "A commitment multiplication proof, CMP for short, allows a player who is committed to secrets s, s' and s = s.s', to prove, without revealing s, s' or s, that indeed s = ss'. CMP is an important building block for secure general multi-party computation as well as threshold cryptography. In the standard cryptographic model, a CMP is typically done interactively using zero-knowledge protocols. In the random oracle model it can be done non-interactively by removing interaction using the Fiat-Shamir heuristic. An alternative non-interactive solution in the distributed setting, where at most a certain fraction of the verifiers are malicious, was presented in [1] for Pedersen's discrete log based commitment scheme. This CMP essentially consists of a few invocations of Pedersen's verifiable secret sharing scheme (VSS) and is secure in the standard model. In the first part of this paper, we improve that CMP by arguing that a building block used in its construction in fact already constitutes a CMP. This not only leads to a simplified exposition, but also saves on the required number of invocations of Pedersen's VSS. Next we show how to construct non-interactive proofs of partial knowledge [8] in this distributed setting. This allows for instance to prove non-interactively the knowledge of l out of m given secrets, without revealing which ones. We also show how to construct efficient non-interactive zero-knowledge proofs for circuit satisfiability in the distributed setting. In the second part, we investigate generalizations to other homomorphic commitment schemes, and show that on the negative side, Pedersen's VSS cannot be generalized to arbitrary (black-box) homomorphic commitment schemes, while on the positive side, commitment schemes based on q-one-way-group-homomorphism [7], which cover wide range of currently used schemes, suffice.", "authors": ["Masayuki Abe", "Ronald Cramer", "Serge Fehr"], "n_citation": 0, "title": "Non-interactive distributed-verifier proofs and proving relations among commitments", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "111bcf9b-7f53-45cd-a36a-5edeb6eb8c24"}
{"abstract": "Differential inclusions-based dynamic feedback neural network models are introduced to solve in real time nonsmooth convex optimization problems restricted on a closed convex subset of R n . First,a differential inclusion-based dynamic feedback neural network model for solving unconstrained optimization problem is established, and its stability and convergence are investigated, then based on the preceding results and the method of successive approximation, differential inclusions-based dynamic feedback neural network models for solving in real time nonsmooth optimization problem on a closed convex subset are successively constructed, and its dynamical behavior and optimization capabilities are analyzed rigorously.", "authors": ["Shiji Song", "Guocheng Li", "Xiaohong Guan"], "n_citation": 0, "title": "Differential Inclusions-Based Neural Networks for Nonsmooth Convex Optimization on a Closed Convex Subset", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "112f89ec-a089-458e-b3c0-4e4ac4c54c27"}
{"abstract": "This paper describes a new algorithm for combinatorial optimization problems and presents the results of our experiments. HOLSA -Heuristic Oscillating Local Search Algorithm- is a neighborhood search algorithm using an evaluation function f inspired from A * , a best-first strategy, a pruning of states as in B&B and operators performing variable steps. All these caracteristics lead to an oscillation principle whereby the search alternates between improving the economic function and satisfying the constraints. We specify how to compute the start state, the evaluation function and the variable steps in order to implement the general outline of HOLSA. Its performance is tested on the multidimensional knapsack problem, using randomly generated problems and classical test problems of the litterature. The experiments show that HOLSA is very efficient, according to the quality of the solutions as well as the search speed, at least on the class of problems studied in this paper. Moreover with large problems, and a limited number of generated nodes, we show that it is better than Branch & Bound, simulated annealing, tabu search and GRASP, both for the quality of the solution and the computational time.", "authors": ["Jean Marc Labat", "Laurent Mynard"], "n_citation": 0, "title": "Oscillation, heuristic ordering and pruning in neighborhood search", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "11bd1e8e-c23b-41ef-b746-adc24f090c45"}
{"abstract": "We aim at a procedure of automatic generation of valency frames for verbs not covered in VALLEX, a lexicon of Czech verbs. We exploit the classification of verbs into syntactico-semantic classes. This article describes our first step to automatically identify verbs of communication and to assign the prototypical frame to them. The method of identification is evaluated against two versions of VALLEX and FrameNet 1.2. For the purpose of frame generation, a new metric based on the notion of frame edit distance is outlined.", "authors": ["V\u00e1clava Bene\u0161ov\u00e1", "Ondrej Bojar"], "n_citation": 0, "title": "Czech verbs of communication and the extraction of their Frames", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "11c533ef-3d2a-4bca-af52-795e194cc0fb"}
{"authors": ["Vladimir B. Berikov", "Gennady Lbov"], "n_citation": 0, "title": "Bayesian model of recognition on a finite set of events", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "1277607a-fabf-4ccd-ad86-41b0e9659bfe"}
{"abstract": "This study aims to identify dependency structure in Korean sentences with the cascaded chunking strategy. In the first stages of the cascade, we find chunks of NP and guess grammatical relations (GRs) using Support Vector Machine (SVM) classifiers for every possible modifier-head pairs of chunks in terms of GR categories as subject, object, complement, adverbial, and etc. In the next stage, we filter out incorrect modifier-head relations in each cascade for its corresponding GR using the SVM classifiers and the characteristics of the Korean language such as distance, no-crossing and case property. Through an experiment with a tree and GR tagged corpus for training the proposed parser, we achieved an overall accuracy of 85.7% on average.", "authors": ["Songwook Lee"], "n_citation": 0, "title": "Cascaded grammatical relation-driven parsing using support vector machines", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1278539b-52b6-4d11-b67f-aa975176f8bf"}
{"abstract": "The statistical learning theory has formulated the Structural Risk Minimization (SRM) principle, based upon the functional form of risk bound on the generalization performance of a learning machine. This paper addresses the application of this formula, which is equivalent to a complexity penalty, to model selection tasks for decision trees, whereas the quantization of the machine capacity for decision trees is estimated using an empirical approach. Experimental results show that, for either classification or regression problems, this novel strategy of decision tree pruning performs better than alternative methods. We name classification and regression trees pruned by virtue of this methodology as Statistical Learning Intelligent Trees (SLIT).", "authors": ["Zhou Yang", "Wenjie Zhu", "Liang Ji"], "n_citation": 0, "title": "SLIT : Designing Complexity Penalty for Classification and Regression Trees Using the SRM Principle", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "141e83b5-9800-4712-9924-23970a22243c"}
{"abstract": "Solving constraint optimization problems is computationally so expensive that it is often impossible to provide a guaranteed optimal solution, either when the problem is too large, or when time is bounded. In these cases, local search algorithms usually provide good solutions. However, and even if an optimality proof is unreachable, it is often desirable to have some guarantee on the quality of the solution found, in order to decide if it is worthwile to spend more time on the problem. This paper is dedicated to the production of intervals, that bound as precisely as possible the optimum of Valued Constraint Satisfaction Problems (VCSP). Such intervals provide an upper bound on the distance of the best available solution to the optimum i.e., on the quality of the optimization performed. Experimental results on random VCSPs and real problems are given.", "authors": ["S. De Givry", "G\u00e9rard Verfaillie", "Thomas Schiex"], "n_citation": 0, "title": "Bounding the optimum of Constraint optimization Problems", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "14559b71-203a-4bcf-ac4c-e28a997573b4"}
{"abstract": "This paper investigates the relations among different partial consistencies which have been proposed for pruning the domains of the variables in constraint systems over the real numbers. We establish several properties of the filtering achieved by the algorithms based upon these partial consistencies. Especially, we prove that: 1) 2B-Consistency (or Hull consistency) algorithms actually yield a weaker pruning than Box-consistency; 2) 3B-Consistency algorithms perform a stronger pruning than Box-consistency. This paper also provides an analysis of both the capabilities and the inherent limits of the filtering algorithms which achieve these partial consistencies.", "authors": ["H\u00e9l\u00e8ne Collavizza", "Fran\u00e7ois Delobel", "Michel Rueher"], "n_citation": 0, "title": "A note on partial consistencies over continuous domains", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "1548fe36-08ee-486e-ba3d-288784efffa0"}
{"abstract": "We describe a resource bounding technique called the Flow Balance Constraint (FBC) to tightly bound the amount of available resource for a set of partially ordered events with piecewise constant resource impact. We provide an efficient algorithm for calculating FBC and bound its complexity. We compare this technique with two existing resource bounding techniques, the Balance Constraint (BC) due to Laborie and the Resource Envelope (E t ) due to Muscettola. We prove that using FBC to halt search under chronological search with a static variable and value order generates smaller search trees than either BC or E t . We also show that E t  and BC are not strictly comparable in terms of the size of the search trees generated under chronological search with a static variable and value order. We then show how to generalize FBC to construct tighter resource bounds but at increased computational cost.", "authors": ["Jeremy Frank"], "n_citation": 0, "title": "Bounding the resource availability of partially ordered events with constant resource impact", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1554c833-8a87-4c99-b61d-1befe5532e6e"}
{"abstract": "Hyavrinen and Oja have proposed an offline Fast-ICA algorithm. But it converge slowly in online form. By using the online whitening algorithm, and applying nature Riemannian gradient in Stiefel manifold, we present in this paper an extended online Fast-ICA algorithm, which can perform online blind source separation (BSS) directly using unwhitened observations. Computer simulation resluts are given to demonstrate the effectiveness and validity of our algorithm.", "authors": ["Gang Wang", "Nini Rao", "Zhilin Zhang", "Quanyi Mo", "Pu Wang"], "n_citation": 0, "title": "An Extended Online Fast-ICA Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1568ace6-d028-4074-a903-43c4853679ca"}
{"abstract": "In least squares support vector (LS-SVM), the key challenge lies in the selection of free parameters such as kernel parameters and tradeoff parameter. However, when a large number of free parameters are involved in LS-SVM, the commonly used grid search method for model selection is intractable. In this paper, SLOO-MPS is proposed for tuning multiple parameters for LS-SVM to overcome this problem. This method is based on optimizing the smooth leave-one-out error via a gradient descent algorithm and feasible to compute. Extensive empirical comparisons confirm the feasibility and validation of the SLOO-MPS.", "authors": ["Liefeng Bo", "Ling Wang", "Licheng Jiao"], "n_citation": 0, "title": "Multiple parameter selection for LS-SVM using smooth leave-one-out error", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "15e5065a-9926-4163-b237-554025b5c70e"}
{"abstract": "Soft constraints based on semirings are a generalization of classical constraints, where tuples of variables' values in each soft constraint are uniquely associated to elements from an algebraic structure called semiring. This framework is able to express, for example, fuzzy, classical, weighted, valued and over-constrained constraint problems. Classical constraint propagation has been extended and adapted to soft constraints by defining a schema for soft local consistency [BMR97]. On the other hand, in [Apt99a,Apt99b] it has been proved that most of the well known constraint propagation algorithms for classical constraints can be cast within a single schema. In this paper we combine these two schema and we show how the framework of [Apt99a,Apt99b] can be used for soft constraints. In doing so, we generalize the concept of soft local consistency, and we prove some convenient properties about its termination.", "authors": ["S. Bistarelli", "Rosella Gennari", "Fabio Rossi"], "n_citation": 0, "title": "Constraint propagation for soft constraints : Generalization and termination conditions", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "15f3629b-5586-4a7e-8614-49faa0f8ec6e"}
{"abstract": "We present a theoretical and computational framework for matching of two-dimensional articulated shapes. Assuming that articulations can be modeled as near-isometries, we show an axiomatic construction of an articulation-invariant distance between shapes, formulated as a generalized multidimensional scaling (GMDS) problem and solved efficiently. Some numerical results demonstrating the accuracy of our method are presented.", "authors": ["Alexander M. Bronstein", "Michael M. Bronstein", "Alfred M. Bruckstein", "Ron Kimmel"], "n_citation": 50, "title": "Matching two-dimensional articulated shapes using generalized multidimensional scaling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1654607f-0598-4c3e-a8ff-5d73d004e44b"}
{"abstract": "Language identification (LID) based on phonotactic modeling is presented in this paper. Approaches using phoneme strings and strings of units automatically derived by an Ergodic HMM (EHMM) are compared. The phoneme recognizers were trained on 6 languages from OGI multi-language-corpus and Czech SpeechDat-E. The LID results are obtained on 4 languages. The results show superiority of Czech phoneme recognizer while used in LID and promising trends using the EHMM-derived units.", "authors": ["Pavel Matejka", "Igor Sz\u00f6ke", "Petr Schwarz", "Jan Cernocky"], "n_citation": 0, "title": "Automatic language identification using phoneme and automatically derived unit strings", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "173cdffe-96f5-4249-a42a-452626a6b6c1"}
{"abstract": "This paper presents an approach to automated marking up of texts with emotional labels. The approach considers in parallel two possible representations of emotions: as emotional categories and emotional dimensions. For each representation, a corpus of example texts previously annotated by human evaluators is mined for an initial assignment of emotional features to words. This results in a List of Emotional Words (LEW) which becomes a useful resource for later automated mark up. The proposed algorithm for automated mark up of text mirrors closely the steps taken during feature extraction, employing for the actual assignment of emotional features a combination of the LEW resource, the ANEW word list, and WordNet for knowledge-based expansion of words not occurring in either. The algorithm for automated mark up is tested and the results are discussed with respect to three main issues: relative adequacy of each one of the representations used, correctness and coverage of the proposed algorithm, and additional techniques and solutions that may be employed to improve the results.", "authors": ["Virginia Francisco", "Pablo Gerv\u00e1s"], "n_citation": 0, "title": "Automated mark up of affective information in english texts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "17d5be4f-6414-43e2-8a86-8cbd9fe92382"}
{"abstract": "In this paper, an edge-based matching algorithm is proposed that combines the generalized Hough transform (GHT) and the Chamfer matching to complement weakness of either method. First, the GHT is used to find approximate object positions and orientations, and then these parameters are refined using the Chamfer matching with distance interpolation. Matching accuracy is further enhanced by using a subpixel algorithm. The algorithm was successfully tested on many images containing various electronic components.", "authors": ["Tai-Hoon Cho"], "n_citation": 0, "title": "Object matching using generalized hough transform and chamfer matching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "17dcfc27-5f9f-4a1a-9224-390e7adcbe16"}
{"abstract": "Ring signature allows to specify a set of possible signers without revealing which member actually produced the signature. This concept was first formalized in 2001 by Rivest, Shamir, and Tauman[3]. Authors of [3] also proposed two versions of ring signature scheme. However, to achieve the goal of anonymity, each user should do much computation in the initial procedure: they should do much work to generate their private and public keys, e.g. in the RSA version, each user should find n i  such that it is the product of two distinct large prime and compute his private/public keys. Moreover, one should extend the one-way trapdoor functions to a common domain since these functions are computed in different domains. This paper's main contribution is to present a version of ring signature scheme which uses a common modulus. Thus, Our proposed scheme is much more efficient in the setup procedure. Further more, the size of public and private keys are reduced.", "authors": ["Chong-zhi Gao", "Zheng-an Yao", "Lei Li"], "n_citation": 0, "title": "A ring signature scheme based on the Nyberg-Rueppel signature scheme", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "184af100-3608-4191-a5fe-d6917e458812"}
{"abstract": "The motion vector prediction (MVP) is an important part of video coding. In the original median predictor, if the neighbor blocks of current block are intra-mode coded, their motion vectors (MVs) will be set to zeros for MVP of current block. This is not very precise for sequences with strong motion. This paper propose an improved motion vector prediction (MVP) scheme for H.264. In the proposed scheme, when there are intra-mode macroblocks beside current block, more MV of the neighbor inter-mode block is utilized instead of zero MVs of intra-mode macroblocks for MVP of current block. The experimental results show that the improved scheme achieves better coding efficiency than the original median predictor. Meanwhile the point obtained by the proposed MVP scheme is closer to the global minimum point, the following fast motion estimation (FME) computation complexity is reduced.", "authors": ["D. Liu", "Debin Zhao", "Qiang Wang", "Wen Gao"], "n_citation": 50, "title": "An Improved Motion Vector Prediction Scheme for Video Coding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1866337d-0edf-4c51-a48e-b7fc0f035205"}
{"abstract": "SmartKom is a multi-modal dialogue system which combines speech with gesture and facial expression. In this paper, we want to deal with one of those phenomena which can be observed in such elaborated systems that we want to call 'offtalk', i.e., speech that is not directed to the system (speaking to oneself, speaking aside). We report the classification results of first experiments which use a large prosodic feature vector in combination with part-of-speech information.", "authors": ["Anton Batliner", "Viktor Zeissler", "Elmar N\u00f6th", "Heinrich Niemann"], "n_citation": 0, "title": "Prosodic classification of offtalk: First experiments", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "18adc243-15e7-4318-b1d7-35c8c88c76c1"}
{"authors": ["Michal Irani", "P. Anandan"], "n_citation": 0, "title": "About direct methods", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "1928eaf1-09cf-472d-8b94-babd1b9acdcf"}
{"abstract": "One of the most appealing features of constraint programming is its rich constraint language for expressing combinatorial optimization problems. This paper demonstrates that traditional combinators from constraint programming have natural counterparts for local search, although their underlying computational model is radically different. In particular, the paper shows that constraint combinators, such as logical and cardinality operators, reification, and first-class expressions can all be viewed as differentiable objects. These combinators naturally support elegant and efficient modelings, generic search procedures, and partial constraint satisfaction techniques for local search. Experimental results on a variety of applications demonstrate the expressiveness and the practicability of the combinators.", "authors": ["Pascal Van Hentenryck", "Laurent Michel", "Liyuan Liu"], "n_citation": 50, "title": "Constraint-based combinators for local search", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1944dc28-691f-4b3c-aedf-23ceaee6d63f"}
{"abstract": "In this paper, a novel method called Twi-Map Support Vector Machines (TMSVM) for multi-classification problems is presented. Our ideas are as follows: Firstly, the training data set is mapped into a high-dimensional feature space. Secondly, we calculate the distances between the training data points and hyperplanes. Thirdly, we view the new vector consisting of the distances as new training data point. Finally, we map the new training data points into another high-dimensional feature space with the same kernel function and construct the optimal hyperplanes. In order to examine the training accuracy and the generalization performance of the proposed algorithm, One-against-One algorithm, Fuzzy Least Square Support Vector Machine (FLS-SVM) and the proposed algorithm are applied to five UCI data sets. Comparison results obtained by using three algorithms are given. The results show that the training accuracy and the testing one of the proposed algorithm are higher than those of One-against-One and FLS-SVM.", "authors": ["Zhifeng Hao", "Bo Liu", "Xiaowei Yang", "Yanchun Liang", "Feng Zhao"], "n_citation": 0, "title": "Twi-map support vector machine for multi-classification problems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "19855a63-442a-449f-b335-17cc182a5d33"}
{"abstract": "The particle swarm optimization (PSO) has been used to train neural networks. But the particles collapse so quickly that it exits a potentially dangerous stagnation characteristic, which would make it impossible to arrive at the global optimum. In this paper, a hybrid PSO with simulated annealing and Chaos search technique (HPSO) is adopted to solve this problem. The HPSO is proposed to train radial basis function (RBF) neural network. Benchmark function optimization and dataset classification problems (Iris, Glass, Wine and New-thyroid) experimental results demonstrate the effectiveness and efficiency of the proposed algorithm.", "authors": ["Haichang Gao", "Boqin Feng", "Yun Hou", "Li Zhu"], "n_citation": 0, "title": "Training RBF Neural Network with Hybrid Particle Swarm Optimization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "19b1b795-5e58-4471-9b71-31a84dcae870"}
{"abstract": "The French nuclear park comprises 58 nuclear reactors distributed through the national territory on 19 geographical sites. They must be repeatedly stopped, for refueling and maintenance. The scheduling of these outages has to comply with various constraints, regarding safety, maintenance logistic, and plant operation, whilst it must contribute to the producer profit maximization. This industrial problem appears to be a hard combinatorial problem that conventional methods used up to now by Electricite de France (mainly based on Mixed Integer Programming) fail to solve properly. We present in this paper a new approach for modeling and solving this problem, combining Constraint Programming (CP) and Local Search. CP is used to find solutions to the outage scheduling problem, while Local Search is used to improve solutions with respect to a heuristic cost criterion. It leads to find solutions as good as with the conventional approaches, but taking into account all the constraints and in very reduced computing time.", "authors": ["Mohand Ou Idir Khemmoudj", "Marc Porcheron", "Hachemi Bennaceur"], "n_citation": 50, "title": "When constraint programming and local search solve the scheduling problem of electricit\u00e9 de france nuclear power plant outages", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "19cf4b84-70ab-4c32-8721-8bfb91fa50f4"}
{"abstract": "The classical view on the process of mutation and affinity maturation that occurs in GCs assumes that their major role is to generate high affinity levels of serum Abs, as well as a dominant pool of high affinity memory B cells, through a very efficient selection process. Here we present a model that considers different types of structures where a mutation selection process occurs, with the aim at discussing the evolution of Germinal Center reactions. Based on the results of this model, we suggest that in addition to affinity maturation, the diversity generated during the GC reaction may have also been important in the evolution towards the presently observed highly organized structure of GC in higher vertebrates.", "authors": ["Jose Faro", "Jaime Combad\u00e3o", "Isabel Gordo"], "n_citation": 0, "title": "Did germinal centers evolve under differential effects of diversity vs affinity", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "19db682a-f854-4e01-a366-4480e839a741"}
{"abstract": "The investigation of some well known noise estimation techniques is presented. The estimated noise is applied in our noise suppression system that is generally used for speech recognition tasks. Moreover, the algorithms are developed to take part in front-end of Distributed Speech Recognition (DSR). Therefore we have proposed some modifications of noise estimation techniques that are quickly adaptable on varying noise and do not need so much information from past segments. We also minimized the algorithmic delay. The robustness of proposed algorithms were tested under several noisy conditions.", "authors": ["Petr Motlicek", "Lukas Burget"], "n_citation": 50, "title": "Efficient noise estimation and its application for robust speech recognition", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "19e81cd6-d120-4441-8d1e-d238ddee8a80"}
{"abstract": "Ad hoc networks have recently been used as a communications medium on tasks that require high levels of communication and coordination. We study the problem of determining an optimal route for a group of wireless units, such that the total connection time among nodes in the resulting mobile ad hoc network is maximized, subject to a limit on the traveled distance. This problem, called the cooperative communication problem in mobile ad hoc networks (CCPM), is modeled using combinatorial optimization and mathematical programming techniques. Applications occur on the coordination of rescue groups, geographical exploration and recognition, unmanned air vehicles (UAVs), and mission coordination. The problem is shown to be NP-hard, and a dynamic programming algorithm is proposed for the problem. Mathematical programming models are presented and computational experiments performed.", "authors": ["Carlos Alberto Serpa de Oliveira"], "n_citation": 0, "title": "Discrete optimization models for cooperative communication in ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1a43bad0-5b65-4f41-8c93-f7c4979f3c3d"}
{"abstract": "Perceptual user interfaces are becoming important nowadays, because they offer a more natural interaction with the computer via speech recognition, haptics, computer vision techniques and so on. In this paper we present a visual-based interface (VBI) that analyzes users' facial gestures and motion. This interface works in real-time and gets the images from a conventional webcam. Due to this, it has to be robust recognizing gestures in webcam standard quality images. The system automatically finds the user's face and tracks it through time for recognizing the gestures within the face region. Then, a new information fusion procedure has been proposed to acquire data from computer vision algorithms and its results are used to carry out a robust recognition process. Finally, we show how the system is used to replace a conventional mouse for human computer interaction. We use the head's motion for controlling the mouse's motion and eyes winks detection to execute the mouse's events.", "authors": ["Cristina Manresa-Yee", "Javier Varona", "Francisco J. Perales"], "n_citation": 50, "title": "Towards hands-free interfaces based on real-time robust facial gesture recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1a629032-7d96-4175-b103-bb47e750a997"}
{"abstract": "We describe a parallel algorithm for modular exponentiation y \u2261 x k  mod n. Then we discuss timing attacks against an implementation of the proposed parallel algorithm for modular exponentiation. When we have two processors, which perform modular exponentiation, an exponent k is scattered into two partial exponents k (0)  and k (1) , where k (0)  and k (1)  are derived by bitwise AND operation from k such that k (0)  = k ^ (0101... 01) 2  and k (1)  = k ^ (1010...10) 2 . Two partial modular exponentiations g 0  \u2261 x k(0)  mod n and y 1  \u2261 x k(1)  mod n are performed in parallel using two processors. Then we can obtain y by computing y \u2261 y 0 y 1  mod n. In general, the hamming weight of k (0)  and k (1)  are smaller than that of k. Thus fast computation of modular exponentiation y \u2261 x k  mod n can be achieved. Moreover we show a timing attack against an implementation of this algorithm. We perform a software simulation of the attack and analyze security of the parallel implementation.", "authors": ["Yasuyuki Sakai", "Kouichi Sakurai"], "n_citation": 0, "title": "Timing attack against implementation of a parallel algorithm for modular exponentiation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1baebd1e-002a-40d2-8b63-58e968f08de5"}
{"abstract": "A combination of Abstract Interpretation (AI) with Integer Linear Programming (ILP) has been successfully used to determine precise upper bounds on the execution times of real-time programs, commonly called worst-case execution times (WCET). The task solved by interpretation is to verify as many local safety properties as possible, safety properties who correspond to the absence of timing accidents. Timing accidents, e.g. cache misses, are reasons for the increase of the execution time of an individual instruction in an execution state. This article attempts to give the answer to the frequently encountered claim, one could have done it by Model Checking (MC)!. It shows that it is the characteristic property of abstract interpretation, which proves AI to be applicable and successful, namely that it only needs one fixpoint iteration to compute invariants that allow the derivation of many safety properties. MC seems to encounter an exponential state-space explosion when faced with the same problem. ILP alone has also been used to model a processor architecture and a program whose upper bounds for execution times was to be determined. It is argued why the only ILP-only approach found in the literature has not led to success.", "authors": ["Reinhard Wilhelm"], "n_citation": 0, "title": "Why AI + ILP is good for WCET, but MC is not, nor ILP alone", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1bb4de74-f979-450d-bfb6-0f2b7873b924"}
{"abstract": "In 1984 Hopfield showed that the time evolution of a symmetric Hopfield neural networks are a motion in state space that seeks out minima in the energy function (i.e., equilibrium point set of Hopfield neural networks). Because high-order Hopfield neural networks have more extensive applications than Hopfield neural networks, the paper will discuss the convergence of high-order Hopfield neural networks. The obtained results ensure that high-order Hopfield neural networks ultimately converge to the equilibrium point set. Our result cancels the requirement of symmetry of the connection weight matrix and includes the classic result on Hopfield neural networks, which is a special case of high-order Hopfield neural networks. In the end, A example is given to verify the effective of our results.", "authors": ["Yi Shen", "Xiaojun Zong", "Minghui Jiang"], "n_citation": 0, "title": "High-order hopfield neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1c51c87d-8306-4b34-9656-afc80fb05b36"}
{"abstract": "We describe a side-channel attack on a substitution block, which is usually implemented as a table lookup operation. In particular, we have investigated smartcard implementations. The attack is based on the identifying equal intermediate results from power measurements while the actual values of these intermediates remain unknown. A powerful attack on substitution blocks can be mounted if the same table is used in multiple iterations and if cross-iteration comparisons are possible. Adversaries can use the method as a part of reverse engineering tools on secret algorithms. In addition to the described method, other methods have to be employed to completely restore the algorithm and its accompanying secret key. We have successfully used the method in a demonstration attack on a secret authentication and session-key generation algorithm implemented on SIM cards in GSM networks. The findings provide guidance for designing smartcard solutions that are secure against this kind of attack.", "authors": ["Roman Novak"], "n_citation": 0, "title": "Side-channel attack on substitution blocks", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1c650b54-08ce-4d1d-9689-8bd65b2c5998"}
{"abstract": "The nonverbal content of speech carries information about the physiological and psychological condition of the speaker. Psychological stress is a pathological element of this condition, of which the cause is accepted to be workload. Objective, quantifiable correlates of stress are searched for by means of measuring the acoustic modifications of the voice brought about by workload. Different voice features from the speech signal to be influenced by stress are: loudness, fundamental frequency, jitter, zero-crossing rate, speech rate and high-energy frequency ratio. To examine the effect of workload on speech production an experiment was designed. 108 native speakers of Dutch were recruited to participate in a stress test (Stroop test). The experiment and the analysis of the test results will be reported in this paper.", "authors": ["L\u00e9on J. M. Rothkrantz", "Pascal Wiggers", "Jan-Willem A. Van Wees", "Robert J. van Vark"], "n_citation": 0, "title": "Voice stress analysis", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1cc4f841-0358-4063-b8d2-7711d594f113"}
{"abstract": "MPEG-21 BSDL offers a solution for exposing the structure of a binary media resource as an XML description, and for the generation of a tailored media resource using a transformed XML description. The main contribution of this paper is the introduction of a real-time work flow for the XML-driven adaptation of H.264/AVC bitstreams in the temporal domain. This real-time approach, which is in line with the vision of MPEG-21 BSDL, is made possible by two key technologies: BFlavor (BSDL + XFlavor) for the efficient generation of XML descriptions and Streaming Transformations for XML (STX) for the efficient transformation of these descriptions. Our work flow is validated in several applications, all using H.264/AVC bitstreams: the exploitation and emulation of temporal scalability, as well as the creation of video skims using key frame selection. Special attention is paid to the deployment of hierarchical B pictures and to the use of placeholder slices for synchronization purposes. Extensive performance data arc also provided.", "authors": ["Wesley De Neve", "Davy De Schrijver", "Davy Van Deursen", "Peter Lambert", "Rik Van de Walle"], "n_citation": 50, "title": "Real-Time BSD-Driven Adaptation Along the Temporal Axis of H.264/AVC Bitstreams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1d0196f6-948f-48f6-b338-aefc60e777a9"}
{"abstract": "This paper discusses methods enhancing the selection of a best parsing tree from the output of natural language syntactic analysis. It presents a method for cutting away redundant parse trees based on the information obtained from a dependency tree-bank corpus. The effectivity of the enhanced parser is demonstrated by results of inter-system parser comparison. The test were run on the standard evaluation grammars (ATIS, CT and PT), our system outperforms the referential implementations.", "authors": ["Ale\u0161 Hor\u00e1k", "Vladim\u00edr Kadlec", "Pavel Smrz"], "n_citation": 0, "title": "Enhancing best analysis selection and parser comparison", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1dac46dd-6cd7-49b6-b584-c78d86c20d95"}
{"abstract": "Constraint Handling Rules (CHR) is a high-level language for writing constraint solvers either from scratch or by modifying existing solvers. An important property of any constraint solver is confluence: The result of a computation should be independent from the order in which constraints arrive and in which rules are applied. In previous work [1], a sufficient and necessary condition for the confluence of terminating CHR programs was given by adapting and extending results about conditional term rewriting systems. In this paper we investigate so-called completion methods that make a non-confluent CHR program confluent by adding new rules. As it turns out, completion can also exhibit inconsistency of a CHR program. Moreover, as shown in this paper, completion can be used to define new constraints in terms of already existing constraints and to derive constraint solvers for them.", "authors": ["Slim Abdennadher", "Thom W. Fr\u00fchwirth"], "n_citation": 0, "title": "On completion of constraint handling rules", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "1dd3ed1a-6889-445c-8742-25d532dc4bd2"}
{"abstract": "The purpose of this study is to identify the hierarchical radial basis function neural networks and select important input features for each sub-RBF neural network automatically. Based on the pre-defined instruction/operator sets, a hierarchical RBF neural network can be created and evolved by using tree-structure based evolutionary algorithm. This framework allows input variables selection, over-layer connections for the various nodes involved. The HRBF structure is developed using an evolutionary algorithm and the parameters are optimized by particle swarm optimization algorithm. Empirical results on benchmark classification problems indicate that the proposed method is efficient.", "authors": ["Yuehui Chen", "Lizhi Peng", "Ajith Abraham"], "n_citation": 0, "title": "Hierarchical Radial Basis Function Neural Networks for Classification Problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1e5f678f-da61-402a-b27e-fb64b8817039"}
{"abstract": "Argumentation is becoming increasingly important in the design and implementation of autonomous software agents. In this paper we discuss our current work on a prototype lightweight Java-based argumentation engine that can be used to implement a non-monotonic reasoning component in Internet or agent-based applications. As far as possible we are aiming towards implementing a general purpose argumentation engine that can be configured to conform to one of a range of semantics.", "authors": ["Daniel Bryant", "Paul Krause"], "n_citation": 50, "title": "An implementation of a lightweight argumentation engine for agent applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1e937ed1-0244-47a3-928b-37301acef102"}
{"authors": ["Tao Chen", "Zhang N", "Y. Wang"], "n_citation": 0, "title": "The role of operation granularity in search-based learning of latent tree models", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "1f57655c-c467-44dd-96dd-b53475d50d9f"}
{"abstract": "This paper deals with the auditory scene analysis via application of ICA in a time-frequency domain. An extension of an original algorithm is presented. This extension consists in Bayesian estimation of a number of independent components via direct implementation of selected grouping principles and via analysis of a structure of the previous time-spans. While the original algorithm is not capable to process sound scenes with fluctuating number of independent sound sources, the presented extension can operate also on sound scenes with the fluctuating number of sound sources.", "authors": ["Ladislava Janku"], "n_citation": 0, "title": "Auditory scene analysis via application of ICA in a time-frequency domain", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1f62228f-18d5-4b20-ac41-e07cf9f39750"}
{"authors": ["Daniele Gianni", "Steve McKeever", "Nicolas P. Smith"], "n_citation": 0, "title": "euHeartDB: A Web-Enabled Database for Geometrical Models of the Heart", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "1f9dd854-6aec-46d0-948c-5d5460758e19"}
{"abstract": "We propose a strategy to support Word Sense Disambiguation (WSD) which is designed specifically for multilingual applications, such as Machine Translation. Co-occurrence information extracted from the translation context, i.e., the set of words which have already been translated, is used to define the order in which disambiguation rules produced by a machine learning algorithm are applied. Experiments on the English-Portuguese translation of seven verbs yielded a significant improvement on the accuracy of a rule-based model: from 0.75 to 0.79.", "authors": ["Lucia Specia", "Maria das Gra\u00e7as", "Volpe Nunes"], "n_citation": 0, "title": "Exploiting the translation context for multilingual WSD", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "204bd14c-800c-4e3e-843e-0e388e390adc"}
{"abstract": "Tailor made security is being enabled by more options for specifying security policies and enhanced possibilities for negotiating security. On the other side these new options raise the complexity of transactions and systems: Users can be overwhelmed, which can lead to less security than before. This paper describes conclusions from a case study and trial of a personal reachability and security manager for telephone based communication. The device helped to negotiate and balance security requirements. The study analysed how much negotiation and detail users could handle during their day-to-day transactions and how they could be supported. Some results are strongly related to more 'classic' security techniques like access control that are becoming more and more interactive: When users learn to understand, the consequences of their access control decisions and can tune their policies these mature to a satisfying level. When users see advantages for their daily activities they are willing to invest more time into understanding additional complexity.", "authors": ["Kai Rannenberg"], "n_citation": 0, "title": "How much negotiation and detail can Users handle? Experiences with security negotiation and the granularity of access control in communications", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "215f6f65-10c4-4f62-9b64-44a0df479159"}
{"authors": ["Manuel Barbosa", "Pooya Farshim"], "n_citation": 0, "title": "Strong knowledge extractors for public-key encryption schemes", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "2160f501-a05e-49f2-8f17-4017c3909fd3"}
{"abstract": "A comprehensive strategy for bidirectional associative memories (BAMs) is presented, which enhances storage capacity greatly. The design strategy combines the dummy augmentation encoding method with optimal gradient descent algorithm. The proposed method increases the storage capacity performance of BAM to its upper limit compared with original Kosko method and optimal gradient descent algorithm. Computer simulations and comparison are given based on three methods to demonstrate the performance improvement of the proposed strategy.", "authors": ["Gengsheng Zheng", "Sidney Nascimento Givigi", "Weiyu Zheng"], "n_citation": 0, "title": "A new strategy for designing bidirectional associative memories", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2165cdf2-37c1-49df-9c45-d17b12bfbb8e"}
{"abstract": "A new specification of an immune network system is proposed. The model works on a set of antibodies from the binary shape-space and it is able to build a stable network and learn new patterns as well. A set of rules based on diversity of the repertoire of patterns which control relations of stimulation and suppression is proposed. The model is described and the results of simple experiments with the implementation of the model without and with presentation of antigens are presented.", "authors": ["Krzysztof Trojanowski", "Marcin Sasin"], "n_citation": 0, "title": "The idiotypic network with binary patterns matching", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2252546c-95ea-4b01-aca8-51094a8c43ed"}
{"abstract": "There are many types of digital watermarking algorithms, but each type corresponds with a certain detecting method to detect the watermark. However, the embedding method is usually unknown, so that it is not possible to know whether the hidden information exists or not. An extensive digital watermarking detecting method based on the known template is proposed in this paper. This method extracts some feature parameters form the spatial, DCT and DWT domains of the image and template, and then use some detecting strategies on those parameters to detect the watermark. The experiment result shows that the correct detecting rate is more than 97%. Obviously, the extensive digital watermarking detection method can be realized, and the method is valuable in theory and practice.", "authors": ["Yang Feng", "Senlin Luo", "Limin Pan"], "n_citation": 0, "title": "An Extensive Method to Detect the Image Digital Watermarking Based on the Known Template", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "228921ab-b472-4cae-ab78-e38de3dcc5d4"}
{"abstract": "Nowadays, the presence of virtual characters is less and less surprising in daily life. However, there is a lack of resources and tools available in the area of visual speech technologies for minority languages. In this paper we present an application to animate in real time virtual characters from live speech in Basque. To get a realistic face animation, the lips must be synchronized with the audio. To accomplish this, we have compared different methods for obtaining the final visemes through HMM based speech recognition techniques. Finally, the implementation of a real prototype has proven the feasibility to obtain a quite natural animation in real time with a minimum amount of training data.", "authors": ["Maider Lehr", "Andoni Arruti", "Amalia Ortiz", "David Oyarzun", "Michael Obach"], "n_citation": 50, "title": "Speech driven facial animation using HMMs in basque", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "23535323-7ff4-48a9-a68f-37ff8d01b231"}
{"abstract": "As Web sites begin to realize the advantages of engaging users in more extended interactions involving information and communication, the log files recording Web usage become more complex. While Web usage mining provides for the syntactic specification of structured patterns like association rules or (generalized) sequences, it is less clear how to analyze and visualize usage data involving longer patterns with little expected structure, without losing an overview of the whole of all paths. In this paper, concept hierarchies are used as a basic method of aggregating Web pages. Interval-based coarsening is then proposed as a method for representing sequences at different levels of abstraction. The tool STRATDYN that implements these methods uses X 2  testing and coarsened stratograms. Stratograms with uniform or differential coarsening provide various detail-and-context views of actual and intended Web usage. Relations to the measures support and confidence, and ways of analyzing generalized sequences are shown. A case study of agent-supported shoppingin an E-commerce site illustrates the formalism.", "authors": ["Bettina Berendt"], "n_citation": 50, "title": "Detail and context in Web usage mining: Coarsening and visualizing sequences", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "237cbfd5-1f7d-43f1-9897-c344d99a0eab"}
{"abstract": "In on-line applications, reinforcement learning based algorithms allow to take into account the environment information in order to propose an action policy for the overall optimization objectives. In this work, it is presented a learning algorithm based on reinforcement learning and temporal differences allowing the on-line parameters adjustment for identification tasks. As a consequence, the reinforcement signal is generically defined in order to minimize the temporal difference.", "authors": ["Mariela Cerrada", "Jose Aguilar", "A. Titli"], "n_citation": 0, "title": "Reinforcement Learning-Based Tuning Algorithm Applied to Fuzzy Identification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "23a0330b-c8a4-494e-8fca-7d8f9ce08725"}
{"abstract": "Existing constraint programming systems offer a fixed set of inference engines implementing search strategies such as single, all, and best solution search. This is unfortunate, since new engines cannot be integrated by the user. The paper presents first-class computation spaces as abstractions with which the user can program inference engines at a high level. Using computation spaces, the paper covers several inference engines ranging from standard search strategies to techniques new to constraint programming, including limited discrepancy search, visual search, and saturation. Saturation is an inference method for tautology-checking used in industrial practice. Computation spaces have shown their practicability in the constraint programming system Oz.", "authors": ["Christian Schulte"], "n_citation": 0, "title": "Programming constraint inference engines", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "23c8e072-8b4d-4ef3-9b5a-bd4f28c433de"}
{"abstract": "Ripple Down Rules is a practical methodology to build knowledge-based systems, which has proved successful in a wide range of commercial applications. However, little work has been done on its theoretical foundations. In this paper, we formalise the key features of the method. We present the process of building a correct knowledge base as a discovery scenario involving a user, an expert, and a system. The user provides data for classification. The expert helps the system to build its knowledge base incrementally, using the output of the latter in response to the last datum provided by the user. In case the system's output is not satisfactory, the expert guides the system to improve its future performance while not affecting its ability to properly classify past data. We examine under which conditions the sequence of knowledge bases constructed by the system eventually converges to a knowledge base that faithfully represents the target classification function. Our results are in accordance with the observed behaviour of real-life systems.", "authors": ["Tri M. Cao", "Eric Martin", "Paul Compton"], "n_citation": 0, "title": "On the convergence of incremental knowledge base construction", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2452bf21-b59f-471a-a06b-ed951123a6dc"}
{"abstract": "Buyer-seller watermarking protocol is a combination of traditional watermarking and fingerprinting techniques. For example, in applications where multimedia content is electronically distributed over a network, the content owner can embed a distinct watermark (a fingerprint), in each copy of the data that is distributed. If unauthorized copies of the data are found, then the origin of the copy can be determined by retrieving the unique watermark corresponding to each buyer. Recently, Ju and Kim proposed an anonymous buyer-seller watermarking protocol, where a buyer can purchase contents anonymously, but the anonymity can be controlled. They used two trusted parties: the watermark certification authority and the judge. The significance of this protocol is that it offered anonymity to watermarking protocol. But this protocol has the problem that honest buyers can be found as guilty, because sellers can recreate the same contents as the buyer's one if he/she colludes with the watermark certification authority and the judge. Thus this scheme must assume existence of the trusted third parties for its security. In this paper, we show shortcomings of this protocol and suggest a buyer-seller watermarking protocol that provides security of buyers and sellers without trusted third party.", "authors": ["Jae-Gwi Choi", "Kouichi Sakurai", "Ji-Hwan Park"], "n_citation": 0, "title": "Does it need trusted third party? Design of Buyer-Seller watermarking protocol without trusted third party", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "24afbdae-01d6-4424-91ce-8170f7df0559"}
{"abstract": "We investigate a minimalistic model of the idiotypic network of B-lymphocytes where idiotypes are represented by bitstrings encoding the nodes of a network. A node is occupied if a lymphocyte clone of the corresponding idiotype exists at the given moment, otherwise it is empty. There is a continuous influx of B-lymphocytes of randomly (by mutation) generated idiotype from the bone marrow. B-lymphocytes are stimulated to proliferate if its receptors (antibodies) are cross-linked by complementary structures. Unstimulated lymphocytes die. Thus, the links of the network connect nodes encoded by complementary bitstrings allowing for a few mismatches. The random evolution leads to a network of highly organized architecture depending on only few parameters. The nodes can be classified into different groups with clearly distinct properties. We report on the building principles which allow to calculate analytically characteristics as the size and the number of links between the groups previously found by simulations.", "authors": ["Holger Schmidtchen", "Ulrich Behn"], "n_citation": 50, "title": "Randomly evolving idiotypic networks : Analysis of building principles", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "252a8a46-6646-45f4-addc-f4935ac8ff63"}
{"abstract": "This paper focuses on the strategies adopted to tackle problematic input and ease communication between modules in a Spanish railway information dialogue system for spontaneous speech. The paper describes the design and tuning considerations followed by the understanding module, both from a language processing and semantic information extraction point of view. Such strategies aim to handle the problematic input received from the speech recogniser, which is due to spontaneous speech as well as recognition errors.", "authors": ["Victoria Arranz", "N\u00faria Castell", "Montserrat Civit"], "n_citation": 0, "title": "Strategies to overcome problematic input in a Spanish dialogue system", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2589cb7d-3eb0-4e09-8c9b-9f90df9c88f3"}
{"abstract": "Based on the linear matrix inequality (LMI), new sufficient conditions on the global exponential stability and asymptotic stability of bidirectional associative memory neural networks with variable delay are presented, and exponential converging velocity index is estimated. Furthermore, the results in this paper are less conservative than the ones reported so far in the literature. One example is given to illustrate the feasibility of our main results.", "authors": ["Minghui Jiang", "Yi Shen", "Xiaoxin Liao"], "n_citation": 0, "title": "An LMI-Based Approach to the Global Stability of Bidirectional Associative Memory Neural Networks with Variable Delay", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "25be6f24-ed6c-43ab-a29e-aafda6c6d4ea"}
{"abstract": "We generalize the Weil descent construction of the GHS attack to arbitrary Artin-Schreier extensions. We give a formula for the characteristic polynomial of Frobenius of the obtained curves and prove that the large cyclic factor of the input elliptic curve is not contained in the kernel of the composition of the conorm and norm maps. As an application we almost square the number of elliptic curves which succumb to the basic GHS attack, thereby weakening curves over F 2 155 further. We also discuss other possible extensions or variations of the GHS attack and conclude that they are not likely to yield further improvements.", "authors": ["Florian Hess"], "n_citation": 0, "title": "The GHS attack revisited", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2695b5ee-f8aa-48db-b733-6afe55c7e418"}
{"abstract": "Polygonal hybrid systems are a subclass of planar hybrid automata which can be represented by piecewise constant differential inclusions. Here, we identify and compute an important object of such systems' phase portrait, namely invariance kernels. An invariant set is a set of initial points of trajectories which keep rotating in a cycle forever and the invariance kernel is the largest of such sets. We show that this kernel is a non-convex polygon and we give a non-iterative algorithm for computing the coordinates of its vertices and edges. Moreover, we present a breadth-first search algorithm for solving the reachability problem for such systems. Invariance kernels play an important role in the algorithm.", "authors": ["Gordon J. Pace", "Gerardo Schneider"], "n_citation": 0, "title": "Model checking polygonal differential inclusions using invariance kernels", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "26ba20f6-f079-43cd-9636-b66bea99336a"}
{"abstract": "Changing customers' behavior require a dramatic change for future manufacturing control concepts. Emerging technologies for distributed application, networking, and intelligent information processing pave the way to reach economic lot size one manufacturing systems. Holonic Manufacturing System provides an approach aiming at this objective. The approach proposed in this work deals with the enhancement of Holons by Web Services for seamless integration in business processes and facilitated reconfigurable shop floor control.", "authors": ["Klaus Glanzer", "Thomas Schmidt", "Gerald Wippel", "Christoph Dutzler"], "n_citation": 0, "title": "Integration of shop floor Holons with automated business processes", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "26ed8467-041f-4801-a771-82c2fc81df04"}
{"authors": ["Boris Goldengorin", "Dmitry Krushinsky"], "n_citation": 50, "title": "A computational study of the pseudo-boolean approach to the p-median problem applied to cell formation", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "2735e911-bc60-4524-83e5-36d489d643f8"}
{"abstract": "We study the synchronization phenomena in a pair of Hindmarsh-Rose(HR) neurons that connected by electrical coupling and chemical coupling and combinations of electrical and chemical coupling. We find that excitatory synapses can antisynchronize two neurons and enough strong inhibition can foster phase synchrony. Investigating the affection of combination of chemical and electrical coupling on network of two HR neurons shows that combining chemical coupling and positive electrical coupling can promotes phase synchrony, and conversely, combining chemical coupling and negative electrical coupling can promotes antisynchrony.", "authors": ["Ying Wu", "Jian-Xue Xu", "Wuyin Jin"], "n_citation": 0, "title": "Synchronous behaviors of two coupled neurons", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "27ac70ad-267f-4761-b297-3786d52cc2ea"}
{"abstract": "we present a color image enhancement method. The proposed method enhances the brightness and contrast of an input image using the low pass and band pass images in Laplacian pyramid, respectively. For color images, our method enhances the color tone by increasing the saturation adpatively according to the intensity of an input image. The major parameters required in our method are automatically set by the human preference data, therefore, the proposed method runs fully automatically without user interaction. Moreover, due to the simplicity and efficiency of the proposed method, a real time implementation and the enhanced results of the image quality was validated through the experiments on various images and video sequences.", "authors": ["Yeul-Min Baek", "Hyoung-Joon Kim", "Jin-Aeon Lee", "Sang-Guen Oh", "Whoi-Yul Kim"], "n_citation": 0, "title": "Color Image Enhancement Using the Laplacian Pyramid", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "285e75c7-55cc-441e-b052-4dc592eb7b4c"}
{"abstract": "The increased usage of mobile devices coupled with an unprecedented demand for information has pushed the scalability problem of pull-based data service to the focus. A broadcast model of streaming data over a wireless medium has been proposed as a viable alternative for information dissemination. In the streaming broadcast model, servers broadcast.data in an asynchronous and unacknowledged mode while clients process personalized and complex queries locally, relieving the load on the server. We address the query processing of streamed XML data, which is fragmented into manageable chunks for easier synchronization. Although there has been some work done in defining algebras that model XQueries on XML documents, no work has been done in defining query algebras for fragmented XML stream data. We define a model for processing fragmented XML stream data, using the concept of holes and fillers. This model offers the flexibility required by the server to disseminate data in manageable fragments, whenever they become available, and to send repetitions, replacements and removal of fragments. We then present a query algebra for XQuery that operates on this streamed XML data model. The XML fragments are operated upon in a continuous pipelined fashion without the need of materializing the transmitted document at the client site.", "authors": ["Sujoe Bose", "Leonidas Fegaras", "David Levine", "Vamsi Chaluvadi"], "n_citation": 0, "title": "A query algebra for fragmented XML stream data", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2874342a-4818-4ec9-89a0-3a627d0af6fd"}
{"authors": ["Nicola Strisciuglio", "Mario Vento", "Nicolai Petkov", "Katrin Amunts", "Lucio Grandinetti", "Th. Lippert", "Nikolai Petkov"], "n_citation": 0, "title": "Bio-Inspired Filters for Audio Analysis", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "288eab80-6614-4ece-8f09-cc7646512555"}
{"authors": ["Florian Mendel", "Christian Rechberger", "Martin Schl\u00e4ffer"], "n_citation": 50, "title": "Cryptanalysis of Twister", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "28a809c0-8627-4ed8-841e-e3f05dd3741a"}
{"abstract": "This paper describes a system of semantic primes necessary for the large-scale semantic representation of event types, encoded as verbal predicates. The system of semantic primes is compiled via mapping modeling elements of the Natural Semantic Metalanguage (NSM), the Semantic Minimum - Dictionary of Bulgarian (SMD), and the Role and Reference Grammar (RRG). The so developed system of semantic primes is a user-defined extension to the metalanguage, adopted in the Unified Eventity Representation (UER), a graphical formalism, introducing the object-oriented design to linguistic semantics.", "authors": ["Milena Slavcheva"], "n_citation": 0, "title": "Semantic representation of events : Building a semantic primes component", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "28b8af4a-6c68-450a-8ba8-af6a35b3ad43"}
{"abstract": "We investigate the certification of temporal properties of untrusted code. This kind of certification has many potential applications, including high confidence extension of operating system kernels. The size of a traditional, proof-based certificate tends to expand drastically because of the state explosion problem. Abstraction-carrying Code (ACC) obtains smaller certificates at the expense of an increased verification time. In addition, a single ACC certificate may be used to certify multiple properties. ACC uses an abstract interpretation of the mobile program as a certificate. A client receiving the code and the certificate will first validate the abstraction and then run a model checker to verify the temporal property. We have developed ACCEPT/C, a certifier of reachability properties for an intermediate language program compiled from C source code, demonstrating the practicality of ACC. Novel aspects of our implementation include: 1) the use of a Boolean program as a certificate; 2) the preservation of Boolean program abstraction during compilation; 3) the encoding of the Boolean program as program assertions in the intermediate program; and 4) the semantics-based validation of the Boolean program via a verification condition generator (VCGen). Our experience of applying ACCEPT/C to real programs, including Linux and NT drivers, shows a significant reduction in certificate size compared to other techniques of similar expressive power; the time spent on model checking is reasonable.", "authors": ["Songtao Xia", "James Hook"], "n_citation": 0, "title": "Certifying temporal properties for compiled C programs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "299da5fe-6cf3-404f-9461-611059837e4d"}
{"abstract": "This paper proposes an algorithm for satisfying systems of linear equality and inequality constraints with hierarchical strengths or preferences. Basically, it is a numerical method that incrementally obtains the LU decompositions of linear constraint systems. To realize this, it introduces a novel technique for analyzing hierarchical systems of linear constraints. In addition, it improves performance by adopting techniques that utilize the sparsity and disjointness of constraint systems. Based on this algorithm, the HiRise constraint solver has been designed and implemented for the use of constructing interactive graphical user interfaces. This paper shows that HiRise is scalable up to thousands of simultaneous constraints in real-time execution.", "authors": ["Hiroshi Hosobe"], "n_citation": 50, "title": "A scalable linear constraint solver for user interface construction", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "2a6b73da-cdbb-4691-bc66-69d41183420e"}
{"abstract": "In this paper, new architectures and comprehensive design methodologies of Genetic Algorithms (GAs) based Fuzzy Relation-based Fuzzy Neural Networks (FRFNN) are introduced and the dynamic search-based GAs is introduced to lead to rapidly optimal convergence over a limited region or a boundary condition. The proposed FRFNN is based on the Fuzzy Neural Networks (FNN) with the extended structure of fuzzy rules being formed within the networks. In the consequence part of the fuzzy rules, three different forms of the regression polynomials such as constant, linear and modified quadratic are taken into consideration. The structure and parameters of the FRFNN are optimized by the dynamic search-based GAs. The proposed model is contrasted with the performance of conventional FNN models in the literature.", "authors": ["Sungkwun Oh", "Byoung-Jun Park", "Witold Pedrycz"], "n_citation": 0, "title": "Design of Fuzzy Neural Networks Based on Genetic Fuzzy Granulation and Regression Polynomial Fuzzy Inference", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2a8af8a2-e428-4d47-a0ec-8f589f7aec11"}
{"abstract": "This paper presents our experience with the lexico-semantic annotation of the Prague Dependency Treebank (PDT). We have used the Czech WordNet (CWN) as an annotation lexicon (repository of lexical meanings) and we annotate each word which is included in the CWN. Based on the error analysis we have performed some experiments with modification of the annotation lexicon (CWN) and consequent re-annotation of occurrences of selected lemmas. We present the results of the annotations and improvements achieved by our corrections.", "authors": ["Eduard Bej\u010dek", "Petra M\u00f6llerov\u00e1", "Pavel Stran\u00e1k"], "n_citation": 0, "title": "The lexico-semantic annotation of PDT : Some results, problems and solutions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2aa0e05e-b1d3-4ddc-a844-fa2fb55a1208"}
{"abstract": "Keyphrase extraction is a task with many applications in information retrieval text mining, and natural language processing. In this paper, a keyphrase extraction approach based on neural network ensemble is proposed. To determine whether a phrase is a keyphrase, the following features of a phrase in a given document are adopted: its term frequency, whether to appear in the title, abstract or headings (subheadings), and its frequency appearing in the paragraphs of the given document. The approach is evaluated by the standard information retrieval metrics of precision and recall. Experiment results show that the ensemble learning can significantly increase the precision and recall.", "authors": ["Jiabing Wang", "Hong Peng", "Jing-Song Hu", "Jun Zhang"], "n_citation": 0, "title": "Ensemble Learning for Keyphrases Extraction from Scientific Document", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2ac04b47-26be-4616-8f7d-275db193763d"}
{"abstract": "This paper proposes a novel adaptive classifier combination scheme based on the cascade of classifier selection and fusion, called adaptive classifier combination scheme (ACCS). In the proposed scheme, system working environment is learned and the environmental context is identified. GA is used to search most effective classifier systems for each identified environmental context. The group of selected classifiers is combined based on GA model for reliable fusion. The knowledge of individual context and its associated chromosomes representing the optimal classifier combination is stored in the context knowledge base. Once the context knowledge is accumulated the system can react to dynamic environment in real time. The proposed scheme has been tested in area of face recognition using standard FERET database, taking illumination as an environmental context. Experimental result showed that using context awareness in classifier combination provides robustness to varying environmental conditions.", "authors": ["Mi Young Nam", "Suman Sedai", "Phill Kyu Rhee"], "n_citation": 0, "title": "Cascade of fusion for adaptive classifier combination using context-awareness", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2acd7dbd-c91f-4350-87eb-29471a6b5bac"}
{"abstract": "We present a generic framework for the automatic and modular inference of sound class invariants for class-based object oriented languages. The idea is to derive a sound class invariant as a conservative ion of the class semantics. In particular we show how a class invariant can be characterized as the solution of a set of equations extracted from the program source. Once a static analysis for the method bodies is supplied, a solution for the former equation system can be iteratively computed. Thus, the class invariant can be automatically inferred. Moreover, our framework is modular since it allows the derivation of class invariants without any hypothesis on the instantiation context and, in the case of subclassing, without accessing to the parent code.", "authors": ["Francesco Logozzo"], "n_citation": 0, "title": "Automatic inference of class invariants", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2ad2d4af-f900-4a17-a1e8-b03ee255700e"}
{"abstract": "The development of object-oriented software starts from requirements expressed commonly as Use Cases. The requirements are then converted into a conceptual or analysis model. Analysis is a fundamental stage because the conceptual model can be shown to satisfy the requirements and becomes the skeleton on which the complete system is built. Most of the use of software patterns until now has been at the design stage and they are applied to provide extensibility and flexibility. However, design patterns don't help avoid analysis errors or make analysis easier. Analysis patterns can contribute more to reusability and software quality than the other varieties. Also, their use contributes to simplifying the development of the analysis model. In particular, a new type of analysis pattern is proposed, called a Semantic Analysis Pattern (SAP), which is in essence a miniapplication, realizing a few Use Cases or a small set of requirements. Using SAPs, a methodology is developed to build the conceptual model in a systematic way.", "authors": ["Eduardo B. Fernandez", "Xiaohong Yuan"], "n_citation": 128, "title": "Semantic analysis patterns", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "2b05b088-8a65-436c-854a-9ec182edba4b"}
{"abstract": "This paper presents a novel clustering algorithm for the structure learning of fuzzy neural networks. Our novel clustering algorithm uses the reward and penalty mechanism for the adaptation of the fuzzy neural networks prototypes for every training sample. This new clustering algorithm can on-line partition the input data, pointwise update the clusters, and self-organize the fuzzy neural structure. No prior knowledge of the input data distribution is needed for initialization. All rules are self-created, and they automatically grow with more incoming data. Our learning algorithm shows that supervised clustering algorithms can be used for the structure learning for the on-line self-organizing fuzzy neural networks. The control of the inverted pendulum is finally used to demonstrate the effectiveness of our learning algorithm.", "authors": ["Haisheng Lin", "Xiao Zhi Gao", "Xianlin Huang", "Zhuoyue Song"], "n_citation": 0, "title": "A Fuzzy Neural Networks with Structure Learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2b0a426f-1ace-4288-b954-ac0f8d13e7db"}
{"abstract": "The min-max modular support vector machine (M 3 -SVM) was proposed for dealing with large-scale pattern classification problems. M 3 -SVM divides training data to several sub-sets, and combine them to a series of independent sub-problems, which can be learned in a parallel way. In this paper, we explore the use of the geometric relation among training data in task decomposition. The experimental results show that the proposed task decomposition method leads to faster training and better generalization accuracy than random task decomposition and traditional SVMs.", "authors": ["Kai-An Wang", "Hai Zhao", "Bao-Liang Lu"], "n_citation": 0, "title": "Task decomposition using geometric relation for min-max modular SVMs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2b5dd657-71c7-474f-94be-f48410fa7dcd"}
{"authors": ["G\u00e9rard Eizenberg", "Jean-Jacques Quisquater"], "n_citation": 0, "title": "Panel session: Watermarking", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "2bca70bc-4703-437a-9a7a-18cf8bf4cc67"}
{"abstract": "Kernel based nonlinear feature extraction approaches, kernel principal component analysis (KPCA) and kernel independent component analysis (KICA), are used for radar high range resolution profiles (HRRP) feature extraction. The time-shift uncertainty of HRRP is handled by a correlation kernel function, and the kernel basis vectors are chosen via a modified LBG algorithm. The classification performance of support vector machine (SVM) classifier based on KPCA and KICA features for measured data are evaluated, which shows that the KPCA and KICA based feature extraction approaches can achieve better classification performance and are more robust to noise as well, comparing with the adaptive Gaussian classifier (AGC).", "authors": ["Hongwei Liu", "Hongtao Su", "Zheng Bao"], "n_citation": 50, "title": "Radar high range resolution profiles feature extraction based on kernel PCA and kernel ICA", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2bf699f1-7619-48ba-9a14-62e6a411cd19"}
{"abstract": "We present an algorithm for correcting communication errors using delivered and undelivered messages. It is used to suggest corrective measures to remove errors introduced by typographical errors in message passing systems like PVM and MPI. The paper focuses on the validity of the algorithm by proving that for a nontrivial number of errors the algorithm can suggest changes to correct the errors. The algorithm has been implemented as a tool in Millipede (Multi Level Interactive Parallel Debugger), which is a support environment developed to assist programmers to debug message passing programs at different abstraction levels.", "authors": ["Jan B\u00e6kgaard Pedersen", "Alan Wagner"], "n_citation": 50, "title": "Correcting errors in message passing systems", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "2c6770df-a7b4-4922-8e65-a6e9b8b1ea06"}
{"abstract": "Data warehouses are designed mostly as centralized systems, and the majority of update maintenance algorithms are tailored for this specific model. Maintenance methods have been proposed either under the assumption of a single view data warehouse, a multi-view centralized model, or a multi-view distributed system with strict synchronization restrictions. We argue that extending this model to a multi-view distributed one, is a practical generalization of the data warehouse system, and the basis for a growing number of applications based on the idea of cooperative views. In this paper we develop a general framework for modeling the maintenance of multi-views in a distributed, decentralized data warehouse, together with an efficient incremental algorithm for view maintenance. To our knowledge, there is no other proposal for a method that incorporates individually and asynchronously updates to different views that are related to each other through derivation dependencies.", "authors": ["Ioana Stanoi", "Dakshi Agrawal", "A. El Abbadi"], "n_citation": 0, "title": "Modeling and maintaining multi-view data warehouses", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "2cac795c-323d-4f3e-b98a-64a53f635192"}
{"abstract": "In this study the usability of two versions of a web based electronic list of literature and information system for blind and visually disabled people were evaluated. Because of the access possibilities of the focus group the applicability for a speech controlled Interface (screen reader, speech controlled web browser) were one point of interest. Furthermore there was focus on the integration of different methods of usability testing to meet the special interests of blind and visually impaired students. These methods were an experimental observation, as well as the collection of subjective user data in an interview and questioner. This paper discusses the influence of characteristics of html-based systems like the link structure and the depth of the indexes on several measures of usability. Because of the results, directions for further research will be presented.", "authors": ["Stefan Riedel", "Wolfgang W\u00fcnschmann"], "n_citation": 0, "title": "Evaluation of a Web based information system for blind and visually impaired students: A descriptive study", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2cd45f3e-788b-4aa0-be8a-30012964c45b"}
{"abstract": "By introducing the Regular Membership Constraint, Gilles Pesant pioneered the idea of basing constraints on formal languages. The paper presented here is highly motivated by this work, taking the obvious next step, namely to investigate constraints based on grammars higher up in the Chomsky hierarchy. We devise an arc-consistency algorithm for context-free grammars, investigate when logic combinations of grammar constraints are tractable, show how to exploit non-constant size grammars and reorderings of languages, and study where the boundaries run between regular, context-free, and context-sensitive grammar filtering.", "authors": ["Meinolf Sellmann"], "n_citation": 53, "title": "The theory of grammar constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d06e9d9-365e-43f3-88aa-e2b97d0b7675"}
{"abstract": "We study the issue of adding a recursion operator to constraint query languages for linear spatial databases. We introduce a language with a bounded inflationary fixpoint operator which is closed and captures the set of polynomial time computable queries over linear constraint databases. This is the first logical characterization of the class of PTIME queries in this context. To prove the result, we develop original techniques to perform arithmetical and geometric operations with constraints.", "authors": ["St\u00e9phane Grumbach", "Gabriel M. Kuper"], "n_citation": 0, "title": "Tractable recursion over geometric data", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "2ddcd6b1-392b-45af-a01d-c9e8f66febcb"}
{"abstract": "This paper presents the Monotone-Pruning algorithm (MP) for computing the minimal coverability set of Petri nets. The original Karp and Miller algorithm (K&M) unfolds the reachability graph of a Petri net and uses acceleration on branches to ensure termination. The MP algorithm improves the K&M algorithm by adding pruning between branches of the K&M tree. This idea was first introduced in the Minimal Coverability Tree algorithm (MCT), however it was recently shown to be incomplete. The MP algorithm can be viewed as the MCT algorithm with a slightly more aggressive pruning strategy which ensures completeness. Experimental results show that this algorithm is a strong improvement over the K&M algorithm. \u00a9 2011 Springer-Verlag.", "authors": ["Pierre-Alain Reynier", "Fr\u00e9d\u00e9ric Servais"], "n_citation": 50, "references": ["22bf363a-dba5-4e97-9215-e9827fe45702", "7f3e4e71-8717-4456-b960-4272bbafc779", "cc5a9112-6a88-4860-9f5b-af230f4b1862"], "title": "Minimal coverability set for Petri nets: Karp and Miller algorithm with pruning", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "2e3fdd09-7a2a-4b02-a389-80cfb2a0aade"}
{"abstract": "In this paper, we introduce a new lattice reduction technique applicable to the narrow, but important class of Hypercubic lattices, (L \u2245 Z N ). Hypercubic lattices arise during transcript analysis of certain GGH, and NTRUSign signature schemes. After a few thousand signatures, key recovery amounts to discovering a hidden unitary matrix U, from its Gram matrix G = UU T . This case of the Gram Matrix Factorization Problem is equivalent to finding the shortest vectors in the hypercubic lattice, L G , defined by the quadratic form G. Our main result is a polynomial-time reduction to a conjecturally easier problem: the Lattice Distinguishing Problem. Additionally, we propose a heuristic solution to this distinguishing problem with a distributed computation of many relatively short vectors.", "authors": ["Michael Szydlo"], "n_citation": 0, "title": "Hypercubic Lattice reduction and analysis of GGH and NTRU signatures", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2e60fa99-c08a-4849-ba4a-eedf6e351add"}
{"abstract": "In order to verify semialgebraic programs, we automatize the Floyd/Naur/Hoare proof method. The main task is to automatically infer valid invariants and rank functions. First we express the program semantics in polynomial form. Then the unknown rank function and invariants are abstracted in parametric form. The implication in the Floyd/Naur/Hoare verification conditions is handled by abstraction into numerical constraints by Lagrangian relaxation. The remaining universal quantification is handled by semidefinite programming relaxation. Finally the parameters are computed using semidefinite programming solvers. This new approach exploits the recent progress in the numerical resolution of linear or bilinear matrix inequalities by semidefinite programming using efficient polynomial primal/dual interior point methods generalizing those well-known in linear programming to convex optimization. The framework is applied to invariance and termination proof of sequential, nondeterministic, concurrent, and fair parallel imperative polynomial programs and can easily be extended to other safety and liveness properties.", "authors": ["Patrick Cousot"], "n_citation": 0, "title": "Proving program invariance and termination by parametric abstraction, Lagrangian relaxation and semidefinite programming", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2e77c444-f7da-4bf0-922e-7c5d95512e28"}
{"abstract": "TypedAssembly Languages (TALs) can be used to validate the safety of assembly-language programs. However, typing rules are usually trusted as axioms. In this paper, we show how to build semantic models for typing judgments in TALs based on an induction technique, so that both the type-safety theorem and the typing rules can be proved as lemmas in a simple logic. We demonstrate this technique by giving a complete model to a sample TAL. This model allows a typing derivation to be interpreted as a machine-checkable safety proof at the machine level.", "authors": ["Gang Tan", "Andrew W. Appel", "Kedar N. Swadi", "Dinghao Wu"], "n_citation": 0, "title": "Construction of a semantic model for a typed assembly language", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2e77fed3-c69b-44bc-8290-a67fb1340556"}
{"abstract": "Given two documents in the form of texts, we present a notion of maximal analogy representing a generalized event sequence of documents with a maximal set of events. They are intended to be used as extended indices of documents to automatically organize a document database from various viewpoints. The maximal analogy is defined so as to satisfy a certain consistency condition and a cost condition. Under the consistency condition, a term in an event sequence is generalized to more term independently of its occurrence positions. The cost condition is introduced so that meaningless similarities between documents are never concluded. As the cost function is monotone, we can present an optimized bottom-up search procedure to discover a maximal analogy under an upper bound of cost. We also show some experimental results based on which we discuss a future plan.", "authors": ["Makoto Haraguchi", "Shigetora Nakano", "Masaharu Yoshioka"], "n_citation": 0, "title": "Discovery of maximal analogies between stories", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2f2aea86-0693-4bfb-b0da-afb14ea07b9b"}
{"abstract": "Inter-block backtracking (IBB) computes all the solutions of sparse systems of non-linear equations over the reals. This algorithm, introduced in 1998 by Bliek et al., handles a system of equations previously decomposed into a set of (small) k x k sub-systems, called blocks. Partial solutions are computed in the different blocks and combined together to obtain the set of global solutions. When solutions inside blocks are computed with interval-based techniques, IBB can be viewed as a new interval-based algorithm for solving decomposed equation systems. Previous implementations used Ilog Solver and its IlcInterval library. The fact that this interval-based solver was more or less a black box implied several strong limitations. The new results described in this paper come from the integration of IBB with the interval-based library developed by the second author. This new library allows IBB to become reliable (no solution is lost) while still gaining several orders of magnitude w.r.t. solving the whole system. We compare several variants of IBB on a sample of benchmarks, which allows us to better understand the behavior of IBB. The main conclusion is that the use of an interval Newton operator inside blocks has the most positive impact on the robustness and performance of IBB. This modifies the influence of other features, such as intelligent backtracking and filtering strategies.", "authors": ["Bertrand Neveu", "Gilles Chabert", "Gilles Trombettoni"], "n_citation": 0, "title": "When interval analysis helps inter-block backtracking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2f8e1d46-8731-42e8-b671-2c804b27f13c"}
{"abstract": "We develop a dynamic modal logic that can be used to model scenarios where agents negotiate over the allocation of a finite number of indivisible resources. The logic includes operators to speak about both preferences of individual agents and deals regarding the reallocation of certain resources. We reconstruct a known result regarding the convergence of sequences of mutually beneficial deals to a Pareto optimal allocation of resources, and discuss the relationship between reasoning tasks in our logic and problems in negotiation. For instance, checking whether a given restricted class of deals is sufficient to guarantee convergence to a Pareto optimal allocation for a specific negotiation scenario amounts to a model checking problem; and the problem of identifying conditions on preference relations that would guarantee convergence for a restricted class of deals under all circumstances can be cast as a question in modal logic correspondence theory.", "authors": ["Ulle Endriss", "Eric Pacuit"], "n_citation": 50, "title": "Modal logics of negotiation and preference", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "30515bbf-a644-4984-8c39-772acdf39877"}
{"abstract": "Model checking is often performed by checking a transformed property on a suitable finite-state abstraction of the source program. Examples include abstractions resulting from symmetry reduction, data independence, and predicate abstraction. The two programs are linked by a structural relationship, such as simulation or bisimulation, guaranteeing that if the transformed property holds on the abstract program, the property holds on the original program. Recently, several algorithms have been developed to automatically generate a deductive proof of correctness from a model checker. A natural question, therefore, is how to 'lift' a deductive proof that is generated for an abstract program back into the original program domain. In this paper, we show how this can be done for general temporal properties, relative to several types of abstraction relationships between the two programs. We develop simplifications of the lifting scheme for common types of abstractions, such as predicate abstraction. We also show how one may generate easily checkable lifted proofs, which find use in applications such as proof-carrying code, and in the use of model checkers as decision procedures in theorem proving.", "authors": ["Kedar S. Namjoshi"], "n_citation": 0, "title": "Lifting temporal proofs through abstractions", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "305c0d66-93e7-4dde-9f1c-74514da3f780"}
{"abstract": "A number of distributed applications require communication services with Quality of Service (QoS) guarantees. The QoS provisioning issue in the Internet has been addressed by the IETF with the definition of the Integrated Services (IntServ) and Differentiated Services (Diffserv) frameworks. Resource reservation mechanisms on which these models are based are totally time-unaware. Yet, we believe that, in some cases, associating a time interval to network resource reservations could be useful for both users and network providers. In this paper we present a distributed scheme for time-dependent reservations in QoS-enabled IP networks. We also show how the standard signalling protocol RSVP may support this new reservation style, with only a few minor modifications. Finally, we present a first prototype implementation of the major component of the proposed architecture and we provide some hints on future applicability scenarios of the advance reservation paradigm and its impact on related topics such as policing and charging techniques in QoS-enabled IP networks.", "authors": ["Roberto Canonico", "Simon Pietro Romano", "Mauro Sellitto", "Giorgio Ventre"], "n_citation": 50, "title": "A scheme for time-dependent resource reservation in QoS-enabled IP networks", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "3078266e-cfce-4662-b0be-f46954d62b11"}
{"abstract": "Attribute value taxonomies (AVTs) have been used to perform AVT-guided decision tree learning on partially or totally missing data. In many cases, user-supplied AVTs are used. We propose an approach to automatically generate an AVT for a given dataset using a genetic algorithm. Experiments on real world datasets demonstrate the feasibility of our approach, generating AVTs which yield comparable performance (in terms of classification accuracy) to that with user supplied AVTs.", "authors": ["Jinu Joo", "Jun Zhang", "Jihoon Yang", "Vasant Honavar"], "n_citation": 0, "title": "Generating AVTs using GA for learning decision tree classifiers with missing data", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "30bc5932-eb7b-4141-b6b5-ead5ed7c519c"}
{"abstract": "In real-life temporal scenarios, uncertainty and preferences are often essential, coexisting aspects. We present a formalism where temporal constraints with both preferences and uncertainty can be defined. We show how three classical notions of controllability (strong, weak and dynamic), which have been developed for uncertain temporal problems, can be generalised to handle also preferences. We then propose algorithms that check the presence of these properties and we prove that, in general, dealing simultaneously with preferences and uncertainty does not increase the complexity beyond that of the separate cases. In particular, we develop a dynamic execution algorithm, of polynomial complexity, that produces plans under uncertainty that are optimal w.r.t. preference.", "authors": ["Francesca Rossi", "Kristen Brent Venable", "Neil Yorke-Smith"], "n_citation": 0, "title": "Controllability of soft temporal constraint problems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "310ea273-868e-4926-94dc-18d0d2f52fab"}
{"abstract": "The universal recognition that it is critical to improve the performance of existing systems and protocols with the understanding to practical service experiences motivates us to discuss this issue in the context of peer-to-peer (P2P) streaming. With the benefit of both practical traces from traditional client-server (C/S) service systems and logs from P2P live broadcasting system, in this paper we first characterize end user behaviors in terms of online duration and reveal the statistically positive correlation between elapsed online duration and expected remaining online time. Then we explore the feasibility to improve the quality of streaming service over P2P networks by proposing Low Disruption Tree Construction (LDTC) algorithm to take the online duration information into account. when peers self-organize into the service overlay. The experiment results show that LDTC could achieve higher stability of video date delivery tree and in turn improve the quality of streaming service.", "authors": ["Yun Tang", "Lifeng Sun", "Jian-Guang Luo", "Yuzhuo Zhong"], "n_citation": 0, "title": "Characterizing User Behavior to Improve Quality of Streaming Service over P2P Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "316ddef9-5b0e-4175-896c-9f372facef83"}
{"authors": ["Jimmy Ho-Man Lee"], "n_citation": 0, "title": "Algebraic properties of CSP model operators", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "319a8ff0-8006-4f1e-b66e-71a398ac4859"}
{"abstract": "We introduce a constraint-based framework for studying infinite qualitative simulations concerned with contingencies such as time, space, shape, size, abstracted into a finite set of qualitative relations. To define the simulations we combine constraints that formalize the background knowledge concerned with qualitative reasoning with appropriate inter-state constraints that are formulated using linear temporal logic. We implemented this approach in a constraint programming system (ECL i PS e ) by drawing on the ideas from bounded model checking. The implementation became realistic only after several rounds of optimizations and experimentation with various heuristics. The resulting system allows us to test and modify the problem specifications in a straightforward way and to combine various knowledge aspects. To demonstrate the expressiveness and simplicity of this approach we discuss in detail two examples: a navigation problem and a simulation of juggling.", "authors": ["Krzysztof R. Apt", "Sebastian Brand"], "n_citation": 50, "title": "Infinite Qualitative Simulations by means of Constraint Programming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "31bae652-1544-4c11-ac8b-df8e1842ba40"}
{"abstract": "Holonic Manufacturing Systems have emerged over the last decade as strategy for manufacturing control system design to cope with rapid changes in manufacturing environment. Resource and component agents integrated in a holarchy are proposed in this paper to dynamically perform co-operative job routing using a distributed algorithm based on the theory of constraints. Members of the holarchy negotiate and compromise on the optimal production flow in order to meet commitments made to each other. Being performed in a distributed manner, the architecture can increase the agility and responsiveness of an integrated system. This flexible structure has been implemented in an open agent environment using JADE agent platform. The performance based results of the simulation experiments are presented and discussed in the paper.", "authors": ["Leonid Sheremetov", "Jorge Mart\u00ednez", "Juan E. Guerra"], "n_citation": 0, "title": "Agent architecture for dynamic job routing in holonic environment based on the theory of constraints", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "32174704-7728-4bd6-b52d-b15103210d89"}
{"abstract": "To optimize performance of applications running over wireless channels, novel state-of-the-art wireless access technologies provide a number of advanced features including dynamic adaptation of protocol parameters at different layers. To exchange control information among non-adjacent layers a cross-layer signalling protocol is needed. Then, the control information should be used by a certain performance control entity to determine the set of protocol parameters resulting in best possible performance of an application in a given wireless channel and traffic conditions. For wireless access technologies with dynamic adaptation of protocol parameters design of the cross-layer performance control system is proposed. Functionalities of components of the system are isolated and described in details.", "authors": ["Dmitri Moltchanov"], "n_citation": 0, "title": "The structure of the reactive performance control system for wireless channels", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3220f0f5-ddd6-4090-842a-7a8a8192cc4f"}
{"abstract": "In this article, we introduce a simple formal semantics for floating-point numbers with errors which is expressive enough to be formally compared to the other methods. Next, we define formal semantics for interval, stochastic, automatic differentiation and error series methods. This enables us to formally compare the properties calculated in each semantics to our reference, simple semantics. Most of these methods having been develpped to verify numerical intensive codes, we also discuss their adequacy to the formal validation of softwares and to static analysis. Finally, this study is completed by experimental results.", "authors": ["Matthieu Martel"], "n_citation": 0, "title": "An overview of semantics for the validation of numerical programs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3226a1a6-f36d-478c-a98d-ad03d604e870"}
{"abstract": "Permutation masks were proposed for reducing the number of holes in Hamming negative selection when applying the r-contiguous or r-chunk matching rule. Here, we show that (randomly determined) permutation masks re-arrange the semantic representation of the underlying data and therefore shatter self-regions. As a consequence, detectors do not cover areas around self regions, instead they cover randomly distributed elements across the space. In addition, we observe that the resulting holes occur in regions where actually no self regions should occur.", "authors": ["Thomas Stibor", "Jonathan Timmis", "Claudia Eckert"], "n_citation": 0, "title": "On permutation masks in hamming negative selection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "324356b1-b1b6-4623-8a9c-c469dd02f7f6"}
{"abstract": "It is wildly recognized that whether the selected kernel matches the data controls the performance of kernel-based methods. Ideally it is expected that the data is linearly separable in the kernel induced feature space, therefore, Fisher linear discriminant criterion can be used as a kernel optimization rule. However, the data may not be linearly separable even after kernel transformation in many applications, a nonlinear classifier is preferred in this case, and obviously the Fisher criterion is not the best choice as a kernel optimization rule. Motivated by this issue, in this paper we present a novel kernel optimization method by maximizing the local class linear separability in kernel space to increase the local margins between embedded classes via localized kernel Fisher criterion, by which the classification performance of nonlinear classifier in the kernel induced feature space can be improved. Extensive experiments are carried out to evaluate the efficiency of the proposed method.", "authors": ["Bo Chen", "Hongwei Liu", "Zheng Bao"], "n_citation": 0, "title": "A Kernel Optimization Method Based on the Localized Kernel Fisher Criterion", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "333e9e01-604b-44b8-919f-e498e407bd3c"}
{"abstract": "A wide range of counting and occurrence constraints can be specified with just two global primitives: the RANGE constraint, which computes the range of values used by a sequence of variables, and the ROOTS constraint, which computes the variables mapping onto a set of values. We focus here on the ROOTS constraint. We show that propagating the ROOTS constraint completely is intractable. We therefore propose a decomposition which can be used to propagate the constraint in linear time. Interestingly, for all uses of the ROOTS constraint we have met. this decomposition does not destroy the global nature of the constraint as we still prune all possible values. In addition, even when the ROOTS constraint is intractable to propagate completely, we can enforce bound consistency in linear time simply by enforcing bound consistency on the decomposition. Finally, we show that specifying counting and occurrence constraints using ROOTS is effective and efficient in practice on two benchmark problems from CSPLib.", "authors": ["Christian Bessiere", "Emmanuel Hebrard", "Brahim Hnich", "Zeynep Kiziltan", "Toby Walsh"], "n_citation": 50, "title": "The ROOTS constraint", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "33d45336-e120-4109-b25b-8dc426edfe64"}
{"abstract": "An approach to intelligent email categorization has been proposed using fast machine learning algorithms. The categorization is based on not only the body but also the header of an email message. The meta-data (e.g. sender name, organization, etc.) provide additional information that can be exploited and improve the categorization capability. Results of experiments on real email data demonstrate the feasibility of our approach. In particular, it is shown that categorization based only on the header information is comparable or superior to that based on all the information in a message.", "authors": ["Jihoon Yang", "Sungyong Park"], "n_citation": 50, "title": "Email categorization using fast machine learning algorithms", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "345b822f-bbb8-4fe7-b1e6-6b2fffe15792"}
{"abstract": "Instances of a polytypic or generic program for a concrete recursive type often exhibit a recursion scheme that is derived from the recursion scheme of the instantiation type. In practice, the programs obtained from a generic program are usually terminating, but the proof of termination cannot be carried out with traditional methods as term orderings alone, since termination often crucially relies on the program type. This problem is tackled by an adaption of type-based termination to generic programming, and a framework for sized polytypic programming is described.", "authors": ["Andreas Abel"], "n_citation": 0, "title": "Towards generic programming with sized types", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3475fd72-aa81-464b-abb4-c5fa80abd75e"}
{"abstract": "There is no need to show the importance of the filtering techniques to solve constraint satisfaction problems i.e. to find values for problem variables subject to constraints that specify which combinations of values are consistent. They can be used during a preprocessing step to remove once and for all some local inconsistencies, or during the search to efficiently prune the search tree. Recently, in [5], a comparison of the most practicable filtering techniques concludes that restricted path consistency (RPC) is a promising local consistency that requires little additional cpu time compared to arc consistency while removing most of the path inverse inconsistent values. However, the RPC algorithm used for this comparison (presented in [1] and called RPC1 in the following) has a non optimal worst case time complexity and bad average time and space complexities. Therefore, we propose RPC2, a new RPC algorithm with O(end 2 ) worst case time complexity and requiring less space than RPC1 in practice. The second aim of this paper is to extend RPC to new local consistencies, k-RPC and Max-RPC, and to compare their pruning efficiency with the other practicable local consistencies. Furthermore, we propose and study a Max-RPC algorithm based on AC-6 that we used for this comparison.", "authors": ["Romuald Debruyne", "Christian Bessiere"], "n_citation": 0, "title": "From restricted path consistency to max-restricted path consistency", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "347d11dc-2730-4723-80aa-619100b3626c"}
{"abstract": "Real-world constraint problems abound with uncertainty. Problems with incomplete or erroneous data are often simplified at present to tractable deterministic models, or modified using error correction methods, with the aim of seeking a solution. However, this can lead us to solve the wrong problem because of the approximations made, an outcome of little help to the user who expects the right problem to be tackled and correct information returned. The certainty closure framework aims at fulfilling these expectations of correct, reliable reasoning in the presence of uncertain data. In this short paper we give an intuition and brief overview of the framework. We define the certainty closure to an uncertain constraint problem and show how it can be derived by transformation to an equivalent certain problem. We outline an application of the framework to a real-world network traffic analysis problem.", "authors": ["Neil Yorke-Smith", "Carmen Gervet"], "n_citation": 50, "title": "On constraint problems with incomplete or erroneous data", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "35ae45d4-30fa-49fc-847c-57fcf350cccf"}
{"abstract": "Several authors have proposed using code modification as a technique for enforcing security policies such as resource limits, access controls, and network information flows. However, these approaches are typically ad hoc and are implemented without a high level abstract framework for code modification. We propose using reflection as a mechanism for implementing code modifications within an abstract framework based on the semantics of the underlying programming language. We have developed a reflective version of Java called Kava that uses byte-code rewriting techniques to insert pre-defined hooks into Java class files at load time. This makes it possible to specify and implement security policies for mobile code in a more abstract and flexible way. Our mechanism could be used as a more principled way of enforcing some of the existing security policies described in the literature. The advantages of our approach over related work (SASI, JRes, etc.) are that we can guarantee that our security mechanisms cannot be bypassed, a property we call strong non-bypassability, and that our approach provides the high level abstractions needed to build useful security policies.", "authors": ["Ian Welch", "Robert J. Stroud"], "n_citation": 0, "title": "Using reflection as a mechanism for enforcing security policies in mobile code", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "35c0b8fb-82d1-4127-9ed6-4196f4df0cdf"}
{"abstract": "The generic group model has recently been used to prove the security of certain asymmetric encryption and signature schemes. This paper presents results that show that there exist problems in that are provably hard in the generic group model but easy to solve whenever the random encoding function is replaced with a specific encoding function (or one drawn from a specific set of encoding functions). In particular we show that there exist cryptographic schemes that are provably hard in the generic group model but easy to break in practice.", "authors": ["Alexander W. Dent"], "n_citation": 0, "title": "Adapting the weaknesses of the random oracle model to the generic group model", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "36017f02-688d-47a0-8348-8db470b3da2a"}
{"abstract": "In wireless communication systems, a conventional call admission control (CAC) mechanism determines whether a node can be admitted to the network by firstly monitoring the received interference plus noise and estimate the achievable signal-to-interference-plus-noise ratio (SINR). However, in the presence of power control, the SINR may vary over time, thus, rendering the conventional CAC inaccurate. The maximum achievable SINR for a new node in a general wireless system depends on the link gains amongst all the co-channel interfering nodes involved. Thus, one of the challenges of CAC in a power-controlled wireless system is the estimation of maximum achievable SINR when information about global link gains is not available. By ignoring the white noise factor, we present a predictor for the maximum achievable signal-to-interference ratio (SIR) of a new node trying to gain access to the medium. Using the SIR predictor we then calculate an optimal active link protection margin, which together with a SIR. threshold would constitute an enhanced threshold value for the new node to attain. By doing so current active communication links would be protected from performance degradation should the maximum achievable SIR value common to all the nodes be lower than the SIR threshold. The accuracy of the predictor is evaluated by means of simulation in terms of mean error and root-mean-square error. Together with finding the corresponding optimal active link protection margin, efficient CAC mechanism to ensure stability of the feasible system can be maintained over a wide range of operating SIR values.", "authors": ["Choong Ming Chin", "Moh Lim Sim", "Sverrir Olafsson"], "n_citation": 0, "title": "Predictive call admission control algorithm for power-controlled wireless systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "362ad203-0512-49ae-91fc-217a2465160a"}
{"abstract": "This paper presents a two level lexical stress assignment model for out of vocabulary Slovenian words used in our text-to-speech system. First, each vowel (and consonant 'r') is determined, whether it is stressed or unstressed, and a type of lexical stress is assigned for every stressed vowel (and consonant 'r'). We applied a machine-learning technique (decision trees or boosted decision trees). Then, some corrections are made on the word level, according the number of stressed vowels and the length of the word. For data sets we used the MULTEXT-East Slovene Lexicon, which was supplemented with lexical stress marks. The accuracy achieved by decision trees significantly outperforms all previous results. However, the sizes of the trees indicate that the accentuation in the Slovenian language is a very complex problem and a simple solution in the form of relatively simple rules is not possible.", "authors": ["Tomaz Sef", "Maja Skrjanc", "Matjaz Gams"], "n_citation": 0, "title": "Automatic lexical stress assignment of unknown words for highly inflected Slovenian language", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "367ae402-7f83-4291-9f45-21ae0beec6b8"}
{"abstract": "We consider the model checking problem for Process Rewrite Systems (PRSs), an infinite-state formalism (non Turing-powerful) which subsumes many common models such as Pushdown Processes and Petri Nets. PRSs can be adopted as formal models for programs with dynamic creation and synchronization of concurrent processes, and with recursive procedures. The model-checking problem for PRSs w.r.t. action-based linear temporal logic (ALTL) is undecidable. However, decidability for some interesting fragment of ALTL remains an open question. In this paper we state decidability results concerning generalized acceptance properties about infinite derivations (infinite term rewriting) in PRSs. As a consequence, we obtain decidability of the model-checking (restricted to infinite runs) for PRSs and a meaningful fragment of ALTL.", "authors": ["Laura Bozzelli"], "n_citation": 50, "title": "Model checking for Process rewrite systems and a class of action-based regular properties", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "368010dd-f015-422f-ae0d-0833bcf3832a"}
{"abstract": "In this article, we explore how neighborhoods for the Large Neighborhood Search (LNS) framework can be automatically defined by the volume of propagation of our Constraint Programming (CP) solver. Thus we can build non trivial neighborhoods which will not be reduced to zero by propagation and whose size will be close to a parameter of the search. Furthermore, by looking at the history of domain reductions, we are able to deduce even better neighborhoods. This idea is validated by numerous experiments with the car sequencing problem. The result is a powerful and completely automatic method that is able to beat our hand-written neighborhoods both in term of performance and of stability. This is in fact the first time for us that a completely generic code is better than a hand-written one.", "authors": ["Laurent Perron", "Paul Shaw", "Vincent Furnon"], "n_citation": 0, "title": "Propagation guided large neighborhood search", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "369bc41c-24d8-4095-a430-496b69c1e506"}
{"abstract": "Gaussian Processes (GPs) have state of the art performance in regression. In GPs, all the basis functions are required for prediction; hence its test speed is slower than other learning algorithms such as support vector machines (SVMs), relevance vector machine (RVM), adaptive sparseness (AS), etc. To overcome this limitation, we present a backward elimination algorithm, called GPs-BE that recursively selects the basis functions for GPs until some stop criterion is satisfied. By integrating rank-1 update, GPs-BE can be implemented at a reasonable cost. Extensive empirical comparisons confirm the feasibility and validity of the proposed algorithm.", "authors": ["Liefeng Bo", "Ling Wang", "Licheng Jiao"], "n_citation": 0, "title": "Sparse Gaussian Processes Using Backward Elimination", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "369bfd60-1b13-4d08-aef2-abdc0ed39f6d"}
{"abstract": "Using hyperspheres as antibody recognition regions is an established abstraction which was initially proposed by theoretical immunologists for use in the modeling of antibody-antigen interactions. This ion is also employed in the development of many artificial immune system algorithms. Here, we show several undesirable properties of hyperspheres, especially when operating in high dimensions and discuss the problems of hyperspheres as recognition regions and how they have affected overall performance of certain algorithms in the context of real-valued negative selection.", "authors": ["Thomas Stibor", "Jonathan Timmis", "Claudia Eckert"], "n_citation": 0, "title": "On the use of hyperspheres in artificial immune systems as antibody recognition regions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "36ae1468-0b05-45a6-be3d-9ad2026d3e83"}
{"abstract": "Automatic labeling of white matter fibres in diffusion-weighted brain MRI is vital for comparing brain integrity and connectivity across populations, but is challenging. Whole brain tractography generates a vast set of fibres throughout the brain, but it is hard to cluster them into anatomically meaningful tracts, due to wide individual variations in the trajectory and shape of white matter pathways. We propose a novel automatic tract labeling algorithm that fuses information from tractography and multiple hand-labeled fibre tract atlases. As streamline tractography can generate a large number of false positive fibres, we developed a top-down approach to extract tracts consistent with known anatomy, based on a distance metric to multiple hand-labeled atlases. Clustering results from different atlases were fused, using a multi-stage fusion scheme. Our \u201clabel fusion\u201d method reliably extracted the major tracts from 105-gradient HARDI scans of 100 young normal adults.", "authors": ["Yan Jin", "Yonggang Shi", "Liang Zhan", "Junning Li", "Greig I. de Zubicaray", "Katie L. McMahon", "Nicholas G. Martin", "Margaret J. Wright", "Paul M. Thompson"], "n_citation": 0, "title": "Automatic population HARDI white matter tract clustering by label fusion of multiple tract atlases", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "36bdf479-7c11-47be-b4f6-953e92598616"}
{"abstract": "Multiview video coding (MVC) plays an important role in three-dimensional audio-video (3DAV) systems. Multiview video display systems are built to provide interactive video services and quality of services (QoS) provided by the system is currently under consideration. MVC encoder uses advanced coding schemes and group of GOP (GoGOP) structure to pursue high compressibility. There is a conflict between compressibility and access ability, i.e., QoS of interaction. In this paper, several evaluation functions are proposed to measure the load and access ability of multiview video system. A nonlinear multipurpose mathematical model based on these functions is provided for interaction analysis. On considering the model, the access ability is a factor to be taken into account for encoder when high compressibility is the primary, and so is the compressibility when trying to achieve high access ability.", "authors": ["You Yaug", "Gangyi Jiang", "Mei Yu", "Zhu Peng"], "n_citation": 0, "title": "A Mathematical Model for Interaction Analysis Between Multiview Video System and User", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3754a3a4-debe-4f4b-b17f-e69e853028a8"}
{"abstract": "Concrete semantics used for abstract interpretation analyses are generally expressed as fixpoints. Checking a property on this kind of semantics can be done by intersecting the fixpoint with a specification related to the property. In this paper, we show how to produce a new, reverse analysis from this specification. The result of this analysis, expressed as a lower closure operator, is then used to guide the initial analysis. With this approach, we can refine the result given by the direct abstract analysis. We show that this method enables to deduce forward analyses from backward analyses (and vice-versa), and to combine them iteratively in a way similar to the forward-backward combination of analyses.", "authors": ["Damien Masse"], "n_citation": 0, "title": "Property checking driven abstract interpretation-based static analysis", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "376cf710-eae8-4884-bc01-e8743da645ef"}
{"abstract": "Mobile ad-hoc networks (MANETs) are especially vulnerable to attacks because of lacking infrastructure and data transfer using radio communication. In the past various types of MANET attacks have been detected and analyzed. In this article primary attack methods (black hole, wormhole, rushing, and Sybil) to MANET routing using the Ad-hoc On-demand Distance Vector (AODV) protocol are modelled and analyzed in a consistent way. This process comprises a semi-formal analysis of pre-conditions (costs, probability of success, required skills) and resulting damage from each attack by means of attack trees. We thereby get a uniform way of modelling and analyzing various attacks to MANET routing that allows to directly compare the different attacks and to assess the risks.", "authors": ["Peter Ebinger", "Tobias Bucher"], "n_citation": 50, "title": "Modelling and analysis of attacks on the MANET routing in AODV", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37a6a451-7829-4f24-9892-cc425ed4f453"}
{"abstract": "For multi-hop wireless networks using IEEE 802.11 most TCP performance degradation results from hidden, exposed terminal problems and bandwidth waste caused by exponential backoff of retransmission timer due to node's mobility, not from collisions. However, in normal DCF algorithm, a failed user increases its contention window (CW) exponentially, thus it reduces the success probability of exposed terminal nodes. That is, these problems will cause burst data transmissions frequently in a particular node which already was successful in packet transmission, because the probability of successful packet transmission rate would be increased. To solve these problems, in this paper, we propose an efficient contention window control (ECWC) scheme to increase TCP performance in wireless multi-hop network. The proposed ECWC scheme is suggested to reduce the hidden and exposed terminal problems of wireless multi-hop network. That is, the proposed scheme increases the number of backoff retransmissions to increase the successful probability rate of MAC transmission, and fixes the contention window at a predetermined value.", "authors": ["Byungjoo Park", "In Huh", "Haniph A. Latchman"], "n_citation": 0, "title": "Performance improvement of TCP with an efficient contention window control mechanism (ECWC) in IEEE 802.11 based multi-hop wireless networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37e1430e-904e-4753-a6c9-6a5211fcfa64"}
{"abstract": "Pipelining is a well-known performance enhancing technique in computer science. Point multiplication is the computationally dominant operation in curve based cryptography. It is generally computed by repeatedly invoking some curve (group) operation like doubling, tripling, halving, addition of group elements. Such a computational procedure may be efficiently computed in a pipeline. More generally, let II be a computational procedure, which computes its output by repeatedly invoking processes from a set of similar processes. Employing pipelining technique may speed up the running time of the computational procedure. To find pipeline sequence by trial and error method is a nontrivial task. In the current work, we present a general methodology, which given any such computational procedure II can find a pipelined version with improved computational speed. To our knowledge, this is the first such attempt in curve based cryptography, where it can be used to speed up the point multiplication methods using inversion-free explicit formula for curves over prime fields. As an example, we employ the proposed general methodology to derive a pipelined version of the hyperelliptic curve binary algorithm for point multiplication and obtain a performance gain of 32% against the ideal theoretical value of 50%.", "authors": ["Kishan Chand Gupta", "Pradeep Kumar Mishra", "Pinakpani Pal"], "n_citation": 0, "title": "A general methodology for pipelining the point multiplication operation in curve based cryptography", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37e8890a-6277-4a7a-961c-68ca2735c78a"}
{"abstract": "In this paper we present a method for mapping 3D unknown environments from stereo images. It is based on a dense disparity image obtained by a process of window correlation. To each image in the sequence a geometrical rectification process is applied, which is essential to remove the conical perspective of the images obtained with a photographic camera. This process corrects the errors in coordinates x and y to obtain a better matching for the map information. The mapping method is an application of the geometrical rectification and the 3D reconstruction, whose main purpose is to obtain a realistic appearance of the scene.", "authors": ["A. J. Gallego S\u00e1nchez", "R. Molina Carmona", "C. Villagr\u00e1 Arnedo"], "n_citation": 0, "title": "Three-dimensional mapping from stereo images with geometrical rectification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "389ee991-bc42-4cd1-aba4-95e50f57e815"}
{"abstract": "We introduce SensorDCSP, a naturally distributed benchmark based on a real-world application that arises in the context of networked distributed systems. In order to study the performance of Distributed CSP (DisCSP) algorithms in a truly distributed setting, we use a discrete-event network simulator, which allows us to model the impact of different network traffic conditions on the performance of the algorithms. We consider two complete DisCSP algorithms: asynchronous backtracking (ABT) and asynchronous weak commitment search (AWC). In our study of different network traffic distributions, we found that, random delays, in some cases combined with a dynamic decentralized restart strategy, can improve the performance of DisCSP algorithms. More interestingly, we also found that the active introduction of message delays by agents can improve performance and robustness, while reducing the overall network load. Finally, our work confirms that AWC performs better than ABT on satisfiable instances. However, on unsatisfiable instances, the performance of AWC is considerably worse than ABT.", "authors": ["C\u00e8sar Fern\u00e1ndez", "Ram\u00f3n B\u00e9jar", "Bhaskar Krishnamachari", "Carla P. Gomes"], "n_citation": 0, "title": "Communication and computation in distributed CSP algorithms", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3902c21f-dd82-4853-b13c-2f2d64d94a99"}
{"authors": ["Jean-Francois Puget"], "n_citation": 0, "title": "Constraint programming next challenge: Simplicity of use", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "394cf406-d48c-4abe-b5dc-2f638121e9db"}
{"abstract": "Nowadays, most of documents are produced in digital format, in which they can be easily accessed and copied. Document copy detection is a very important tool for protecting the author's copyright. We present PPChecker, a document copy detection system based on plagiarism pattern checking. PPChecker calculates the amount of data copied from the original document to the query document, based on linguistically-motivated plagiarism patterns. Experiments performed on CISI document collection show that PPChecker produces better decision information for document copy detection than existing systems.", "authors": ["Namoh Kang", "Alexander F. Gelbukh", "Sangyong Han"], "n_citation": 0, "title": "PPChecker : Plagiarism pattern checker in document copy detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3962375e-1b8e-46d9-bd66-214f0ed51270"}
{"abstract": "With the development of network technology, multimedia applications in various video forms are widely used in network services. In order to leverage video QoS, it becomes a pressing problem to monitor and control video QoS during network transmission of video. In this paper, we propose a monitoring and control framework for video QoS over IP and mobile network. Also, we develop a low computational complexity and more effective video quality assessment (VQA) method based on human visual system (HVS), Improved Human Visual Model (I-HVM), and propose Adaptive and Dynamic Sampling Strategy (ADSS) of video feature, to monitor video quality at both ends of our framework. The experimental results show that our framework can monitor well video QoS over IP and mobile network. Consequence, to leverage video QoS, dynamic control can be applied to transmission decision of video service according to the monitoring results of video QoS by our framework.", "authors": ["Bingjun Zhang", "Lifeng Sun", "Xiaoyu Cheng"], "n_citation": 0, "title": "Video QoS Monitoring and Control Framework over Mobile and IP Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "39de1c47-24e1-4b83-875c-1b22a0512aa9"}
{"abstract": "Constructive Inductive Learning, CIL, aims at learning more accurate or comprehensive concept descriptions by generating new features from the basic features initially given. Most of the existing CIL systems restrict the kinds of functions that can be applied to construct new features, because the search space of feature candidates can be very large. However, so far, no constraint has been applied to combining the basic features. This leads to generating many new but meaningless features. To avoid generating such meaningless features, in this paper, we introduce meta-attributes into CIL, which represent domain knowledge about basic features and allow to eliminate meaningless features. We also propose a Constructive Inductive learning system using Meta-Attributes, CIMA, and experimentally show it can significantly reduce the number of feature candidates.", "authors": ["Kouzou Ohara", "Yukio Onishi", "Noboru Babaguchi", "Hiroshi Motoda"], "n_citation": 0, "title": "Constructive Inductive learning based on meta-attributes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3a5e587b-bfe5-4a25-8722-ca66d0e509d1"}
{"abstract": "The generalization of the satisfiability problem with arbitrary quantifiers is a challenging problem of both theoretical and practical relevance. Being PSPACE-complete, it provides a canonical model for solving other PSPACE tasks which naturally arise in AI. Effective SAT-based solvers have been designed very recently for the special case of boolean constraints. We propose to consider the more general problem where constraints are arbitrary relations over finite domains. Adopting the viewpoint of constraint-propagation techniques so successful for CSPs, we provide a theoretical study of this problem. Our main result is to propose quantified arc-consistency as a natural extension of the classical CSP notion.", "authors": ["Lucas Bordeaux", "Eric Monfroy"], "n_citation": 0, "title": "Beyond NP: Arc-consistency for quantified constraints", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3bc01f29-14c4-46fa-a44d-c6caa5ff0bfc"}
{"authors": ["Florian Mendel", "Christian Rechberger", "Martin Schl\u00e4ffer"], "n_citation": 0, "title": "MD5 is Weaker than Weak: Attacks on Concatenated Combiners", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "3bdae6c2-dc23-4deb-8409-e87cb6616d81"}
{"abstract": "We propose a model for learning the articulated motion of human arm. The goal is to generate plausible trajectories of joints that mimic the human movement using deformation information. The trajectories are then mapped to constraint space. These constraints can be the space of start and end configuration of the human body and task-specific constraints such as avoiding an obstacle, picking up and putting down objects. This movement generalization is a step forward from existing systems that can learn single gestures only. Such a model can be used to develop humanoid robots that move in a human-like way in reaction to diverse changes in their environment. The model proposed to accomplish this uses a combination of principal component analysis (PCA) and a special type of a topological map called the dynamic cell structure (DCS) network. Experiments on a kinematic chain of 2 joints show that this model is able to successfully generalize movement using a few training samples for both free movement and obstacle avoidance.", "authors": ["Stephan Al-Zubi", "Gerald Sommer"], "n_citation": 0, "title": "Learning deformations of human arm movement to adapt to environmental constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3be38038-5ea9-4859-9bb3-72d6f1e942e7"}
{"abstract": "We combine compositional reasoning and reachability analysis to formally verify the safety of a recent cache coherence protocol. The protocol is a detailed implementation of token coherence, an approach that decouples correctness and performance. First, we present a formal and abstract specification that captures the safety substrate of token coherence, and highlights the symmetry in states of the cache controllers and contents of the messages they exchange. Then, we prove that this specification is coherent, and check whether the implementation proposed by the protocol designers is a refinement of the abstract specification. Our refinement proof is parametric in the number of cache controllers, and is compositional as it reduces the refinement checks to individual controllers using a specialized form of assume-guarantee reasoning. The individual refinement obligations are discharged using refinement maps and reachability analysis. While the formal proof justifies the intuitive claim by the designers about the ease of verifiability of token coherence, we report on several bugs in the implementation, and accompanying modifications, that were missed by extensive prior simulations.", "authors": ["Sebastian Burckhardt", "Rajeev Alur", "Milo M. K. Martin"], "n_citation": 0, "title": "Verifying safety of a token coherence implementation by parametric compositional refinement", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3bf1f2ac-5859-441c-9ec6-bbbf96cb6fd0"}
{"abstract": "Where have we been and where are we going? Three types of answers will be discussed: consistent progress, oscillations and discontinuities. Moore's Law provides a convincing demonstration of consistent progress, when it applies. Speech recognition error rates are declining by lOx per decade; speech coding rates are declining by 2x per decade. Unfortunately, fields do not always move in consistent directions. Empiricism dominated the field in the 1950s, and was revived again in the 1990s. Oscillations between Empiricism and Rationalism may be inevitable, with the next revival of Rationalism coining in the 2010s, assuming a 40-year cycle. Discontinuities are a third logical possibility. From time to time, there will be fundamental changes that invalidate fundamental assumptions. As petabytes become a commodity (in the 2010s), old apps like data entry (dictation) will be replaced with new priorities like data consumption (search).", "authors": ["Kenneth Ward Church"], "n_citation": 0, "title": "Speech and language processing: Can we use the past to predict the future?", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "3c25d7c9-522b-4559-be8e-82d56440f80f"}
{"abstract": "In this paper, we propose a novel relevance feedback approach using adaptive clustering based on region representation. Performance of content based image retrieval system is usually very low because of the semantic gap between the low level feature representation and the user's high level concept in a query image. Semantically relevant images may exhibit very different visual characteristics, and maybe scattered in several clusters. Our main goal is finding semantically related clusters to reduce this semantic gap. Our method consists of region based clustering process and cluster-merging process. All segmented regions of relevant images are grouped into semantically related clusters, and clusters are merged by estimating the number of the clusters. We form representatives of clusters as the optimal query. A region based image similarity measure is used to calculate the distance between the multipoint optimal query and an image in the database. Experiments have demonstrated that the proposed approach is effective in improving the performance of image similarity retrieval system.", "authors": ["Deok-Hwan Kim", "Seok-Lyong Lee"], "n_citation": 0, "title": "Relevance feedback using adaptive clustering for region based image similarity retrieval", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c58dfae-b3ee-4256-9164-acb6205375ad"}
{"abstract": "Text embedded in images or videos is indispensable to understand multimedia information. In this paper we propose a new text detection method using the texture feature derived from text strokes. The method consists of four steps: wavelet multiresolution decomposition, thresholding and pixel labeling, text detection using texture features from strokes, and refinement of mask image. Experiment results show that our method is effective.", "authors": ["C. Zhu", "Weiqiang Wang", "Qianhui Ning"], "n_citation": 0, "title": "Text Detection in Images Using Texture Feature from Strokes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c5efffd-95e0-40a5-929a-15d8d594b7f5"}
{"abstract": "This paper is concerned with the optimization method of the cascade architectures of fuzzy neural networks. The structure of the network that deals with a selection of a subset of input variables and their distribution across the individual logic processors (LPs) is optimized with the use of genetic algorithms (GA). We discuss random signal-based learning employing simulated annealing (SARSL), a local search technique, aimed at further refinement of the connections of the neurons (GA-SARSL). A standard data set is discussed with respect to the performance of the constructed networks and their interpretability.", "authors": ["Chang-Wook Han", "Jung-Il Park"], "n_citation": 0, "title": "Simulated Annealing Based Learning Approach for the Design of Cascade Architectures of Fuzzy Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3c8c2741-4795-4127-b379-471f5007157e"}
{"authors": ["W.K. Chan", "Tsong Yueh Chen", "Shing Chi Cheung", "T. H. Tse", "Zhenyu Zhang"], "n_citation": 50, "title": "Towards the testing of power-aware software applications for wireless sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "3cd5e0c6-ea9c-45ad-883e-1807cfcdad1b"}
{"abstract": "The selection of the distance measure to separate the objects of the knowledge space is critical in many classification algorithms. In this paper, we analyze the distance measures reported in the literature for the problem of HIV prediction. We propose a new distance for HIV viral sequences, based on the mutations with regard to the HXB2 reference sequence. In a first step, we reduce data dimensionality in order to subsequently analyze the distance measure's performance in terms of its ability to separate classes. \u00a9 2008 Springer Berlin Heidelberg.", "authors": ["Isis Bonet", "Abdel Rodr\u00edguez", "Ricardo Grau", "Mar\u00eda Matilde Garc\u00eda", "Yvan Y. Saez", "Ann Now\u00e9"], "n_citation": 50, "title": "Comparing distance measures with visual methods", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "3d9bb1d8-4a0a-4a27-8393-a8af826c8cb7"}
{"abstract": "Dedicated Short Range Communications (DSRC) enabled road vehicles are on the brink of actualizing an important application of mobile ad hoc networks. It is crucial that the messages exchanged between the vehicles and between the vehicles and specialized infrastructure be reliable, accurate and confidential. To this end, we propose to identify the security threats inherent in the emerging DSRC Wireless Access in Vehicular Environments (WAVE) architecture. We rank the identified threats according to the European Telecommunications Standards Institute's (ETSI) threat analysis methodology. We also discuss possible countermeasures to the most critical threats.", "authors": ["Christine Laurendeau", "Michel Barbeau"], "n_citation": 0, "title": "Threats to security in DSRC/WAVE", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3db85b92-a8f4-4af9-a341-4bb03c16bdea"}
{"abstract": "Structure-Sweetness Relationships of Aspartame derivatives have been established using fingerprint descriptors by GUHA method. GUHA is the acronym for General Unary Hypotheses Automaton. The glucophoric hypotheses on the reasons for sweetness of aspartame derivatives were generated. Moreover, new results on sweetness receptor site topology have been found. The results were confirmed both by theoretical studies of other authors and chemical evidence. New knowledge obtained can be used for tailoring new aspartame analogous as artificial sweeteners.", "authors": ["Jaroslava Halova", "Premysl Zak", "Pavel Stopka", "Tomoaki Yuzuri", "Yukino Abe", "Kazuhisa Sakakibara", "Hiroko Suezawa", "Minoru Hirota"], "n_citation": 0, "title": "Structure-sweetness Relationships of Aspartame derivatives by GUHA", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3e73c125-079b-4e0b-bc93-d8b3721212de"}
{"abstract": "Multi-category classification is a most interesting problem in the fields of pattern recognition. A one-step method is presented to deal with the multi-category problem. The proposed method converts the problem of classification to the function regression and is applied to solve the converted problem by least squares support vector machines. The novel method classifies the samples in all categories simultaneously only by solving a set of linear equations. Demonstrations of computer experiments are given and good performance is achieved in the simulations.", "authors": ["Jingqing Jiang", "Chunguo Wu", "Yanchun Liang"], "n_citation": 0, "title": "Multi-category classification by least squares support vector regression", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3efc3e08-c52b-4b5d-ba70-287d96bfe438"}
{"abstract": "Three-valued models and logics have been recently advocated as being more suitable to reason about automatically-generated ions of reactive systems than traditional 2-valued models such as standard Kripke structures or Labeled Transition Systems. Indeed, ions specified in 3-valued models are able to distinguish properties that are true, false and unknown of the concrete system, and hence their analysis can yield correctness proofs and counter-examples that can be both guaranteed to be sound. In this paper, we study several 3-valued modeling formalisms proposed in the literature and show that they have the same expressiveness, in the sense that any model specified in any of these formalisms can be translated into a model specified in any other We also show that the complexity of the model checking and generalized model checking problems does not change from one formalism to the other.", "authors": ["Patrice Godefroid", "Radha Jagadeesan"], "n_citation": 103, "title": "On the expressiveness of 3-valued models", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "3f75dab0-68a4-43df-b38f-4a77ef8fbebf"}
{"abstract": "We propose a new method for amorphous bio-compatible computing using deoxyribozyme logic gates [1] in which oligonucleotides act as enzymes on other oligonucleotides, yielding oligonucleotide products. Moreover, these reactions can be controlled by inputs that are also oligonucleotides. We interpret these reactions as logic gates, and the concentrations of chemical species as signals. Since these reactions are homogeneous, i.e., they use oligonucleotides as both inputs and outputs, we can compose them to construct complex logic circuits. Thus, our system for chemical computation offers functionality similar to conventional electronic circuits with the potential for deployment inside of living cells. Previously, this technology was demonstrated in closed-system batch reactions, which limited its computational ability to simple feed-forward circuits. In this work, we go beyond closed systems, and show how to use thermodynamically open reactors to build biomolecular circuits with feedback. The behavior of an open chemical system is determined both by its chemical reaction network and by the influx and efflux of chemical species. This motivates a change in design process from that used with closed systems. Rather than focusing solely on the stoichiometry of the chemical reactions, we must carefully examine their kinetics. Systems of differential equations and the theory of dynamical systems become the appropriate tools for designing and analyzing such systems. Using these tools, we present an inverter. Next, by introducing feedback into the reaction network, we construct devices with a sense of state. We show how a combination of analytical approximation techniques and numerical methods allows us to tune the dynamics of these systems. We demonstrate a flip-flop which exhibits behavior similar to the RS flip-flop of electronic computation. It has two states in which the concentration of one oligonucleotide is high and the other is low or vice versa. We describe how to control the state of the flip-flop by varying the concentration of the substrates. Moreover, there are large regions of parameter space in which this behavior is robust, and we show how to tune the influx rates as a function of the chemical reaction rates in a way that ensures bistability.", "authors": ["Clint Morgan", "Darko Stefanovic", "Cristopher Moore", "Milan N. Stojanovic"], "n_citation": 0, "title": "Building the components for a biomolecular computer", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3f82305c-29f0-4a64-af34-1592bb0d0f9a"}
{"abstract": "Intelligent software agents are a promising approach to improve information logistics in manufacturing enterprises. This paper deals with the application of agents in the area of process planning and production control. Thus, enterprises will be able to fulfil the requirement of flexible, reliable and fault-tolerant manufacturing. Fulfilment of these requirements is a prerequisite for successful participation in modern business alliances like supply chains and virtual enterprises. Thus, agent-based improvements of information logistics enable enterprises to face the challenges of competition successfully. Current research activities focus on the development of agent-based systems for integrated process planning and production control. They led to the IntaPS approach, which is presented in this paper. Furthermore, the integration of different agent-based systems in context of collaborative manufacturing in supply chains will be discussed.", "authors": ["Berend Denkena", "Michael Zwick", "Peer-Oliver Woelk"], "n_citation": 0, "title": "Multiagent-based process planning and scheduling in context of supply chains", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "409a069c-2528-4244-acfc-ca2a6f07f2f2"}
{"abstract": "Several models for transliteration pair acquisition have been proposed to overcome the out-of-vocabulary problem caused by transliterations. To date, however, there has been little literature regarding a framework that can accommodate several models at the same time. Moreover, there is little concern for validating acquired transliteration pairs using up-to-date corpora, such as web documents. To address these problems, we propose a hybrid model for transliteration pair acquisition. In this paper, we concentrate on a framework for combining several models for, transliteration pair acquisition. Experiments showed that our hybrid model was more effective than each individual transliteration pair acquisition model alone.", "authors": ["Oh Jong-Hoon", "Key-Sun Choi", "Hitoshi Isahara"], "n_citation": 0, "title": "A hybrid model for extracting transliteration equivalents from parallel corpora", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "40ac64dc-ec1c-4fa1-b8e6-ff0d19fefa30"}
{"abstract": "With the ever increasing sophistication of attacking techniques, intrusion detection has been a very active field of research. We are designing and implementing a prototype intrusion detection system (IADIDF) that is composed of distributed agents. This paper describes the function of entities, defines the communication and alert mechanism. There are three main agents include Detectors, Managers and Communicators. Each agent operates cooperatively yet independently of the others, providing for efficiency alerts and distribution of resources. Communication mechanism is composed of three layers, which are Transport, Hosts and MessageContent layers. All entities of the prototype are developed in C program under Linux platform. Then, we analyze system performance, advantages of the prototype, and come to a conclusion that the operating of agents will not impact system heavily.", "authors": ["Ye Du", "Huiqiang Wang", "Yonggang Pang"], "n_citation": 0, "title": "A useful system prototype for intrusion detection: Architecture and experiments", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "41ac6c0a-d26f-4f0c-9c72-39a13a107a47"}
{"abstract": "The paper describes the ITC-irst approach for handling spoken dialogue interactions over the telephone. In particular, we have used the technology to develop a telemedicine prototype system that provides health-care to chronic patients. First, the system architecture will be summarized, then we will describe an automatic service for home monitoring of patients affected by hypertension pathology. Patients must periodically introduce data into a database containing their personal medical records. The collected data are managed, according to well established medical guidelines, by a software agent connected to a relational database, which contains the detailed history of each patient.", "authors": ["Ivano Azzini", "Toni Giorgino", "Daniele Falavigna", "Roberto Gretter"], "n_citation": 0, "title": "Application of spoken dialogue technology in a medical domain", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "42b8bbce-0a98-4e47-b1b4-ef217c7be45b"}
{"abstract": "We discuss Topic Detection, a sub-task of the Topic Detection and Tracking (TDT) Project, and present a system that uses domain-informed techniques to group news reports into clusters that capture the narrative of events in the news domain. We present an initial evaluation of this system, and describe an application of these techniques for the clustering of live news feeds. We conclude that these approaches promise more coherent and useful clusters and suggest some areas of future work.", "authors": ["Cormac Flynn", "John Dunnion"], "n_citation": 0, "title": "Event clustering in the news domain", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "42c358bc-c086-406f-a52f-87d0ff9c1392"}
{"abstract": "In this paper, we propose a new architecture of Fuzzy Polynomial Neural Networks (FPNN) by means of genetically optimized Fuzzy Polynomial Neuron (FPN) and discuss its comprehensive design methodology involving mechanisms of genetic optimization, especially Genetic Algorithms (GAs). The conventional FPNNs developed so far are based on mechanisms of self-organization and evolutionary optimization. The proposed FPNN gives rise to a structurally optimized network and comes with a substantial level of flexibility in comparison to the one we encounter in conventional FPNNs. It is shown that the proposed genetic algorithms-based Fuzzy Polynomial Neural Networks is more useful and effective than the existing models for nonlinear process. We experimented with Medical Imaging System (MIS) dataset to evaluate the performance of the proposed model.", "authors": ["Sungkwun Oh", "In-Tae Lee", "Jeoung-Nae Choi"], "n_citation": 0, "title": "Design of Fuzzy Polynomial Neural Networks with the Aid of Genetic Fuzzy Granulation and Its Application to Multi-variable Process System", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "42c3d002-da0a-4e04-ae02-efafc080e446"}
{"abstract": "This paper introduces a new method to prune the domains of the variables in constrained optimization problems where the objective function is defined by a sum y = \u03a3x i , and where variables x i  are subject to difference constraints of the form x j  - x i  < c. An important application area where such problems occur is deterministic scheduling with the mean flow time as optimality criteria. Classical approaches perform a local consistency filtering after each reduction of the bound of y. The drawback of these approaches comes from the fact that the constraints are handled independently. We introduce here a global constraint that enables to tackle simultaneously the whole constraint system, and thus, yields a more effective pruning of the domains of the x i  when the bounds of y are reduced. An efficient algorithm, derived from Dikjstra's shortest path algorithm, is introduced to achieve interval consistency on this global constraint.", "authors": ["Jean-Charles R\u00e9gin", "Michel Rueher"], "n_citation": 0, "title": "A global constraint combining a sum constraint and difference constraints", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "430204ba-fc6e-4328-93a6-6602427947cc"}
{"authors": ["Joon S. Park", "Pratheep Chandramohan", "Joseph Giordano"], "n_citation": 50, "title": "Component-abnormality detection and immunization for survivable systems in large distributed environments", "venue": "international conference on software engineering", "year": 2004, "id": "435ba39d-37e8-4e50-98eb-9fd817e426cf"}
{"abstract": "The Adaptive Constraint Engine (ACE) seeks to automate the application of constraint programming expertise and the extraction of domain-specific expertise. Under the aegis of FORR, an architecture for learning and problem-solving, ACE learns search-order heuristics from problem solving experience. This paper describes ACE's approach, as well as new experimental results on specific problem classes. ACE is both a test-bed for CSP research and a discovery environment for new algorithms.", "authors": ["Susan L. Epstein", "Eugene C. Freuder", "Richard J. Wallace", "Anton Morozov", "Bruce Samuels"], "n_citation": 0, "title": "The Adaptive Constraint Engine", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4390e351-fea5-42f2-b877-75cf9addfd43"}
{"abstract": "In this paper, we introduce the notion of event-oriented k-times revocable if and only if linked group signatures (k-EoRiffL group signatures). In k-EoRiffL group signatures, signers can sign on behalf of a group anonymously and unlinkably up to a permitted number of times (k) per event. No party, even the group manager, can revoke the anonymity of the signer. On the other hand, everyone can identify the signer if he signs more than k times for a particular event. We then show that k-EoRiffL group signatures can be used for k-times anonymous authentication(k-TAA), compact e-cash, e-voting, etc. We formally define security model for the new notion and propose constant-size construction, that is, size of our construction is independent of the size of the group and the number of permitted usage k. Our construction is secure based on the q-strong Diffie-Hellman assumption and the y-DDHI assumption.", "authors": ["Man Ho Au", "Willy Susilo", "Siu-Ming Yiu"], "n_citation": 0, "title": "Event-oriented k-times revocable-iff-linked group signatures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "43ab8ba4-a8c1-4891-ac66-175aeb7bb5a8"}
{"abstract": "The visual secret sharing (VSS for short) scheme is a secret image sharing scheme. A secret image is visually revealed from overlapping shadow images without additional computations. However, the contrast of reconstructed image is much lost. By means of the reversing operation (reverse black and white), Viet and Kurosawa used the traditional VSS scheme to design a VSS scheme which the secret image is almost perfectly reconstructed. Two drawbacks of the Viet-Kurosawa scheme are: (1) one can only reconstruct an almost ideal-contrast image but not an ideal-contrast image (2) the used traditional VSS scheme must be a perfect black scheme. This paper shows a real perfect contrast VSS scheme such that black and white pixels are all perfectly reconstructed within finite runs, no matter what type (perfect black or non-perfect black) of the traditional VSS scheme is.", "authors": ["Ching-Nung Yang", "Chung-Chun Wang", "Tse-Shih Chen"], "n_citation": 50, "title": "Real perfect contrast visual secret sharing schemes with reversing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "43c360ad-d3d9-4eab-8819-d9e7f01c5b90"}
{"authors": ["Joost Vennekens", "Marc Denecker", "Maurice Bruynooghe"], "n_citation": 50, "title": "Embracing events in causal modelling: Interventions and counterfactuals in CP-logic", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "43ea30af-3c14-4302-a4e5-176eba941ef3"}
{"abstract": "This paper studies the strength and direction of phase synchronization among Neural Networks (NNs). First, a nonlinear lumped-parameter cerebral cortex model is addressed and used to generate epileptic surrogate EEG signals. Second, a method that can be used to calculate the strength and direction of phase synchronization among NNs is described including the phase estimation, synchronization index and phase coupling direction. Finally, simulation results show the method addressed in this paper can be used to estimate the phase coupling direction among NNs.", "authors": ["Yan Li", "Xiaoli Li", "Gaoxiang Ouyang", "Xinping Guan"], "n_citation": 50, "title": "Strength and direction of phase synchronization of neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "440f6885-0f8f-4a9b-8f3f-58fe4e7828a5"}
{"abstract": "In this paper we propose an efficient OT N  1  scheme in the bounded storage model, which is provably secure without complexity assumptions. Under the assumption that a public random string of M bits is broadcasted, the protocol is secure against any computationally unbounded dishonest receiver who can store \u03c4M bits, r   2, if the sender and receiver can store N.O(\u221akM) bits, we are able to construct a protocol for OT N  1  which has almost the same complexity as in OT 2  1  scheme. Ding's protocol was constructed by using the interactive hashing protocol which is introduced by Noar, Ostrovsky, Venkatesan and Yung [15] with very large round-complexity. We propose an efficiently extended interactive hashing and analyze its security. This protocol answers partially an open problem raised in [10].", "authors": ["Dowon Hong", "Ku-Young Chang", "Heuisu Ryu"], "n_citation": 0, "title": "Efficient oblivious transfer in the bounded-storage model", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "44762951-ce2b-4159-a831-bc9ee426a071"}
{"abstract": "By combining the adaptive control and linear feedback with the updated laws, an approach of adaptive synchronization and parameters identification of recurrently delayed neural networks with all the parameters unknown is proposed based on the invariance principle of functional differential equations. This approach supplies a systematic and analytical procedure for adaptive synchronization and parameters identification of such uncertain networks, and it is also simple to implement in practice. Theoretical proof and numerical simulation demonstrate the effectiveness and feasibility of the proposed technique.", "authors": ["Jin Zhou", "Tianping Chen", "Lan Xiang"], "n_citation": 0, "title": "Adaptive synchronization of delayed neural networks based on parameters identification", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "44ea7334-252b-4b27-844a-e5439eff446c"}
{"abstract": "A Key Distribution Center enables secure communications among groups of users in a network by providing common keys that can be used with a symmetric encryption algorithm to encrypt and decrypt messages the users wish to send to each other. A Distributed Key Distribution Center is a set of servers of a network that jointly realize a Key Distribution Center. In this paper we propose an unconditionally secure scheme to set up a robust Distributed Key Distribution Center. Such a distributed center keeps working even if some minority of the servers malfunction or misbehave under the control of a mobile adversary. Our scheme for a distributed key distribution center is constructed using unconditionally secure proactive verifiable secret sharing schemes. We review the unconditionally secure verifiable secret sharing scheme described by Stinson and Wei, discuss a problem with the proactive version of that scheme, and present a modified version which is proactively secure.", "authors": ["Paolo D'Arco", "Douglas R. Stinson"], "n_citation": 0, "title": "On unconditionally secure robust Distributed Key Distribution Centers", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "455fe965-017e-473e-a871-e2fd2aba6f95"}
{"abstract": "It is argued that human-denoting nouns in European languages forming pairs like English steward vs. stewardess, or Spanish jefe vs. jefa 'chief', or German Student vs. Studentin 'student', or Russian moskvic vs. moskvicka 'Muscovite' may be featured in factographic databases conjointly as Sex-Dependent Nouns?a special part of speech. Each SDN has two forms, maybe coinciding, selected when necessary by the sex of the denoted person. SDN notion ensures a kind of universality for translation between various languages, being especially convenient in languages with gender of nouns implied by sex. We base our reasoning on Spanish, French, Russian, German, and English examples.", "authors": ["Igor A. Bolshakov", "Sof\u00eda N. Galicia-Haro"], "n_citation": 0, "title": "Featuring of sex-dependent nouns in databases oriented to european languages", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4595af45-e27a-4917-bdce-6d67b2eaceb1"}
{"abstract": "Coordination infrastructures can be used for the general' purpose support of WfMSs (workflow management systems). Suitably-expressive coordination artifacts can be specialised as workflow engines, encapsulating workflow rules expressed in terms of coordination laws. In this paper, we focus on the issue of inter-organisational workflow (IOW), and show how the issue of multiple, interdependent, distributed workflows requires coordination artifacts to be linkable, so as to create a network of inter-connected coordination flows. After discussing a model of workflow engine based on ReSpecT tuple centres, we introduce a distributed workflow architecture based on TuCSoN, exploiting a logic-based workflow language. In particular, we focus on the definition of a scoping mechanism, and show how this enable workflows to be dynamically governed and distributed upon a coordination infrastructure based on artifact linkability. An example of a VE (virtual enterprise) workflow is finally discussed.", "authors": ["Andrea Omicini", "Alessandro Ricci", "Nicola Zaghini"], "n_citation": 0, "title": "Distributed workflow upon linkable coordination artifacts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "45ed3898-8f42-4ca6-8e4e-f4213de390ca"}
{"abstract": "We introduce the operation of domain compression for complete refinements of finite abstract domains. This provides a systematic method for simplifying abstract domains in order to isolate the most abstract domain, when it exists, whose refinement toward completeness for a given semantic function returns a given domain. Domain compression is particularly relevant to compare abstractions in static program analysis and abstract model checking. In this latter case we consider domain compression in predicate abstraction of transition systems.", "authors": ["Roberto Giacobazzi", "Isabella Mastroeni"], "n_citation": 0, "title": "Domain compression for complete abstractions", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "46e08b69-99e4-4237-8cac-657a01a6f096"}
{"abstract": "It is a standard result in the theory of quantum error-correcting codes that no code of length n can fix more than n/4 arbitrary errors, regardless of the dimension of the coding and encoded Hilbert spaces. However, this bound only applies to codes which recover the message exactly. Naively, one might expect that correcting errors to very high fidelity would only allow small violations of this bound. This intuition is incorrect: in this paper we describe quantum error-correcting codes capable of correcting up to [(n - 1)/2] arbitrary errors with fidelity exponentially close to 1, at the price of increasing the size of the registers (i.e., the coding alphabet). This demonstrates a sharp distinction between exact and approximate quantum error correction. The codes have the property that any t components reveal no information about the message, and so they can also be viewed as error-tolerant secret sharing schemes. The construction has several interesting implications for cryptography and quantum information theory. First, it suggests that secret sharing is a better classical analogue to quantum error correction than is classical error correction. Second, it highlights an error in a purported proof that verifiable quantum secret sharing (VQSS) is impossible when the number of cheaters t is n/4. In particular, the construction directly yields an honest-dealer VQSS scheme for t = [(n - 1)/2]. We believe the codes could also potentially lead to improved protocols for dishonest-dealer VQSS and secure multi-party quantum computation. More generally, the construction illustrates a difference between exact and approximate requirements in quantum cryptography and (yet again) the delicacy of security proofs and impossibility results in the quantum model.", "authors": ["Claude Cr\u00e9peau", "Daniel Gottesman", "Adam D. Smith"], "n_citation": 50, "title": "Approximate quantum error-correcting codes and secret sharing schemes", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "471191c3-b7bf-4bf8-a9e2-83fece5c1ed3"}
{"abstract": "This paper presents the results of a set of experiments aiming to characterize the behaviour of the IP data service over a real UMTS network. According to our empirical measurements, packet losses are not frequent, meaning that UMTS link-level retransmission mechanisms are adequate to cope with data corruption through the wireless link. However, ARQ loss recovery mechanisms produce a high variability of the packet delay, which can also affect the performance of the IP data service. By analysing actual UMTS traces, we explore the behaviour of the packet delay through the UMTS network, explaining the effects of the underlying mechanisms. Finally, we derive a simple model to imitate the behaviour of UMTS Internet access. This model could be very useful in simulation or emulation experiments.", "authors": ["Jos\u00e9 Manuel Cano-Garc\u00eda", "Eva Gonz\u00e1lez-Parada", "E. Casilari"], "n_citation": 50, "title": "Experimental analysis and characterization of packet delay in UMTS networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "471b68ad-208a-4c57-9049-760e38c8e4cd"}
{"abstract": "This paper is concerned with the use of statistical information about dependency path length for sentence compression. The sentence compression method employed here requires a quantity called inter-phrase dependency strength. In the training process, original sentences are parsed, and the number of tokens is counted for each pair of phrases, connected with each other by a dependency path of certain length, that survive as a modifier-modified phrase pair in the corresponding compressed sentence in the training corpus. The statistics is exploited to estimate the inter-phrase dependency strength required in the sentence compression process. Results of subjective evaluation shows that the present method outperforms the conventional one of the same framework where the distribution of dependency distance is used to estimate the inter-phrase dependency strength.", "authors": ["Kiwamu Yamagata", "Satoshi Fukutomi", "Kazuyuki Takagi", "Kazuhiko Ozeki"], "n_citation": 0, "title": "Sentence compression using statistical information about dependency path length", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "476b99b4-040b-4110-9c0b-e1d935283974"}
{"abstract": "The disparity energy model can explain physiological properties of binocular neurons in early visual cortex quantitatively. Therefore, many physiologically-plausible models for binocular stereopsis employed the disparity energy model as a model neuron. These models can explain a variety of psychological data. However, most of them cannot handle with stereo transparency. Here, we develop a simple model for transparency perception with the disparity energy model, and examine the ability to detect overlapping disparities. Computer simulations showed that the model properties of transparency detection are consistent with many psychophysical findings.", "authors": ["Osamu Watanabe"], "n_citation": 0, "title": "A Neural Model for Stereo Transparency with the Population of the Disparity Energy Models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4781dc51-c1ea-4f7b-be4c-6d1c1ca97ad1"}
{"abstract": "Some of the best known models for software reliability are based on non homogeneous Poisson processes. Here, we analyze the application of the Chains-of-Rare-Events model to model grouped failures production. As it has been previously shown, this model can be analyzed as a compound Poisson with a Poisson Truncated at Zero as the compounding distribution. We introduce the mode estimator for the parameter of the Poisson Truncated at Zero. This estimator has the important characteristic of quickly reaching stability around the true value. We apply this model to several data and compare it with a non homogenous Poisson process model, and the Poisson distribution compounded by a geometric model.", "authors": ["N\u00e9stor Rub\u00e9n Barraza", "Jonas D. Pfefferman", "Bruno Cernuschi-Frias", "F\u00e9lix Cernuschi"], "n_citation": 0, "title": "An application of the Chains-of-Rare-Events model to software development failure prediction", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "4829dcb7-009d-4e37-a2a5-4118a0546360"}
{"abstract": "In this paper, a new pre-processing step is proposed in the resolution of SAT instances, that recovers and exploits structural knowledge that is hidden in the CNF. It delivers an hybrid formula made of clauses together with a set of equations of the form y = f(x 1 ,...,x n ) where f is a standard connective operator among (V, A, \u21d4) and where y and x i  are boolean variables of the initial SAT instance. This set of equations is then exploited to eliminate clauses and variables, while preserving satisfiability. These extraction and simplification techniques allowed us to implement a new SAT solver that proves to be the most efficient current one w.r.t. several important classes of instances.", "authors": ["Richard Ostrowski", "\u00c9ric Gr\u00e9goire", "Bertrand Mazure", "Lakhdar Sais"], "n_citation": 119, "title": "Recovering and exploiting structural knowledge from CNF formulas", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4861b476-6ba6-4c3c-956f-12312a86448d"}
{"abstract": "Conway's game of Life provides interesting problems in which modelling issues in constraint programming can be explored. The problem of finding a maximum density stable pattern ('still-life') is discussed. A formulation of this problem as a constraint satisfaction problem with 0-1 variables and non-binary constraints is compared with its dual graph translation into a binary CSP. The success of the dual translation is surprising, from previously-reported experience, since it has as many variables as the non-binary CSP and very large domains. An important factor is the identification of many redundant constraints: it is shown that these can safely be removed from a dual graph translation if arc consistency is maintained during search.", "authors": ["Barbara M. Smith"], "n_citation": 0, "title": "A dual graph translation of a problem in 'life'", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "48b1c23b-e334-4cfa-a5cf-0b45e7ab2359"}
{"abstract": "In this article, we present a simple, effective method to learning for an MLP that is based on approximating the Hessian using only local information, specifically, the correlations of output activations from previous layers of hidden neurons. This approach of training the hidden layer weights with the Hessian approximation combined with the training of the final output layer of weights using the pseudoinverse [1] yields improved performance at a fraction of the computational and structural complexity of conventional learning algorithms.", "authors": ["E. J. Teoh", "Cheng Xiang", "Kay Chen Tan"], "n_citation": 0, "title": "A Fast Learning Algorithm Based on Layered Hessian Approximations and the Pseudoinverse", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "48db06cf-8a5e-4fa1-8d2c-c8909e286505"}
{"abstract": "Efficient broadcasting protocols aim to determine a small set of forward nodes to ensure full coverage. Position based broadcast oriented protocols, such as BIP, LBIP, DBIP and LDBIP work well in static and quasi static environment. While before they can be applied in general case where nodes move even during the broadcast process, the impact of mobility should be considered and mobility control mechanism is needed. In existing mobility management, each node periodically sends Hello message and based on received messages construct local view which may be updated at actual transmission time. In this paper, we proposed proactive and predictive mobility control mechanism: node will only send request to collect neighbors' info before transmission which conserves energy consumption of periodical Hello messages; once receiving location request command, nodes will send twice at certain interval; based on received locations, nodes will predict neighbors' position at future actual transmission time, use predicted local view to construct spanning tree and do efficient broadcast operation. We propose localized broadcast oriented protocols with mobility prediction for MANETs, and simulation result shows new protocols can achieve high coverage ratio.", "authors": ["Hui Xu", "Manwoo Jeon", "Shu Lei", "Jinsung Cho", "Sungyoung Lee"], "n_citation": 0, "title": "Localized broadcast oriented protocols with mobility prediction for mobile ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "49b3ae6e-d00a-4b75-8a8e-76152b10d9fd"}
{"abstract": "We define and construct simulatable commitments. These are commitment schemes such that there is an efficient interactive proof system to show that a given string c is a legitimate commitment on a given value v, and furthermore, this proof is efficiently simulatable given any proper pair (c, v). Our construction is provably secure based on the Decisional Diffie-Hellman (DDH) assumption. Using simulatable commitments, we show how to efficiently transform any public coin honest verifier zero knowledge proof system into a proof system that is concurrent zero-knowledge with respect to any (possibly cheating) verifier via black box simulation. By efficient we mean that our transformation incurs only an additive overhead (both in terms of the number of rounds and the computational and communication complexity of each round), and the additive term is close to optimal (for black box simulation): only w(log n) additional rounds, and w(log n) additional public key operations for each round of the original protocol, where n is a security parameter, and w(log n) can be any superlogarithmic function of n independent of the complexity of the original protocol. The transformation preserves (up to negligible additive terms) the soundness and completeness error probabilities, and the new proof system is proved secure based on the DDH assumption, in the standard model of computation, i.e., no random oracles, shared random strings, or public key infrastructure is assumed.", "authors": ["Daniele Micciancio", "Erez Petrank"], "n_citation": 0, "title": "Simulatable commitments and efficient concurrent zero-knowledge", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "49fd15dc-b9ff-4a2f-953d-71680427422a"}
{"authors": ["Zbyn\u011bk Zaj\u00edc", "Luk\u00e1\u0161 Machlica", "Lud\u011bk M\u00fcller"], "n_citation": 0, "title": "Initialization of fMLLR with Sufficient Statistics from Similar Speakers", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "4a028c95-dbea-41c9-b3fa-72c595041064"}
{"abstract": "Science is one of the most creative forms of human reasoning. The recent epistemological and cognitive studies concentrate on the concept of abduction as a means to originate and refine new ideas. Traditional cognitive science accounts concerning abduction aim at illustrating discovery and creativity processes in terms of theoretical and internal aspects, by means of computational simulations and/or abstract cognitive models. A neglected issue, worth of a deepest investigation inside artificial intelligence, is that discovery is often related to a complex cognitive task involving the use and the manipulation of external world. Concrete manipulations of external world is a fundamental passage in the process of knowledge extraction and hypotheses generation: by a process of manipulative abduction it is possible to build prostheses for human minds, by interacting with external objects and representations in a constructive way, and so by creating implicit knowledge through doing. This kind of embodied and unexpressed knowledge holds a key role in the subsequent processes of scientific comprehension and discovery. This paper aims at illustrating the close relationship between external representations and creative processes in scientific explorations and understanding of phenomena.", "authors": ["Lorenzo Magnani", "Matteo Piazza", "Riccardo Dossena"], "n_citation": 50, "title": "The extra-theoretical dimension of discovery. Extracting knowledge by abduction", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4a0768a6-7cc3-4de1-8dba-970f30d0ed98"}
{"abstract": "In this paper we introduce a system for the robust analysis of English using the apporach of Dynamic Syntax, in which the syntactic process is modelled as the word-by-word construction of a semantic representation. We argue that the inherent incrementality of the approach, in contrast with the essentially static assumptions of standard generative grammar, has clear advantages for the task of language modelling. To demonstrate its potential we show that this syntactic approach consistently outperforms a standard trigram model in word recovery tasks on parsable sentences. Furthermore, these results are achieved without recourse to hand-prepared training data.", "authors": ["David Tugwell"], "n_citation": 0, "title": "Language modelling with dynamic syntax", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4a26f92e-3f57-4b47-a685-9f09ee2dc264"}
{"abstract": "In this paper, a method of improving the learning time and convergence rate is proposed to exploit the advantages of artificial neural networks and fuzzy theory to neuron structure. This method is applied to the XOR problem, n bit parity problem, which is used as the benchmark in neural network structure, and recognition of digit image in the vehicle plate image for practical image recognition. As a result of experiments, it does not always guarantee the convergence. However, the network was improved the learning time and has the high convergence rate. The proposed network can be extended to an arbitrary layer. Though a single layer structure is considered, the proposed method has a capability of high speed during the learning process and rapid processing on huge patterns.", "authors": ["Kwang-Baek Kim", "Sungshin Kim", "Young-Hoon Joo", "Am-Sok Oh"], "n_citation": 0, "title": "Enhanced fuzzy single layer perceptron", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4a4b3a09-0913-4a70-b653-806c1c699937"}
{"abstract": "Promises of ubiquitous control of the physical environment by large-scale wireless sensor networks open avenues for new applications that are expected to redefine the way we live and work. Most of recent research has concentrated on developing techniques for performing relatively simple tasks in small-scale sensor networks assuming some form of centralized control. The main contribution of this work is to propose a new way of looking at large-scale sensor networks, motivated by lessons learned from the way biological ecosystems are organized. Indeed, we believe that techniques used in small-scale sensor networks are not likely to scale to large networks; that such large-scale networks must be viewed as an ecosystem in which the sensors/effectors are organisms whose autonomous actions, based on local information, combine in a communal way to produce global results. As an example of a useful function, we demonstrate that fully distributed consensus can be attained in a scalable fashion in massively deployed sensor networks where individual motes operate based on local information, making local decisions that are aggregated across the network to achieve globally-meaningful effects.", "authors": ["Kennie H. Jones", "Kenneth N. Lodding", "Stephan Olariu", "Larry Wilson", "Chunsheng Xin"], "n_citation": 0, "title": "Biology-inspired distributed consensus in massively-deployed sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4a770bf5-932e-4969-9de2-09b4ee5d5839"}
{"abstract": "As electricity markets evolve, there is a need for new modelling approaches that simulate how electric power markets could evolve over time and how participants in these markets may act and react to the changing economic, financial and regulatory environment in which they operate. To study electricity markets behaviour and evolution we propose a multi-agent simulator where agents represent several entities that can be found in electricity markets, such as generators, consumers, market operators and network operators, but also entities that are emerging with the advent of liberalization, such as traders. The simulator probes the possible effects of market rules and conditions by simulating the strategic behaviour of participants. In this paper a special attention is devoted to the behaviour and characteristics of Seller, Buyer and Trader agents, highlighting their strategies and decision processes in order to gain advantage facing the new emerging competitive market.", "authors": ["Isabel Pra\u00e7a", "Carlos Ramos", "Zita Vale", "Manuel Cordeiro"], "n_citation": 0, "title": "An agent-based simulator for electricity markets: Seller, Buyer, and Trader players", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "4a8366b3-603b-448a-b698-5ad2d13ff8ed"}
{"abstract": "One common design for routing protocols in mobile ad hoc networks is to use positioning information. We combine the class of randomized position-based routing strategies called AB (Above-Below) algorithms with face routing to form AB:FACE2:AB routing algorithms, a new class of hybrid routing algorithms in mobile ad hoc networks. Our experiments on unit disk graphs, and their associated Yao sub-graphs and Gabriel sub-graphs, show that the delivery rates of the AB:FACE2:AB algorithms are significantly better than either class of routing algorithms alone when routing is subject to a threshold count beyond which the packet is dropped. The best delivery rates were obtained on the Yao subgraph. With the appropriate choice of threshold, on non-planar graphs, the delivery rates are equivalent to those of face routing (with no threshold) while, on average, discovering paths to their destinations that are several times shorter.", "authors": ["Thomas Fevens", "Alaa Eddien Abdallah", "Badr Naciri Bennani"], "n_citation": 0, "title": "Randomized AB-face-AB routing algorithms in mobile ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4adc25fa-2cf2-4919-a1b3-c75f667494f2"}
{"abstract": "This paper is about the automated production of dialogue models. The goal is to propose and validate a methodology that allows the production of finalized dialogue models (i.e. dialogue models specific for given applications) in a few hours. The solution we propose for such a methodology, called the Rapid Dialogue Prototyping Methodology (RDPM), is decomposed into five consecutive main steps, namely: (1) producing the task model; (2) deriving the initial dialogue model; (3) using a Wizard-of-Oz experiment to instantiate the initial dialogue model; (4) using an internal field test to refine the dialogue model; and (5) using an external field test to evaluate the final dialogue model. All five steps will be described in more detail in the document.", "authors": ["Trung H. Bui", "Martin Rajman", "Miroslav Melichar"], "n_citation": 0, "title": "Rapid dialogue prototyping methodology", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4b1edd25-653c-4557-8dc1-3360f0763824"}
{"abstract": "Activation function is a crucial factor in independent component analysis (ICA) and the best one is the score function defined on the probability density function (pdf) of the source. However, in FastICA, the activation function has to be selected from several predefined choices according to the prior knowledge of the sources, and the problem of how to select or optimize activation function has not been solved yet. In this paper, self-adaptive FastICA is presented based on the generalized Gaussian model (GGM). By combining the optimization of the GGM parameter and that of the demixing vector, a general framework for self-adaptive FastICA is proposed. Convergence and stability of the proposed algorithm are also addressed. Simulation results show that self-adaptive FastICA is effective in parameter optimization and has better accuracy than traditional FastICA.", "authors": ["Gang Wang", "Xin Xu", "Dewen Hu"], "n_citation": 0, "title": "Self-adaptive FastICA based on generalized gaussian model", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4bb66a53-5e25-4207-9b81-1250dca1e256"}
{"abstract": "This paper presents a coordination model, the Actor, Role and Coordinator (ARC) model, to address three main concerns inherent in a pervasive Open Distributed and Embedded (ODE) system: dynamicity, scalability, and stringent QoS requirements. The model treats a pervasive ODE system as a composition of concurrent computation and coerced coordination. In particular, concurrent computation is modeled as Actors, while coerced coordination specifies the system's QoS requirements by mapping them to coordination constraints. The coordination constraints are transparently imposed on actors through message manipulations, which are carried out by the roles and coordinators. The coordinators are responsible for the coordination among roles, while the roles in our model provide abstractions for coordinated behaviors that may be shared by multiple actors and further assume local coordination responsibilities for the actors playing the roles. The role's behavior abstraction decouples the syntactic dependencies between the coordinators and the actors, thus shielding the coordinator layer from the dynamicity of underlying actors inherent in ODE systems. This paper also formally defines the role and coordinator behaviors and the composition of the actor computation model with the proposed coerced coordination model. Our formal study has shown that the ARC system is closed under composition and recursion.", "authors": ["Shangping Ren", "Yue Yu", "Nianen Chen", "Kevin Marth", "Pierre-Etienne Poirot", "Limin Shen"], "n_citation": 0, "title": "Actors, roles and coordinators : A coordination model for open distributed and embedded systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4c103658-5fe7-4732-a10f-8176bb068233"}
{"abstract": "This paper presents an efficient implementation technique for presenting multivariate audit data needed by statistical-based intrusion detection systems. Multivariate data analysis is an important tool in statistical intrusion detection systems. Typically, multivariate statistical intrusion detection systems require visualization of the multivariate audit data in order to facilitate close inspection by security administrators during profile creation and intrusion alerts. However, when applying these intrusion detection schemes to web-based Internet applications, the space complexity of the visualization process is usually prohibiting due to the large number of resources managed by the web server. In order for the approach to be adopted effectively in practice, this paper presents an efficient technique that allows manipulation and visualization of a large amount of multivariate data. Experimental results show that our technique greatly reduces the space requirement of the visualization process, thus allowing the approach to be adopted for monitoring web-based Internet applications.", "authors": ["Zhi Guo", "Kwok-Yan Lam", "Siu-Leung Chung", "Ming Gu", "Jiaguang Sun"], "n_citation": 0, "title": "Efficient presentation of multivariate audit data for intrusion detection of web-based Internet services", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "4c381d86-99a7-4d33-bc34-56411b202aa5"}
{"abstract": "In this paper, we have studied the global attractivity of the equilibrium of Cohen-Grossgerg model with both finite and infinite delays. Criteria for global attractivity are also derived by means of Lyapunov functionals. As a corollary, we show that if the delayed system is dissipative and the coefficient matrix is VL-stable, then the global attractivity of the unique equilibrium is maintained provided the delays are small. Estimates on the allowable sizes of delays are also given. Applications to the Hopfield neural networks with discrete delays are included.", "authors": ["Tao Xiang", "Xiaofeng Liao", "Jian Huang"], "n_citation": 0, "title": "Global attractivity of cohen-grossberg model with delays", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4e71a46d-3e8e-4c28-83c2-aa507f8ae8b4"}
{"abstract": "Rebuilding three-dimensional objects represented by a set of points is a classical problem in computer graphics. Multiple applications like medical imaging or industrial techniques require finding shape from scattered data. Therefore, the reconstruction of a set of points that represents a shape has been widely studied, depending on data source and reconstruction's objectives. This purpose of this paper is to provide an automatic reconstruction from an unorganized cloud describing an unknown shape in order to provide a solution that will allow to compute the object's volume and to deform it with constant volume. The main idea in this paper consists in filling the object's interior with an equipotential surface resulting of the fusion of potential field primitives also called metaballs or blobs. Nevertheless, contrary to most of usual rebuilding methods based on implicit primitives blending, we do not compute any medial axis to set the primary objects. Indeed, a fast voxelization is used to find a contour from the discrete shape and to determine interior areas. Then, the positioning of implicit primitives rely on a multilayer system. Finally, a controlled fusion of the isosurfaces guarantees the lack of any holes and a respectful contour of the original object, such that we obtain a complete shape filling.", "authors": ["Vincent B\u00e9n\u00e9det", "Dominique Faudot"], "n_citation": 0, "title": "An alternative to medial axis for the 3D reconstruction of unorganized set of points using implicit surfaces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4e72b71f-34d9-4f86-aa71-93c4f2147bb0"}
{"abstract": "We extend the logic and semantics of authorization due to Abadi, Lampson, et al. to support restricted delegation. Our formal model provides a simple interpretation for the variety of constructs in the Simple Public Key Infrastructure (SPKI), and lends intuition about possible extensions. We discuss both extensions that our semantics supports and extensions that it cautions against.", "authors": ["Jon Howell", "David Kotz"], "n_citation": 101, "title": "A formal semantics for SPKI", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "4ec0711d-c218-45d3-9fd1-29dcdb14716e"}
{"abstract": "The expansion of the WWW and the growth of data sources lead to the proliferation of heterogeneous data (texts, images, videos, sounds and relational views). We call these data complex data. In order to explore them, we need to carry out their integration into a unified format. Collecting, structuring and storing constitute the different tasks of complex data integration. There exists many approaches for data integration like mediated schemes and wrappers, or warehousing. In this paper, we propose a new approach for complex data integration that uses both classical warehousing approach and multi-agents systems (MAS) technology. We consider the different tasks of the data integration process as services offered by actors called agents. To validate this approach, we have implemented a multi-agent system for complex data integration named SMAIDoC. One of the advantage of The MAS technology is that it provides an evolutive structure to our system.", "authors": ["Omar Boussaid", "Fadila Bentayeb", "Amandine Duffoux", "Frederic Clerc"], "n_citation": 0, "title": "Complex data integration based on a multi-agent system", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "4ec8065b-ae43-4b6d-8bac-bb5c0a81bb04"}
{"abstract": "We present a new method to simulate deformable volumetric objects interactively using finite elements. With quadratic basis functions and a non-linear strain tensor, we are able to model realistic local compression as well as large global deformation. The construction of the differential equations is described in detail including the Jacobian matrix required to solve the non-linear system. The results show that the bending of solids is reflected more realistically than with the linear refinement previously used in computer graphics. At the same time higher frame rates are achieved as the number of elements can be drastically reduced. Finally, an application to virtual tissue simulation is presented with the objective to improve surgical training.", "authors": ["Johannes Mezger", "Wolfgang Strasser"], "n_citation": 0, "title": "Interactive soft object simulation with quadratic finite elements", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f42f6bc-0dda-4bc4-a77e-0dc611972ca2"}
{"abstract": "We study the impact of malicious synchronization on computer systems that serve customers periodically. Systems supporting automatic periodic updates are common in web servers providing regular news update, sports scores or stock quotes. Our study focuses on the possibility of launching an effective low rate attack on the server to degrade performance measured in terms of longer processing time and request drops due to timeouts. The attackers are assumed to behave like normal users and send one request per update cycle. The only parameter utilized in the attack is the timing of the requests sent. By exploiting the periodic nature of the updates, a small number of attackers can herd users' update requests to a cluster and arrive in a short period of time. Herding can be used to discourage new users from joining the system and to modify the user arrival distribution, so that the subsequent burst attack will be effective. While the herding based attacks can be launched with a small amount of resource, they can be easily prevented by adding a small random component to the length of the update interval.", "authors": ["Mun Choon Chan", "Ee-Chien Chang", "Liming Lu", "Peng Song Ngiam"], "n_citation": 50, "title": "Effect of malicious synchronization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f44da8b-92dd-457f-92cb-38fe4b0bd611"}
{"abstract": "This paper presents a robust approach to solve Hoist Scheduling Problems (HSPs) based on an integration of Constraint Logic Programming (CLP) and Mixed Integer Programming (MIP). By contrast with previous dedicated models and algorithms for solving classes of HSPs, we define only one model and run different solvers. The robust approach is achieved by using a CLP formalism. We show that our models for different classes of industrial HSPs are all based on the same generic model. In our hybrid algorithm search is separated from the handling of constraints. Constraint handling is performed by constraint propagation and linear constraint solving. Search is applied by labelling of boolean and integer variables. Computational experience shows that the hybrid algorithm, combining CLP and MIP solvers, solves classes of HSPs which cannot be handled by previous dedicated algorithms. For example, the hybrid algorithm derives an optimal solution, and proves its optimality, for multiple-hoists scheduling problems.", "authors": ["Robert Rodosek", "Mark Wallace"], "n_citation": 0, "title": "A generic model and hybrid algorithm for Hoist Scheduling Problems", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "4f44ea27-a4c0-4a6d-a939-a1bf9e581180"}
{"abstract": "Rebeca is an actor-based language which has been successfully applied to model concurrent and distributed systems. The semantics of Rebeca in labeled transition system is not compositional. In this paper, we investigate the possibility of mapping Rebeca models into a coordination language, Reo, and present a natural mapping that provides a compositional semantics of Rebeca. To this end, we consider reactive objects in Rebeca as components in Reo, and specify their behavior using constraint automata as black-box components within Reo circuits. Modeling coordination and communication among reactive objects as Reo circuits, and the behavior of reactive objects as constraint automata, provides a compositional semantics for Rebeca. Although the result is a compositional model, its visual representation in Reo shows very well that it still reflects the tight coupling inherent in the communication mechanism of object-based paradigms, whereby the real control and coordination is built into the code of the reactive objects themselves. We describe an alternative design that overcomes this deficiency. This illustrates the differences between objects and components, and the challenges in moving from object-based to component-based designs.", "authors": ["Marjan Sirjani", "Mohammad Mahdi Jaghoori", "Christel Baier", "Farhad Arbab"], "n_citation": 0, "title": "Compositional semantics of an actor-based language using constraint automata", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4f9eea4d-3fde-428e-82f7-a604b95a7415"}
{"abstract": "Service discovery is a basic requirement for mobile ad hoc networks to provide service efficiently with dynamically changing network topology. In this paper, we propose a service discovery protocol with maximal area disjoint path (SDMAD) for mobile ad hoc networks. The goal of SDMAD is to exploit multiple servers simultaneously and do parallel delivery. One key feature of SDMAD is that there is no contention between the multiple paths discovered by it. Moreover, SDMAD considers service discovery and routing jointly to reduce control overhead. Extensive simulations show that with SDMAD the performance of service delivery is greatly improved over that with other service discovery protocols.", "authors": ["Shihong Zou", "Le Tian", "Shiduan Cheng", "Yu Lin"], "n_citation": 0, "title": "A service discovery protocol with maximal area disjoint paths for mobile ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "4fad10dd-8157-4107-87e0-15a414608364"}
{"abstract": "When we started to deal with the unit selection technique in ARTIC TTS, the question of the choice of the unit type used within the system was being dealt with. Although the basic version of our TTS system is based on triphones, we decided on the use of diphones in unit selection - mainly due to our concerns about the susceptibility of the unit selection technique to segmentation inaccuracies, and due to a limited experience with the overall system behaviour. However, we also planned to examine the possibilities of the use of triphones. As the first version of our unit selection is being built at present, this paper will examine whether the use of diphones can bring a significant advantage over the use of triphones, and whether there is a clear reason why one type of units behaves better than the other.", "authors": ["Daniel Tihelka", "Jindrich Matousek"], "n_citation": 0, "title": "Diphones vs. Triphones in czech unit selection TTS", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4feecc1d-b126-49b3-90b3-a9462c66c88c"}
{"abstract": "[Context and motivation] This research preview presents ongoing work on a free software requirements modeling tool called reqT that is developed in an educational context. [Question/problem] The work aims to engage computer science students in Requirements Engineering (RE) through a tool that captures essential RE concepts in executable code. [Principal ideas] Requirements are modeled using an internal DSL in the Scala programming language that blends natural language strings with a graph-oriented formalism. [Contribution] The metamodel of reqT and its main features are presented and modeling examples are provided together with a discussion on initial experiences from student projects, limitations and directions of further research.", "authors": ["Bj\u00f6rn Regnell"], "n_citation": 0, "title": "reqT.org \u2013 Towards a Semi-Formal, Open and Scalable Requirements Modeling Tool", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "50600dbb-369f-4a21-8576-a9aca1e622df"}
{"abstract": "Perspective scenarios of e-commerce applications, in particular B2B applications, based on the exchange of XML documents open new research issues in the field of information systems and XML data management. In fact, information systems will have to generate XML documents, obtained by querying the underlying DBMS. This paper informally introduces the ERX-Query Language (ERX-QL), under development as part of ERX Data Management System (developed at University of Bergamo). ERX-QL deals with the problem of formulating declarative queries to extract data from within a database and directly generate XML documents. This way, ERX-QL naturally deals with recursive and nested XML structures. Furthermore, the rich extended ER database provided by the ERX system makes ERX-QL rich and powerful, thus suitable, with minor changes, to be adopted on classical RDBMSs.", "authors": ["Giuseppe Psaila"], "n_citation": 50, "title": "ERX-QL: Querying an entity-relationship DB to obtain XML documents", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5062fe8b-e374-4d12-8e39-e6f8e96e5beb"}
{"abstract": "Complex dynamical behavior in a four-neuron recurrent neural network with discrete delays is investigated in this paper. The dissipativity of the system and stability of equilibrium point are studied by mens of Lyapunov theory. Stable fixed point, periodic and quasi-periodic orbits, and chaotic motion are observed in system via numerical calculation. With the change of the slope and threshold of activation function, as well as time delay and synaptic weight, the system passes from stable to periodic and then to chaotic regimes. Interestingly, the system returns to periodic or stable regimes by further changing these parameter values. Furthermore, some numerical evidences, such as phase portraits, bifurcation diagrams, power spectrum density, are given to confirm chaos.", "authors": ["Haigeng Luo", "Xiaodong Xu", "Xiaoxin Liao"], "n_citation": 0, "title": "Numerical Analysis of a Chaotic Delay Recurrent Neural Network with Four Neurons", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "509bef21-7f33-4b26-95c3-05875bf0ec40"}
{"abstract": "Recently, Particle Swarm Optimization(PSO) has been widely applied for training neural network. To improve the performance of PSO for high-dimensional solution space which always occurs in training NN, this paper introduces a new paradigm of particle swarm optimization named stochastic PSO (S-PSO). The feature of the S-PSO is its high ability for exploration. Consequently, when swarm size is relatively small, S-PSO performs much better than traditional PSO in training of NN. Hence if S-PSO is used to realize training of NN, computational cost of training can be reduced significantly.", "authors": ["Yangmin Li", "Xin Chen"], "n_citation": 0, "title": "A New Stochastic PSO Technique for Neural Network Training", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "50cf45b4-6ad5-476d-8aca-686a2dee0266"}
{"abstract": "Signatures are evolving profiles of entities extracted from streams of transactional data. For a stream of credit card transactions, for example, an entity might be a credit card number and a signature the average purchase amount. Signatures provide a high-level view of data in a transactional data warehouse and help data analysts focus their attention on interesting subsets of the data in such warehouses. Traditional databases are not designed for such applications. They impose overhead for services not necessary in such applications, such as indexing, declarative querying, and transaction support. Hancock is a C-based domain-specific programming language with an embedded domain-specific database designed for computing signatures. In this paper, we describe Hancock's database mechanism, evaluate its performance, and compare an application written in Hancock with an equivalent application written in Daytona [5], a very efficient relational database system.", "authors": ["Kathleen Fisher", "Colin R. Goodall", "Karin H\u00f6gstedt", "Anne Rogers"], "n_citation": 0, "title": "An application-specific database", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5132da7f-5cf8-46d6-91de-84b3ca7c0a47"}
{"abstract": "This paper describes the design and implementation of a module of emotions and personality for synthetic actors. Here are presented the results of previous researches, which were the basis of this project. With this information, a model for emotion generation using personality traits was designed in three stages, and implemented using fuzzy logic, FSMs, and probability theory. Finally, the functionalities of the module were shown using a demo version implemented with the videogame engine Unreal\u00ae 2 Runtime.", "authors": ["Diana Arellano T\u00e1vara", "Andreas Meier"], "n_citation": 0, "title": "Agents with personality for videogames", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5149a312-7e0c-4858-a9fb-bc938e24a2e0"}
{"abstract": "Reiter's default logic can not handle inconsistencies and incoherences and thus is not satisfactory enough in commonsense reasoning. In the paper we propose a new variant of default logic named FDL in which the existence of extension is guaranteed and the trivial extension is avoided. Moreover, Reiter's default extensions are reserved and can be identified from the other extensions in FDL. Technically, we develop a paraconsistent and monotonic reasoning system based on resolution as the underlying logic of FDL. The definition of extension is also modified in the manner that conflicts between justifications of the used defaults and the conclusions of the extension, which we call justification conflicts, are permitted, so that justifications can not be denied by subsequent defaults and the existence of extension is guaranteed. Then we select the desired extensions as preferred ones according to the criteria that justification conflicts should be minimal.", "authors": ["Zhangang Lin", "Yue Ma", "Zuoquan Lin"], "n_citation": 50, "title": "A fault-tolerant default logic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "51bb5056-fa31-4c32-be10-55b51cea031c"}
{"abstract": "In this paper, we propose a new self-growing hierarchical principal components analysis self-organizing neural networks model. This dynamically growing model expands the ability of the PCASOM model that represents the hierarchical structure of the input data. It overcomes the shortcoming of the PCASOM model in which the fixed the network architecture must be defined prior to training. Experiment results showed that the proposed model has better performance in the tradition clustering problem.", "authors": ["Stones Lei Zhang", "Zhang Yi", "Jian Lv"], "n_citation": 0, "title": "Growing Hierarchical Principal Components Analysis Self-Organizing Map", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "522b0a7f-4628-4a5a-ba22-8fffe1d992a6"}
{"abstract": "Scheduling problems, e.g., a job-shop scheduling, are classical NP-hard problems. In the paper a two-level adaptation method is proposed to solve the scheduling problem in a dynamically changing and uncertain environment. It is applied to the heterarchical multi-agent architecture developed by Valckenaers et al. Their work is improved by applying machine learning techniques, such as: neurodynamic programming (reinforcement learning + neural networks) and simulated annealing. The paper focuses on manufacturing control, however, a lot of these ideas can be applied to other kinds of decision-making, as well.", "authors": ["Bal\u00e1zs Csan\u00e1d Cs\u00e1ji", "Botond K\u00e1d\u00e1r", "L\u00e1szl\u00f3 Monostori"], "n_citation": 0, "title": "Improving multi-agent based scheduling by neurodynamic programming", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "52b546ec-749d-4c31-9d23-ad67bdf0a335"}
{"abstract": "In this paper, we want to exploit the knowledge obtained from those detected objects which are incorporated into the background model since they cease their movement. These motionless foreground objects should be handled in security domains such as video surveillance. This paper uses an adaptive background modelling algorithm for moving-object detection. Those detected objects which present no motion are identified and added into the background model, so that they will be part of the new background. Such motionless agents are included for further appearance analysis and agent categorization.", "authors": ["Ivan Huerta", "Daniel Rowe", "J. Gonzalez", "Juan Jos\u00e9 Villanueva"], "n_citation": 0, "title": "Efficient incorporation of motionless foreground objects for adaptive background segmentation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "535407c7-c312-422b-be52-18ccc9c0ac21"}
{"abstract": "Symbolic model checking methods have been extended recently to the verification of probabilistic systems. However, the representation of the transition matrix may be expensive for very large systems and may induce a prohibitive cost for the model checking algorithm. In this paper, we propose an approximation method to verify quantitative properties on discrete Markov chains. We give a randomized algorithm to approximate the probability that a property expressed by some positive LTL formula is satisfied with high confidence by a probabilistic system. Our randomized algorithm requires only a succinct representation of the system and is based on an execution sampling method. We also present an implementation and a few classical examples to demonstrate the effectiveness of our approach.", "authors": ["Thomas Herault", "Richard Lassaigne", "Fr\u00e9d\u00e9ric Magniette", "Sylvain Peyronnet"], "n_citation": 0, "title": "Approximate probabilistic model checking", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "53888055-761a-4c69-89a6-f9ee0027fe35"}
{"abstract": "Pitching-shot is known to be a root-shot for subsequent baseball video content analysis, e.g., event or highlight detection, and video structure parsing. In this paper, we integrate multiple feature analysis and fuzzy classification techniques to achieve pitching-shot detection in commercial baseball video. The adopted features include color (e.g., field color percentage and dominant color), temporal motion, and spatial activity distribution. On the other hand, domain knowledge of the baseball game forms the basis for fuzzy inference rules. Experiment results show that our detection rate is capable of achieving 95.76%.", "authors": ["Wen-Nung Lie", "Guo-Shiang Lin", "Sheng-Lung Cheng"], "n_citation": 0, "title": "Pitching Shot Detection Based on Multiple Feature Analysis and Fuzzy Classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "53d93768-0e95-4351-97ef-8c2f73078ae5"}
{"abstract": "We propose some global constraints for lexicographic orderings on vectors of variables. These constraints are very useful for breaking a certain kind of symmetry arising in matrices of decision variables. We show that decomposing such constraints carries a penalty either in the amount or the cost of constraint propagation. We therefore present a global consistency algorithm which enforces a lexicographic ordering between two vectors of n variables in O(nb) time, where b is the cost of adjusting the bounds of a variable. The algorithm can be modified very slightly to enforce a strict lexicographic ordering. Our experimental results on a number of domains (balanced incomplete block design, social golfer, and sports tournament scheduling) confirm the efficiency and value of these new global constraints.", "authors": ["Alan M. Frisch", "Brahim Hnich", "Zeynep Kiziltan", "Ian Miguel", "Toby Walsh"], "n_citation": 0, "title": "Global constraints for lexicographic orderings", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "53e35f20-fcaa-4c30-9535-5af0a5265ef6"}
{"abstract": "In this paper we analyse the security of a new key exchange protocol proposed in [3], which is based on mutually learning neural networks. This is a new potential source for public key cryptographic schemes which are not based on number theoretic functions, and have small time and memory complexities. In the first part of the paper we analyse the scheme, explain why the two parties converge to a common key, and why an attacker using a similar neural network is unlikely to converge to the same key. However, in the second part of the paper we show that this key exchange protocol can be broken in three different ways, and thus it is completely insecure.", "authors": ["Alexander Klimov", "Anton Mityagin", "Adi Shamir"], "n_citation": 0, "title": "Analysis of neural cryptography", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "54797bad-0920-4de1-9451-e9278a794e88"}
{"abstract": "Inductive and deductive inference are both essential ingredients of scientific activity. This paper takes a database-centred view some of the crucial issues arising in any attempt to combine these two modes of inference. It explores how a recently-proposed class of database systems (that support the execution of composite tasks, each of whose steps may involve knowledge discovery, as an inductive process, and or query answering, as a deductive one) might deliver significant benefits in the context of a case study where the specific characteristics of such systems can be more vividly perceived as being relevant and nontrivial.", "authors": ["Marcelo A. T. Arag\u00e3o", "Alvaro A. A. Fernandes"], "n_citation": 0, "title": "Seamlessly supporting combined knowledge discovery and query answering: A case study", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "548f9fff-fbc9-45a5-a29a-ff444fbc117e"}
{"abstract": "In this paper we discuss to what extent the choice of one particular Part-of-Speech (PoS) tagger determines the results obtained by a word sense disambiguation (WSD) system. We have chosen several PoS taggers and two WSD methods. By combining them, and using different kind of information, several experiments have been carried out. The WSD systems have been evaluated using the corpora of the lexical sample task of SENSEVAL-3 for English. The results show that some PoS taggers work better with one specific method. That is, selecting the right combination of these tools, could improve the results obtained by a WSD system.", "authors": ["Lorenza Moreno-Monteagudo", "Rub\u00e9n Izquierdo-Bevi\u00e1", "Patricio Mart\u00ednez-Barco", "Armando Su\u00e1rez"], "n_citation": 0, "title": "A study of the influence of PoS tagging on WSD", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "54d6735b-155a-4d9c-bf20-cf656856bd51"}
{"abstract": "In this paper, a delayed Lagrangian network is presented for solving quadratic programming problems. Based on some known results, the delay interval is determined to guarantee the asymptotic stability of the delayed neural network at the optimal solution. One simulation example is provided to show the effectiveness of the approach.", "authors": ["Qingshan Liu", "Jun Wang", "Jinde Cao"], "n_citation": 50, "title": "A Delayed Lagrangian Network for Solving Quadratic Programming Problems with Equality Constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5519551f-cc66-4e61-a08c-c3e8c277dca1"}
{"abstract": "The job shop scheduling with release dates and due dates is considered. An effective algorithm based on GENET neural network model is presented for solving this type of problem. Two heuristics are embedded into the progressive stochastic search scheme in GENET network for the objective of minimizing the weighted number of tardy jobs. Experimental results indicate that the presented algorithm is competent to attain significant reductions in number of tardy jobs in relation to priority heuristic algorithms.", "authors": ["Xin Feng", "Ho-fung Leung", "Lixin Tang"], "n_citation": 50, "title": "An effective algorithm based on GENET neural network model for job shop scheduling with release dates and due dates", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "55303e8a-f84e-43d2-887f-6096422d4540"}
{"abstract": "This paper presents approaches and solutions for agent-based manufacturing systems developed within the AgenTec project. The developed technologies, methods and tools for automation systems will realise plug and produce.", "authors": ["Arnulf Braatz", "Michael H\u00f6pf", "Arno Ritter"], "n_citation": 0, "title": "AgenTec: Concepts for agent technology in automation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "55d26cb7-63ca-47fa-ad29-c24fe190e243"}
{"abstract": "This paper proposes and analyzes a message scheduling scheme and corresponding capacity allocation method for the distributed hard real-time communication on dual Wireless LANs. By making the superframe of one network precede that of the other by half, the dual network architecture can minimize the effect of deferred beacon and reduce the worst case waiting time by half. The effect of deferred beacon is formalized and then directly considered to decide polling schedule and capacity vector. Simulation results executed via ns-2 show that the proposed scheme improves the schedulability by 36 % for real-time messages and allocates 9 (% more bandwidth to non-real-time messages by enhancing achievable throughput for the given stream sets, compared with the network whose bandwidth is just doubled.", "authors": ["Junghoon Lee", "Mikyung Kang", "Gyung-Leen Park"], "n_citation": 50, "title": "Design of a hard real-time guarantee scheme for dual ad hoc mode IEEE 802.11 WLANs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "56508659-b318-449a-badf-d0407ad7b370"}
{"authors": ["Ullrich Hustadt", "Dmitry Tishkovsky", "Frank Wolter", "Michael Zakharyaschev"], "n_citation": 50, "title": "Automated reasoning about metric and topology", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "566559c9-cb0d-4d54-80e4-ca3979faf67e"}
{"abstract": "Tractability results for structural subproblems have generally been considered for explicit relations listing the allowed assignments. In this paper we define a representation which allows us to express constraint relations as either an explicit set of allowed labelings, or an explicit set of disallowed labelings, whichever is smaller. We demonstrate a new structural width parameter, which we call the interaction width, that when bounded allows us to carry over well known structural decompositions to this more concise representation. Our results naturally derive new structurally tractable classes for SAT.", "authors": ["Chris Houghton", "David A. Cohen", "M. Green"], "n_citation": 50, "title": "The effect of constraint representation on structural tractability", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "56d3bafd-c93d-4d72-8bbe-77dc45d608dd"}
{"abstract": "In this paper, we study impulsive fuzzy cellular neural net).. works. Criteria are obtained for the existence and exponential stability of a unique equilibrium of fuzzy cellular neural networks impulsive state displacements at fixed instants of time.", "authors": ["Tingwen Huang", "Marco Roque-Sol"], "n_citation": 0, "title": "Stability of Fuzzy Cellular Neural Networks with Impulses", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "56eb4827-741f-49fa-8a6f-c047a343ec6f"}
{"abstract": "When developing situation awareness applications we begin by constructing an OWL ontology to capture a language of discourse for the domain of interest. Such an ontology, however, is never sufficient for fully representing the complex knowledge needed to identify what is happening in an evolving situation - this usually requires general implication afforded by a rule language such as SWRL. This paper describes the application of SWRL/OWL to the representation of knowledge intended for a supply logistics scenario. The rules are first presented in an abstract syntax based on n-ary predicates. We then describe a process to convert them into a representation that complies with the binary-only properties of SWRL. The application of the SWRL rules is demonstrated using our situation awareness application, SAWA, which can employ either Jess or BaseVISor as its inference engine. We conclude with a summary of the issues encountered in using SWRL along with the steps taken in resolving them.", "authors": ["Christopher J. Matheus", "Kenneth Baclawski", "Mieczyslaw M. Kokar", "Jerzy J. Letkowski"], "n_citation": 0, "title": "Using SWRL and OWL to capture domain knowledge for a situation awareness application applied to a supply logistics scenario", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5716e4c8-5850-4b7b-86d7-4517aad9ce02"}
{"authors": ["Tetsuya Hatano", "Atsuko Miyaji", "Takashi Sato"], "n_citation": 0, "title": "T-Robust Scalable Group Key Exchange Protocol with O(logn) Complexity", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "57f2dee2-3196-473e-a686-df060e3414ed"}
{"abstract": "Many feature extraction algorithms have been proposed for time series classification. However, most of the proposed algorithms in time series data mining community belong to the unsupervised approach, without considering the class separability capability of features that is important for classification. In this paper we propose a supervised feature extraction approach by selecting discriminating wavelet coefficients to improve the time series classification accuracy. After wavelet transformation, few wavelet coefficients with higher class separability capability are selected as features. We apply three feature evaluation criteria, i.e., Fisher's discriminant ratio, divergence, and Bhattacharyya distance. Experiments performed on several benchmark time series datasets demonstrate the effectiveness of the proposed approach.", "authors": ["Hui Zhang", "Tu Bao Ho", "Mao-Song Lin", "Xuefeng Liang"], "n_citation": 0, "title": "Feature Extraction for Time Series Classification Using Discriminating Wavelet Coefficients", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "57f9b7f3-607a-4678-ba7b-d85152580ab8"}
{"abstract": "In this paper we analyze the statistical distribution of the keystream generator used by the stream ciphers RC4 and RC4A. Our first result is the discovery of statistical biases of the digraphs distribution of RC4/RC4A generated streams, where digraphs tend to repeat with short gaps between them. We show how an attacker can use these biased patterns to distinguish RC4 keystreams of 2 26  bytes and RC4A keystreams of 2 26.5  bytes from randomness with success rate of more than 2/3. Our second result is the discovery of a family of patterns in RC4 keystreams whose probabilities in RC4 keystreams are several times their probabilities in random streams. These patterns can be used to predict bits and words of RC4 with arbitrary advantage, e.g., after 2 45  output words a single bit can be predicted with probability of 85%, and after 2 50  output words a single byte can be predicted with probability of 82%, contradicting the unpredictability property of PRNGs.", "authors": ["Itsik Mantin"], "n_citation": 0, "title": "Predicting and distinguishing attacks on RC4 keystream generator", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5801f54f-f5ab-4276-9c6c-d13bc99ea251"}
{"abstract": "Since it is difficult to replace batteries of the sensor nodes that are once deployed in the field, energy saving is one of the most critical issues for object tracking in wireless sensor networks. It is desirable that only the nodes surrounding the mobile target should be responsible for observing the target to save the energy consumption and extend the network lifetime as well by reducing the number of participating nodes. The number of nodes participating in object tracking can be reduced by an accurate estimation of the target location. In this paper, we propose a tracking method that can reduce the number of participating sensor nodes for target tracking. We show that our tracking method performs well in terms of energy saving regardless of mobility pattern of the mobile target.", "authors": ["Hyunsook Kim", "Eunhwa Kim", "Kijun Han"], "n_citation": 50, "title": "An energy efficient tracking method in wireless sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "588bb75e-82f5-4489-be37-a9bf8ca58f97"}
{"abstract": "The RTS/CTS access scheme, designed to reduce the number of collisions in a IEEE 802.11 network, is known to exhibit problems due to masked nodes, the imbalance between the interference range and the communication range of the nodes, and scenarios in which nodes are unnecessarily silenced, thus preventing parallel transmissions to take place. We present an approach for enhancing the performance of the IEEE 802.11 MAC protocol by selectively discarding or delaying specifically marked RTS and CTS packets. By dropping the circularity-satisfied RTS, we allow certain parallel transmissions to proceed, even if there is a non-zero risk of collision. By delaying the circularity-satisfied CTS, we allow a neighboring parallel transmission to continue. One important feature of the circularity approach is that it is fully compatible with the IEEE 802.11 standard. We implemented the circularity approach in ns-2 simulator. Through a series of experiments, we show that the circularity approach provides a significant improvement in the throughput and end-to-end delay of the network, and contributes to a reduction of the number of collisions in most scenarios.", "authors": ["Mohammad Zubair Ahmad", "Damla Turgut", "R. Bhakthavathsalam"], "n_citation": 0, "title": "Circularity-based medium access control in mobile ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "58aa0dba-beb4-41e6-8352-9a2f0170dad4"}
{"abstract": "This paper presents the results of a case study, concerning the propagation of a global disjunctive resource constraint, when the resource is over-loaded. The problem can be seen as a partial constraint satisfaction problem, in which either the resource constraint or the due dates of some jobs have to be violated. Global constraint propagation methods are introduced to efficiently deal with this situation. These methods are applied to a well-known operations research problem: minimizing the number of late jobs on a single machine, when jobs are subjected to release dates and due dates. Dominance criteria and a branch and bound procedure are developed for this problem. 960 instances are generated with respect to different characteristics (number of jobs, overload ratio, distribution of release dates, of due dates and of processing times). Instances with 60 jobs are solved in 23 seconds on average and 90% of the instances with 100 jobs are solved in less than 1 hour.", "authors": ["P. Baptiste", "C. Le Pape", "Laurent P\u00e9ridy"], "n_citation": 0, "title": "Global constraints for partial CSPs : A case-study of resource and due date constraints", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "58b5a752-6c14-405e-96ff-5b1a8f42d514"}
{"abstract": "This paper presents a novel approach of palmprint authentication for multimedia security by using the differential operation. In this approach, a differential operation is first conducted to the palmprint image in horizontal direction. And then the palmprint is encoded according to the sign of the value of each pixel of the differential image. This code is called DiffCode of the palmprint. The size of DiffCode is 128 bytes, which is the smallest one among the existing palmprint features and suitable for multimedia security. The similarity of two DiffCode is measured using their Hamming distance. This approach is tested on the public PolyU Palmprint Database anti the EER is 0.6%, which is comparable with the existing palmprint recognition methods.", "authors": ["Xiangqian Wu", "Kuanquan Wang", "David Zhang"], "n_citation": 0, "title": "Differential Operation Based Palmprint Authentication for Multimedia Security", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "58fb1f1f-85b6-4ed2-8714-b23eeb8f7133"}
{"abstract": "Visualizing the color changing and fading process of ancient Chinese wall paintings to tourists and researchers is of great value in both education and preservation. But previously, because empirical knowledge from artists was not introduced, it is infeasible to simulate the color changing and fading in the absence of physical and chemical knowledge of color changing and fading of frescoes. In this paper, however, empirical knowledge from artists is formalized. Since the improved system can reflect knowledge from artists in addition to the previous physical and chemical modeling, the simulation results are faithful to the color aging process of frescoes. In this article, first, the former related works is reviewed. Then, the formalization of empirical knowledge from artists is narrated. After that, the simulation results are shown and discussed. And finally, future research is suggested.", "authors": ["Xifan Shi", "Dongming Lu", "Jianming Liu"], "n_citation": 0, "title": "Color Changing and Fading Simulation for Frescoes Based on Empirical Knowledge from Artists", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "59090238-e245-45b0-9bf3-b101dbc3bc1c"}
{"abstract": "In the framework of natural gradient, a novel Kurtosis-Dependent Parameterized Independent Component Analysis (KDPICA) algorithm is proposed, which can separate the mixture of super- and sub-Gaussian sources. Two kinds of new probability density models are proposed, which can provide wider ranges especially for sub-Gaussian kurtosis. According to kurtosis value of source and whitening, model parameters are adaptively calculated which can be used to estimate super- and sub-Gaussian source distributions and its corresponding score functions directly. According to stability analysis, the ranges of model parameters are fixed which confirm KDPICA algorithm stable. The experiment shows the proposed algorithm has better performance than some proposed algorithms.", "authors": ["Xiaofei Shi", "Jidong Suo", "Chang Liu", "Li Li"], "n_citation": 0, "title": "A Novel Kurtosis-Dependent Parameterized Independent Component Analysis Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "59244075-4def-48e0-91db-343b1d3d829c"}
{"abstract": "A central issue in computational neuroscience is to answer why neural systems can process information extremely fast. Here we investigate the effect of noise and the collaborative activity of a neural population on speeding up computation. We find that 1) when input noise is Poissonian, i.e., its variance is proportional to the mean, and 2) when the neural ensemble is initially at its stochastic equilibrium state, noise has the 'best' effect of accelerating computation, in the sense that the input strength is linearly encoded by the number of neurons firing in a short-time window, and that the neural system can use a simple strategy to read out the stimulus rapidly and accurately.", "authors": ["Si Wu", "Jianfeng Feng", "Shun-ichi Amari"], "n_citation": 0, "title": "The Ideal Noisy Environment for Fast Neural Computation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "592bd41b-f036-46ee-b63f-8f645a3c4a3d"}
{"abstract": "A variant of Schnorr's signature scheme called RDSA has been proposed by I. Biehl, J. Buchmann, S. Hamdy and A. Meyer in order to be used in finite abelian groups of unknown order such as the class group of imaginary quadratic orders. We describe in this paper a total break of RDSA under a plain known-message attack for the parameters that were originally proposed. It recovers the secret signature key from the knowledge of less than 10 signatures of known messages, with a very low computational complexity. We also compare a repaired version of RDSA with GPS scheme, another Schnorr variant with similar properties and we show that GPS should be preferred for most of the applications.", "authors": ["Pierre-Alain Fouque", "Guillaume Poupard"], "n_citation": 50, "title": "On the security of RDSA", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "594f2f46-437d-4527-a5fe-b72b1e9d041e"}
{"abstract": "Constraint Satisfaction Problems and Propositional Satisfiability, are frameworks widely used to represent and solve combinatorial problems. A concept of primary importance in both frameworks is that of constraint propagation. In this paper we study and compare the amount of propagation that can be achieved, using various methods, when translating a problem from one framework into another. Our results complement, extend, and tie together recent similar studies. We provide insight as to which translation is preferable, with respect to the strength of propagation in the original problem and the encodings.", "authors": ["Yannis Dimopoulos", "Kostas Stergiou"], "n_citation": 0, "title": "Propagation in CSP and SAT", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "59f5231e-7078-46ef-baa7-46d0a663f860"}
{"abstract": "Automatic dialogue systems get easily confused if speech is recognized which is not directed to the system. Besides noise or other people's conversation, even the user's utterance can cause difficulties when he is talking to someone else or to himself (Off-Talk). In this paper the automatic classification of the user's focus of attention is investigated. In the German SmartWeb project, a mobile device is used to get access to the semantic web. In this scenario, two modalities are provided -speech and video signal. This makes it possible to classify whether a spoken request is addressed to the system or not: with the camera of the mobile device, the user's gaze direction is detected; in the speech signal, prosodic features are analyzed. Encouraging recognition rates of up to 93 % are achieved in the speech-only condition. Further improvement is expected from the fusion of the two information sources.", "authors": ["Christian Hacker", "Anton Batliner", "Elmar N\u00f6th"], "n_citation": 0, "title": "Are you looking at Me, are you talking with Me : Multimodal classification of the focus of attention", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "59fd5b04-7eef-45fe-9227-4320c380c16c"}
{"abstract": "In this paper, We propose a faster algorithm for the following instruction scheduling problem: Given a set of UET (Unit Execution Time) instructions with precedence constraints in the form of a DAG(Directed Acyclic Graph), latency constraints where latencies between any two instructions are restricted to be either 0 or 1, timing constraints in the form of individual integer release times and dead-lines and a single RISC processor, find a feasible schedule which satisfies all constraints. The time complexity of our algorithm is O(n 2  log n) + min{O(ne), O(n 2.376 )}, where n is the number of instructions and e is the number of edges in the precedence graph. Our algorithm is faster than the existing algorithm which runs in O(n 3 \u03b1(n)) time, where a(n) is the inverse of Ackermann function. In addition, our algorithm can be used to solve the maximum lateness minimization problem in O(n 2  log 2  n + min{ne,n 2.376 }) time.", "authors": ["Hui Wu", "Joxan Jaffar", "Roland Yap"], "n_citation": 0, "title": "Instruction scheduling with timing constraints on a single RISC processor with 0/1 latencies", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "5a72a91d-e8a8-4ce8-b66c-826677a6a742"}
{"abstract": "In [Walsh and Stergiou, 1999] enforcing arc consistency (AC) in the dual encoding was shown to strictly dominate enforcing AC on the hidden or GAC on the original problem. We introduce a dual encoding that requires only a small subset of the original constraints to be stored in extension, while the remaining constraints can be stored intensionally. In this paper we present a theoretical comparison between the pruning achieved by enforcing AC on this dual encoding, versus enforcing GAC and dual arc consistency on the standard encoding. We show how the covering based encoding retains the dominance over enforcing GAC on the original problem, while using less space than the existing dual encoding.", "authors": ["Sivakumar Nagarajan", "Scott D. Goodwin", "Abdul Sattar", "John Thornton"], "n_citation": 0, "title": "On dual encodings for non-binary constraint satisfaction problems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "5abe305b-fff4-4788-ad7d-50387a1e4f0a"}
{"abstract": "With the Peak Ground Velocity 283 records in three dimensions, the velocity attenuation relationship with distance was discussed by neural network in this paper. The earthquake magnitude, epicenter distance, site intensity and site condition were considered as basic input element for the network. By using Bayesian Regularization Back Propagation Neural Networks (BRBPNN), the over-fitting phenomenon was reduced to some extent. The horizontal velocity was discussed. The PGV predicted by neural networks can simulate the detail difference with distance, while the PGV given by other traditional attenuation relationship only give a reduction relation with distance. The importance of each input factor was compared by the square weight of the input layer of the network. The order may be earthquake magnitude, epicenter distance and soil condition.", "authors": ["Ben-Yu Liu", "Liao-Yuan Ye", "Mei-ling Xiao", "Sheng Miao"], "n_citation": 50, "title": "Peak Ground Velocity Evaluation by Artificial Neural Network for West America Region", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5ac9b1a6-12fe-4b7e-a671-308e66d19009"}
{"abstract": "This paper presents an algorithm that achieves hyper-arc consistency for the soft alldifferent constraint. To this end, we prove and exploit the equivalence with a minimum-cost flow problem. Consistency of the constraint can be checked in O(nm) time, and hyper-arc consistency is achieved in O(m) time, where n is the number of variables involved and m is the sum of the cardinalities of the domains. It improves a previous method that did not ensure hyper-arc consistency.", "authors": ["Willem Jan van Hoeve"], "n_citation": 0, "title": "A hyper-arc consistency algorithm for the soft alldifferent constraint", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5ae3d5c8-85ba-4b09-8609-62ca2e4017be"}
{"abstract": "A (1,2)-robust combiner for a cryptographic primitive P is a construction that takes two candidate schemes for P and combines them into one scheme that securely implement P even if one of the candidates fails. Robust combiners are a useful tool for ensuring better security in applied cryptography, and also a handy tool for constructing cryptographic protocols. For example, we discuss using robust combiners for obtaining universal schemes for cryptographic primitives (a universal scheme is an explicit construction that implements P under the sole assumption that P exists). In this paper we study what primitives admit robust combiners. In addition to known and very simple combiners for one-way functions and equivalent primitives, we show robust combiners for protocols in the world of public key cryptography, namely for Key Agreement(KA). The main point we make is that things are not as nice for Oblivious Transfer (OT) and in general for secure computation. We prove that there are no transparent black-box robust combiners for OT, giving an indication to the difficulty of finding combiners for OT. On the positive side we show a black box construction of a. (2, 3)-robust combiner for OT, as well as a generic construction of (1, n)-robust OT-combiners from any (1,2)-robust OT-combiner.", "authors": ["Danny Harnik", "Joe Kilian", "Moni Naor", "Omer Reingold", "Alon Rosen"], "n_citation": 0, "title": "On robust combiners for oblivious transfer and other primitives", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5bbde549-2f5e-4d62-af0f-76c629f0e94f"}
{"abstract": "A new approach to analyze and synthesize texture regions in video coding is presented, where texture blocks in video sequence are synthesized using graph cut technique. It first identifies the texture regions by video segmentation technique, and then calculates their motion vectors by motion vector (MV) scaling technique like temporal direct mode. After the correction of these MVs, texture regions are predicted from forward and/or backward reference frames by the corrected MVs. Furthermore, Overlapped Block Motion Compensation (OBMC) is applied to these texture regions to reduce block artifacts. Finally, the texture blocks are stitched together along optimal seams to reconstruct the current texture block using graph cuts. Experimental results show that the proposed method can achieve compared visual quality for texture regions with H.264/AVC, while spending fewer bits.", "authors": ["Yongbing Zhang", "Xiangyang Ji", "Debin Zhao", "Wen Gao"], "n_citation": 50, "title": "Video Coding by Texture Analysis and Synthesis Using Graph Cut", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5be450e7-07c1-4dec-b535-bb8507e486a8"}
{"abstract": "Dialogue modelling attempts to determine the way in which a dialogue is developed. The dialogue strategy (i.e., the system behaviour) of an automatic dialogue system is determined by the dialogue model. Most dialogue systems use rule-based dialogue strategies, but recently, the probabilistic models have become very promising. We present probabilistic models based on the dialogue act concept, which uses user turns, dialogue history and semantic information. These models are evaluated as dialogue act labelers. The evaluation is carried out on a railway information task.", "authors": ["Carlos D. Mart\u00ednez-Hinarejos", "Francisco Casacuberta"], "n_citation": 0, "title": "Evaluating a probabilistic dialogue model for a railway information task", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5c185eec-52dd-4d51-a61e-002b3216988c"}
{"abstract": "In this paper, we essentially drop the requirement of Lipschitz condition on the activation functions. By employing Lyapunov functional and several new inequalities, some new criteria concerning global exponential stability for a class of generalized neural networks with time-varying delays are obtained, which only depend on physical parameters of neural networks. Since these new criteria do not require the activation functions to be differentiable, bounded or monotone nondecreasing and the connection weight matrices to be symmetric, they are mild and more general than previously known criteria.", "authors": ["Gang Wang", "Huaguang Zhang", "Chonghui Song"], "n_citation": 0, "title": "New Criteria of Global Exponential Stability for a Class of Generalized Neural Networks with Time-Varying Delays", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5c814bbb-fc62-470a-8ae6-c4da533c0816"}
{"abstract": "In this paper a multiresolution wavelet kernel function (MWKF) is proposed for support vector regression. It is different from traditional SVR that the process of reducing dimension is utilized before increasing dimension. The nonlinear mapping \u03a6(x) from the input space S to the feature space has explicit expression based on dimensionality reduction and wavelet multiresolution analysis. This wavelet kernel function can be represented by inner product. This method guarantee that quadratic program of support vector regression has feasible solution and need not parameter selecting in kernel function. Numerical experiments demonstrate the effectiveness of this method.", "authors": ["Fengqing Han", "Dacheng Wang", "Chuan-Dong Li", "Xiaofeng Liao"], "n_citation": 0, "title": "A Multiresolution Wavelet Kernel for Support Vector Regression", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5d074f18-23b3-446c-aa0a-e390c0322e23"}
{"abstract": "The purpose of this paper is to analyze the evolution of typhoon cloud patterns in the spatio-temporal domain using statistical learning models. The basic approach is clustering procedures for extracting hidden states of the typhoon, and we also analyze the temporal dynamics of the typhoon in terms of transitions between hidden states. The clustering procedures include both spatial and spatio-temporal clustering procedures, including K-means clustering, Self-Organizing Maps (SOM), Mixture of Gaussians (MoG) and Generative Topographic Mapping (GTM) combined with Hidden Markov Model (HMM). The result of clustering is visualized on the Evolution Map on which we analyze and visualize the temporal structure of the typhoon cloud patterns. The results show that spatio-temporal clustering procedures outperform spatial clustering procedures in capturing the temporal structures of the evolution of the typhoon.", "authors": ["Asanobu Kitamoto"], "n_citation": 0, "title": "Evolution map: Modeling state transition of typhoon image sequences by spatio-temporal clustering", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5dbaaa9b-3822-4af2-91dc-4d627bccaa7e"}
{"abstract": "We explore the use of prosodic features beyond pauses, including duration, pitch, and energy features, for automatic sentence segmentation of ICSI meeting data. We examine two different approaches to boundary classification: score-level combination of independent language and prosodic models using HMMs, and feature-level combination of models using a boosting-based method (BoosTexter). We report classification results for reference word transcripts as well as for transcripts from a state-of-the-art automatic speech recognizer (ASR). We also compare results using the lexical model plus a pause-only prosody model, versus results using additional prosodic features. Results show that (1) information from pauses is important, including pause duration both at the boundary and at the previous and following word boundaries; (2) adding duration, pitch, and energy features yields significant improvement over pause alone; (3) the integrated boosting-based model performs better than the HMM for ASR conditions; (4) training the boosting-based model on recognized words yields further improvement.", "authors": ["Jachym Kolar", "Elizabeth Shriberg", "Yang Liu"], "n_citation": 0, "title": "Using prosody for automatic sentence segmentation of multi-party meetings", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5dd4db7f-8841-41de-bf93-4bcb7e1fe102"}
{"abstract": "The paper introduces value precedence on integer and set sequences. A useful application of the notion is in breaking symmetries of indistinguishable values, an important class of symmetries in practice. Although value precedence can be expressed straightforwardly using if-then constraints in existing constraint programming systems, the resulting formulation is inefficient both in terms of size and runtime. We present two propagation algorithms for implementing global constraints on value precedence in the integer and set domains. Besides conducting experiments to verify the feasibility and efficiency of our proposal, we characterize also the propagation level attained by various usages of the global constraints as well as the conditions when the constraints can be used consistently with other types of symmetry breaking constraints.", "authors": ["Jimmy Ho-Man Lee"], "n_citation": 0, "title": "Global constraints for integer and set value precedence", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5ddf63a0-97e9-4ac0-8d05-67c7cc2ae704"}
{"abstract": "A new theoretical result on the global exponential stability of recurrent neural networks with delay is presented. It should be noted that the activation functions of recurrent neural network do not require to be bounded. The presented criterion, which has the attractive feature of possessing the structure of linear matrix inequality (LMI), is a generalization and improvement over some previous criteria. A example is given to illustrate our results.", "authors": ["Yi Shen", "Minghui Jiang", "Xiaoxin Liao"], "n_citation": 0, "title": "A generalized LMI-based approach to the global exponential stability of recurrent neural networks with delay", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5e0d729b-71d3-4924-9ab9-2d3a1c8cc0ac"}
{"abstract": "In this paper we present experiments with data-driven part-of-speech taggers trained and evaluated on the annotated Helsinki Corpus of Swahili. Using four of the current state-of-the-art data-driven taggers, TnT, MBT, SVMTool and MXPOST, we observe the latter as being the most accurate tagger for the Kiswahili dataset.We further improve on the performance of the individual taggers by combining them into a committee of taggers. We observe that the more naive combination methods, like the novel plural voting approach, outperform more elaborate schemes like cascaded classifiers and weighted voting. This paper is the first publication to present experiments on data-driven part-of-speech tagging for Kiswahili and Bantu languages in general.", "authors": ["Guy De Pauw", "Gilles-Maurice de Schryver", "Peter Waiganjo Wagacha"], "n_citation": 0, "title": "Data-driven part-of-speech tagging of kiswahili", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5e4ce006-d185-4289-9985-097b2f7162ca"}
{"abstract": "According to biological and neurophysiologic research, there is a bloom bursting of synapses in brain's physiological growing process of newborn infants. These jillion nerve connections will be pruned and the dendrites of neurons can change their conformation in infants' proceeding cognition process. Simulating this pruning process, a new neural network structure evolution algorithm is proposed based on e and m projections in information geometry and model selection criterion. This structure evolution process is formulated in iterative e, m projections and stopped by using model selection criterion. Experimental results prove the validation of the algorithm.", "authors": ["Yun-Hui Liu", "Siwei Luo", "Ziang Lv", "Hua Huang"], "n_citation": 0, "title": "A Neural Network Structure Evolution Algorithm Based on e, m Projections and Model Selection Criterion", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5f3717b6-ce00-4692-b157-12faab209b56"}
{"abstract": "This paper investigate the problems of global exponential stability and exponential convergence rate for delayed impulsive Hopfield type neural networks. By using the method of Lyapunov functions, some sufficient conditions for ensuring global exponential stability of these networks are derived, and the estimate of exponential convergence rate is also obtained. A numerical example is worked out to illustrate the obtained results.", "authors": ["Bingji Xu", "Qun Wang", "Yi Shen", "Xiaoxin Liao"], "n_citation": 0, "title": "Global exponential stability of delayed impulsive hopfield type neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5fcbb833-f42c-4dc3-bb3b-defebe54951e"}
{"abstract": "A text in Hungarian Sign Language is analyzed in this paper. 3  The discourse-semantic framework we use is a generalized version of van Eijck-Kamp's DRT [1], called Lifelong DRT, where the crucial innovation is regarding the interpreter's information state containing the mutual background knowledge shared by speaker and interpreter as a gigantic lifelong DRS where all pieces of the interpreter's lexical, cultural/encyclopedic and interpersonal knowledge are accessible whose mobilization the exhaustive interpretation of the given text requires.", "authors": ["G\u00e1bor Alberti", "Helga M. Szab\u00f3"], "n_citation": 0, "title": "Discourse-semantic analysis of Hungarian Sign Language", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "60056429-126a-4cef-92ba-3b9b81b47833"}
{"abstract": "In this paper we define partial symmetry breaking, a concept that has been used in many previous papers without being the main topic of any research. This paper is the first systematic study of partial symmetry breaking in constraint programming. We show experimentally that performing symmetry breaking with only a subset of all symmetries can result in greatly reduced run-times. We also look at the consequences of using partial symmetry breaking in terms of variable and value ordering heuristics. Finally, different methods of selecting symmetries are considered before presenting a general algorithm for selecting subsets of symmetries.", "authors": ["Iain McDonald", "Barbara M. Smith"], "n_citation": 50, "title": "Partial symmetry breaking", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6073ba23-4745-4a17-9049-8b829740bc05"}
{"abstract": "There are many situations in which listening to a text produced by a text-to-speech system is easier or safer than reading, for example when driving a car. Technical documents, such as conference articles, manuals etc., usually are comprised of relatively plain and unequivocal sentences. These documents usually contain words and terms unknown to the listener because they are full of domain specific terminology. In this paper, we propose a system that allows the users to interrupt the reading upon hearing an unknown or confusing term by a non-speech acoustic gesture (e.g. uhm?). Upon this interruption, the system provides a definition of the term, retrieved from Wikipedia, the Free Encyclopedia. The selection of the non-speech gestures has been made with a respect to the cross-cultural applicability and language independence. In this paper we present a set of novel tools enabling this kind of interaction.", "authors": ["Adam J. Sporka", "Pavel Zikovsky", "Pavel Slav\u00edk"], "n_citation": 50, "title": "Explicative document reading controlled by non-speech audio gestures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "60fa1e9b-3bad-4458-86d5-7f8a6cadfc05"}
{"abstract": "We present a new approach method for gender identification on the teeth based on PCA (principal component analysis) representation using geometric features of teeth such as the size and shape of the jaws, size of the teeth and teeth structure. In this paper we try to set forth the foundations of a biometric system for automatic evaluation of gender identification using dental geometric features. To create a gender identification system, a template based on PCA is created from dental data collected the plaster figures of teeth which were done at dental hospital, department of oral medicine. Templates of dental images based on PCA representation include the 18 principal components as the features for gender identification. The PCA basis vectors reflects well the features for gender identification in the whole of teeth. The classification for gender identification is generated based on the nearest neighbor (NN) algorithm. The gender identification performance in dental images of 50 person was 76%. The identified values in females and males were 79.3% and 71.4%, respectively.", "authors": ["Young-Suk Shin"], "n_citation": 0, "title": "Gender identification on the teeth based on principal component analysis representation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "611aca66-515a-4372-ab0d-6799817d4339"}
{"abstract": "The aim of our work is to provide support for reading (or skimming) scientific papers. In this paper we report on the task to identify concepts or terms with positive attributions in scientific papers. This task is challenging as it requires the analysis of the relationship between a concept or term and its sentiment expression. Furthermore, the context of the expression needs to be inspected. We propose an incremental knowledge acquisition framework to tackle these challenges. With our framework we could rapidly (within 2 days of an expert's time) develop a prototype system to identify positive attributions in scientific papers. The resulting system achieves high precision (above 74%) and high recall rates (above 88%) in our initial experiments on corpora of scientific papers. It also drastically outperforms baseline machine learning algorithms trained on the same data.", "authors": ["Son Bao Pham", "A. Hoffmann"], "n_citation": 0, "title": "Extracting positive attributions from scientific papers", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "617c67bb-de93-4806-bc7f-6b8a24c7db85"}
{"abstract": "We study how digital signature schemes can generate signatures as short as possible, in particular in the case where partial message recovery is allowed. We give a concrete proposition named OPSSR that achieves the lower bound for message expansion, and give an exact security proof of the scheme in the ideal cipher model. We extend it to the multi-key setting. We also show that this padding can be used for an asymmetric encryption scheme with minimal message expansion.", "authors": ["Louis Granboulan"], "n_citation": 50, "title": "Short signatures in the random oracle model", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "61d8e179-695d-43ec-bf81-08a3df809e25"}
{"abstract": "Historically, discrete minimization problems in constrained logical programming were modeled with the help of an isolated bounding constraint on the objective that is to be decreased. To overcome this frequently inefficient way of searching for improving solutions, the notion of optimization constraints was introduced. Optimization constraints can be viewed as global constraints that link the objective with other constraints of the problem at hand. We present an arc-consistency (actually: hyper-arc-consistency) algorithm for the minimum weight all different constraint which is an optimization constraint that consists in the combination of a linear objective with an all different constraint.", "authors": ["Meinolf Sellmann"], "n_citation": 0, "title": "An arc-consistency algorithm for the minimum weight all different constraint", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "62a619b4-3943-4c19-834f-b995fd423a9b"}
{"abstract": "For efficiency reasons, neighbourhoods in local search are often shrunk by only considering moves modifying variables that actually contribute to the overall penalty. These are known as conflicting variables. We propose a new definition for measuring the conflict of a variable in a model and apply it to the set variables of models expressed in existential second-order logic extended with counting (\u2203SOL + ). Such a variable conflict can be automatically and incrementally evaluated. Furthermore, this measure is lower-bounded by an intuitive conflict measure, and upper-bounded by the penalty of the model. We also demonstrate the usefulness of the approach by replacing a built-in global constraint by an \u2203SOL +  version thereof, while still obtaining competitive results.", "authors": ["Magnus \u00c5gren", "Pierre Flener", "Justin Pearson"], "n_citation": 50, "title": "Inferring variable conflicts for local search", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "62b43f56-dd79-4fb4-8874-0e0528982974"}
{"abstract": "This paper deals with a lemmatization technique and its using for phonetic transcription of exceptional words. The lemmatizer is based on language morphology and uses a lexicon of basic word forms and a set of inversion derivation rules to acquire lemmatization rules, which are essential for finding word bases. The lemmatization algorithm and its necessary modifications for transcription of exceptional words are described. The main goal of the designed system is to save computer memory for exceptional lexicon storing. The experimental results showed that it is possible to save from 18.3% (English) to 98.4% (Finnish) of the full lexicon size. Hence, the described technique can be applied with advantage for high inflectional and agglutinative languages.", "authors": ["Jakub Kanis", "L. M\u00fcller"], "n_citation": 0, "title": "Using the Lemmatization technique for phonetic transcription in text-to-speech system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "62b9a527-292d-4243-be38-981ffd6cb1c0"}
{"abstract": "Combining two well-known techniques - pushdown automata and oracles - results in a new class of parsers (oracle pushdown automata) having many advantages. It makes possible to combine easily different parsing techniques handling different language aspects into a single parser. Such composition moreover preserves simplicity of design of the combined parts. It opens new ways of parsing for linguistic purposes.", "authors": ["Michal Zemlicka"], "n_citation": 0, "title": "Parsing with oracle", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "637bca7a-1c18-4beb-845a-2f302d7ccada"}
{"authors": ["Dan Suciu"], "n_citation": 0, "title": "Typechecking for semistructured data", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "63cf2d1e-5012-452f-9ee3-b8d59fbd0afb"}
{"authors": ["Iain S. Duff", "Efstratios Gallopoulos", "Di Serafino D", "B Ucar"], "n_citation": 0, "title": "Topic 10: Parallel Numerical Algorithms", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "645b8b39-f482-40df-af6c-a4bc6cce6af5"}
{"abstract": "We introduce a data processing pipeline designed to generate registration markers from range scan data. This approach uses curvature maps and histogram-templates to identify local surface features. The noise associated with real-world scans is addressed using a (common) Gauss filter and expansion-segmentation. Experimental results are presented for data from The Digital Michelangelo Project.", "authors": ["John Rugis", "Reinhard Klette"], "n_citation": 0, "title": "Surface registration markers from range scan data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "651b146c-e0a8-4cf2-970b-26f36bd4409c"}
{"abstract": "We consider the Stable Marriage Problem and the Stable Roommates Problem in presence of ties and incomplete preference lists. They can be solved by centralized algorithms, but this requires to make public preference lists, something that members would prefer to avoid for privacy reasons. This motivates a distributed formulation to keep privacy. We propose a distributed constraint approach that solves all the considered problems, keeping privacy.", "authors": ["Ismel Brito", "Pedro Meseguer"], "n_citation": 0, "title": "Distributed stable matching problems with ties and incomplete lists", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65e63ce2-8a64-42bd-814c-4d9579a93eca"}
{"abstract": "Recently multi-block collision attacks (MBCA) were found on the Merkle-Damgard (MD)-structure based hash functions MD5, SHA-0 and SHA-1. In this paper, we introduce a new cryptographic construction called 3C devised by enhancing the MD construction. We show that the 3C construction is at least as secure as the MD construction against single-block and multi-block collision attacks. This is the first result of this kind showing a generic construction which is at least as resistant as MD against MBCA. To further improve the resistance of the design against MBCA, we propose the 3C+ design as an enhancement of 3C. Both these constructions are very simple adjustments to the MD construction and are immune to the straight forward extension attacks that apply to the MD hash function. We also show that 3C resists some known generic attacks that work on the MD construction. Finally, we compare the security and efficiency features of 3C with other MD based proposals.", "authors": ["Praveen Gauravaram", "William Millan", "Ed Dawson", "Kapali Viswanathan"], "n_citation": 0, "title": "Constructing secure hash functions by enhancing merkle-damg\u00e5rd construction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65f52bff-2813-4d79-b72e-786899adeb84"}
{"abstract": "This paper is devoted to global robust stability analysis of the general recurrent neural networks with time-varying parametric uncertainty and time-varying delays. To remove the dependence on the size of time-delays, Lyapunov-Razumikhin stability theorem and LMI approach are applied to derive the global robust stability conditions for the neural networks. Then delay-dependent global robust stability criteria are developed based on integrating Lyapunov-Krasovskii functional method and LMI approach. These stability criteria are in term of the solvability of linear matrix inequalities.", "authors": ["Jun Xu", "Daoying Pi", "Yong-Yan Cao"], "n_citation": 0, "title": "Global Robust Stability of General Recurrent Neural Networks with Time-Varying Delays", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "662a1e07-d00e-4bde-940b-6171c6f0b509"}
{"abstract": "We present a comparative study between Expectation-Maximization (EM) trained probabilistic neural network (PNN) with random initialization and with initialization from Global k-means. To make the results more comprehensive, the algorithm was tested on both homoscedastic and heteroscedastic PNNs. Normally, user have to define the number of clusters through trial and error method, which makes random initialization to be of stochastic nature. Global k-means was chosen as the initialization method because it can autonomously find the number of clusters using a selection criterion and can provide deterministic clustering results. The proposed algorithm was tested on benchmark datasets and real world data from the cooling water system in a power plant.", "authors": ["Roy Kwang Chang", "Chu Kiong Loo", "M. V. C. Rao"], "n_citation": 0, "title": "Autonomous and Deterministic Probabilistic Neural Network Using Global k-Means", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "669826dd-44d2-4eea-9fc6-f0eec5465d8d"}
{"abstract": "A fundamental tenet of user-centered design is that the needs, wants, limitations, and contexts of end users are central to the process of creating products and services that can be used and understood by the people who will use them. Most of the time these end users aren\u2019t all that different from the people designing the technology. But as the differences increase between designers and the people they\u2019re designing for, understanding and empathizing with users becomes harder and even more important. As we build software for people and communities with vastly diverse backgrounds, cultures, languages, and education, we need to stretch our ideas of what users want and need and how best to serve them.     The Technology for Emerging Markets (TEM) group at Microsoft Research India seeks to address the needs and aspirations of people in the developing world who are just beginning to use computing technologies and services as well as those for whom access to computing still remains largely out of reach. Much of this work can be described as designing for constraint: constraints in education, in infrastructure, in financial resources, in languages and in many other areas. In this talk, I will describe some work from our group that explores how we have tried to manage these constraints to create software and systems for people and communities often overlooked by technologists.", "authors": ["Edward Cutrell"], "n_citation": 0, "title": "Innovating in india: designing for constraint, computing for inclusion (keynote)", "venue": "international conference on software engineering", "year": 2014, "id": "66a31d1e-d30c-4d94-8beb-c610781c39a8"}
{"abstract": "In this paper a novel distributed video coding scheme was proposed based on Heegard-Berger coding theorem, rather than Wyner-Ziv theorem. The main advantage of HB coding is that the decoder can still decode and output a coarse reconstruction, even if side information degrade or absent. And if side information present or upgrade at decoder, a better reconstruction can be achieved. This robust feature can solve the problem lies in Wyner-Ziv video coding that the encoder can hardly decide the bit rate because rate-distortion was affected by the side information known only at the decoder. This feature also leaded to our HB video coding scheme with 2 decoding level of which we first reconstruct a coarse reconstruction frame without side information, and do motion search in previous reconstructed frame to find side information, then reconstruct a fine reconstruction frame through HB decoding again, with side information available.", "authors": ["Xiaopeng Fan", "\u00c1ngel Oscar", "Yan Chen", "Jiantao Zhou"], "n_citation": 0, "title": "Heegard-Berger video coding using LMMSE estimator", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "66b57631-cdb6-4fe4-a106-1e7bffc5ba9f"}
{"abstract": "Autonomous, highly distributed control architectures are composed of a significant number of holons/agents that can reason and act on behalf of represented processes or artifacts in a coordinated manner. Depending on the social organization capabilities of the agents, the autonomous system could evolve into complex agent organizations called temporal holarchies. Cost-based negotiation supports the holarchy formation. Dynamic hierarchical teamworks architecture of middle-agents is described to increase robustness of the architecture.", "authors": ["Francisco P. Maturana", "Pavel Tichy", "Petr Slechta", "Raymond J. Staron", "Fred M. Discenzo", "Kenwood H. Hall", "Vladim\u00edr Mar\u00edk"], "n_citation": 0, "title": "Cost-based dynamic reconfiguration system for evolving holarchies", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "671ffca7-af58-497e-8c79-aa4c8ad84ee2"}
{"abstract": "In this paper, a kind of relation between CNN (cellular neural network) and GIM (Gibbs image model) is noted. Based on this relation, a new approach for CNN's template design is proposed, this approach is valid to many questions that could be processed with GIM, such as segmentation, edge detection and restoration. We also discuss the learning algorithm and hardware annealing jointed with the new approach. Simulations of some examples are shown in order to validate effectiveness of new approach.", "authors": ["Jianye Zhao", "Hongling Meng", "Daoheng Yu"], "n_citation": 0, "title": "A novel CNN template design method based on GIM", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6786075e-97fb-417b-b57d-97ffe28d5a87"}
{"abstract": "The contribution of this paper is in demonstrating the impact of AND/OR search spaces view on solutions counting. In contrast to the traditional (OR) search space view, the AND/OR search space displays independencies present in the graphical model explicitly and may sometimes reduce the search space exponentially. Empirical evaluation focusing on counting demonstrates the spectrum of search and inference within the AND/OR search spaces.", "authors": ["Rina Dechter", "Robert Mateescu"], "n_citation": 0, "title": "The impact of AND/OR search spaces on constraint satisfaction and counting", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "67a7f507-8d5c-4f51-97dd-0a8bd98187c7"}
{"abstract": "We consider the problem of bounded model checking for linear temporal logic with past operators (PLTL). PLTL is more attractive as a specification language than linear temporal logic without past operators (LTL) since many specifications are easier to express in PLTL. Although PLTL is not more expressive than LTL, it is exponentially more succinct. Our contribution is a new more efficient encoding of the bounded model checking problem for PLTL based on our previously presented encoding for LTL. The new encoding is linear in the bound. We have implemented the encoding in the NuSMV 2.1 model checking tool and compare it against the encoding in NuSMV by Benedetti and Cimatti. The experimental results show that our encoding performs significantly better than this previously used encoding.", "authors": ["Timo Latvala", "Armin Biere", "Keijo Heljanko", "Tommi A. Junttila"], "n_citation": 0, "title": "Simple is better: Efficient bounded model checking for past LTL", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "67ac5960-920f-4a5e-99ea-180c05e0b67c"}
{"abstract": "Model-based applications in engineering, such as diagnosis, configuration or interactive decision-support systems, require embedded constraint solvers with challenging capabilities. Not only consistency checking and solving, but also the computation of (minimal) conflicts and explanations are required. Moreover, realistic models of engineered systems often require the usage of very expressive constraint languages, which mix continuous and discrete variable domains, linear and non-linear equations, inequations, and even procedural constraints. A positive feature of the models of typical engineered systems is, however, that their corresponding constraint problems have a bounded and even relatively small density (induced width). We present here our relational constraint solver RCS that has been specifically designed to address these requirements. RCS is based on variable elimination, exploiting the low-density property. To analyze a set of constraints, RCS builds a so-called aggregation tree by joining the input constraints and eliminating certain variables after every single join. The aggregation tree is then used to compute solutions, as well as explanations and conflicts. We also report some preliminary experimental results obtained with a prototype implementation of this framework.", "authors": ["Jakob Mauss", "Frank Seelisch", "Mugur Tatar"], "n_citation": 0, "title": "A relational constraint solver for model-based engineering", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "67d56be1-bbb1-4a09-a638-a2221ffbb9e7"}
{"abstract": "Inspired by AND/OR search spaces for graphical models recently introduced, we propose to augment Ordered Decision Diagrams with AND nodes, in order to capture function decomposition structure. This yields AND/OR multi-valued decision diagram (AOMDD) which compiles a constraint network into a canonical form that supports polynomial time queries such as solution counting, solution enumeration or equivalence of constraint networks. We provide a compilation algorithm based on Variable Elimination for assembling an AOMDD for a constraint network starting from the AOMDDs for its constraints. The algorithm uses the APPLY operator which combines two AOMDDs by a given operation. This guarantees the complexity upper bound for the compilation time and the size of the AOMDD to be exponential in the treewidth of the constraint graph, rather than pathwidth as is known for ordered binary decision diagrams (OBDDs).", "authors": ["Robert Mateescu", "Rina Dechter"], "n_citation": 50, "title": "Compiling constraint networks into AND/OR multi-valued decision diagrams (AOMDDs)", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "681f0c50-ebd1-4497-986e-eeba8961e16b"}
{"abstract": "In a previous work we have presented a formal framework devoted to show the relevance of choreography and orchestration in the design of service oriented applications. Even if useful to start a formal investigation of the relationship between choreography and orchestration, the proposed framework was not suitable to specify real case studies. In fact, it simply permitted to specify all possible computations abstracting away from the conditions driving the choice of the actual behaviour. In this paper we tackle this problem by introducing the notion of state variables. The addition of state requires a substantial modification of the entire framework because the same state variable, at the level of choreography, can be actually stored in distributed orchestrators that will need to synchronize in order to maintain consistent views. In order to faithfully investigate this problem we also need to modify the formal model at the orchestration level, moving from synchronous to asynchronous communication as the latter is the communication modality of the ordinary communication infrastructures.", "authors": ["Nadia Busi", "Roberto Gorrieri", "Claudio Guidi", "Roberto Lucchi", "Gianluigi Zavattaro"], "n_citation": 144, "title": "Choreography and orchestration conformance for system design", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "68b59a9a-bdaa-40fc-85ac-4c2642a277b5"}
{"abstract": "We investigate the efficiency of BDD-like data-structures for timed automata verification. We find that the efficiency is highly sensitive to the variable designs and canonical form definitions. We explore the two issues in details and propose to use CRD (Clock-Restriction Diagram) for timed automata state-space representation. We compare two canonical forms for zones, develop a procedure for quick zone-containment detection, and discuss the effect of variable-ordering of CRD. We implement our idea in our tool red 4.1 and carry out experiments to compare with other tools and red's previous version in both forward and backward analysis.", "authors": ["Farn Wang"], "n_citation": 0, "title": "Efficient verification of timed automata with BDD-like data-structures", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "6952006a-44d3-4ce8-8dbf-d95537a82051"}
{"abstract": "Constraint Handling Rules (CHR) allow one to specify and implement both propagation and simplification for user-defined constraints. Since a propagation rule is applicable again and again, we present in this paper for the first time an operational semantics for CHR that avoids the termination problem with propagation rules. In previous work [AFM96], a sufficient and necessary condition for the confluence of terminating simplification rules was given inspired by results about conditional term rewriting systems. Confluence ensures that the solver will always compute the same result for a given set of constraints independent of which rules are applied. The confluence of propagation rules was an open problem. This paper shows that we can also give a sufficient and a necessary condition for confluence of terminating CHR programs with propagation rules based on the more refined operational semantics.", "authors": ["Slim Abdennadher"], "n_citation": 0, "title": "Operational semantics and confluence of constraint propagation Rules", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "696a7f41-1eae-454a-9e93-3fb16d5a4431"}
{"abstract": "In this paper, we consider the statistical decision processes behind a linear and a differential cryptanalysis. By applying techniques and concepts of statistical hypothesis testing, we describe precisely the shape of optimal linear and differential distinguishers and we improve known results of Vaudenay concerning their asymptotic behaviour. Furthermore, we formalize the concept of sequential distinguisher and we illustrate potential applications of such tools in various statistical attacks.", "authors": ["Pascal Junod"], "n_citation": 0, "title": "On the optimality of linear, differential, and sequential distinguishers", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "69914e7f-051a-47ec-a74d-d8b1624d133e"}
{"abstract": "Practical pattern classification and knowledge discovery problems require selecting a useful subset of features from a much larger set to represent the patterns to be classified. Exhaustive evaluation of possible feature subsets is usually infeasible in practice because of the large amount of computational effort required. Bio-inspired algorithms offer an attractive approach to find near-optimal solutions to such optimization problems. This paper presents an approach to feature subset selection using bio-inspired algorithms. Our experiments with several benchmark real world patted classification problems demonstrate the feasibility of this approach to feature subset selection in the automated design of neural networks for pattern classification and knowledge discovery.", "authors": ["Keunjoon Lee", "Jinu Joo", "Jihoon Yang", "Vasant Honavar"], "n_citation": 0, "title": "Experimental Comparison of Feature Subset Selection Using GA and ACO Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "69a5e103-66b1-4164-ae89-b071ae0db928"}
{"abstract": "In case of application to high-dimensional pattern recognition task, Independent Component Analysis (ICA) often suffers from two challenging problems. One is the small sample size problem. The other is the choice of basis functions (or independent components). Both problems make ICA classifier unstable and biased. To address the two problems, we propose an enhanced ICA algorithm using a cascaded ensemble learning scheme, named as Random Independent Subspace (RIS). A random resampling technique is used to generate a set of low dimensional feature subspaces in the original feature space and the whiten feature space, respectively. One classifier is constructed in each feature subspace. Then these classifiers are combined into an ensemble classifier using a final decision rule. Extensive experimentations performed on the FERET database suggest that the proposed method can improve the performance of ICA classifier.", "authors": ["Jian Cheng", "Kongqiao Wang", "Yen-Wei Chen"], "n_citation": 0, "title": "A cascaded ensemble learning for independent component analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "69cd8cab-7bee-48e5-a54b-361c76695ee6"}
{"abstract": "We present a sound and complete rule set for determining whether sorting by document order and duplicate removal operations in the query plan of XPath expressions are unnecessary. Additionally we define a deterministic automaton that illustrates how these rules can be translated into an efficient algorithm. This work is an important first step in the understanding and tackling of XPath/XQuery optimization problems that are related to ordering and duplicate removal.", "authors": ["Jan Hidders", "Philippe Michiels"], "n_citation": 0, "title": "Avoiding unnecessary ordering operations in XPath", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6a2c6e2c-7089-456e-8621-27445d4d6f11"}
{"authors": ["Claude-Guy Quimper", "Toby Walsh"], "n_citation": 85, "title": "Global grammar constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6a4e74e8-ff73-443f-a4ac-27249a7c32c6"}
{"abstract": "We study the global cardinality constraint (gcc) and propose an O(n 1.5 d) algorithm for domain consistency and an O(cn + dn) algorithm for range consistency where n is the number of variables, d the number of values in the domain, and c an output dependent variable smaller than or equal to n. We show how to prune the cardinality variables in O(n 2 d + n 2.66 ) steps, detect if g cc is universal in constant time and prove that it is NP-Hard to maintain domain consistency on extended-GCC.", "authors": ["Claude-Guy Quimper", "Alejandro L\u00f3pez-Ortiz", "Peter van Beek", "Alexander Golynski"], "n_citation": 0, "title": "Improved algorithms for the global cardinality constraint", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6a670868-2420-4292-ad37-950eb4b616fb"}
{"abstract": "Korean displays various types of relative clauses including head internal and external relative clauses (HIRC and HERC). In particular, the treatment of HIRC has received less attention from computational perspectives even though it is frequently found in both text and spoken languages. This paper shows that a typed feature structure grammar of HPSG (together with the semantic representations of Minimal Recursion Semantics) offers us a computationally feasible and applicable way of deep-parsing both the HIRC and HERC in the language.", "authors": ["Jong-Bok Kim"], "n_citation": 50, "title": "Parsing head internal and external relative clause constructions in Korean", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6aa22c14-f3d1-4a7e-b6d2-d6205ca5ef96"}
{"abstract": "Network attacks often employ scanning to locate vulnerable hosts and services. Fast and accurate detection of local scanners is key to containing an epidemic in its early stage. Existing scan detection schemes use statically determined detection criteria, and as a result do not respond well to traffic perturbations. We present two adaptive scan detection schemes, Success Based (SB) and Failure Based (FB), which change detection criteria based on traffic statistics. We evaluate the proposed schemes analytically and empirically using network traces. Against fast scanners, the adaptive schemes render detection precision similar to the traditional static schemes. For slow scanners, the adaptive schemes are much more effective, both in terms of detection precision and speed. SB and FB have non-linear properties not present in other schemes. These properties permit a lower Sustained Scanning Threshold and a robustness against perturbations in the background traffic.", "authors": ["Ahren Studer", "Chenxi Wang"], "n_citation": 0, "title": "Adaptive detection of local scanners", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6ab7cb21-496f-4cc4-b1ff-92f2cbda85f9"}
{"abstract": "Long leading-time streamflow forecast is a complex non-linear procedure. Traditional methods are easy to get slow convergence and low efficiency. The biased wavelet neural network (BWNN) based on BP learning algorithm is proposed and used to forecast monthly streamlfow. It inherits the multiresolution capability of wavelets analysis and the nonlinear input-output mapping trait of artificial neural networks. With the new set of biased wavelets, BWNN can effectively cut down the redundancy from multiresolution calculation. The learning rate and momentum coefficient are employed in BP algorithm to accelerate convergence and avoid falling into local minimum. BWNN is applied to Fengtan reservoir as case study. Its simulation performance is compared with the results obtained from autoregressive integrated moving average, genetic algorithm, feedforward neural network and traditional wavelet neural network models. It is shown that BWNN has high model efficiency index, low computing redundancy and provides satisfying forecast precision.", "authors": ["Fang Liu", "Jianzhong Zhou", "Fang-Peng Qiu", "Junjie Yang"], "n_citation": 50, "title": "Biased Wavelet Neural Network and Its Application to Streamflow Forecast", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6adcbb47-03df-4592-a4e9-ae92e0753fcf"}
{"abstract": "This paper presents two approaches to spoken document retrieval-search in LVCSR recognition lattices and in phoneme lattices. For the former one, an efficient method of indexing and search of multi-word queries is discussed. In phonetic search, the indexation of tri-phoneme sequences is investigated. The results in terms of response time to single and multi-word queries are evaluated on ICSI meeting database.", "authors": ["Lukas Burget", "Jan Cernocky", "Michal Fapso", "Martin Karafiat", "Pavel Matejka", "Petr Schwarz", "Pavel Smrg", "Igor Sz\u00f6ke"], "n_citation": 0, "title": "Indexing and search methods for spoken documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6bbdcc08-209e-4805-a579-ea4b15dabb52"}
{"abstract": "We study the problem of computing multicast trees with minimal bandwidth consumption in multi-hop wireless mesh networks. For wired networks, this problem is known as the Steiner tree problem, and it has been widely studied before. We demonstrate in this paper, that for multihop wireless mesh networks, a Steiner tree does not offer the minimal bandwidth consumption, because it neglects the wireless multicat advantage. Thus, we re-formulate the problem in terms of minimizing the numbrer of transmissions, rather than the edge cost of multicast trees. We show that the new problem is also NP-complete and we propose heuristics to compute good approximations for such bandwidth-optimal trees. Our simulation results show that the proposed heuristics offer a lower bandwidth consumption compared with Steiner trees.", "authors": ["Pedro M. Ruiz", "Antonio Fernandez G\u00f3mez-Skarmeta"], "n_citation": 0, "title": "Heuristic algorithms for minimum bandwith consumption multicast routing in wireless mesh networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6c2ae68f-b75c-4598-b394-0fbd5d31fc4e"}
{"abstract": "One of the main drawbacks of many machine learning techniques, such as neural networks or ensemble methods, is the incomprehensibility of the model produced. One possible solution to this problem is to consider the learned model as an oracle and generate a new model that mimics the semantics of the oracle by expressing it in the form of rules. In this paper we analyse experimentally the influence of pruning, the size of the invented dataset and the confidence of the examples in order to obtain shorter sets of rules without reducing too much the accuracy of the model. The experiments show that the factors analysed affect the mimetic model in different ways. We also show that by combining these factors in a proper way the quality of the mimetic model improves significantly wrt. other previous reports on the mimetic method.", "authors": ["Ricardo Blanco-Vega", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Maria Josh Ramirez-Quintana"], "n_citation": 0, "title": "Analysing the trade-off between comprehensibility and accuracy in mimetic models", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6c3454b6-6841-4fdf-adca-b05094c39c76"}
{"abstract": "In this paper we describe the initial stages of the ASR component of the MALACH (Multilingual Access to Large Spoken Archives) project. This project will attempt to provide improved access to the large multilingual spoken archives collected by the Survivors of the Shoah Visual History Foundation (VHF) by advancing the state of the art in automated speech recognition. In order to train the ASR system, it is neccesary to manually transcribe a large amount of speech data, identify the appropriate vocabulary, and obtain relevant text for language modeling. We give a detailed description of the speech annotation process; show the specific properties of the spontaneous speech contained in the archives; and present a baseline speech recognition results.", "authors": ["Josef Psutka", "Pavel Ircing", "Josef Psutka", "V. Radov\u00e1", "William J. Byrme", "Jan Hajic", "Samuel Gustman", "Bhuvana Ramabhadran"], "n_citation": 50, "title": "Automatic transcription of Czech language oral history in the MALACH project: Resources and initial experiments", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6c991680-4b28-4dbf-a0cc-941c9aede16b"}
{"abstract": "Flood of information sometimes makes it difficult to extract useful knowledge from databases, libraries and WWW. This paper presents an intelligent method for extraction of word senses from human factors in knowledge discovery, which utilizes the integrated Korean noun and verb networks through the selectional restriction relations in sentences. Integration of Korean Noun Networks into the SENKOV(Semantic Networks for Korean Networks) system will play an important role in both computational linguistic applications and psycholinguistic models of language processing.", "authors": ["Yoo-Jin Moon", "Minkoo Kim", "Young-Ho Hwang", "Pankoo Kim", "Kijoon Choi"], "n_citation": 0, "title": "Extraction of word senses from human factors in knowledge discovery", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6cb11646-b898-43f1-8cdc-b0d1ba20b859"}
{"abstract": "This paper provides theoretical foundations for the group signature primitive. We introduce strong, formal definitions for the core requirements of anonymity and traceability. We then show that these imply the large set of sometimes ambiguous existing informal requirements in the literature, thereby unifying and simplifying the requirements for this primitive. Finally we prove the existence of a construct meeting our definitions based only on the sole assumption that trapdoor permutations exist.", "authors": ["Mihir Bellare", "Daniele Micciancio", "Bogdan Warinschi"], "n_citation": 599, "title": "Foundations of group signatures: Formal definitions, simplified requirements, and a construction based on general assumptions", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "6d745a92-ef5b-482f-a45c-cdfa8fcae5c4"}
{"abstract": "The min-max modular network has been shown to be an efficient classifier, especially in solving large-scale and complex pattern classification problems. Despite its high modularity and parallelism, it suffers from quadratic complexity in space when a multiple-class problem is decomposed into a number of linearly separable problems. This paper proposes two new pruning methods and an integrated process to reduce the redundancy of the network and optimize the network structure. We show that our methods can prune a lot of redundant modules in comparison with the original structure while maintaining the generalization accuracy.", "authors": ["Yang Yang", "Bao-Liang Lu"], "n_citation": 0, "title": "Structure pruning strategies for min-max modular network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6d7f81cb-8674-46f8-9548-d849505eb01a"}
{"abstract": "In this paper we describe the deployment of an offshore wireless sensor network and the lightweight intelligence that was integrated into the data acquisition and forwarding software. Although the conditions were harsh and the hardware readily available, the deployment managed to characterise an extended period due the some unique algorithms In-situ adaptation to differences in data types, hardware condition and user requirements are demonstrated and analysis of performance carried out. Statistical tests, local feedback and global genetic style material exchange ensure limited resources such as battery and bandwidth are used efficiently by manipulating data at the source.", "authors": ["Chris Roadknight", "Antonio Gonzalez", "Laura Parrot", "Steve Boult", "Ian W. Marshall"], "n_citation": 0, "title": "An intelligent sensor network for oceanographic data acquisition", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "6d93ae7f-8a54-4c4b-b037-fcc41f37a8e4"}
{"abstract": "DPA-countermeasures are one of the essential technology for implementing elliptic curve cryptosystems (ECC) on smart cards. Not only standard DPA but also recently proposed refined power analysis (RPA) and zero value analysis (ZVA) should be considered. Itoh, Izu and Takenaka proposed a secure and efficient countermeasure (the randomized initial point countermeasure, RIP) in order to resist these attacks. Then, Mamiya, Miyaji and Morimoto improved the efficiency. This paper also aims at improving RIP in another direction. As a result, compared to the original RIP, about 28% improvement can be established. In other words, the proposed countermeasure has almost no penalty from a non DPA-resistant scalar multiplication.", "authors": ["Kouichi Itoh", "Tetsuya Izu", "Masahiko Takenaka"], "n_citation": 50, "title": "Improving the randomized initial point countermeasure against DPA", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6dc18c9b-cc5b-44d8-8fe6-c2ad012df96e"}
{"abstract": "We show that the same methodology used to study phase transition behaviour in NP-complete problems works with a polynomial problem class: establishing arc consistency. A general measure of the constrainedness of an ensemble of problems, used to locate phase transitions in random NP-complete problems, predicts the location of a phase transition in establishing arc consistency. A complexity peak for the AC3 algorithm is associated with this transition. Finite size scaling models both the scaling of this transition and the computational cost. On problems at the phase transition, this model of computational cost agrees with the theoretical worst case. As with NP-complete problems, constrainedness - and proxies for it which are cheaper to compute - can be used as a heuristic for reducing the number of checks needed to establish arc consistency in AC3.", "authors": ["Ian P. Gent", "Ewan MacIntyre", "Patrick Prosser", "Paul Shaw", "Toby Walsh"], "n_citation": 0, "title": "The constrainedness of Arc consistency", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "6dccfd3e-9a43-46d3-be3c-83de69c093f2"}
{"abstract": "This paper presents a novel independent component analysis algorithm that separates mixtures using serially updating geodesic method. The geodesic method is derived from the Stiefel manifold, and an on-line version of this method that can directly treat with the unwhitened observations is obtained. Simulation of artificial data as well as real biological data reveals that our proposed method has fast convergence.", "authors": ["Tao Yu", "Huaizong Shao", "Qicong Peng"], "n_citation": 0, "title": "An ICA Learning Algorithm Utilizing Geodesic Approach", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6df5f7ae-dae2-4e26-bbc0-84e7ef9ae88f"}
{"abstract": "Although negative conclusions are presented implicitly in Normal Hybrid Probabilistic Programs (NHPP) [26] through the closed world assumption, representing and reasoning with explicit negation is needed in NHPP to allow the ability to reason with incomplete knowledge. In this paper we extend the language of NHPP to explicitly encode classical negation in addition to non-monotonic negation. The semantics of the extended language is based on the answer set semantics of traditional logic programming [9]. We show that the proposed semantics is a natural extension to the answer set semantics of traditional logic programming [9]. In addition, the proposed semantics is reduced to stable probabilistic model semantics of NHPP [26]. The importance of that is computational methods developed for NHPP can be applied to the proposed language. Furthermore, we show that some commonsense probabilistic knowledge can be easily represented in the proposed language.", "authors": ["Emad Saad"], "n_citation": 0, "title": "Incomplete knowledge in hybrid probabilistic logic programs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6e18e45e-b89c-4ead-8f7b-932cfe566db6"}
{"abstract": "We present the currently simplest, most efficient, optimally resilient, adaptively secure, and proactive threshold RSA scheme. A main technical contribution is a new rewinding strategy for analysing threshold signature schemes. This new rewinding strategy allows to prove adaptive security of a proactive threshold signature scheme which was previously assumed to be only statically secure. As a separate contribution we prove that our protocol is secure in the UC framework.", "authors": ["Jes\u00fas F. Almansa", "Ivan Damg\u00e5rd", "Jesper Buus Nielsen"], "n_citation": 0, "title": "Simplified threshold RSA with adaptive and proactive security", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6e743411-c507-414e-9735-5e8937c50b17"}
{"abstract": "Path broken, which is caused by node mobility, leads to the rerouting operations and the degradation of network performance. How to select a reliable path becomes a critical issue in designing routing protocol. The metric for routing selection in terms of path availability which is based on link availability can be used to select reliable paths. Based on the random walk-based mobility model, we propose a prediction method for calculating the link availability at any time (LA(t)) on the basis of the duration T during which two nodes keep their movement status unchanged for the first time. Meanwhile, a general method to calculate the average LET (r ) is developed as well as the general expression of LA(t) is given. Simulation results show that the results of our method are more approximate to the practical results as compared with those of other methods.", "authors": ["Jianxin Wang", "Xianman Zhu", "Jianer Chen"], "n_citation": 0, "title": "Link availability at any time in MANET", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6ef36e7a-a4db-45c8-a08e-f82fa96d0faa"}
{"abstract": "This paper proposes a novel routing protocol dedicated to IVC. This routing protocol, named as geographic pattern routing (GPR). The protocol does not need to search route frequently, so sufficient bandwidth is reserved for application packets, and QoS is easily guaranteed. It has ingredients to support geographic pattern search and packet forwarding assisted by geographic pattern, and it considers whether sufficient nodes are on the path represented by geographic pattern to allow packets forwarding to the destination. Besides vehicle location, it does not need other information. The simulation shows that GPR is better than other routing protocols.", "authors": ["Jiang Hao", "Luo Jian-jin", "Kun Mean Hou", "Chen Lijia"], "n_citation": 50, "title": "Geographic pattern routing for MANETOR in IVC", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6f28e832-f270-4dd3-bbd9-543ab9da1d9d"}
{"abstract": "In this paper we present a new framework for over constrained problems. We suggest to define an over-constrained network as a global constraint. We introduce two new lower bounds of the number of violations, without making any assumption on the arity of constraints.", "authors": ["Jean-Charles R\u00e9gin", "Thierry Petit", "Christian Bessiere", "Jean-Francois Puget"], "n_citation": 56, "title": "An original constraint based approach for solving over constrained problems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "6f45b3a1-feb5-4222-b62c-db57f3615179"}
{"abstract": "We show that if a minimal-time solution exists for a fundamental distributed computation primitive, synchronizing a general directed network of finite-state processors, then there must exist an extraordinarily fast O(ED log 2  D (log 2  n) 2  ) algorithm in the RAM model of computation for exactly determining the diameter of a general directed graph. The proof is constructive. This result interconnects two very distinct areas of computer science: distributed protocols on networks of intercommunicating finite-state machines and standard algorithms on the usual RAM model of computation.", "authors": ["Darin Goldstein", "Kojiro Kobayashi"], "n_citation": 0, "title": "On the complexity of the most general firing squad synchronization problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6f6e21c0-5b8a-4749-8e2a-bc4c499e77e2"}
{"abstract": "In this paper, we propose a new algorithm to establish Generalized Arc Consistency (GAC) on positive table constraints, i.e. constraints defined in extension by a set of allowed tuples. Our algorithm visits the lists of valid and allowed tuples in an alternative fashion when looking for a support (i.e. a tuple that is both allowed and valid). It is then able to jump over sequences of valid tuples containing no allowed tuple and over sequences of allowed tuples containing no valid tuple. Our approach, that can be easily grafted to any generic GAC algorithm, admits on some instances a behaviour quadratic in the arity of the constraints whereas classical approaches, i.e. approaches that focus on either valid or allowed tuples, admit an exponential behaviour. We show the effectiveness of this approach, both theoretically and experimentally.", "authors": ["Christophe Lecoutre", "Radoslaw Szymanek"], "n_citation": 0, "title": "Generalized arc consistency for positive table constraints", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6fef5ecd-572f-4b3c-b315-99a36a2b4ea9"}
{"abstract": "By using the continuation theorem for Mawhin's coincidence degree and some analytical techniques, several sufficient conditions are obtained ensuring existence of periodic solution of BAM neural networks with variant coefficients. delays and impulse.", "authors": ["Hui Wang", "Xiaofeng Liao", "Chuandong Li", "Degang Yang"], "n_citation": 0, "title": "Existence of Periodic Solution of BAM Neural Network with Delay and Impulse", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7002cd7d-b28d-4197-b5a2-3eadc01e5d4a"}
{"abstract": "The minor component is the eigenvector associated with the smallest eigenvalue of the covariance matrix of the input data. The minor component analysis (MCA) is a statistical method for extracting the minor component. Many neural networks have been proposed to solve MCA. However, there exists the problem of the divergence of the norm of the weight vector in these neural networks. In this paper, a modification to the well known MCA EXIN algorithm is presented by adjusting the learning rate. The modified MCA EXIN algorithm can guarantee that the norm of the weight vector of the neural network converges to a constant. Mathematical proofs and simulation results are given to show the convergence of the algorithm.", "authors": ["Dezhong Peng", "Zhang Yi", "Xiaolin Xiang"], "n_citation": 0, "title": "A modified MCA EXIN algorithm and its convergence analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "70141b35-50f6-4500-8f23-7cc5faca49b8"}
{"abstract": "In this paper we describe a complete method for building a perceptual user interface in indoor uncontrolled environments. The overall system uses two calibrated cameras and does initialization: it detects user, takes his/her measurements, builds a 3D-Model. It performs matching/tracking for: trunk, head, left arm, right arm and hands. The system is waiting for a user in a predefined posture, once the user has been detected he/she is analysed to take measurements are taken and a 3D-Model is built. Tracking is carried out by a Particle Filter algorithm splited in three steps: tracking of head-trunk, tracking of left arm and tracking of right arm. This proposed divide and conquer solution improves computation time without getting better or similar results than sequential solution. The matching process uses two sub-matching functions, one to compute color and another to compute shape one. Finally the system provides numerical values for joints and end effectors to be used for interactive applications.", "authors": ["Jose Maria Buades Rubio", "Francisco J. Perales", "Manuel Gonz\u00e1lez Hidalgo", "Javier Varona"], "n_citation": 50, "title": "Upper body tracking for interactive applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7031de2e-4b56-46cb-9334-b1ee70b94e45"}
{"abstract": "The syntactic and semantic complexity of the so-called numeral classifier (Num-CI) constructions in Korean challenges theoretical as well as computational linguists. We provide a constraint-based analysis of these constructions within the framework of HPSG with the semantic representations of MRS (Minimal Recursion Semantics) and reports its implementation in the LKB (Linguistic Knowledge Building) system.", "authors": ["Jong-Bok Kim", "Jaehyung Yang"], "n_citation": 0, "title": "Processing korean numeral classifier constructions in a typed feature structure grammar", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "709ee30c-a883-4a5e-827c-e4a07bc040e5"}
{"abstract": "We consider communication sessions in which a pair of parties begin by running an authenticated key-exchange protocol to obtain a shared session key, and then secure successive data transmissions between them via an authenticated encryption scheme based on the session key. We show that such a communication session meets the notion of a secure channel protocol proposed by Canetti and Krawczyk [9] if and only if the underlying authenticated encryption scheme meets two new, simple definitions of security that we introduce, and the key-exchange protocol is secure. In other words, we reduce the secure channel requirements of Canetti and Krawczyk to easier to use, stand-alone security requirements on the underlying authenticated encryption scheme. In addition, we relate the two new notions to existing security notions for authenticated encryption schemes.", "authors": ["Chanathip Namprempre"], "n_citation": 0, "title": "Secure channels based on authenticated encryption schemes: A simple characterization", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "70baf5f4-1664-4610-9f3e-a30b9987134b"}
{"abstract": "The cutting operation of 3D surface meshes plays an important role in surgery simulators. One of the important requirements for surgical simulators is the visual reality. We propose a new strategy for cutting on surface meshes: refinement and separate strategy consisting of the refinement followed by the separation of the refined mesh element.The proposed strategy gives the faithful representation of interaction paths of a surgical tool.", "authors": ["Huynh Quang Huy Viet", "Takahiro Kamada", "Hiromi T. Tanaka"], "n_citation": 0, "title": "An adaptive 3D surface mesh cutting operation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "71a423c0-cdaf-4009-aaaa-3d8c286cc633"}
{"authors": ["Zbyn\u011bk Zaj\u00edc", "Luk\u00e1\u0161 Machlica", "Lud\u011bk M\u00fcller"], "n_citation": 0, "title": "Robust Statistic Estimates for Adaptation in the Task of Speech Recognition", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "71b94682-176f-4ef0-bc4b-ed01e493db84"}
{"abstract": "In railway simulation, given timetables have to be checked against various criteria, mainly correctness and robustness. Most existing approaches use classical centralized simulation techniques. This work goes beyond that in two main aspects: We use constraint satisfaction to get rid of deadlock problems and the simulation is done distributedly for better performance. This should make it possible to solve very large railway simulation problems.", "authors": ["Hans Schlenker"], "n_citation": 0, "title": "Distributed constraint-based railway simulation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "727f6cf0-1b55-4fbf-888c-0d5b5ea0b1d6"}
{"abstract": "We introduce an advanced architecture of genetically optimized Hybrid Fuzzy Neural Networks (gHFNN) and develop a comprehensive design methodology supporting their construction. The gHFNN architecture results from a synergistic usage of the hybrid system generated by combining Fuzzy Neural Networks (FNN) with Polynomial Neural Networks (PNN). We distinguish between two types of the linear fuzzy inference rule (TSK fuzzy rule)-based FNN structures showing how this taxonomy depends upon the type of a fuzzy partition of input variables. As to the consequence part of the gHFNN, the development of the PNN dwells on two general optimization mechanisms: the structural optimization is realized via GAs whereas in case of the parametric optimization we proceed with a standard least square method-based learning.", "authors": ["Sungkwun Oh", "Byoung-Jun Park", "Hyun-Ki Kim"], "n_citation": 0, "title": "Genetically optimized hybrid fuzzy neural networks based on TSK fuzzy rules and polynomial neurons", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "72f0f7d8-c9da-454d-beb5-13e62a683984"}
{"abstract": "Building an effective classifer involves choosing the model class with the appropriate learning bias as well as the right level of complexity within that class. These two aspects have rarely been addressed together: typically, model class (or algorithm) selection is performed on the basis of default settings, while model instance (or complexity) selection is investigated within the confines of a single model class. We study the impact of model complexity on algorithm selection and show how the relative performance of candidate algorithms changes drastically with the choice of complexity parameters.", "authors": ["Melanie Hilario"], "n_citation": 0, "title": "Model complexity and algorithm selection in classification", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "72fd70b0-d1ae-4a4c-bff9-89f009ef4971"}
{"abstract": "We present an interface between the ECL i PS e  constraint logic programming system and the GAP computational abstract algebra system. The interface provides a method for efficiently dealing with large numbers of symmetries of constraint satisfaction problems for minimal programming effort. We also report an implementation of SBDS using the GAP-ECL i PS e  interface which is capable of handling many more symmetries than previous implementations and provides improved search performance for symmetric constraint satisfaction problems.", "authors": ["Ian P. Gent", "Warwick Harvey", "Tom Kelsey"], "n_citation": 115, "title": "Groups and constraints: Symmetry breaking during search", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "73051e19-80f0-4556-9fbb-b708090bb42e"}
{"abstract": "Much progress has been made in terms of boosting the effectiveness of backtrack style search methods. In addition, during the last decade, a much better understanding of problem hardness, typical case complexity, and backtrack search behavior has been obtained. One example of a recent insight into backtrack search concerns so-called heavy-tailed behavior in randomized versions of backtrack search. Such heavy-tails explain the large variations in runtime often observed in practice. However, heavy-tailed behavior does certainly not occur on all instances. This has led to a need for a more precise characterization of when heavy-tailedness does and when it does not occur in backtrack search. In this paper, we provide such a characterization. We identify different statistical regimes of the tail of the runtime distributions of randomized backtrack search methods and show how they are correlated with the sophistication of the search procedure combined with the inherent hardness of the instances. We also show that the runtime distribution regime is highly correlated with the distribution of the depth of inconsistent subtrees discovered during the search. In particular, we show that an exponential distribution of the depth of inconsistent subtrees combined with a search space that grows exponentially with the depth of the inconsistent subtrees implies heavy-tailed behavior.", "authors": ["Carla P. Gomes", "C\u00e8sar Fern\u00e1ndez", "Bart Selman", "Christian Bessiere"], "n_citation": 50, "title": "Statistical regimes across constrainedness regions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7319aeea-fc2f-4e1d-97d2-d139b0374af2"}
{"abstract": "Digital watermarking applies to variety of media including image, video, audio and text. Because of the nature of digital text, its watermarking methods are special. Moreover, these methods basically depend on the script used in the text. This paper reviews application of digital watermarking to Farsi (Persian) and similar scripts (like Arabic, Urdu and Pashto) which are substantially different from English and other western counterparts, especially in using connected alphabets. Focusing on the special characteristics of these scripts, application of common methods used for text watermarking is studied. By comparing the results, suitable methods which results in the highest payload will be presented.", "authors": ["Ali Khodami", "Khashayar Yaghmaie"], "n_citation": 0, "title": "Persian Text Watermarking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "73409f28-b8d2-4ebc-ac95-0e3f48dd5f45"}
{"abstract": "The paradigm of Multilayered Extended Semantic Networks (MultiNet) is one of the most thoroughly described knowledge representantion systems along the line of semantic networks. The conceptual representation of MultiNet is characterized by embedding its nodes into a multidimensional space of layer attributes. These layer attributes play an important part during the syntactico-semantic analysis of natural language texts and during the inferential answer finding in question answering systems. The paper demonstrates the automatic generation of complex layer information for conceptual nodes and their use in the assimilation of knowledge pieces into a larger knowledge base.", "authors": ["Sven Hartrumpf", "Hermann Helbig"], "n_citation": 0, "title": "The generation and use of layer information in Multilayered Extended Semantic Networks", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "734c1080-ee44-41e3-a4a5-58985c62f75a"}
{"abstract": "A novel text-independent verification system based on the fractional Brownian motion (M\u2015dim\u2015fBm) for automatic speaker recognition (ASR) is presented in this paper. The performance of the proposed M\u2015dim\u2015fBm was compared to those achieved with the GMM (Gaussian Mixture Models) classifier using the mel-cepstral coefficients. We have used a speech database - obtained from fixed and cellular phones - uttered by 75 different speakers. The results have shown the superior performance of the M\u2015dim\u2015fBm classifier in terms of recognition accuracy. In addition, the proposed classifier employs a much simpler modeling structure as compared to the GMM.", "authors": ["Ricardo Sant Ana", "R. Coelho", "Abraham Alcaim"], "n_citation": 0, "title": "A new classifier for speaker verification based on the fractional Brownian motion process", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "737f5a1b-8479-4179-862a-9edcb0091ba9"}
{"authors": ["Roberta Caccialupi", "Licia Calvi", "Maria Cassella", "Georgia Conte"], "n_citation": 50, "title": "Usability evaluation of a multimedia archive: B@bele", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "739aad75-cf82-4bdc-b188-e8bafa130eda"}
{"abstract": "This paper elaborates on a solution to represent authorization and delegation in a graphical way, allowing users to better interpret delegation relationships. We make use of Weighted Trust Graph (WTG) as an instrument to represent delegation and authorization, extending it to cope with more complicated concepts, and providing a graphical representation of the level of confidence that exists between two entities regarding a resource or attribute. We represent the level of confidence for each pair of entities as a point in an axis diagram, as a set of points, or as a set of triangular regions depending on the accuracy we need. Then, we use the same diagram to represent the set of acceptable confidence level, that we call authorization policy set. In this way, a single diagram can be used to decide about authorization, thus providing a powerful tool for systems in which interaction of users is needed.", "authors": ["Isaac Agudo", "Javier Lopez", "Jos\u00e9 Antonio Montenegro"], "n_citation": 0, "title": "Graphical representation of authorization policies for weighted credentials", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "74035c1b-10b1-405e-b212-6650c2acbf37"}
{"abstract": "The multi-modal characteristics of Web image make it possible to unify keywords and visual features for image retrieval in Web context. Most of the existing methods about the integration of these two features focus on the interactive relevance feedback technique, which needs the user's interaction (i.e. a two-step interactive search). In this paper, an approach based on association rule and clustering techniques is proposed to unify keywords and visual features in a different manner, which seamlessly implements the integration within one-step search. The proposed approach considers both Query By Keyword (QBK) mode and Query By Example (QBE) mode and need not the user's interaction. The experiment results show the proposed approach remarkably improve the retrieval performance compared with the pure search only based on keywords or visual features, and achieve a retrieval performance approximate to the two-step interactive search without requiring the user's additional interaction.", "authors": ["Ruhan He", "Hai Jin", "Wenbing Tao", "Aobing Sun"], "n_citation": 0, "title": "Unifying Keywords and Visual Features Within One-Step Search for Web Image Retrieval", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "744fb0d4-c7eb-4a31-bb13-fe334f5483ad"}
{"abstract": "A job shop scheduling problem with fuzzy due dates is discussed. The membership function of a fuzzy due date assigned to each job denotes the degree of satisfaction of a decision maker for the completion time of this job. The performance criterion of proposed problem is to maximize the minimum degree of satisfaction over given jobs, and it is an NP-complete problem. Thus artificial neural network is considered to search optimal jobs schedule.", "authors": ["Yuan Xie", "Jianying Xie", "Jie Li"], "n_citation": 50, "title": "Fuzzy due dates job shop scheduling problem based on neural network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "74653f39-97dd-4525-b26a-35a69be275c1"}
{"abstract": "We present a method for succinctly structuring neural networks having a few thousands weights. Here structuring means weight sharing where weights in a network are divided into clusters and weights within the same cluster are constrained to have the same value. Our method employs a newly developed weight sharing technique called bidirectional clustering of weights (BCW), together with second-order optimal criteria for both cluster merge and split. Our experiments using two artificial data sets showed that the BCW method works well to find a succinct network structure from an original network having about two thousands weights in both regression and classification problems.", "authors": ["Kazumi Saito", "Ryohei Nakano"], "n_citation": 0, "title": "Structuring neural networks through bidirectional clustering of weights", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "74d3e2af-edfa-40ce-b559-cbd64a6c42a2"}
{"abstract": "Geographical routing protocols have several desirable features for use in ad hoc and sensor networks but are susceptible to voids and localization errors. Virtual coordinate systems which overlay a coordinate system on the nodes offer an alternative that is more resilient to localization errors. However, we show that it is vulnerable to different forms of the void problem where packets reach nodes with no viable next hop in the forwarding set. In addition, it is possible for nodes with the same coordinates to arise at different points in the network in the presence of voids. This paper identifies and analyzes these problems. It also compares several existing routing protocols based on Virtual Coordinate systems. Finally, we propose a routing algorithm that uses geographic routing in the greedy phase and virtual coordinates with backtracking to overcome voids and achieve high connectivity in the greedy phase with higher overall path quality and more resilience to localization errors. We demonstrate these properties using extensive simulation studies.", "authors": ["Ke Liu", "Nael B. Abu-Ghazaleh"], "n_citation": 0, "title": "Virtual coordinates with backtracking for void traversal in geographic routing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "74f58e72-e477-4cc3-bfcb-e8d6a8c7cd38"}
{"abstract": "A kernel orthogonal subspace projection (KOSP) algorithm has been developed for nonlinear approximating subpixel proportion in this paper. The algorithm applies linear regressive model to the feature space induced by a Mercer kernel, and can therefore be used to recursively construct the minimum mean squared-error regressor. The algorithm includes two steps: the first step is to select the feature vectors by defining a global criterion to characterize the image data structure in the feature space; and the second step is the projection onto the feature vectors and then apply the classical linear regressive algorithm. Experiments using synthetic data degraded by an AVIRIS image have been carried out, and the results demonstrate that the proposed method can provide excellent proportion estimation for hyperspectral images. Comparison with support vector regression (SVR) and radial basis function neutral network (RBF) had also been given, and the experiments show that the proposed algorithm slightly outperform than RBF and SVR.", "authors": ["Bo Wu", "Liangpei Zhang", "Pingxiang Li", "Jinmu Zhang"], "n_citation": 50, "title": "Nonlinear Estimation of Hyperspectral Mixture Pixel Proportion Based on Kernel Orthogonal Subspace Projection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7595d85e-68bc-42bc-94cb-b625b6a826d6"}
{"abstract": "Traditional approaches for extractive summarization score/classify sentences based on features such as position in the text, word frequency and cue phrases. These features tend to produce satisfactory summaries, but have the inconvenience of being domain dependent. In this paper, we propose to tackle this problem representing the sentences by word sequences (n-grams), a widely used representation in text categorization. The experiments demonstrated that this simple representation not only diminishes the domain and language dependency but also enhances the summarization performance.", "authors": ["Esa\u00fa Villatoro-Tello", "Luis Villase\u00f1or-Pineda", "Manuel Montes-y-G\u00f3mez"], "n_citation": 50, "title": "Using word sequences for text summarization", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "75a1f9d7-7c5f-437c-a4f2-7e9ecb2dedb5"}
{"abstract": "In this paper we present a dependency analyser able to compute syntax recognition and analysis according to dependency grammars. The analyser is able to deal with nonprojective constructions, it has means to express the level of word-order freedom and its limitations. The level of word-order freedom and the level of robustness (correctness) of sentences can be specified as parameters of the analysis. Data specification language and grammar definition language are also presented.", "authors": ["Tom\u00e1\u0161 Holan"], "n_citation": 50, "title": "Dependency analyser configurable by measures", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "75dde60e-5c4b-413e-b7c3-58bf40b95540"}
{"abstract": "During the boosting of networking multimedia applications in recent years, secure transmission of video streams becomes highly demanded by many hot applications, such as confidential video conference and pay-TV. In this paper, we present a quality-controllable encryption method for H.264 coded video streams. Our goal has been to provide an efficient way to scramble the video streams to prevent illegal users from plagiarizing. By making use of the property of H.264 specification that Intra coded blocks are divided into three different types with different sizes, our algorithm provides the flexibility of scrambling the video up to certain level, which may be manually specified by the user or automatically determined by the system according to the networking traffic condition. Our design ensures that even the deepest scrambling level adds trivial performance overhead to the standard H.264 encoding/decoding process.", "authors": ["Guang-Ming Hong", "Chun Yuan", "Yi Wang", "Yuzhuo Zhong"], "n_citation": 50, "title": "A Quality-Controllable Encryption for H.264/AVC Video Coding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7626e011-c990-4b30-9e4c-b2bed6c69d06"}
{"abstract": "Support vector machines are currently very popular approaches to supervised learning. Unfortunately, the computational load for training and classification procedures increases drastically with size of the training data set. In this paper, a method called cooperative clustering is proposed. With this procedure, the set of data points with pre-determined size near the border of two classes is determined. This small set of data points is taken as the set of support vectors. The training of support vector machine is performed on this set of data points. With this approach, training efficiency and classification efficiency are achieved with small effects on generalization performance. This approach can also be used to reduce the number of support vectors in regression problems.", "authors": ["Shengfeng Tian", "Shaomin Mu", "Chuanhuan Yin"], "n_citation": 50, "title": "Cooperative Clustering for Training SVMs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "76a414d4-55da-4ab0-beb2-8f9b930be98c"}
{"abstract": "Random instances are widely used as benchmarks in evaluating algorithms for finite-domain constraint satisfaction problems (CSPs). We present an analysis that shows why deciding satisfiability of instances from some distributions is challenging for current complete methods. For a typical random CSP model, we show that when constraints are not too tight almost all unsatisfiable instances have a structural property which guarantees that unsatisfiability proofs in a certain resolution-like system must be of exponential size. This proof system can efficiently simulate the reasoning of a large class of CSP algorithms which will thus have exponential running time on these instances.", "authors": ["David G. Mitchell"], "n_citation": 50, "title": "Resolution complexity of random constraints", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "76b7addb-fc95-4620-9da3-720eafc11147"}
{"abstract": "We present a capability calculus for checking partial confluence of channel-communicating concurrent processes. Our approach automatically detects more programs to be partially confluent than previous approaches and is able to handle a mix of different kinds of communication channels, including shared reference cells.", "authors": ["Tachio Terauchi", "Alex Aiken"], "n_citation": 50, "title": "A capability calculus for concurrency and determinism", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "76c1de86-b016-46e6-9db4-e1d5714bf4cb"}
{"abstract": "MAG is a synchronous stream cipher submitted to the E-CRYPT eSTREAM project. The design criterion for the cipher is cellular automata, although it can be modelled as a word-based shift-register with a single word of memory. Cryptanalysis of MAG reveals serious structural weaknesses within the cipher. This paper presents simple distinguishing attacks against MAG with an 80-bit or 128-bit key that can, under certain circumstances, be carried out by hand. The approach is extended to a partial-key recovery attack. For the 80-bit and 128-bit keys, we recover 40 key bits and 32 key bits respectively from about 32 bytes keystream. A proposed modification to MAG, intended to prevent an earlier distinguishing attack, has no effect upon our distinguisher but instead allows a full key recovery attack for both 80-bit and 128-bit keys using around thirty-two bytes of keystream and a practical precomputation. Therefore the modification actually weakens an already insecure cipher.", "authors": ["Leonie Simpson", "Matthew Henricksen"], "n_citation": 0, "title": "Improved cryptanalysis of MAG", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "777d5e46-0472-40ca-9333-5bc34d3ba89e"}
{"abstract": "We present a simple Data Mining Logic (DML) that can express common data mining tasks, like Find Boolean association rules or Find inclusion dependencies. At the center of the paper is the problem of characterizing DML queries that are amenable to the levelwise search strategy used in the a-priori algorithm. We relate the problem to that of characterizing monotone first-order properties for finite models.", "authors": ["Toon Calders", "Jef Wijsen"], "n_citation": 0, "title": "On monotone Data Mining languages", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "77ab4985-d7d0-4d00-b4e5-4fffbf4d17aa"}
{"abstract": "This paper is focused on improving visual Czech speech synthesis. Our aim was the design of a highly natural and realistic talking head with a realistic 3D face model, improved co-articulation, and a realistic model of inner articulatory organs (teeth, the tongue and the palate). Besides very good articulation our aim was also expression of the mimic and emotions of the talking head. The intelligibility was verified by the listening test and the results of this test were analysed. Firstly, the face model reconstruction from real data is presented. A 3D computer vision was employed in order to obtain a model of an arbitrary face. The stereovision technique that is used to reconstruct the model is described in detail in Section 2. Details concerning a visual speech synthesis are discussed in Section 3. Special features of the visual speech synthesis of the Czech language are mentioned, too. Furthermore, the design of a talking head including the solution of co-articulation problem is presented. Next, the modeling of expression and emotions in animation is described. The last part of the paper, Section 4, contains results of the performed listening test.", "authors": ["Zdenek Krnoul", "Milos Zelezny"], "n_citation": 50, "title": "Realistic face animation for a Czech talking head", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "77c5fe26-c92a-4f94-9476-23d87132ad24"}
{"abstract": "In this work we show how our intonation corpus driven intonation modelling methodology MEMOInt can help in the graphical visualization of the complex relationships between the different prosodic features which configure the intonational aspects of natural speech. MEMOInt has already been used successfully for the prediction of synthetic F0 contours in the presence of the usual data scarcity problems. Now, we report on the possibilities of using the information gathered in the modelling phase in order to provide a graphical view of the relevance of the various prosodic features which affect the typical F0 movements. The set of classes which group the intonation patterns found in the corpus can be structured in a tree in which the relation between the classes and the prosodic features of the input text is hierarchically correlated. This visual outcome shows to be very useful to carry out comparative linguistic studies of prosodic phenomena and to check the correspondence between previous prosodic knowledge on a language and the real utterances found in a given corpus.", "authors": ["David Escudero-Mancebo", "Valent\u00edn Carde\u00f1oso-Payo"], "n_citation": 0, "title": "Visualization of prosodic knowledge using corpus driven MEMOInt intonation modelling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "77db5a69-99d0-436f-9336-afecb9a4712c"}
{"abstract": "Sequence compression is one of the most promising tools for strengthening pseudo-random generators used in stream ciphers. Indeed, adding compression components can thwart algebraic attacks aimed at LFSR-based stream ciphers. Among such components are the Shrinking Generator and the Self-Shrinking Generator, as well as recent variations on Bit-Search-based decimation. We propose a general model for compression used to strengthen pseudo-random sequences. We show that there is a unique (up to length-preserving permutations) construction that reaches an optimal trade-off between output rate and security against several attacks.", "authors": ["Aline Gouget", "Herv\u00e9 Sibert"], "n_citation": 0, "title": "How to strengthen pseudo-random generators by using compression", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7807f38f-ee16-4252-bda1-22e9356ea94e"}
{"abstract": "Despite the recent surge of research in query processing over data streams, little attention has been devoted to defining precise semantics for continuous queries over streams. We first present an abstract semantics based on several building blocks: formal definitions for streams and relations, mappings among them, and any relational query language. From these basics we define a precise interpretation for continuous queries over streams and relations. We then propose a concrete language, CQL (for Continuous Query Language), which instantiates the abstract semantics using SQL as the relational query language and window specifications derived from SQL-99 to map from streams to relations. We have implemented most of the CQL language in a Data Stream Management System at Stanford, and we have developed a public repository of data stream applications that includes a wide variety of queries expressed in CQL.", "authors": ["Arvind Arasu", "Shivnath Babu", "Jennifer Widom"], "n_citation": 0, "title": "CQL: A language for Continuous queries over streams and relations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7844c301-7f59-40d0-b8e5-3d254d7ce6ab"}
{"abstract": "Bluetooth is a widespread solution for personal area networks (PAN). However, it currently lacks in spontaneous and energy efficient device communication which could make it more successful. To allow spontaneous networking, communication nodes should have the ability to timely discover each other. Moreover, in order to improve energy efficiency and increase battery lifetime the energy spent during discovery procedures should be minimized while guaranteeing high probability of successfully completing the discovery and the information transfer procedure. In this paper we derive performance considerations on the behavior of the system through experiments in a physical testbed where devices move while discovering nodes in their neighborhood and then exchange information. These considerations can be used for accurately model neighbor discovery process and information transfer in Bluetooth networks.", "authors": ["Laura Galluccio", "Alessandro Leonardi", "Antonio Matera"], "n_citation": 0, "title": "Improving energy efficiency and responsiveness in bluetooth networks : A performance assessment", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "78b99e2f-d034-4d1d-8ed2-125a7f71d633"}
{"abstract": "We provide two new construction methods for nonlinear resilient functions. The first method is a simple modification of an elegant construction due to Zhang and Zheng and constructs n-input, m-output resilient S-boxes with degree d > m. We prove by an application of the Griesmer bound for linear error correcting codes that the modified Zhang-Zheng construction is superior to the previous method of Cheon in Crypto 2001. Our second construction uses a sharpened version of the Maiorana-McFarland technique to construct nonlinear resilient functions. The nonlinearity obtained by our second construction is better than previously known construction methods.", "authors": ["Kishan Chand Gupta", "Palash Sarkar"], "n_citation": 0, "title": "Improved construction of nonlinear resilient S-boxes", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "79200f56-cec8-48c7-801d-e350a74f7a53"}
{"abstract": "In this paper, a node pruning algorithm based on optimal brain surgeon is proposed for feedforward neural networks. First, the neural network is trained to an acceptable solution using the standard training algorithm. After the training process, the orthogonal factorization is applied to the output of the nodes in the same hidden layer to identify and prune the dependant nodes. Then, a unit-based optimal brain surgeon(UB-OBS) pruning algorithm is proposed to prune the insensitive hidden units to further reduce the size of the neural network, and no retraining is needed. Simulations are presented to demonstrate the effectiveness of the proposed approach.", "authors": ["Jinhua Xu", "Daniel W. C. Ho"], "n_citation": 50, "title": "A Node Pruning Algorithm Based on Optimal Brain Surgeon for Feedforward Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "796d1935-1db4-4e9d-9a30-9e283cc5c588"}
{"abstract": "The multi-view video is a collection of multiple videos, capturing the same scene at different viewpoints. If we acquire multi-view videos from multiple cameras, it is possible to generate scenes at arbitrary view positions. It means that users can change their viewpoints freely and can feel visible depth with view interaction. Therefore, the multi-view video can be used in a variety of applications including three-dimensional TV (3DTV), free viewpoint TV, and immersive broadcasting. However, since the data size of the multi-view video linearly increases as the number of cameras, it is necessary to develop an effective framework to represent, process, and display multi-view video data. In this paper, we propose inter-camera coding methods of multi-view video using layered depth image (LDI) representation. The proposed methods represents various information included in multi-view video hierarchically based on LDI. In addition, we reduce a large amount of multi-view video data to a manageable size by exploiting spatial redundancies among multiple videos and reconstruct the original multiple viewpoints successfully from the constructed LDI.", "authors": ["Seung-Uk Yoon", "Eun-Kyung Lee", "Sung-Yeol Kim", "Yo-Sung Ho", "Kugjin Yun", "Sukhee Cho", "Namho Hur"], "n_citation": 50, "title": "Inter-camera Coding of Multi-view Video Using Layered Depth Image Representation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7989c5ce-c843-490c-8d99-915e46d74282"}
{"abstract": "We introduce the first El Gamal based mix-net in which each mix-server partially decrypts and permutes its input, i.e., no re-encryption is necessary. An interesting property of the construction is that a sender can verify non-interactively that its message is processed correctly. We call this sender verifiability. The mix-net is provably UC-secure against static adversaries corrupting any minority of the mix-servers. The result holds under the decision Diffie-Hellman assumption, and assuming an ideal bulletin board and an ideal zero-knowledge proof of knowledge of a correct shuffle. Then we construct the first proof of a decryption-permutation shuffle, and show how this can be transformed into a zero-knowledge proof of knowledge in the UC-framework. The protocol is sound under the strong RSA-assumption and the discrete logarithm assumption. Our proof of a shuffle is not a variation of existing methods. It is based on a novel idea of independent interest, and we argue that it is at least as efficient as previous constructions.", "authors": ["Douglas Wikstr\u00f6m"], "n_citation": 0, "title": "A sender verifiable mix-net and a new proof of a shuffle", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "79e891eb-b1d9-47cb-a9f2-a2c5c70d7842"}
{"abstract": "This paper follows the research direction that has received a growing interest recently, namely application of knowledge discovery methods to complex data representations. Among others, there have been methods proposed for learning in expressive, hybrid languages, combining relational component with terminological (description logics) component. In this paper we present a novel approach to frequent pattern discovery over the knowledge base represented in such a language, the combination of the basic subset of description logics with D\u00a3-safe rules, that can be seen as a subset of Semantic Web Rule Language. Frequent patterns in our approach are represented as conjunctive D\u00a3-safe queries over the hybrid knowledge base. We present also an illustrative example of our method based on the financial dataset.", "authors": ["Joanna J\u00f3zefowska", "Agnieszka Lawrynowicz", "Tomasz Lukaszewski"], "n_citation": 0, "title": "Towards discovery of frequent patterns in description logics with rules", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7a223290-1891-4123-a50e-a31bdfcd2eef"}
{"abstract": "Aligned parallel corpora are very important linguistic resources useful in many text processing tasks such as machine translation, word sense disambiguation, dictionary compilation, etc. Nevertheless, there are few available linguistic resources of this type, especially for fiction texts, due to the difficulties in collecting the texts and high cost of manual alignment. In this paper, we describe an automatically aligned English-Spanish parallel corpus of fiction texts and evaluate our method of alignment that uses linguistic data-namely, on the usage of existing bilingual dictionaries-to calculate word similarity. The method is based on the simple idea: if a meaningful word is present in the source text then one of its dictionary translations should be present in the target text. Experimental results of alignment at paragraph level are described.", "authors": ["Alexander F. Gelbukh", "Grigori Sidorov", "Jos\u00e9 \u00c1ngel Vera-F\u00e9lix"], "n_citation": 50, "title": "Paragraph-level alignment of an english-spanish parallel corpus of fiction texts using bilingual dictionaries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7a2e4c50-0a8d-45aa-b565-872e857ae672"}
{"abstract": "In recent work, a general framework for specifying correspondences between logic programs under the answer-set semantics has been defined. The framework captures different notions of equivalence, including well-known ones like ordinary, strong, and uniform equivalence, as well as refined ones based on the projection of answer sets where not all parts of an answer set are of relevance. In this paper, we describe an implementation to verify program correspondences in this general framework. The system, called ccT, relies on linear-time constructible reductions to quantified propositional logic and uses extant solvers for the latter language as back-end inference engines.", "authors": ["Johannes Oetsch", "Martina Seidl", "Hans Tompits", "Stefan Woltran"], "n_citation": 0, "title": "ccT : A correspondence-checking tool for logic programs under the answer-set semantics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7a560299-ddf5-49ff-9c41-81d72657ec82"}
{"authors": ["Mario Lamberger", "Florian Mendel", "Christian Rechberger", "Vincent Rijmen", "Martin Schlaeffer"], "n_citation": 0, "title": "Rebound Distinguishers: Results on the Full Whirlpool Compression Function", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "7a592a92-c838-462f-a89e-2c57f342493f"}
{"abstract": "The aim of this paper is to demonstrate that CP could be a better candidate than MIP for solving the master problem within a Benders decomposition approach. Our demonstration is based on a case study of a workforce scheduling problem encountered in a large call center of Bouygues Telecom, a French mobile phone operator. Our experiments show that CP can advantageously replace MIP for the implementation of the master problem due to its greater ability to efficiently manage a wide variety of constraints such as the ones occurring in time tabling applications.", "authors": ["Thierry Benoist", "Etienne Gaudin", "Beno\u00eet Rottembourg"], "n_citation": 50, "title": "Constraint programming contribution to benders decomposition: A case study", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7b462f24-4971-4693-a0c8-8da4b3c93bd8"}
{"abstract": "Unreliable speech recognition, especially in noisy environments and the need for more natural interaction between man and machine have motivated the development of multimodal systems using speech, pointing, gaze, and facial expressions. In this paper we present a new approach to fuse multimodal information streams using agents. A general framework based on this approach that allows for rapid application development is described. Since anaphora very often occur in natural discourse a special agent for anaphora resolution was developed within this framework.", "authors": ["L\u00e9on J. M. Rothkrantz", "Pascal Wiggers", "Frans Flippo", "Dimitri Woei-A-Jin", "Robert J. van Vark"], "n_citation": 0, "title": "Multimodal dialogue management", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "7b6131a7-6a99-4d67-9f7d-3e8b964b89c0"}
{"abstract": "This paper describes a novel fast correlation attack of stream ciphers. The salient feature of the algorithm is the absence of any pre-processing or iterative phase, an usual feature of existing fast correlation attacks. The algorithm attempts to identify a number of bits of the original linear feedback shift register (LFSR) output from the received bits of the ciphertext. These are then used to construct a system of linear equations which are subsequently solved to obtain the initial conditions. The algorithm is found to perform well for LFSRs of large sizes but having sparse polynomials. It may be noted that such polynomials have low Hamming weight which is one more than the number of feedback connections or taps of the corresponding LFSR. Its performance is good in situations even where limited cipherlength is available. Another important contribution of the paper is a modification of the approach when the LFSR outputs are combined by a function which is correlation immune and perhaps, unknown to the decrypter.", "authors": ["Sarbani Palit", "Bimal K. Roy", "Arindom De"], "n_citation": 0, "title": "A fast correlation attack for LFSR-based stream ciphers", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7bea822d-db17-4a7c-bd8a-ccd4ec6d3990"}
{"abstract": "We propose a logical framework for describing, reasoning about, and simulating transaction models that relax some of the ACID (Atomicity-Consistency-Isolation-Durability) properties of classical transactions. Such extensions, usually called advanced transaction models (ATMs), have been proposed for dealing with new database applications involving long-lived, endless, and cooperative activities. Our approach appeals to non-Markovian theories, in which one may refer to past states other than the previous one. We specify an ATM as a suitable non-Markovian theory of the situation calculus, and its properties, including the relaxed ACID properties, as formulas of the same calculus. We use our framework to formalize classical and closed nested transactions. We first formulate each ATM and its properties as a theory of a certain kind and formulas of the situation calculus, respectively. We then define a legal database log as one whose actions are all possible and in which all the Commit and Rollback actions must occur whenever they are possible. After that, we show that the known properties of the ATM, including the (possibly relaxed) ACID constraints, are properties of legal logs and logical consequences of the theory corresponding to that ATM. Finally, we show how to use such a specification as a background theory for transaction programs written in the situation calculus based programming language GOLOG.", "authors": ["Iluju Kiringa"], "n_citation": 0, "title": "Simulation of advanced transaction models using GOLOG", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7ca57873-b0af-4ef2-9f3c-ee368bafda74"}
{"abstract": "A rudimentary approach to mitigate interference issues in license-exempt 802.16 systems is presented. This approach operates by permitting each Base Station (BS), and associated Subscriber Stations (SSs) to remain inactive for a specified fraction of the time. Other systems can then transmit with a reduced likelihood of interference. A simulator was developed to determine how this system performs. The results show that the throughput of the system is very sensitive to the fraction of time each BS is active; the system throughput is maximised when each BS is active less than 40% of the time for the scenarios studied. The results demonstrate a discrepancy between uplink and downlink throughput which can be attributed to the greater amount of overheads in the uplink. Finally, the results show that broadcast information being transmitted periodically at full power has a significant detrimental impact on the system.", "authors": ["Omar Ashagi", "Sean Murphy", "Liam Murphy"], "n_citation": 0, "title": "Mitigating interference between IEEE 802.16 systems operating in license-exempt mode", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7d281d0d-0609-458d-a974-7fbf59d353d8"}
{"abstract": "We present a recursive total least squares (RTLS) algorithm for multilayer feedforward neural networks. So far, recursive least squares (RLS) has been successfully applied to training multilayer feedforward neural networks. If the input data contains additive noise, the results from RLS could be biased. Such biased results can be avoided by using the RTLS algorithm. The RTLS algorithm described in this paper performs better than RLS algorithm over a wide range of SNRs and involves approximately the same computational complexity of O(N 2 ) as the RLS algorithm.", "authors": ["Nakjin Choi", "Jun-Seok Lim", "Koeng-Mo Sung"], "n_citation": 0, "title": "An efficient recursive total least squares algorithm for training multilayer feedforward neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7d406ea5-3a4c-4881-aaf0-da129202fedf"}
{"abstract": "This paper addresses how to use public-keys of several different signature schemes to generate 1-out-of-n signatures. Previously known constructions are for either RSA-keys only or DL-type keys only. We present a widely applicable method to construct a 1-out-of-n signature scheme that allows mixture use of different flavors of keys at the same time. The resulting scheme is more efficient than previous schemes even if it is used only with a single type of keys. With all DL-type keys, it yields shorter signatures than the ones of the previously known scheme based on the witness indistinguishable proofs by Cramer, et al. With all RSA-type keys, it reduces both computational and storage costs compared to that of the Ring signatures by Rivest, et al.", "authors": ["Masayuki Abe", "Miyako Ohkubo", "Koutarou Suzuki"], "n_citation": 0, "title": "1-Out-of-n signatures from a variety of keys", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7db001c2-f681-46c3-97d9-9f558362e3a9"}
{"abstract": "The Hierarchical Layer Graph (HL graph) is a promising network topology for wireless networks with variable transmission ranges. It was introduced and analyzed by Meyer auf der Heide et al. 2004. In this paper we present a distributed, localized and resource-efficient algorithm for constructing this graph. The qualtiy of the HL graph depends on the domination radius and the publication radius, which affect the amount of interference in the network. These parameters also determine whether the HL graph is a c-spanner, which implies an energy-efficient topology. We investigate the performance on randomly distributed node sets and show that the restrictions on these parameters derived from a worst case analysis are not so tight using realistic settings. Here, we present the results of our extensive experimental evaluation, measuring congestion, dilation and energy. Congestion includes the load that is induced by interfering edges. We distinguish between congestion and realistic congestion where we also take the signal-to-interference ratio into account. Our experiments show that the HL graph contains energy-efficient paths as well as paths with a few number of hops while preserving a low congestion.", "authors": ["Stefan R\u00fchrup", "Christian Schindelhauer", "Klaus Volbert"], "n_citation": 0, "title": "Performance analysis of the hierarchical layer graph for wireless networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7dc25312-66d1-4d95-873b-6110100d6a90"}
{"abstract": "In this work we investigate bounds on throughput and delay performance of a scheduling protocol that derives its decisions from codes traditionally used to correct or detect errors in the information carried over a noisy channel. In this paper we study the particular cases in which the Reed-Solomon and Hermitian code constructions are used. It is found that Hermitian codes outperform Reed-Solomon codes in minimum throughput guarantee and delay metrics when the number of nodes is in the order of thousands. The relative minimum distance of the code used to schedule the transmissions is identified as an important property that can be used to identify codes that can enable scheduling patterns with better minimum performance guarantees. Furthermore, the terminology of error control coding is used to present a more general and constructive framework for the study of code-based scheduling protocols.", "authors": ["Carlos H. Rentel", "Thomas Kunz"], "n_citation": 50, "title": "Reed-solomon and hermitian code-based scheduling protocols for wireless ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7df2c869-c446-482f-9ab5-a3e223928670"}
{"abstract": "This paper presents a new approach method to recognize facial expressions in various internal states using independent component analysis (ICA). We developed a representation of facial expression images based on independent component analysis for feature extraction of facial expressions. This representation consists of two steps. In the first step, we present a representation based on principal component analysis (PCA) excluded the first 2 principal components to reflect well the changes in facial expressions. Second, ICA representation from this PCA representation was developed. Finally, classification of facial expressions in various internal states was created on two dimensional structure of emotion with pleasure/displeasure dimension and arousal/sleep dimension. The proposed algorithm demonstrates the ability to discriminate the changes of facial expressions in various internal states. This system is possible to use in cognitive processes, social interaction and behavioral investigations of emotion.", "authors": ["Young-Suk Shin"], "n_citation": 0, "title": "Facial expression recognition in various internal states using independent component analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7e55fb8e-f54d-48de-a3df-c0bd03b4fa03"}
{"abstract": "Many important combinatorial optimization problems can be expressed as constraint satisfaction problems with soft constraints. When problems are too difficult to be solved exactly, approximation methods become the best option. Mini-bucket Elimination (MBE) is a well known approximation method for combinatorial optimization problems. It has a control parameter z that allow us to trade time and space for accuracy. In practice, it is the space and not the time that limits the execution with high values of z. In this paper we introduce a new propagation phase that MBE should execute at each bucket. The purpose of this propagation is to jointly process as much information as possible. As a consequence, the undesirable lose of accuracy caused by MBE when splitting functions into different mini-buckets is minimized. We demonstrate our approach in scheduling, combinatorial auction and max-clique problems, where the resulting algorithm MBE p  gives important percentage increments of the lower bound (typically 50% and up to 1566%) with only doubling the cpu time.", "authors": ["Emma Rollon", "Javier Larrosa"], "n_citation": 0, "title": "Mini-bucket elimination with bucket propagation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7f2049cc-6962-4f4b-8942-b2687606a98e"}
{"abstract": "Here we present an efficient implementation strategy and some general design criteria for the standard nonlinear combiner model. This model combines the output sequences of several independent Linear Feedback Shift Registers (LFSRs) using a Boolean function to produce the running key sequence. The model is well studied and a standard target for many cryptanalytic attacks. The naive bitwise software implementation of the LFSRs is not efficient. In this paper we explore an efficient block oriented software implementation technique to make it competitive with the recently proposed fast stream ciphers. Our proposed specifications on this model can resist the fast correlation attacks. To evaluate our design criteria and implementation techniques, we carry out the security and performance analysis considering a specific scheme based on this model.", "authors": ["Sandeepan Chowdhury", "Subhamoy Maitra"], "n_citation": 0, "title": "Efficient software implementation of lfsr and Boolean function and its application in nonlinear combiner model", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "7f5da28b-41df-491d-b0bd-5e83e431a99c"}
{"abstract": "In this work, we studied the dynamics of modified FitzHugh-Nagumo (MFHN) neuron model. This model shows how the potential difference between spine head and its surrounding medium vacillates between a relatively constant period called the silent phase and large scale oscillation reffered to as the active phase or bursting. We investigated bifurcation in the dynamics of two MFHN neurons coupled to each other through an electrical coupling. It is found that the variation in coupling strength between the neurons leads to different types of bifurcations and the system exhibits the existence of fixed point, periodic and chaotic attractor.", "authors": ["Deepak Mishra", "Abhishek Yadav", "Sudipta Ray", "Prem Kumar Kalra"], "n_citation": 0, "title": "Nonlinear dynamical analysis on coupled modified fitzhugh-nagumo neuron model", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "7fe21fde-49f2-4319-87a0-b1e204b1adf9"}
{"abstract": "The issue of value invention in logic programming embraces many scenarios, such as logic programming with function symbols, object oriented logic languages, inter-operability with external sources of knowledge, set unification. This paper introduces a framework embedding value invention in a general context. The class of programs having a suitable (but, in general, not decidable) 'finite grounding property' is identified, and the class of 'value invention restricted' programs is introduced. Value invention restricted programs have the finite grounding property and can be decided in polynomial time. They are, in a sense, the broadest polynomially decidable class having this property, whenever no assumption can be made about the nature of invented values (while this latter is the case in the specific literature about logic programming with function symbols). Relationships with existing formalisms are eventually discussed; in particular, value invention restricted programs subsume \u03c9-restricted programs and are incomparable with finitary programs.", "authors": ["Francesco Calimeri", "Susanna Cozza", "Giovambattista Ianni"], "n_citation": 0, "title": "Decidable fragments of logic programming with value invention", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "803928a8-69d0-4e08-a666-c86912284b11"}
{"abstract": "The 3rd Generation Partnership Project (3GPP) has proposed the IP Multimedia Subsystem (IMS) as a key element in the next-generation network (NGN) converged architecture supporting multimedia services. Extending the IMS towards provisioning support for location based services (LBS) will enable enhanced services and offer new revenues to the system. However, conveying location information in the IMS and connecting the IMS with a real positioning system are still open issues. This paper presents the design and implementation of an IMS Location Server (ILS) integrating IMS with a positioning system. From the IMS perspective, the ILS serves as a service enabler for LBS. Considerable work has been done by the IETF in the area of location information transport based on the Session Initiation Protocol (SIP). This paper proposes some improvements in this area. In order to demonstrate proof-of-concept in enhancing IMS-based services, a Location-aware Push-to-Talk (LaPoC) prototype service has been developed. The service has been integrated and tested with the Ericsson Mobile Positioning System (MPS). The paper also gives the results of performance measurements including traffic load analysis and session establishment time.", "authors": ["Miran Mosmondor", "Lea Skorin-Kapov", "Renato Filjar"], "n_citation": 50, "title": "Location conveyance in the IP multimedia subsystem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8083de5c-ba89-4379-8348-8bc8352a80e7"}
{"abstract": "By utilizing the Lyapunov function method to analyze stability of discrete time Hopfield neural networks with delays and obtain some new sufficient conditions for the global exponential stability of the equilibrium point for such networks. It is shown that the proposed conditions rely on the connection matrices and network parameters. The presented conditions are testable and less conservative than some given in the earlier references.", "authors": ["Qiang Zhang", "Wenbing Liu", "Xiaopeng Wei"], "n_citation": 0, "title": "Global exponential stability of discrete time hopfield neural networks with delays", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "81145e1b-8690-4f73-809a-53f4f3da71a4"}
{"abstract": "At Delft University of Technology there is a project running on multimodal interfaces on the interaction of speech and lipreading. A large vocabulary speaker independent speech recognizer for the Dutch language was developed using Hidden Markov Toolkit and the Polyphone database of recorded Dutch speech. To make the system more noise robust audio cues provided by an automatic lip-reading technique were integrated in the system. In this paper we give an outline of both systems and present results of experiments.", "authors": ["Pascal Wiggers", "L\u00e9on J. M. Rothkrantz"], "n_citation": 0, "title": "Integration of speech recognition and automatic lip-reading", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "815290d2-66a9-4a53-aa5b-47ab3def4782"}
{"abstract": "Dynamic neural networks with different time-scales include the aspects of fast and slow phenomenons. Some applications require that the equilibrium points of the designed network be stable. In this paper, the passivity-based approach is used to derive stability conditions for dynamic neural networks with different time-scales. Several stability properties, such as passivity, asymptotic stability, input-to-state stability and bounded input bounded output stability, are guaranteed in certain senses. Numerical examples are also given to demonstrate the effectiveness of the theoretical results.", "authors": ["Alejandro Cruz Sandoval", "Wen Yu"], "n_citation": 0, "title": "Passivity Analysis of Dynamic Neural Networks with Different Time-Scales", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "81dd31d6-c911-4714-9985-2e75e33c110e"}
{"abstract": "In this paper, we integrate orthogonal frequency-division multiplexing (OFDM) technique with vertical Bell Labs layered space-time (V-BLAST) architecture as a promising solution for enhancing the data rates of wireless communication systems, and propose a new blind deconvolution method. A two-stage algorithm is developed to estimate the channel parameters. At first stage, we propose an algorithm based on the second order statistics to decorrelate the sensor signals. After decorrelation, we apply instantaneous demixing algorithm to separate the signals at the second stage. Simulation results demonstrate the validity and the performance of the proposed algorithms.", "authors": ["Feng Jiang", "Liqing Zhang", "Bin Xia"], "n_citation": 0, "title": "Two-Stage Blind Deconvolution for V-BLAST OFDM System", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "81e0de43-0b6e-4d7a-804b-f466a17772e8"}
{"abstract": "While in the sequential world the programmer can concentrate on the algorithmic solution to his given problem, in parallel and distributed systems he also has to consider aspects of communication, synchronization and data movement. In this paper we describe a prototypical middleware solution that enables the clear separation of these aspects. We combine algorithmic skeletons describing the computational aspects with overlapping data distributions describing the communication and synchronization. Both are expressed in a high-level manner. The system automatically coordinates the different activities and allows the programmer to easily change the underlying communication topology.", "authors": ["Thomas Nitsche"], "n_citation": 0, "title": "Coordinating computation with communication", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "81ffa478-4005-445f-9071-44743da783ce"}
{"abstract": "Spurred by advances in cDNA microarray technology, gene expression data are increasingly becoming available. In time-ordered data, the expression levels are measured at several points in time following some experimental manipulation. A gene regulatory network can be inferred by fitting a linear system of differential equations to the gene expression data. As biologically the gene regulatory network is known to be sparse, we expect most coefficients in such a linear system of differential equations to be zero. In previously proposed methods to infer such a linear system, ad hoc assumptions were made to limit the number of nonzero coefficients in the system. Instead, we propose to infer the degree of sparseness of the gene regulatory network from the data, where we determine which coefficients are nonzero by using Akaike's Information Criterion.", "authors": ["Michiel J. L. de Hoon", "Seiya Imoto", "Satoru Miyano"], "n_citation": 0, "title": "Inferring gene regulatory networks from time-ordered gene expression data using differential equations", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8211e6ac-92ae-4c71-91b1-888799d16562"}
{"abstract": "Web Ontology Language (OWL) is a semantic markup language for describing information on the Web so that machines can process and interpret Web content. OWL expresses information ontologies and has the capability to map all the information on the World Wide Web into a semantic, machine-understandable atlas of information. This article addresses personal information modeling in OWL through enhancing the OWL specification to include such information. It introduces a scheme for identifying personal information and presents an OWL-based design to model personal information without introducing modifications. Some restrictions on certain OWL constructs are necessary to integrate personal information in the language.", "authors": ["Sabah Al-Fedaghi", "Majed Y. Ahmad"], "n_citation": 0, "title": "Personal Information Modeling in Semantic Web", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8275860f-51ee-4814-a0de-97e13c0ec5bc"}
{"abstract": "Modern analytics solutions succeed to under- stand and predict phenomenons in a large diversity of software systems, from social networks to Internet-of-Things platforms. This success challenges analytics algorithms to deal with more and more complex data, which can be structured as graphs and evolve over time. However, the underlying data storage systems that support large-scale data analytics, such as time-series or graph databases, fail to accommodate both dimensions, which limits the integration of more advanced analysis taking into account the history of complex graphs, for example. This paper therefore introduces a formal and practical definition of temporal graphs. Temporal graphs provide a compact representation of time-evolving graphs that can be used to analyze complex data in motion. In particular, we demonstrate with our open-source implementation, named GreyCat, that the performance of temporal graphs allows analytics solutions to deal with rapidly evolving large-scale graphs.", "authors": ["Thomas Hartmann", "Francois Fouquet", "Matthieu Jimenez", "Romain Rouvoy", "Yves Le Traon"], "n_citation": 0, "title": "Analyzing Complex Data in Motion at Scale with Temporal Graphs", "venue": "international conference on software engineering", "year": 2017, "id": "82beba29-820f-4f23-a469-5e7a6d36aca7"}
{"abstract": "The Sammon Transform performs data projections in a topology-preserving manner on the basis of an arbitrary distance measure. We use the weights of the observation probabilities of semi-continuous HMMs that were adapted to the current speaker as input. Experiments on laryngectomized speakers with tracheo-esophageal substitute voice, hoarse, and normal speakers show encouraging results. Different speaker groups are separated in 2-D space, and the projection of a new speaker into the Sammon map allows prediction of his or her kind of voice pathology. The method can thus be used as an objective, automated support for the evaluation of voice disorders, and it visualizes them in a way that is convenient for speech therapists.", "authors": ["Tino Haderlein", "Dominik Zorn", "Stefan Steidl", "Elmar N\u00f6th", "Makoto Shozakai", "Maria Schuster"], "n_citation": 0, "title": "Visualization of voice disorders using the sammon transform", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "82e2c83f-f34c-4c9a-be43-4a3588198d48"}
{"abstract": "In this paper, a new approach to structure learning of Bayesian networks (BNs) based on genetic algorithm is proposed. The proposed method explores the wider solution space than the previous method. In the previous method, while the ordering among the nodes of the BNs was fixed their conditional dependencies represented by the connectivity matrix was learned, whereas, in the proposed method, the ordering as well as the conditional dependency among the BN nodes is learned. To implement this method using the genetic algorithm, we represent an individual of the population as a pair of chromosomes: The first one represents the ordering among the BN nodes and the second one represents their conditional dependencies. To implement proposed method new crossover and mutation operations which are closed in the set of the admissible individuals are introduced. Finally, a computer simulation exploits the real-world data and demonstrates the performance of the method.", "authors": ["Jaehun Lee", "Wooyong Chung", "Euntai Kim"], "n_citation": 0, "title": "A New Genetic Approach to Structure Learning of Bayesian Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "838020a1-6c37-4678-ac47-8d84de573460"}
{"abstract": "Multipath is a sought-after mechanism to achieve reliability along the error-prone channel in wireless sensor networks. However, this technique is not energy efficient since sensor networks are subject to strict resource constraints. In this paper, we propose an energy aware method to combine multipath routing with practical network coding technique. Through this method, we can guarantee the same reliability while reduce much energy consumption by decreasing the number of paths needed to delivery data. This method only needs little metadata overhead and some small-scale linear operations. Simulations under different circumstances verify the theoretical results. The paper also discusses other advantages of network coding in multi-receiver case to illustrate our future work.", "authors": ["Shan-Shan Li", "Peidong Zhu", "Xiangke Liao", "Weifang Cheng", "Shaoliang Peng"], "n_citation": 50, "title": "Energy efficient multipath routing using network coding in wireless sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "83ffaf70-895d-4dd1-b35f-dea4afeb971d"}
{"abstract": "In this paper, we propose an improved Poly 1305 MAC, called IPMAC. IPMAC is a refinement of Polyl305 MAC shown by Bernstein. We use only one 16-byte key and one 16-byte nonce for IPMAC while Polyl305 MAC uses two 16-byte keys and one 16-byte nonce, 48-byte in total. The cost of reducing the size of secret keys is almost negligible: only one encryption. Similarly to Poly 1305 MAC, our algorithm correctly and efficiently handles messages of arbitrary bit length.", "authors": ["Dayin Wang", "Dongdai Lin", "Wenling Wu"], "n_citation": 0, "title": "An improved poly1305 MAC", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8450c6e2-041b-4706-9849-e39120a42c16"}
{"authors": ["Thomas Eiter", "Michael Fink", "J\u00e1n Senko"], "n_citation": 50, "title": "A tool for answering queries on action descriptions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "84b25440-84b1-4afd-b264-21f04e6a1aed"}
{"authors": ["Micha\u0142 Chlebiej", "Krzysztof Nowi\u0144ski", "Piotr \u015acis\u0142o", "Piotr Ba\u0142a"], "n_citation": 50, "title": "Reconstruction of Heart Motion from 4D Echocardiographic Images", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "852930c5-9931-4076-bd7c-d2687be2baf2"}
{"abstract": "The paper presents a model for Quality of Service (QoS) signaling for complex multimedia services in the next generation network (NGN), from establishing necessary QoS support at session setup, to further QoS modifications in response to dynamic changes during the session. The model aims to maximize user perceived quality, while taking into account constraints imposed by the user, network, and service itself. While the model is independent of the particular service and network scenario, we take a Networked Virtual Reality (NVR) service as a fairly good representative of intricate media-rich services in the NGN. We further propose a mapping onto the IP Multimedia Subsystem (IMS), as the most prominent NGN-driven part of the current Universal Mobile Telecommunications System (UMTS) architecture; and identify its possible functional enhancements for supporting services such as NVR.", "authors": ["Lea Skorin-Kapov", "Maja Matijasevic"], "n_citation": 50, "title": "End-to-end QoS signaling for future multimedia services in the NGN", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "85bf8cf8-d3ea-4073-9b2f-6b97449c9eb7"}
{"abstract": "Excessive information is known to degrade the classification performance of many machine learning algorithms. Attribute-efficient learning algorithms can tolerate irrelevant attributes without their performance being affected too much. Valiant's projection learning is a way to combine such algorithms so that this desired property is maintained. The archetype attribute-efficient learning algorithm Winnow and, especially, combinations of Winnow have turned out empirically successful in domains containing many attributes. However, projection learning as proposed by Valiant has not yet been evaluated empirically. We study how projection learning relates to using Winnow as such and with an extended set of attributes. We also compare projection learning with decision tree learning and Naive Bayes on UCI data sets. Projection learning systematically enhances the classification accuracy of Winnow, but the cost in time and space consumption can be high. Balanced Winnow seems to be a better alternative than the basic algorithm for learning the projection hypotheses. However, Balanced Winnow is not well suited for learning the second level (projective disjunction) hypothesis. The on-line approach projection learning does not fall far behind in classification accuracy from batch algorithms such as decision tree learning and Naive Bayes on the UCI data sets that we used.", "authors": ["Tapio Elomaa", "Jussi T. Lindgren"], "n_citation": 0, "title": "Experiments with projection learning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "85d4beb6-f883-4d90-bd5c-ddb6afb7a570"}
{"abstract": "Constraint Programming is a powerful programming paradigm with a great impact on a number of important areas such as logic programming [45],concurrent programming [42], artificial intelligence [12], and combinatorial optimization [46]. We believe that constraint programming is also a rich source of many challenging algorithmic problems, and co-operations between the constraint programming and the algorithms communities could be beneficial to both areas.", "authors": ["Fabrizio Grandoni", "Giuseppe F. Italiano"], "n_citation": 50, "title": "Algorithms and constraint programming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "85e7ea55-9b78-4173-a256-9ba40f934341"}
{"abstract": "Real-time rendering of complex 3D scene on mobile devices is a challenging task. The main reason is that mobile devices have limited computational capabilities and are lack of powerful 3D graphics hardware support. In this paper, we propose a remote Image-Based Rendering system for mobile devices to interactively visualize real world and synthetic scenes under wireless network. Our system uses panoramic video as building block of representing scene data. The scene data is compressed with one MPEG like encoding scheme tailored for mobile device. The compressed data is stored on remote server. Our system carefully partitions the rendering task between client and server. The server is responsible for determining the required data for rendering novel views. It streams the required data to client in server pushing manner. After receiving data, mobile client carries out rendering locally using image warping and displays the resultant images onto its small screen. Experimental results show that our system can achieve real time rendering speed on mainstream mobile devices. It allows multiple mobile clients to explore the same or different scenes simultaneously.", "authors": ["Zhongding Jiang", "Mao Y", "Qi Jia", "Nan Jiang", "Junyi Tao", "Xiaochun Fang", "Hujun Bao"], "n_citation": 0, "title": "PanoWalk : A Remote Image-Based Rendering System for Mobile Devices", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "869278b1-e37f-4bea-9e0d-7ccc174934e4"}
{"abstract": "Performance on a statistical language processing task relies upon accurate information being found in a corpus. However, it is known (and this paper will confirm) that many perfectly valid word sequences do not appear in training corpora. The percentage of n-grams in a test document which are seen in a training corpus is defined as n-gram coverage, and work in the speech processing community [7] has shown that there is a correlation between n-gram coverage and word error rate (WER) on a speech recognition task. Other work (e.g. [I]) has shown that increasing training data consistently improves performance of a language processing task. This paper extends that work by examining n-gram coverage for far larger corpora, considering a range of document types which vary in their similarity to the training corpora, and experimenting with a broader range of pruning techniques. The paper shows that large portions of language will not be represented within even very large corpora. It confirms that more data is always better, but how much better is dependent upon a range of factors: the source of that additional data, the source of the test documents, and how the language model is pruned to account for sampling errors and make computation reasonable.", "authors": ["Ben Allison", "David Guthrie", "Louise Guthrie"], "n_citation": 50, "title": "Another look at the data sparsity problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8694282c-9522-4f57-9f91-0aa150545ba7"}
{"authors": ["Hanne Vlaeminck", "Johan Wittocx", "Joost Vennekens", "Marc Denecker", "Maurice Bruynooghe"], "n_citation": 0, "title": "An approximative inference method for solving \u2203\u2200SO satisfiability problems", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "86a8ed5a-01b2-4d6f-898f-0180966b3169"}
{"abstract": "We present a wrapper generation system to extract contents of semi-structured documents which contain instances of a record. The generation is done automatically using general assumptions on the structure of instances. It outputs a set of pairs of left and right delimiters surrounding instances of a field. In addition to input documents, our system also receives a set of symbols with which a delimiter must begin or end. Our system treats semi-structured documents just as strings so that it does not depend on markup and natural languages. It does not require any training examples which show where instances are. We show experimental results on both static and dynamic pages which are gathered from 13 Web sites, markuped in HTML or XML, and written in four natural languages. In addition to usual contents, generated wrappers extract useful information hidden in comments or tags which are ignored by other wrapper generation algorithms. Some generated delimiters contain whitespaces or multibyte characters.", "authors": ["Yasuhiro Yamada", "\u6cf0\u5bdb \u5c71\u7530", "Daisuke Ikeda", "Sachio Hirokawa", "\u4f50\u5343\u7537 \u5ee3\u5ddd"], "n_citation": 0, "title": "Automatic wrapper generation for multilingual Web resources", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "86c8221b-fad5-46f0-ae43-af382a748ff0"}
{"abstract": "In this work we present an approach to adapt speaker-independent recognizers to a new acoustical environment. The recognizers were trained with data which were recorded using a close-talking microphone. These recognizers are to be evaluated with distant-talking microphone data. The adaptation set was recorded with the same type of microphone. In order to keep the speaker-independency this set includes 33 speakers. The adaptation itself is done using maximum a posteriori (MAP) and maximum likelihood linear regression adaptation (MLLR) in combination with the Baum-Welch algorithm. Furthermore the close-talking training data were artificially reverberated to reduce the mismatch between training and test data. In this manner the performance could be increased from 9.9% WA to 40.0% WA in speaker-open conditions. If further speaker-dependent adaptation is applied this rate is increased up to 54.9% WA.", "authors": ["Andreas K. Maier", "Tino Haderlein", "Elmar N\u00f6th"], "n_citation": 0, "title": "Environmental adaptation with a small data set of the target domain", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "86d04088-29a6-4380-9cca-c3fbdf9b8bb6"}
{"abstract": "IP traceback technology is an important means combating Denial of Service (DoS) attacks in Internet. This paper proposes a new IP traceback scheme constituting two parts: the first part is constructing a traceback tree by integrating Deterministic Packet Marking and Probabilistic Packet Marking, and the second part is getting attack routes by analyzing this traceback tree. Basing on performance analysis, we point out that our scheme is both efficient and robust against mark field spoofing.", "authors": ["Fan Min", "Junyan Zhang", "Guo-wie Yang"], "n_citation": 0, "title": "An IP traceback scheme integrating DPM and PPM", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "86d5db6a-d040-45f1-ba89-e3975b45fdd6"}
{"abstract": "Many recent systems tackling Distributed Constraint Satisfaction Problems (DCSPs) lack a theoretically founded specification and safety or liveness property proofs. This may be due to the difficulty of modeling and verifying concurrently running threads and their interaction. In this article we will briefly sketch an approach to the modeling and verification of concurrent algorithms tailored to DCSP solving and based on algebraic Petri nets. We will present a realistic case study on distributed agreement finding and state according termination and consistency properties.", "authors": ["Markus Hannebauer"], "n_citation": 0, "title": "How to model and verify concurrent algorithms for Distributed CSPs", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "86da79d0-6df8-4335-8058-640f07905f75"}
{"abstract": "In this paper, we propose an aggregation scheme, which is called Combine Early and Go Together (CEGT), to reduce the energy cost to transmit sensor data from sources to the sink. In the CEGT, the aggregation point is chosen in such a way that the sum of distances from source nodes to the sink via the aggregation point measured in the number of hops is minimized. Unlike most aggregation schemes, CEGT does not need to construct a tree from each source node to the sink. Simulation results show that our scheme can prolong the network life time and improve the path durability than the conventional schemes such as the gradient path and the shortest path schemes.", "authors": ["Jeongho Son", "Jinsuk Pak", "Kijun Han"], "n_citation": 0, "title": "A detection scheme of aggregation point for directed diffusion in wireless sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "86de3337-7eca-4ca1-92c2-0557dcc9221b"}
{"abstract": "A constraint satisfaction problem (CSP) model can be preprocessed to ensure that any choices made will lead to solutions, without the need to backtrack. This can be especially useful in a real-time process control or online interactive context. The conventional machinery for ensuring backtrack-free search, however, adds additional constraints, which may require an impractical amount of space. A new approach is presented here that achieves a backtrack-free representation by removing values. This may limit the choice of solutions, but we are guaranteed not to eliminate them all. We show that in an interactive context our proposal allows the system designer and the user to collaboratively establish the tradeoff in space complexity, solution loss, and backtracks.", "authors": ["J. Christopher Beck", "Tom Carchrae", "Eugene C. Freuder", "Georg Ringwelski"], "n_citation": 0, "title": "Backtrack-free search for real-time constraint satisfaction", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "87107f08-66c3-453d-9399-747977cbc5cf"}
{"abstract": "This paper addresses the problem of probabilistic modelling of human motion by combining several 2D views. This method takes advantage of 3D information avoiding the use of a complex 3D model. Considering that the main disadvantage of 2D models is their restriction to the camera angle, a solution to this limitation is proposed in this paper. A multi-view Gaussian Mixture Model (GMM) is therefore fitted to a feature space made of Shapes and Stick figures manually labelled. Temporal and spatial constraints are considered to build a probabilistic transition matrix. During the fitting, this matrix limits the feature space only to the most probable models from the GMM. Preliminary results have demonstrated the ability of this approach to adequately estimate postures independently of the direction of motion during the sequence.", "authors": ["Gr\u00e9gory Rogez", "Carlos Orrite", "Jes\u00fas Garc\u00eda Mart\u00ednez", "J. Elias Herrero"], "n_citation": 0, "title": "Probabilistic spatio-temporal 2D-model for pedestrian motion analysis in monocular sequences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "872b220b-f710-4220-8e37-ab142157e727"}
{"abstract": "This paper highlights degeneracy as being an important property in both the immune system and biology in general. From this, degeneracy is chosen as a candidate to inspire artificial immune systems. As a first step in exploiting the power of degeneracy, we follow the conceptual framework approach and build an abstract computational model in order to understand the properties of degenerate detectors free of any application bias. The model we build is based on the activation of T H  cell in the lymph node, as lymph nodes are the sites in the body where the adaptive immune response to foreign antigen in the lymph are activated. The model contains APC, antigen and T H  cell agents that move and interact in a 2-dimensional cellular space. The T H  cell agent receptors are assumed to be degenerate and their response to different antigen agents is measured. Initial observations and results of our model are presented and highlight some of the possibilities of degenerate detector recognition.", "authors": ["Paul S. Andrews", "Jon Timmis"], "n_citation": 50, "title": "A computational model of degeneracy in a lymph node", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "88703664-3ef5-436f-9d32-4b30aa8f1f50"}
{"abstract": "Bound consistency can easily and efficiently be enforced on linear constraint. However, bound consistency techniques deal with every constraint separately. We show that in some cases much stronger bounds can be computed by looking globally at the set of linear constraints. A simple algorithm for computing such bounds is derived from a new decomposition scheme. We prove that these bounds can be as tight as possible. In some cases this can be better than the standard reduced cost fixings. This algorithm has been implemented within a global constraint. Experimental results on balanced incomplete block design (BIBD) show its effectiveness: we were able to significantly improve over the best published results for this class of problems.", "authors": ["Jean-Francois Puget"], "n_citation": 0, "title": "Improved bound computation in presence of several clique constraints", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "88d027bf-76ff-40b5-b509-a7cc310320e3"}
{"abstract": "This paper presents a robust automatic speech recognition (ASR) system as multimedia interface for car navigation. In front-end, we use the minimum-mean square error (MMSE) enhancement to suppress the background in-car noise and then compensate the spectrum components distorted by noise over-reduction by smoothing technologies. In acoustic model training, an immunity learning scheme is adopted. in which pre-recorded car noises are artificially added to clean training utterances to imitate the in-car environment. The immunity scheme makes the system robust to both residual noise and speech enhancement distortion. In the context of Mandarin speech recognition, a special issue is the diversification of Chinese dialects, i.e. the pronunciation difference among accents decreases the recognition performance if the acoustic models are trained with an unmatched accented database. We propose to train the models with multiple accented Mandarin databases to solve this problem. The efficiency of the proposed ASR system is confirmed in evaluations.", "authors": ["Pei Ding", "Lei He", "Xiang Yan", "Rui Zhao", "Jie Hao"], "n_citation": 0, "title": "Robust Mandarin Speech Recognition for Car Navigation Interface", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "89aad6bb-0dcf-485a-bf69-aaa12f0dee66"}
{"abstract": "Situated at the intersection of machine learning and logic programming, inductive logic programming (ILP) has been concerned with finding patterns expressed as logic programs. While ILP initially focussed on automated program synthesis from examples, it has recently expanded its scope to cover a whole range of data analysis tasks (classification, regression, clustering, association analysis). ILP algorithms can this be used to find patterns in relational data, i.e., for relational data mining (RDM). This paper briefly introduces the basic concepts of ILP and RDM and discusses some recent research trends in these areas.", "authors": ["Saso Dzeroski"], "n_citation": 50, "title": "From inductive logic programming to relational data mining", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "89d1974b-4eb0-402a-9df3-955cfb28f179"}
{"abstract": "This paper presents a frame-level adaptive search range scaling strategy for B pictures coding for H.264/AVC from the hardware-oriented viewpoint. After studying the relationship between search range of P and B picture, a simple search range scaling strategy is proposed at first, which is efficient for normal or low motion video. After that, this strategy is extended to high motion video by using the information of intra prediction and motion vector of each P picture to restrict the search range of adjacent B pictures. This adaptive search range scaling strategy can not only reduce approximate 60% search area of B pictures, but also keep almost the same coding performance as the reference software.", "authors": ["Zhigang Yang", "Wen Gao", "Yan Liu", "Debin Zhao"], "n_citation": 0, "title": "Adaptive Search Range Scaling for B Pictures Coding", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8a6c8297-3830-48cc-9cd4-8ab45235ff28"}
{"abstract": "We describe an object-oriented constraint solving toolkit, QOCA, designed for interactive graphical applications. It has a simple yet powerful interface based on the metric space model for constraint manipulation. Currently QOCA supports linear arithmetic constraints and two different metrics: the square of the Euclidean distance and the Manhattan distance. It provides three solvers, all of which rely on keeping the constraints in solved form and relies on novel algorithms for efficient resolving of constraints during direct manipulation. We provide a thorough empirical evaluation of QOCA, both of the interface design and the speed of constraint solving.", "authors": ["Kim Marriott", "S. S. Chok", "Alan Finlay"], "n_citation": 0, "title": "A tableau based constraint solving toolkit for interactive graphical applications", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "8ad53171-8cd4-4da9-862a-4a18bd47f151"}
{"abstract": "Presented paper informs about the current status of Slovak National Corpus, goals and achievements of the project, as well as about technical details concerning the building of representative, balanced, annotated corpus of modem Slovak language.", "authors": ["Alexander Horak", "Lucia Gianitsova", "M\u00e1ria \u0160imkov\u00e1", "Martin Smotlak", "Radovan Garab\u00edk"], "n_citation": 0, "title": "Slovak national corpus", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8b14da29-9e9d-4861-9944-62a23d1de19a"}
{"abstract": "We present a rule-based framework for defining and implementing finite trace monitoring logics, including future and past time temporal logic, extended regular expressions, real-time logics, interval logics, forms of quantified temporal logics, and so on. Our logic, EAGLE, is implemented as a Java library and involves novel techniques for rule definition, manipulation and execution. Monitoring is done on a state-by-state basis, without storing the execution trace.", "authors": ["Howard Barringer", "Allen Goldberg", "Klaus Havelund", "Koushik Sen"], "n_citation": 0, "title": "Rule-based runtime verification", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8b7fed4f-7ae2-4705-af1d-599394fe2256"}
{"abstract": "Combinatorial search methods often exhibit a large variability in performance. We study the cost profiles of combinatorial search procedures. Our study reveals some intriguing properties of such cost profiles. The distributions are often characterized by very long tails or heavy tails. We will show that these distributions are best characterized by a general class of distributions that have no moments (i.e., an infinite mean, variance, etc.). Such non-standard distributions have recently been observed in areas as diverse as economics, statistical physics, and geophysics. They are closely related to fractal phenomena, whose study was introduced by Mandelbrot. We believe this is the first finding of these distributions in a purely computational setting. We also show how random restarts can effectively eliminate heavy-tailed behavior, thereby dramatically improving the overall performance of a search procedure.", "authors": ["Carla P. Gomes", "Bart Selman", "Nuno Crato"], "n_citation": 0, "title": "Heavy-tailed distributions in combinatorial search", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "8bcfa8e6-78dc-4259-8f5f-3b1d7a402bc7"}
{"abstract": "In this paper we propose a new stable learning algorithm for Cellular Neural Networks. Our approach is based on the input-to-state stability theory, so to obtain learning laws that do not need robust modifications. Here we present only a theoretical study, letting experimental evidences for further works.", "authors": ["Marco A. Moreno-Armend\u00e1riz", "Giovanni Egidio Pazienza", "Wen Yu"], "n_citation": 0, "title": "Training Cellular Neural Networks with Stable Learning Algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8c9940f4-bb9f-4bb0-a362-ee7c4129cfd3"}
{"abstract": "Time delays affect the behavior of a dynamic system significantly. Chaos have been found in a simplest neural network system consisted with even one neuron. It is known difficult to analyze chaos theoretically in a neural network system with uncertain time delays. In this paper we consider a neural network consisted with one neuron with uncertain time delays. Stability of the system is analyzed first. Chaos phenomenon of the system is then illustrated by giving the diagram of bifurcation and the largest Lyapunov exponent.", "authors": ["Shangbo Zhou", "Hua Li", "Zhongfu Wu"], "n_citation": 0, "title": "Stability and chaos of a neural network with uncertain time delays", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8cfdba63-274e-4819-94b1-ee9eff0de8f0"}
{"abstract": "In this paper we introduce a novel framework to compute jointly syntactic parses and semantic representations of a written sentence. To achieve this goal, we couple a syntactico-semantic grammar and a knowledge base. The knowledge base is implemented in Description Logics, in a polynomial variant. The grammar is a Range Concatenation Grammar, which combines expressive power and polynomial parsing time, and allows external predicate calls. These external calls are sent to the knowledge base, which is able either to answer these calls or to learn new information, this process taking place during parsing. Thus, only semantically acceptable parses are built, avoiding the costly a posteriori semantic check of all syntactically correct parses.", "authors": ["Beno\u00eet Sagot", "Adil El Ghali"], "n_citation": 0, "title": "Coupling grammar and knowledge base: Range Concatenation grammars and description logics", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8d2cf5a0-bf9e-4901-857c-65dcfd6f5744"}
{"authors": ["Dirk Smeets", "Thomas Fabry", "Jeroen Hermans", "Dirk Vandermeulen", "Paul Suetens"], "n_citation": 0, "title": "Inelastic deformation invariant modal representation for non-rigid 3D object recognition", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "8d47b016-b1b0-4325-9310-40b251550ce2"}
{"abstract": "In undeniable signature schemes, zero-knowledgeness and non-transferability have been identified so far. In this paper, by separating these two notions, we show the first 3-move confirmation and disavowal protocols for Chaum's undeniable signature scheme which is secure against active and concurrent attacks. Our main observation is that while the signer has one public key and one secret key, there exist two witnesses in the confirmation and disavowal proofs of Chaum's scheme.", "authors": ["Kaoru Kurosawa", "Swee-Huay Heng"], "n_citation": 0, "title": "3-move undeniable signature scheme", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8d6c1ca6-3732-4837-9195-c7b9fae1ba43"}
{"abstract": "People are usually more interested in ranking a group of interrelated objects than the actual scores of them. In this paper, we presented an improved relative evaluation criterion, which is suitable for such task and can solve those problems that are difficult to deal with by common absolute criterion. We put forward the improvement algorithm and realize its function approximation by means of BP algorithm. Furthermore, we tested this criterion with the statistic data of SARS. The experimental results indicated that our improved criterion is effective in comparing a set of correlative objects objectively. Finally, we made a conclusion about the goal and efficiency of relative criterion.", "authors": ["Zhiyong Zhang", "Jingang Liu", "Zhongzhi Shi"], "n_citation": 0, "title": "An improved relative criterion using BP algorithm", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8da023db-a1da-4046-8980-11e2441381c1"}
{"abstract": "The nonmonotonic formalism of partial equilibrium logic (PEL) has recently been proposed as a logical foundation for the partial stable and well-founded semantics of logic programs [1,2]. We study certain logical properties of PEL and some techniques to compute partial equilibrium models.", "authors": ["Pedro Cabalar", "Sergei P. Odintsov", "David Pearce", "Agust\u00edn Valverde"], "n_citation": 0, "title": "On the logic and computation of partial equilibrium models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8df3cf57-4fef-4571-9f43-22ea91be6327"}
{"abstract": "Multi-view video systems are often faced with brightness variations across the multi-perspective captured images. To tackle this problem and maintain the coding performance, block-based illumination compensation (BBIC) methods are recently proposed, where a first order affine BBIC model, consisting of a multiplicative factor and an additive offset, is often adopted. However, so far little attention has been paid to the fast algorithms that can reduce the computational overhead of BBIC. Therefore, we propose a fast image local statistics computation scheme using the technique of integral images, which can largely ease the process of computing the BBIC parameters for any blocks of interest. Moreover, a fast progressive integral image generation scheme, seamlessly integrated in a streaming-mode macroblock-based video coding system, is proposed. The experimental results show that the proposed technique achieves an average six-fold speedup, in comparison to the traditional computation methods under typical conditions.", "authors": ["Jiangbo Lu", "Gauthier Lafruit", "Francky Catthoor"], "n_citation": 0, "title": "Streaming-Mode MB-Based Integral Image Techniques for Fast Multi-view Video Illumination Compensation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8e03a3ce-9f5e-4684-b124-7ac6a7d15c84"}
{"abstract": "Feature selection is always an important and difficult issue in pattern recognition, machine learning and data mining. In this paper, a novel approach called resemblance coefficient feature selection (RCFS) is proposed. Definition, properties of resemblance coefficient (RC) and the evaluation criterion of the optimal feature subset are given firstly. Feature selection algorithm using RC criterion and a quantum genetic algorithm is described in detail. RCFS can decide automatically the minimal dimension of good feature vector and can select the optimal feature subset reliably and effectively. Then the efficient classifiers are designed using neural network. Finally, to bring into comparison, 3 methods, including RCFS, sequential forward selection using distance criterion (SFSDC) and a new method of feature selection (NMFS) presented by Tiejun Lu are used respectively to select the optimal feature subset from original feature set (OFS) composed of 16 features of radar emitter signals. The feature subsets, obtained from RCFS, SFSDC and NMFS, and OFS are employed respectively to recognize 10 typical radar emitter signals in a wide range of signal-to-noise rate. Experiment results show that RCFS not only lowers the dimension of feature vector greatly and simplifies the classifier design, but also achieves higher accurate recognition rate than SFSDC, NMFS and OFS, respectively.", "authors": ["Gexiang Zhang", "Laizhao Hu", "Weidong Jin"], "n_citation": 0, "title": "Resemblance coefficient and a quantum genetic algorithm for feature selection", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8e428c2e-19a3-4dd3-9eae-9c779c47ce78"}
{"abstract": "This paper presents a new scheme for synthesizing hand motion to grasp various objects. Hand motion has a variety of expression with its high degrees of freedom and the functional motions are especially complex and difficult to synthesize. This paper focuses on a fact that actual grasp motion varies depending on the features of the object including its size and shape, which give important clues for reproducing a proper hand motion to manipulate them. Based on this idea, we propose a scheme to sample grasp motions and synthesize the whole hand motion including approach to the object, preparation of the hand shape and grab motion. Synthesized animation demonstrated a potential for easily designing functional motions for hand animation.", "authors": ["Yoshihiro Yasumuro", "Masayuki Yamazaki", "Masataka Imura", "Yoshitsugu Manabe", "Kunihiro Chihara"], "n_citation": 0, "title": "Grasp motion synthesis based on object features", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8e7d346c-4a72-452d-b004-7cb112f4c734"}
{"abstract": "Nowadays there is a proliferation of research into multi-agent and holonic systems. These systems are being applied to environments including production, supply chain and warehousing to increase the flexibility, openness and mass-customisation of e-manufacturing operations. One such example is the Holonic Packing Cell demonstrator at Cambridge University's Institute for Manufacturing. However, there is very little basis for evaluating how well such systems have been built or how they will operate once they are deployed into pragmatic shop-floor settings. This paper is an initial step in filling this void by proposing a framework to evaluate holonic systems with respect to: (i) the performance of their controlled operations, (ii) the applicability of the software and control systems, and (iii) the methodology used.", "authors": ["Martyn Fletcher", "Duncan McFarlane", "Alan Thorne", "Dennis Jarvis", "Andrew Lucas"], "n_citation": 0, "title": "Evaluating a Holonic Packing Cell", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "8f89a15a-51d0-4072-9c8e-4b9dc09d94fd"}
{"abstract": "The emergence of computing environments where smart devices are embedded pervasively in the physical world has made possible many interesting applications and has triggered several new research areas. Mobile ad hoc networks (MANET), sensor networks and radio frequency identification (RFID) systems are all examples of such pervasive systems. Operating on an open medium and lacking a fixed infrastructure, these systems suffer from critical security vulnerabilities for which few satisfactory current solutions exist, particularly with respect to availability and denial-of-service. In addition, most of the extant knowledge in network security and cryptography cannot be readily transferred to the newer settings which involve weaker devices and less structured networks. In this paper we investigate the security of pervasive systems and focus on availability issues in malicious environments. We articulate a formal security framework that is tuned for the analysis of protocols for constrained systems and show how this can be used with applications that involve MANET and RFID systems. In our approach we shall use optimistic protocols for which the overhead is minimal when the adversary is passive. When the adversary is active, depending on the application, the additional cost is either used to trace malicious behavior or born by non-constrained components of the system. Our goal is to design mechanisms that will support self-healing and promote a fault-free system state, or a stable system state, in the presence of a Byzantine adversary.", "authors": ["Mike Burmester", "Tri Van Le", "Breno de Medeiros"], "n_citation": 50, "title": "Towards provable security for ubiquitous applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8f8c30ea-d42f-4fb1-8a27-c5b29365edec"}
{"abstract": "We perform a comprehensive study of mappings between constraint satisfaction problems (CSPs) and propositional satisfiability (SAT). We analyse four different mappings of SAT problems into CSPs, and two of CSPs into SAT problems. For each mapping, we compare the impact of achieving arc-consistency on the CSP with unit propagation on the SAT problem. We then extend these results to CSP algorithms that maintain (some level of) arc-consistency during search like FC and MAC, and to the Davis-Putnam procedure (which performs unit propagation at each search node). Because of differences in the branching structure of their search, a result showing the dominance of achieving arc-consistency on the CSP over unit propagation on the SAT problem does not necessarily translate to the dominance of MAC over the Davis-Putnam procedure. These results provide insight into the relationship between propositional satisfiability and constraint satisfaction.", "authors": ["Toby Walsh"], "n_citation": 0, "title": "SAT v CSP", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "909aa898-ed91-498d-ae17-67034ea41da4"}
{"abstract": "The branch of Computer Science known as digital image processing includes different topics of Investigation and Development, as well as applications encompassing different stages that go from data acquisition, enhancement, and segmentation, up to the analysis, classification and interpretation of images [1]. Particularly, the reconstruction of 3D movements from 2D images (photos, filming) is a complex area, which becomes significant when it is a matter of obtaining real time responses. [1, 2, 3]. This project aim is developing a three-dimensional analysis system considering the processing of a soccer ball trajectory and rotational speed for its later computer-generated graphical modeling. The objective of the system is to improve the player's skills and the training methodology, and is framed within the research line of this Institute and within the area of signal and image processing.", "authors": ["Federico Cristina", "Sebasti\u00e1n H. Dapoto", "Claudia Cecilia Russo", "Armando Degiusti", "Mar\u00eda Jos\u00e9 Ab\u00e1solo"], "n_citation": 0, "title": "Mobile path and spin 3D tracking and reconstruction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "90a6d9f8-e953-46d0-900b-bf6fad693839"}
{"abstract": "In this paper, we investigate the complexity of deciding the satisfiability of XPath 2.0 expressions, i.e., whether there is an XML document for which their result is nonempty. Several fragments that allow certain types of expressions are classified as either in PTIME or NP-hard to see which type of expression make this a hard problem. Finally, we establish a link between XPath expressions and partial tree descriptions which are studied in computational linguistics.", "authors": ["Jan Hidders"], "n_citation": 0, "title": "Satisfiability of XPath expressions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "90a7d122-d5c2-477c-b8f8-15d998cc8ea6"}
{"abstract": "A new public-key model for resettable zero-knowledge (rZK) protocols, which is an extension and generalization of the upper-bounded public-key (UPK) model introduced by Micali and Reyzin [EuroCrypt'01, pp. 373-393], is introduced and is named weak public-key (WPK) model. The motivations and applications of the WPK model are justified in the distributed smart-card/server setting and it seems more preferable in practice, especially in E-commerce over Internet. In this WPK model a 3-round (optimal) black-box resettable zero-knowledge argument with concurrent soundness for NP is presented assuming the security of RSA with large exponents against subexponential-time adversaries. Our result improves Micali and Reyzin's result of resettable zero-knowledge argument with concurrent soundness for NP in the UPK model. Note that although Micali and Reyzin' protocol satisfies concurrent soundness in the UPK model, but it does not satisfy even sequential soundness in our WPK model. Our protocol works in a somewhat parallel repetition manner to reduce the error probability and the black-box zero-knowledge simulator works in strict polynomial time rather than expected polynomial time. The critical tools used are: verifiable random functions introduced by Micali, Rabin and Vadhan [FOCS'99, pp. 120-130], zap presented by Dwork and Naor [FOCS'00, pp. 283-293] and complexity leveraging introduced by Canetti, Goldreich, Goldwasser and Micali [STOC'00, pp. 235-244].", "authors": ["Yunlei Zhao", "Xiaotie Deng", "Chan H. Lee", "Hong Zhu"], "n_citation": 50, "title": "Resettable zero-knowledge in the weak public-key model", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "90b00346-8c22-4b04-88e9-ac53cb5248ae"}
{"abstract": "Web-based portals are a convenient and effective mechanism for integrating information from a wide variety of sources, including Web services. However, since availability and performance of Web services cannot be guaranteed. availability of information and overall performance of a portal can vary. In this paper, we describe a framework for developing an autonomic self-healing portal system that relies on the notion of differentiated services (i.e., services that provide common behavior with variable quality of service) in order to survive unexpected traffic loads and slowdowns in underlying Web services. We also present a theoretical performance model that predicts the impact of the framework on existing systems. We demonstrate the framework with an example and provide an evaluation of the technique.", "authors": ["Henri Naccache", "Gerald C. Gannod", "Kevin Gary"], "n_citation": 0, "title": "A self-healing web server using differentiated services", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "90b2b2f0-687c-4844-bc0e-9cb554b65370"}
{"abstract": "The scarcest resource for most of the wireless sensor networks (WSNs) is energy and one of the major factors in energy consumption for WSNs is due to communication. Not only transmission but also reception is the source of energy consumption. The lore to decrease energy consumption is to turn off radio circuit when it is not needed. This is why TDMA has advantages over contention based methods. Time slot assignment algorithm is an essential part of TDMA based systems. Although centralized time slot assignment protocols are preferred in many WSNs, centralized approach is not scalable. In this paper, a new energy efficient and delay sensitive distributed time slot assignment algorithm (DTSM) is proposed for sensor networks under convergecast traffic pattern. DTSM which is developed as part of the military monitory (MILMON) system introduced in [16], aims to operate with low delay and low energy. Instead of collision based periods, it assigns slots by the help of tiny request slots. While traditional slot assignment algorithms do not allow assigning the same slot within two hop neighbors, because of the hidden node problem, DTSM can assign, if assignment is suitable for convergecast traffic. Simulation results have shown that delay and energy consumption performance of DTSM is superior to FPRP, DRAND, and TRAMA which are the most known distributed slot assignment protocols for WSNs or ad hoc networks. Although DTSM has somewhat long execution time, its scalability characteristic may provide application specific time durations.", "authors": ["Ilker Bekmezci", "Fatih Alagoz"], "n_citation": 50, "title": "Delay sensitive and energy efficient distributed slot assignment algorithm for sensor networks under convergecast data traffic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "90b7b8a0-61f4-4ee3-b3bf-f7b886e8f953"}
{"abstract": "We discuss the use of abstract interpretation in the context of automatic program verification requiring precise abstractions. We compare entirely manual versus user-guided abstractions ranging from program-specific abstractions including predicate abstraction to the systematic design of abstract domains and iteration strategies.", "authors": ["Patrick Cousot"], "n_citation": 50, "title": "Automatic verification by abstract interpretation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "90ff6b27-88a0-43a3-8a53-92bf71af1bc8"}
{"abstract": "We propose an approach for incremental modeling of composite Web services. The technique takes into consideration both the functional and non-functional requirements of the composition. While the functional requirements are described using symbolic transition systems-transition systems augmented with state variables, function invocations, and guards; non-functional requirements are quantified using thresholds. The approach allows users to specify an and possibly incomplete specification of the desired service (goal) that can be realized by selecting and composing a set of pre-existing services. In the event that such a composition is unrealizable, i.e. the composition is not functionally equivalent to the goal or the non-functional requirements are violated, our system provides the user with the causes for the failure, that can be used to appropriately reformulate the functional and/or non-functional requirements of the goal specification.", "authors": ["Jyotishman Pathak", "Samik Basu", "Vasant Honavar"], "n_citation": 0, "title": "Modeling web services by iterative reformulation of functional and non-functional requirements", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "91247147-96cd-4ede-a497-da5e45653421"}
{"abstract": "In this article we describe an efficient AES software implementation that is well suited for 8-bit smart cards and resistant against power analysis attacks. Our implementation masks the intermediate results and randomizes the sequence of operations at the beginning and the end of the AES execution. Because of the masking, it is secure against simple power analysis attacks, template attacks and first-order DPA attacks. Due to the combination of masking and randomization, it is resistant against higher-order DPA attacks. Resistant means that a large number of measurements is required for a successful attack. This expected number of measurements is tunable. The designer can choose the amount of randomization and thereby increase the number of measurements. This article also includes a practical evaluation of the countermeasures. The results prove the theoretical assessment of the countermeasures to be correct.", "authors": ["Christoph Herbst", "Elisabeth Oswald", "Stefan Mangard"], "n_citation": 0, "title": "An AES smart card implementation resistant to power analysis attacks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9193cf81-c4ad-4409-bb04-2691c7b837ab"}
{"abstract": "This paper presents the computer aided simulation of a model for the control of an immune response. This model has been developed to investigate the proposed hypothesis that the same cytokine that amplifies an initiated response can eventually lead to its downregulation, if it can act on more than one cell type. The simulation environment is composed of effector cells and regulatory cells; the former, when activated, initiate an immune response, while the latter are responsible for controlling the magnitude of the response. The signalling that coordinates this process is modelled using stimulation and regulation cytokines. Simulation results obtained, in accordance with the motivating idea, are presented and discussed.", "authors": ["Thiago S. Guzella", "Tomaz A. Mota-Santos", "Joaquim Quinteiro Uch\u00f4a", "Walmir M. Caminhas"], "n_citation": 0, "title": "Modelling the control of an immune response through cytokine signalling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "91e49510-816c-4d64-801b-45aef73dd030"}
{"abstract": "We study absolute and relative keys for XML, and investigate their associated decision problems. We argue that these keys are important to many forms of hierarchically structured data including XML documents. In contrast to other proposals of keys for XML, these keys can be reasoned about efficiently. We show that the (finite) satisfiability problem for these keys is trivial, and their (finite) implication problem is finitely axiomatizable and decidable in PTIME in the size of keys.", "authors": ["Peter Buneman", "Susan B. Davidson", "Wenfei Fan", "Carmem S. Hara", "Wang-Chiew Tan"], "n_citation": 0, "title": "Reasoning about keys for XML", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "921214e4-7d3b-418c-8584-7b4517f123d3"}
{"abstract": "The paper presents a web-based network management system, using the mobile agents technology, designed for Taiwan's National Broadband Experimental Networks (NBEN). The mobile agent technology is applied to gather and monitor the network status, and is capable of offering efficient, flexible and intelligent network management functions. Through the friendly graphical interfaces, the network status and related information can be easily collected and displayed dynamically. In the future, more mobile agents with intelligence will be designed and implemented, and the system will be transferred to commercial networks.", "authors": ["Li-Der Chou", "Kun-Chang Shen", "Ko-Chung Tang", "Chi-Chia Kao"], "n_citation": 0, "title": "Implementation of mobile-agent-based network management systems for National Broadband Experimental Networks in Taiwan", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "92305c45-5f7a-4c67-9c88-3cc913d2a4c3"}
{"abstract": "This paper formulates and studies a model of impulsive coupled delayed neural networks. Based on stability theory of impulsive dynamical systems, a simple but less-conservative criterion is derived for global synchronization of such coupled neural networks. Furthermore, the theoretical result is applied to a typical chaotic delayed Hopfied neural networks, and is also illustrated by numerical simulations.", "authors": ["Jin Zhou", "Tianping Chen", "Lan Xiang", "M Liu"], "n_citation": 0, "title": "Global Synchronization of Impulsive Coupled Delayed Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "92774707-3a88-4107-8e85-956af460d7d2"}
{"abstract": "Unstructured P2P systems have gained great popularity in recent years and are currently used by millions of users. One fundamental property of these systems is the lack of structure, which allows decentralized operation and makes it easy for new users to join and participate in the system. However, the lack of structure can also be abused by malicious users. We explore one such attack, that enables malicious users to use unstructured P2P systems to perform Denial of Service (DoS) attacks to third parties. Specifically, we show that a malicious node can coerce a large number of peers to perform requests to a target host that may not even be part of the P2P network, including downloading unwanted files from a target Web Server. This is a classic form of denial-of-service which also has two interesting characteristics: (a) it is hard to identify the originator of the attack, (b) it is even harder to stop the attack. The second property comes from the fact that certain unstructured P2P systems seem to have a kind of memory, retaining knowledge about (potentially false) queries for many days. In this paper we present real-world experiments of Gnutella-based DoS attacks to Web Servers. We explore the magnitude of the problem and present a solution to protect innocent victims against this attack.", "authors": ["Elias Athanasopoulos", "Kostas G. Anagnostakis", "Evangelos P. Markatos"], "n_citation": 0, "title": "Misusing unstructured P2P systems to perform DoS attacks : The network that never forgets", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "931b9e17-7e79-4196-ac2c-f8768d59d2cf"}
{"abstract": "One-time signatures are an important and efficient authentication utility. Various schemes already exist for the classical one-way public-key cryptography. One-time signatures have not been sufficiently explored in the literature in the branch of society-oriented cryptography. Their particular properties make them suitable, as a potential cryptographic primitive, for broadcast communication and group-based applications. In this paper, we try to contribute to filling this gap by introducing several group-based one-time signature schemes of various versions: with proxy, with trusted party, and without trusted party.", "authors": ["Mohamed Al-Ibrahim", "Anton Cerny"], "n_citation": 0, "title": "Proxy and threshold one-time signatures", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9361b96d-ac9c-4559-a8a5-60a3fa9aa153"}
{"abstract": "The class of closed patterns is a well known condensed representations of frequent patterns, and have recently attracted considerable interest. In this paper, we propose an efficient algorithm LCM (Linear time Closed pattern Miner) for mining frequent closed patterns from large transaction databases. The main theoretical contribution is our proposed prefix-preserving closure extension of closed patterns, which enables us to search all frequent closed patterns in a depth-first manner, in linear time for the number of frequent closed patterns. Our algorithm do not need any storage space for the previously obtained patterns, while the existing algorithms needs it. Performance comparisons of LCM with straightforward algorithms demonstrate the advantages of our prefix-preserving closure extension.", "authors": ["Takeaki Uno", "Tatsuya Asai", "Yuzo Uchida", "Hiroki Arimura"], "n_citation": 0, "title": "An efficient algorithm for enumerating Closed patterns in transaction databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "936e35a2-457f-4322-b430-b0467d947364"}
{"abstract": "We investigate techniques for acoustic modeling in automatic recognition of context-independent phoneme strings from the TIMIT database. The baseline phoneme recognizer is based on TempoRAl Patterns (TRAP). This recognizer is simplified to shorten processing times and reduce computational requirements. More states per phoneme and bi-gram language models are incorporated into the system and evaluated. The question of insufficient amount of training data is discussed and the system is improved. All modifications lead to a faster system with about 23.6% relative improvement over the baseline in phoneme error rate.", "authors": ["Petr Schwarz", "Pavel Matejka", "Jan Cernocky"], "n_citation": 0, "title": "Towards lower error rates in phoneme recognition", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9397c6c1-f640-47b1-80ab-958847a7b678"}
{"abstract": "In the manufacturing world, globalisation leads to a trend towards the reduction of batches and product life cycle, and the increase of part diversity, which are in conflict with other requirements, such as the cost reduction achieved with higher productivity. Thus, the challenge is to develop flexible, agile and intelligent management and control architectures that satisfy the referred requirements. The holonic manufacturing and the agent-based manufacturing approaches allow a new approach to the manufacturing problem, through concepts such as modularity, decentralisation, autonomy and re-use of control software components. ADACOR, one of the holonic architectures recently proposed, defines a set of autonomous and intelligent holons aiming to improve the performance of control system in industrial scenarios characterised by the frequent occurrence of unexpected disturbances. The formal modeling and validation of the specifications of the ADACOR-holons and of the interactions between these holons to implement the manufacturing control functions is of critical importance. In this paper, a formal methodology is introduced and applied to model the dynamic behaviour of the ADACOR-holon classes.", "authors": ["Paulo Leit\u00e3o", "Armando W. Colombo", "Francisco Restivo"], "n_citation": 0, "title": "An approach to the formal specification of holonic control systems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "93e81c8d-65f5-42c8-93e5-dac6377b80d7"}
{"abstract": "In this paper, we present an algorithm for speech input statistical translation. This algorithm is a dynamic-programming based algorithm, which uses a word graph in the input as a representation of the acoustic of a given utterance. A beam-search implementation of this algorithm has been made and experimental results with the so called EUTRANS-I task are presented.", "authors": ["Ismael Garc\u00eda-Varea", "Alberto Sanchis", "Francisco Casacuberta"], "n_citation": 0, "title": "A decoding algorithm for speech input statistical translation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9411d056-b21d-4d38-a41e-0832aa1b09b5"}
{"abstract": "We analyze the complexity of optimization problems expressed using valued constraints. This very general framework includes a number of well-known optimization problems such as MAX-SAT, and WEIGHTED MAX-SAT, as well as properly generalizing the classical CSP framework by allowing the expression of preferences. We focus on valued constraints over Boolean variables, and we establish a dichotomy theorem which characterizes the complexity of any problem involving a fixed set of constraints of this kind.", "authors": ["David A. Cohen", "Martin C. Cooper", "Peter Jeavons"], "n_citation": 0, "title": "A complete characterization of complexity for Boolean constraint optimization problems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9422db7d-c912-450c-adc6-7fca961151be"}
{"abstract": "In this paper we apply a Naive Bayes classifier (NB), a Bayesian Network (BAN) and a decision tree inducer (CART) on predicting Pitch Accent tones in Greek text, extracting knowledge from text and linguistic information. It is well established that regarding the performance of machine learning techniques, scale and quality of the corpus are very important. For our purpose we used a database consisted of 5.500 words, distributed in 500 paragraphs. In the present study, pitch accent placement was treated as a binary classification task. Hence, given a word form in its sentential context, it was decide whether it should be unaccented or bear a pitch accent tone.", "authors": ["Panagiotis Zervas", "Nikos Fakotakis", "George K. Kokkinakis"], "n_citation": 0, "title": "Pitch accent prediction from ToBI annotated corpora based on Bayesian learning", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "943edc69-8c85-428b-916d-21919e9bcb8c"}
{"authors": ["Fabrizio Silvestri", "Salvatore Orlando", "Raffaele Perego"], "n_citation": 0, "title": "WINGS: A Parallel Indexer for Web Contents", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "954acbdb-f668-4908-92ee-064e24a47b9a"}
{"abstract": "Studies on evolutionary dynamics of grammar acquisition on the computer have been widely reported in recent years, where an agent learns grammars of other agents through the exchange of sentences between them. Particularly, Nowak et al. [11] generalized an evolutionary theory of language with the universal grammar mathematically. In this paper, we propose a model of language evolution for the emergence of creole based on their theory, and try to discover the critical conditions for creolization. In our experimentation, we utilize the inside-outside (EM) algorithm to find the grammar of a new generation. As a result, we contend that creolization is strongly affected by the popularity of community of the original language, rather than the similarity of original grammars.", "authors": ["Makoto Nakamura", "Satoshi Tojo"], "n_citation": 0, "title": "The emergence of artificial creole by the EM algorithm", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9569466e-0f0e-4b23-93ad-5964d57bd64d"}
{"abstract": "This article presents a cross-lingual study for Hungarian and Finnish about the segmentation of continuous speech on word and phrasal level based on prosodic features. A word level segmenter has been developed which can indicate the word boundaries with acceptable accuracy for both languages. The ultimate aim is to increase the robustness of Automatic Speech Recognizers (ASR) by detection of word and phrase boundaries, and thus significantly decrease the searching space during the decoding process, very time-consuming in case of agglutinative languages, like Hungarian and Finnish. They are however fixed stressed languages, so by stress detection, word beginnings can be marked with reliable accuracy. An algorithm based on data-driven (HMM) approach was developed and evaluated. The best results were obtained by time series of fundamental frequency and energy together. Syllable length was found to be much less effective, hence was discarded. By use of supra-segmental features, word boundaries can be marked with high correctness ratio, if we allow not to find all of them. The method we evaluated is easily adaptable to other fixed-stress languages. To investigate this we adapted the method to the Finnish language and obtained similar results.", "authors": ["Kl\u00e1ra Vicsi", "Gy\u00f6rgy Szasz\u00e1k"], "n_citation": 0, "title": "Prosodic cues for automatic phrase boundary detection in ASR", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "95c659ab-40da-42d2-af92-4a5d29fff560"}
{"abstract": "The paper analyzes both acoustic and linguistic features with different Chinese prosodic boundaries. Then a rule-learning approach was used to do the prosodic boundary labelling. In the paper the prosodic boundaries are classified into four levels, full intonational boundary with strong intonational marking with/without lengthening or change in speech tempo, prosodic phrase boundary with rather weak intonational marking, prosodic word boundary and phone foot boundary. Candidate acoustic and linguistic features related to prosodic boundary were extracted from the corpus to establish an example database. Based on this, a series of comparative experiments is conducted to collect the most effective features from the candidates. Results show that the selected candidates characterize the boundary features efficiently. Final experiments show that rule-learning approach introduced in the paper can achieve better prediction accuracy than the rule and RNN based methods and yet retain the advantage of the simplicity and understandability.", "authors": ["Jianhua Tao"], "n_citation": 0, "title": "Acoustic and linguistic information based Chinese prosodic boundary labelling", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "95fbad18-b039-4940-9d2e-ada64995c5c5"}
{"abstract": "It has long been accepted that dynamic variable ordering heuristics outperform static orderings. But just how dynamic are dynamic variable ordering heuristics? This paper examines the behaviour of a number of heuristics, and attempts to measure the entropy of the search process at different depths in the search tree.", "authors": ["Patrick Prosser"], "n_citation": 0, "title": "The dynamics of dynamic variable ordering heuristics", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "95fef837-b66b-484b-aa4d-ca2ba8fc7c65"}
{"abstract": "Evolving artificial neural network is an important issue in both evolutionary computation (EC) and neural networks (NN) fields. In this paper, a hybrid particle swarm optimization (PSO) is proposed by incorporating differential evolution (DE) and chaos into the classic PSO. By combining DE operation with PSO, the exploration and exploitation abilities can be well balanced, and the diversity of swarms can be reasonably maintained. Moreover, by hybridizing chaotic local search (CLS), DE operator and PSO operator, searching behavior can be enriched and the ability to avoid being trapped in local optima can be well enhanced. Then, the proposed hybrid PSO (named CPSODE) is applied to design multi-layer feed-forward neural network. Simulation results and comparisons demonstrate the effectiveness and efficiency of the proposed hybrid PSO.", "authors": ["Bo Liu", "Ling Wang", "Yihui Jin", "Dexian Huang"], "n_citation": 0, "title": "Designing neural networks using hybrid particle swarm optimization", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "96502076-bde1-4286-9e39-bf1a3c818e43"}
{"abstract": "In this paper, we propose TFIGF, a method which detects peculiar web pages using distribution of words in WWW given a set of keywords. Our TFIGF detects a set of index words which represent a WWW page by estimating their importance in the WWW page and their rareness in WWW. Experiments using both English and Japanese WWW pages clearly show superiority of our approach over a traditional method which employs a limited number of WWW pages in the estimation.", "authors": ["Masayuki Hirose", "Einoshin Suzuki"], "n_citation": 0, "title": "Using WWW-distribution of words in detecting peculiar Web pages", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "965fe4ff-3875-4569-8600-f99dc8839db9"}
{"abstract": "The Constraint Hierarchy (CH) framework is used to tackle multiple criteria selection (MCS), consisting of a set of candidates and a set of, possibly competing, criteria for selecting the best candidate(s). In this paper, we identify aspects of the CH framework for further enhancement so as to model and solve MCS problems more accurately. We propose the Fuzzy Constraint Hierarchies framework, which allows constraints to belong to, possibly, more than one level in a constraint hierarchy to a varying degree. We also propose to replace the standard equality relation = used in valuation comparators of the CH framework by the \u03b1-approximate equality relation =a(\u03b1) for providing more flexible control over the handling of valuations with close error values. These proposals result in three new classes of valuation comparators. Formal properties of the new comparators are given, wherever possible.", "authors": ["R. W. L. Kam", "J. H. M. Lee"], "n_citation": 0, "title": "Fuzzifying the Constraint Hierarchies Framework", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "9697e1f9-6ec8-4aea-a014-d821959cd86a"}
{"abstract": "We propose a new approach for understanding the algorithmspecific empirical hardness of NP-Hard problems. In this work we focus on the empirical hardness of the winner determination problem-an optimization problem arising in combinatorial auctions-when solved by ILOG's CPLEX software. We consider nine widely-used problem distributions and sample randomly from a continuum of parameter settings for each distribution. We identify a large number of distribution-nonspecific features of data instances and use statistical regression techniques to learn, evaluate and interpret a function from these features to the predicted hardness of an instance.", "authors": ["Kevin Leyton-Brown", "Eugene Nudelman", "Yoav Shoham"], "n_citation": 0, "title": "Learning the empirical hardness of optimization problems: The case of combinatorial auctions", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "96c637be-2c2b-4068-8c22-1ffe6b30933a"}
{"abstract": "A new algorithm for learning neural network ensemble is introduced in this paper. The proposed algorithm, called NNEFS, exploits the synergistic power of neural network ensemble and feature subset selection to fully exploit the information encoded in the original dataset. All the neural network components in the ensemble are trained with feature subsets selected from the total number of available features by wrapper approach. Classification for a given intance is decided by weighted majority votes of all available components in the ensemble. Experiments on two UCI datasets show the superiority of the algorithm to other two state of art algorithms. In addition, the induced neural network ensemble has more consistent performance for incomplete datasets, without any assumption of the missing mechanism.", "authors": ["Haixia Chen", "Senmiao Yuan", "Kai Jiang"], "n_citation": 0, "title": "Wrapper approach for learning neural network ensemble by feature selection", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9708d102-d125-4932-afb9-a748a88b2681"}
{"abstract": "In the last years, the completion of the human genome sequencing showed up a wide range of new challenging issues involving raw data analysis. In particular, the discovery of information implicitly encoded in biological sequences is assuming a prominent role in identifying genetic diseases and in deciphering biological mechanisms. This information is usually represented by patterns frequently occurring in the sequences. Because of biological observations, a specific class of patterns is becoming particularly interesting: frequent structured patterns. In this respect, it is biologically meaningful to look at both exact and approximate repetitions of the patterns within the available sequences. This paper gives a contribution in this setting by providing some algorithms which allow to discover frequent structured patterns, either in exact or approximate form, present in a collection of input biological sequences.", "authors": ["Luigi Palopoli", "Giorgio Terracina"], "n_citation": 0, "title": "Discovering frequent structured patterns from string databases: An application to biological sequences", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9746550b-d90a-4e51-87b0-a521b35b2cad"}
{"abstract": "The global exponential stability of reaction-diffusion Hopfield neural networks with distributed delays is studied. Without assuming the boundedness, monotonicity and differentiability of the activation functions, the sufficient conditions were obtained by utilizing Dini's derivative, F-function and extended Hanaly's inequality. These conditions are easy to check and apply in practice and can be regarded as an extension of existing results.", "authors": ["Zhihong Tang", "Yiping Luo", "Feiqi Deng"], "n_citation": 0, "title": "Global exponential stability of reaction-diffusion hopfield neural networks with distributed delays", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "97f6e25c-c7be-4f85-8f08-992a9410ffc9"}
{"abstract": "Wireless nodes in sensor network detect surrounding events and then deliver the sensed information to a base station. Organizing these sensors into clusters enables efficient utilization of the limited network resources. Many clustering algorithms have been proposed such as LEACH, HEED, GAF and so on. While LEACH has many excellent features such as highly adaptive, self-configuring cluster formation, application-specific data aggregation, etc., it does not scale well when the network size or coverage increases. In this paper, the Enhanced Multihop Clustering Algorithm (EMCA) is proposed which utilizes multihop links for both intra-cluster and inter-cluster communication. To model the energy consumption more accurately, each cluster is modeled as a Voronoi Cell instead of a circle. The optimal parameter values are determined to minimize the total energy consumption so as to prolonging the lifetime of the whole network. Numerical results show that when both LEACH and EMCA operate with optimal parameter values, the total energy consumption of EMCA is much smaller than that of LEACH. Moreover, EMCA scales much well when the network scale increases, which proves that EMCA is highly scalable and is especially suitable for relatively large-scale wireless sensor networks.", "authors": ["Ying Qian", "Jinfang Zhou", "Liping Qian", "Kangsheng Chen"], "n_citation": 50, "title": "Prolonging the lifetime of wireless sensor network via multihop clustering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "98d52128-d50f-4f12-b191-7aa7785f887a"}
{"abstract": "In this paper we give an overview of applications of Constraint Programming for IP (Internet Protocol) data networks, and discuss the problem of Resilience Analysis in more detail. In this problem we try to predict the loading of a network in different failure scenarios, without knowing end-to-end flow values throughout the network; the inference is based only on observed link traffic values. The related problem of Traffic Flow Analysis aims to derive a traffic matrix from the observed link traffic data. This is a severely under-constrained problem, we can show that the obtained flow values vary widely in different, feasible solutions. Experimental results indicate that using the same data much more accurate, bounded results can be obtained for Resilience Analysis.", "authors": ["Helmut Simonis"], "n_citation": 0, "title": "Constraint based resilience analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "991ef335-216f-4031-bf45-b5dcc726667b"}
{"authors": ["Li Weng", "Rony Darazi", "Bart Preneel", "Beno\u00eet Macq", "Ann Dooms"], "n_citation": 0, "title": "Robust image content authentication using perceptual hashing and watermarking", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "99498749-33d5-484b-a037-686ee871422d"}
{"abstract": "In order to reduce the gap between low-level image features and high-level image semantics, various long term learning strategies were integrated into content-based image retrieval system. The strategies always use the semantic relationships among images to improve the effectiveness of the retrieval system. This paper proposes a semantic similarity propagation method to mine the hidden semantic relationships among images. The semantic relationships are propagated between the similar images and regions. Experimental results verify the improvement on similarity propagation and image retrieval.", "authors": ["Weiming Lu", "Hong Pan", "Jiangqin Wu"], "n_citation": 0, "title": "Region-Based Semantic Similarity Propagation for Image Retrieval", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9966644b-e67b-4d55-93ea-76c66e9c7bad"}
{"abstract": "Inductive Logic Programming (ILP) is a Machine Learning research field that has been quite successful in knowledge discovery in relational domains. ILP systems use a set of pre-classified examples (positive and negative) and prior knowledge to learn a theory in which positive examples succeed and the negative examples fail. In this paper we present a novel ILP system called April, capable of exploring several parallel strategies in distributed and shared memory machines.", "authors": ["Nuno A. Fonseca", "Fernando M. A. Silva", "Rui Camacho"], "n_citation": 0, "title": "April : An inductive logic programming system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "99b3bbb4-d7a2-48c4-9b17-126b92e5fcb7"}
{"abstract": "In this paper, a novel spatial-temporal position prediction motion-compensated interpolation method (MCI) for frame rate up-conversion is proposed using the transmitted Motion Vectors (MVs). Based on our previous proposed GMPP algorithm, the new method uses the motion vectors correction (MVC) first. Then joint spatial-temporal position prediction algorithm is applied on the transmitted MVs to predict more accurately the positions the interpolated blocks really move to, which makes the MVs used for interpolation more nearer to the true motion. Then the weighted-adaptive spatial-temporal MCI algorithm is used to complete the final interpolation. Applied to the H.264 decoder, the new proposed method can achieve significant increase on PSNR and obvious decrease of the block artifacts, which can be widely used in video streaming and distributed video coding applications.", "authors": ["Jianning Zhang", "Lifeng Sun", "Yuzhuo Zhong"], "n_citation": 0, "title": "A Novel Spatial-Temporal Position Prediction Motion-Compensated Interpolation for Frame Rate Up-Conversion", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "99ed6e5e-16ed-489a-bf6e-a20c846767d9"}
{"abstract": "Locally linear embedding (LLE) is one of the methods intended for dimensionality reduction, which relates to the number K of nearest-neighbors points to be initially chosen. So, in this paper, we want that the parameter K has little influence on the dimension reduction, that is to say, the parameter K can be widely chosen while not influence the effect of dimension reduction. Therefore, we propose a method of improved LLE, which uses new distance computing for weight of K nearest-neighbors points in LLE. Thus, even when the number K is little, the improved LLE can get good results of dimension reduction, while the traditional LLE needs a larger number of K to get the same results. When the number K of the nearest neighbors gets larger, test in this paper has proved that the improved LLE can still get correct results.", "authors": ["Heyong Wang", "Jie Zheng", "Zheng-an Yao", "Lei Li"], "n_citation": 50, "title": "Improved Locally Linear Embedding Through New Distance Computing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9a38e087-bc05-48e5-85dc-1656ce0808d8"}
{"abstract": "Radio Frequency Identification (RFID) system has been studied so much and it may be applicable to various fields. RFID system, however, still has consumer privacy problems under the limitation of low-cost tag implementation. We propose an efficient privacy protection scheme using two hash functions in the tag. We show that our scheme satisfies not only privacy and location history protection of consumers, but also scalability and flexibility of back-end servers. Additionally, we present a practical example to compare performance of several schemes.", "authors": ["Sang-Soo Yeo", "Sung Kwon Kim"], "n_citation": 0, "title": "Scalable and flexible privacy protection scheme for RFID systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9a601451-0903-45a0-b65c-19867935add6"}
{"abstract": "We describe the Zinc modelling language. Zinc provides set constraints, user defined types, constrained types, and polymorphic predicates and functions. The last allows Zinc to be readily extended to different application domains by user-defined libraries. Zinc is designed to support a modelling methodology in which the same conceptual model can be automatically mapped into different design models, thus allowing modellers to easily plug and play with different solving techniques and so choose the most appropriate for that problem.", "authors": ["Maria J. Garc\u00eda de la Banda", "Kim Marriott", "Reza Rafeh", "Mark Wallace"], "n_citation": 0, "title": "The modelling language zinc", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9a7069dc-0f16-4add-8a3d-a3480b792943"}
{"abstract": "The paper introduces the notion of freely completable partial solutions to characterize constraint satisfaction problems that have components which are relatively easy to solve and are only loosely connected to the remaining parts of the problem. Discovering such partial solutions during the solution process can result in strongly pruned search trees. We give a general definition of freely completable partial solutions, and then apply it to resource-constrained project scheduling. In this domain, we suggest a heuristic algorithm that is able to construct freely completable partial schedules. The method - together with symmetry breaking applied before search - has been successfully tested on real-life resource-constrained project scheduling problems containing up to 2000 tasks.", "authors": ["Andr\u00e1s Kov\u00e1cs", "J\u00f3zsef V\u00e1ncza"], "n_citation": 0, "title": "Completable partial solutions in constraint programming and constraint-based scheduling", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9bfaaeda-23fb-4c93-a603-cb3f7365b513"}
{"abstract": "To gain access to account privileges, an intruder masquerades as the proper account user. Information from user feedback helps to improve the accuracy of classifiers used for detecting masquerades. Instead of operating in isolation, the online sequential classifier can request feedback from the user. In the full-feedback policy, the classifier verifies every session; in the feedback-on-alarm policy, the classifier confirms only suspicious sessions. Surprisingly, confirming only a few sessions under the feedback-on-alarm policy is enough to be competitive with verifying all sessions under the full-feedback policy. Experiments on a standard artificial dataset demonstrate that the naive-Bayes classifier boosted by the feedback-on-alarm policy beats the previous best-performing detector and reduces the number of missing alarms by 30%.", "authors": ["Kwong H. Yung"], "n_citation": 0, "title": "Using feedback to improve masquerade detection", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9c1e1d6c-01fa-4146-b6a8-6b2943dccac3"}
{"abstract": "A method is proposed in the present paper for supporting the discovery of causal knowledge by finding causal sentences from a text and chaining them by the operation of our system. The operation of our system called ACkdT relies on the search for sentences containing appropriate natural language phrases. The system consists of two main subsystems. The first subsystem achieves the extraction of knowledge from individual sentences that is similar to traditional information extraction from texts while the second subsystem is based on a causal reasoning process that generates new knowledge by combining knowledge extracted by the first subsystem. In order to speed up the whole knowledge acquisition process a search algorithm is applied on a table of combinations of keywords characterizing the sentences of the text. Our knowledge discovery method is based on the use of our knowledge representation independent method ARISTA that accomplishes causal reasoning on the fly directly from text. The application of the method is demonstrated by the use of two examples. The first example concerns pneumonology and is found in a textbook and the second concerns cell apoptosis and is compiled from a collection of MEDLINE paper abstracts related to the recent proposal of a mathematical model of apoptosis.", "authors": ["John Kontos", "Areti Elmaoglou", "Ioanna Malagardi"], "n_citation": 0, "title": "ARISTA causal knowledge discovery from texts", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9c26d51d-b1f9-46fd-ab25-0621442e3cae"}
{"abstract": "Support Vector Machines is a new and promising technique in statistical learning theory. Recently, this technique produced very interesting results in pattern recognition [1,2,3]. In this paper, one of the first application of Support Vector Machines (SVM) technique for the problem of keyword spotting is presented. It classifies the correct and the incorrect keywords by using linear and Radial Basis Function kernels. This is a first work proposed to use SVM in keyword spotting, in order to improve recognition and rejection accuracy. The obtained results are very promising.", "authors": ["Yassine Ben Ayed", "Dominique Fohr", "Jean Paul Haton", "G\u00e9rard Chollet"], "n_citation": 0, "title": "Keyword spotting using Support Vector Machines", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "9c8d72f9-b318-4b03-8a92-a2bc0f60cbed"}
{"abstract": "The note reports on the current status of an implementation of a rule-based negotiation mechanism in a model e-commerce multi-agent system. Here, we briefly describe the conceptual architecture of the system and its initial implementation utilizing JADE and JESS. A particular negotiation scenario involving English auctions performed in parallel is also discussed.", "authors": ["Costin Badica", "Adriana Badita", "Maria Ganzha", "Alin Iordache", "Marcin Paprzycki"], "n_citation": 0, "title": "Rule-based framework for automated negotiation : Initial implementation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9cbff0f8-607c-4715-8d75-9d4760e5e768"}
{"abstract": "We study the quality of LP-based approximation methods for pure combinatorial problems. We found that the quality of the LP-relaxation is a direct function of the underlying constrainedness of the combinatorial problem. More specifically, we identify a novel phase transition phenomenon in the solution integrality of the relaxation. The solution quality of approximation schemes degrades substantially near phase transition boundaries. Our findings are consistent over a range of LP-based approximation schemes. We also provide results on the extent to which LP relaxations can provide a global perspective of the search space and therefore be used as a heuristic to guide a complete solver.", "authors": ["Lucian Leahu", "Carla P. Gomes"], "n_citation": 0, "title": "Quality of LP-based approximations for highly combinatorial problems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9cf936f9-756c-4786-9f1e-cb9f94685b9e"}
{"abstract": "A new type of neutral neural network (NNN) model is established and the corresponding stability analysis is studied in this paper. By introducing the neutral term into the classical neural network model, the inspiration and associate memory phenomenon can be well described and explained. The stochastic Hopfield NNN model (HNNN) is investigated, respectively. Some criteria for mean square exponential stability and asymptotic stable are provided.", "authors": ["Yumin Zhang", "Lei Guo", "Lingyao Wu", "Chun-Bo Feng"], "n_citation": 50, "title": "On stochastic neutral neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9d0b8afd-821e-47c0-9516-b312a413968a"}
{"abstract": "We consider action domain descriptions whose meaning can be represented by transition diagrams. We introduce several semantic measures to compare such action descriptions, based on preferences over possible states of the world and preferences over some given conditions (observations, assertions, etc.) about the domain, as well as the probabilities of possible transitions. This preference information is used to assemble a weight which is assigned to an action description. As an application of this approach, we study the problem of updating action descriptions with respect to some given conditions. With a semantic approach based on preferences, not only, for some problems, we get more plausible solutions, but also, for some problems without any solutions due to too strong conditions, we can identify which conditions to relax to obtain a solution. We conclude with computational issues, and characterize the complexity of computing the semantic measures.", "authors": ["Thomas Eiter", "Esra Erdem", "Michael Fink", "J\u00e1n Senko"], "n_citation": 0, "title": "Comparing action descriptions based on semantic preferences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9dccec59-5743-4941-aab1-081b911c995c"}
{"abstract": "In this paper, a new QoS model is presented for end-to-end service provisioning in wireless ad hoc networks. Many previous works focus on the packet scheduling mechanism using multiple service classes implemented for traffic prioritization based service differentiation. However, this paper concentrates on a scheme for dynamically selecting a proper one among several forwarding classes that perform different service rate according to service requirements. There service requirements include low delay, high throughput, and low loss. The proposed solution is a new QoS provisioning model called Dynamic Hop Service Differentiation (DHSD). This model supports soft QoS provisioning to reduce network overhead and wireless PHB (WPHB) to achieve the end-to-end QoS required by applications. The proposed QoS model is evaluated using OPNET simulation. We show that this model outperforms both best-effort and strict priority service models in wireless ad hoc network environments.", "authors": ["Joo-Sang Youn", "Seung-Joon Seok", "Chul-Hee Kang"], "n_citation": 0, "title": "New service differentiation model for end-to-end QoS provisioning in wireless ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9e4cae8d-eb96-4572-b663-94b4e5749859"}
{"abstract": "CAST is a coordination model designed to support interactions among agents executing on hosts that make up a mobile ad hoc network (MANET). From an application programmer's point of view, CAST makes it possible for operations to be executed at arbitrary locations in space, at prescribed times which may be in the future, and on remote hosts even when no end-to-end connected route exists between the initiator and target(s) of the operation. To accomplish this, CAST assumes that each host moves in space in accordance with a motion profile which is accurate but which at any given time extends into the future for a limited duration. These motion profiles are freely exchanged among hosts in the network through a gossiping protocol. Knowledge about the motion profiles of the other hosts in the network allows for source routing of operation requests and replies over disconnected routes. In this paper, we present the CAST model and its formalization. We also discuss the feasibility of realizing this model.", "authors": ["Gruia-Catalin Roman", "Radu Handorean", "Rohan Sen"], "n_citation": 0, "title": "Tuple space coordination across space and time", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9e5f30bc-5cdf-428e-b87a-2981bc02ba3d"}
{"abstract": "Sensor nets have many undisputed fields of application. A paradigm of communication is the use of one control channel in the MAC layer. We challenge this paradigm for nodes with very restricted hardware resources. In our model nodes support the use of different channels and use clock synchronisation. We present a simple probabilistic synchronised channel utilisation scheme for wireless communication. The main features are its simplicity, robustness against radio interference, the high throughput caused by less interfering signals, and predictable energy consumption. For this, the channel selection is based on a carefully chosen probability distribution maximising the expected number of successfully delivered packets up to a constant factor, Combined with a standard synchronisation scheme it provides a novel energy-efficient, robust, and fast message delivery service for sensor networks where data gathering is not available due to memory restrictions.", "authors": ["Christian Schindelhauer", "Kerstin Voss"], "n_citation": 0, "title": "Probability distributions for channel utilisation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9e5fd455-bce4-482f-a037-a05be929a42a"}
{"abstract": "Lamport diagrams are partial orders which depict computations of message passing systems. It is natural to consider generalizations of linear time temporal logics over such diagrams. In [MR00], we presented a decidable temporal logic with local temporal modalities and a global 'previous' modality to talk of message receipts. It seems reasonable to extend the logic with a global 'next' modality as well, so that sending of messages may also be easily specified, but this (or other similar attempts) lead to undecidability. Hence we consider ways of restricting the models so as to obtain decidability, while retaining the expressiveness of global 'next' and global 'previous' modalities. For this, we consider Lamport diagrams presented as a sequence of layers. The layers themselves describe finite communication patterns and a diagram is obtained by sequential composition of such parallel processes. The logic is defined appropriately, with layer formulas describing processes within a layer, and temporal formulas describing the sequence of layers in the computation. When the number of events in layers is uniformly bounded and each layer is communication closed, we get decidability. Alternatively, a stronger uniform bound on what we term channel capacity also yields decidability. We present an example of system specification in the logic.", "authors": ["B. Meenakshi 0002", "Ramaswamy Ramanujam"], "n_citation": 50, "title": "Reasoning about layered message passing systems", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9eaadbf3-0eff-4973-ae9e-8a03e58e4e68"}
{"abstract": "Apart from the computer vision community, an always increasing number of scientific domains show a great interest for image analysis techniques. This interest is often guided by practical needs. As examples, we can cite all the medical imagery systems, the satellites images treatment and botanical databases. A common point of these applications is the large image collections that are generated and therefore require some automatic tools to help the scientists. These tools should allow clear structuration of the visual information and provide fast and accurate retrieval process. In the framework of the plant genes expression study we designed a content-based image retrieval (CBIR) system to assist botanists in their work. We propose a new contour-based shape descriptor that satisfies the constraints of this application (accuracy and real-time search). It is called Directional Fragment Histogram (DFH). This new descriptor has been evaluated and compared to several shape descriptors.", "authors": ["Itheri Yahiaoui", "Nicolas Herv\u00e9", "Nozha Boujemaa"], "n_citation": 0, "title": "Shape-Based Image Retrieval in Botanical Collections", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9f0c7532-0809-4205-90d8-00f5b32319e9"}
{"authors": ["Pierpaolo Degano", "Gian Luigi Ferrari", "Letterio Galletta", "Gianluca Mezzetti"], "n_citation": 0, "title": "Types for Coordinating Secure Behavioural Variations", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "9f34608d-d79d-4563-802a-ef0fe29852fc"}
{"abstract": "In this paper, some sufficient conditions of uniform stability and asymptotic stability about second order neural networks with time delays were obtained by the properties of nonnegative matrices and techniques of differential inequalities. In the results, it was assumed that the activation functions satisfy the Lipschitz condition and it was not required that the activation functions were bounded, differentiable and strictly increasing. Moreover, the symmetry of the connection matrix was not also necessary. Thus, we have improved some previous work of other researchers.", "authors": ["Jinan Pei", "Daoyi Xu", "Zhichun Yang", "Wei Zhu"], "n_citation": 0, "title": "Stability analysis of second order hopfield neural networks with time delays", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9f35cf3e-59db-4817-8121-984c20ab250d"}
{"abstract": "Pseudonym systems allow users to interact with multiple organizations anonymously by using pseudonyms. Such schemes are of significant practical relevance because it is the best means of providing privacy for users. In previous works, users transact with a organization by demonstration of possession of a credential issued by the organization or relationship with another credential. However, the information that a user has a credential from a specific organization compromises privacy of the user. In the present paper, we give a formal definition of practical pseudonym system in which the level of privacy provided can be chosen be according to security policies.", "authors": ["Yuko Tamura", "Atsuko Miyaji"], "n_citation": 0, "title": "Anonymity-enhanced pseudonym system", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9fc2b2f6-62e8-49c4-ad7a-ba2a39f8825d"}
{"abstract": "Implementations of cryptographic protocols, such as OpenSSL for example, contain bugs affecting security, which cannot be detected by just analyzing abstract protocols (e.g., SSL or TLS). We describe how cryptographic protocol verification techniques based on solving clause sets can be applied to detect vulnerabilities of C programs in the Dolev-Yao model, statically. This involves integrating fairly simple pointer analysis techniques with an analysis of which messages an external intruder may collect and forge. This also involves relating concrete run-time data with abstract, logical terms representing messages. To this end, we make use of so-called trust assertions. The output of the analysis is a set of clauses in the decidable class H 1 , which can then be solved independently. This can be used to establish secrecy properties, and to detect some other bugs.", "authors": ["Jean Goubault-Larrecq", "Fabrice Parrennes"], "n_citation": 0, "title": "Cryptographic protocol analysis on real C code", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a01f5055-2c4b-44e3-9bed-45976fbbe2f1"}
{"authors": ["Ramesh Jain"], "n_citation": 50, "title": "Digital experience", "venue": "Communications of The ACM", "year": 2001, "id": "a03a52c3-3f78-4255-9bc1-9dc0cc265f38"}
{"abstract": "Among backtracking based algorithms for constraint satisfaction problems (CSPs), algorithms employing constraint propagation, like forward checking (FC) and MAC, have had the most practical impact. These algorithms use constraint propagation during search to prune inconsistent values from the domains of the uninstantiated variables. In this paper we present a general approach to extending constraint propagating algorithms, especially forward checking. In particular, we provide a simple yet flexible mechanism for pruning domain values, and show that with this in place it becomes easy to utilize new mechanisms for detecting inconsistent values during search. This leads to a powerful and uniform technique for designing new CSP algorithms: one simply need design new methods for detecting inconsistent values and then interface them with the domain pruning mechanism. Furthermore, we also show that algorithms following this design can proved to be correct in a simple and uniform way. To demonstrate the utility of these ideas five new CSP algorithms are presented.", "authors": ["Fahiem Bacchus"], "n_citation": 0, "title": "Extending forward checking", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "a0482fd6-4949-480a-a3d3-a3f3861e41aa"}
{"abstract": "In this contribution we transfer a customer purchase incidence model for consumer products which is based on Ehrenberg's repeat-buying theory to Web-based information products. Ehrenberg's repeat-buying theory successfully describes regularities on a large number of consumer product markets. We show that these regularities exist in electronic markets for information goods, too, and that purchase incidence models provide a well founded theoretical base for recommender and alert services. The article consists of two parts. In the first part Ehrenberg's repeat-buying theory and its assumptions are reviewed and adapted for web-based information markets. Second, we present the empirical validation of the model based on data collected from the information market of the Virtual University of the Vienna University of Economics and Business Administration from September 1999 to May 2001.", "authors": ["Andreas Geyer-Schulz", "Michael Hahsler", "Maximillian Jahn"], "n_citation": 0, "title": "A customer purchase incidence model applied to recommender services", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a06dc5bd-e9ea-4cee-8918-8948665e2875"}
{"abstract": "Feed forward Neural Network (FNN) has been widely applied to many fields because of its ability to closely approximate unknown function to any degree of desired accuracy. Gradient techniques, for instance, Back Propagation (BP) algorithm, are the most general learning algorithms. Since these techniques are essentially local optimization algorithms, they are subject to converging at the local optimal solutions and thus perform poorly even on simple problems when forecasting out of samples. Consequently, we presented an adaptive Tabu Search (TS) approach as a possible alternative to the problematical BP algorithm, which included a novel adaptive search strategy of intensification and diversification that was used to improve the efficiency of the general TS. Taking the classical XOR problem and function approximation as examples, a compare investigation was implemented. The experiment results show that TS algorithm has obviously superior convergence rate and convergence precision compared with other BP algorithms.", "authors": ["Yi He", "Yuhui Qiu", "Guangyuan Liu", "Kaiyou Lei"], "n_citation": 0, "title": "Optimizing weights of neural network using an adaptive tabu search approach", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a0b77692-5ec7-45e1-a910-8aaceb22b3de"}
{"abstract": "Process algebraic specifications can provide useful support for the architectural design of software systems due to the possibility of analyzing their properties. In addition to that, such specifications can be exploited to guide the generation of code. What is needed at this level is a general methodology that accompanies the translation process, which in particular should help understanding whether and when it is more appropriate to implement a software component as a thread or as a monitor. The objective of this paper is to develop a systematic approach to the synthesis of correctly coordinating monitors from arbitrary process algebraic specifications that satisfy some suitable constraints. The whole approach will be illustrated by means of the process algebraic specification of a cruise control system.", "authors": ["Edoardo Bont\u00e0", "Marco Bernardo", "Jeff Magee", "Jeff Kramer"], "n_citation": 0, "title": "Synthesizing concurrency control components from process algebraic specifications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a0d5dd1d-523e-44c3-9ce3-3947894120ea"}
{"abstract": "Counting the number of solutions to CSP instances has applications in several areas, ranging from statistical physics to artificial intelligence. We give an algorithm for counting the number of solutions to binary CSPs, which works by transforming the problem into a number of 2-SAT instances, where the total number of solutions to these instances is the same as those of the original problem. The algorithm consists of two main cases, depending on whether the domain size d is even, in which case the algorithm runs in O(1.3247 n  . (d/2) n ) time, or odd, in which case it runs in O(1.3247 n  . ((d 2  + d + 2)/4) n/2 ) if d = 4 . k + 1, and O(1.3247 n  . ((d 2  + d)/4) n/2 ) if d = 4 . k + 3. We also give an algorithm for counting the number of possible 3-colourings of a given graph, which runs in O(1.8171 n ), an improvement over our general algorithm gained by using problem specific knowledge.", "authors": ["Ola Angelsmark", "Peter Jonsson", "Svante Linusson", "Johan Thapper"], "n_citation": 50, "title": "Determining the number of solutions to binary CSP instances", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a1426edf-51e0-47cf-bcb7-ad4f9c0b0e33"}
{"abstract": "Capturing constraint structure is critical in Constraint Programming to support the configuration and adaptation of domain filtering algorithms. To this end, we propose a software model coupling a relational constraint language, a constraint type inference system, and an algorithm configuration system. The relational language allows for expressing constraints from primitive constraints; the type system infers the type of constraint expressions out of primitive constraint types; and the configuration system synthesises algorithms out of primitive routines using constraint types. In this paper, we focus on the issue of constraint type inferencing, and present a method to implement sound and extendible inference systems.", "authors": ["David Lesaint"], "n_citation": 0, "title": "Inferring constraint types in Constraint Programming", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a1f221e7-9a6e-4ebf-ba40-c783a34a9c23"}
{"abstract": "Discourse in formal domains, such as mathematics, is characterized by a mixture of natural language and embedded formal expressions. Based on an investigation of a collected corpus of informal dialogues on naive set theory proofs, we are developing a dependency-based lexicalist grammar for parsing input with different degrees of verbalization of the mathematical content: ranging from symbolic alone to fully worded mathematical expressions. In this paper, we describe our approach to analysis, focusing on the underlying semantic representations.", "authors": ["Magdalena Wolska", "Ivana Kruijff-Korbayov\u00e1"], "n_citation": 0, "title": "Building a dependency-based grammar for parsing informal mathematical discourse", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a27d909e-1ec1-40ff-8f56-6b964cf0b050"}
{"abstract": "This paper presents a dynamic bandwidth allocation scheme for new and handoff voice calls in cellular networks. The main idea of the new scheme is based on monitoring the elapsed real time of handoff calls and according to both a time threshold (t e ) and a dynamically changing bandwidth threshold (B t ) parameters, a handoff call is either prioritized or treated as a new call. Also in this paper, we introduce a crucial general performance metric Z that can be used to measure the performance of different bandwidth allocation schemes and compare them. Z, which is a performance/cost ratio, is a function of the new call blocking probability, handoff call dropping probability and system utilization all together. The results indicate that our scheme outperforms other traditional schemes in terms of performance/cost ratio, and maintains its superiority under different network circumstances.", "authors": ["Idil Candan", "Muhammed Salamah"], "n_citation": 0, "title": "Dynamic time-threshold based scheme for voice calls in cellular networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a320b1ce-51be-4caf-9c5d-2dcd0097c7f1"}
{"abstract": "Rapid development of wireless networks leads to unceasing appearance of new techniques and methods. Recently cooperative relaying emerged as viable and tempting option for future wireless networks. Cooperative transmission attempts to provide spatial diversity for hardware devices that have only one antenna through the joint use of antennas that belong to several nodes. An overview of approaches in this area is presented. For proposed coded cooperative strategy, regions for which cooperation results in a gain in Frame Error Rate (FER) are analyzed.", "authors": ["Vladimir Bashun", "Konstantin Dubkov"], "n_citation": 50, "title": "Cooperative transmission through incremental redundancy", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a383b0f3-7504-4af5-a4ec-2bb942523374"}
{"abstract": "The training of feed-forward Neural Networks (NNs) by back-propagation (BP) is much time-consuming and complex task of great importance. To overcome this problem, we apply Genetic Algorithm (GA) to determine parameters of NN automatically and propose a efficient GA which reduces its iterative computation time for enhancing the training capacity of NN. Proposed GA is based on steady-state model among continuous generation model and used the modified tournament selection, as well as special survival condition. To show the validity of the proposed method, we compare with conventional and the survival-based GA using mathematical optimization problems and set covering problem. In addition, we estimate the performance of training the layered feedforward NN with GA and BP.", "authors": ["Dong-Sun Kim", "Hyunsik Kim", "Duck-Jin Chung"], "n_citation": 50, "title": "A modified genetic algorithm for fast training neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a4383f88-ec2d-4521-9f02-bab5f9e7df5b"}
{"abstract": "In this paper, we propose a nonlinear 2-measure concept, and using it, together with the nonlinear 1-measure proposed earlier by other researchers, to analyze the global convergence of genetic regulatory networks. Two sufficient conditions are derived to ensure globally exponentially convergence of solutions of the networks. The derived conditions provide insight into the dynamical behavior of gene networks.", "authors": ["Hongtao Lu", "Zhizhou Zhang", "Lin He"], "n_citation": 0, "title": "Convergence analysis of genetic regulatory networks based on nonlinear measures", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a5fc137b-4d3b-457b-ac18-c356bb593b69"}
{"abstract": "How to coordinate the processes in a complex component-based software system is a nontrivial issue. Many different coordination approaches exist, each with its own specific advantages and drawbacks. To support their mutual comparison, this paper proposes a formal methodology to automatically evaluate the performance of coordination approaches. This methodology comprises (1) creation of simulation models of coordination approaches, (2) execution of simulation experiments of these models applied to test examples, and (3) automated evaluation of the models against specified requirements. Moreover, in a specific case study, the methodology is used to evaluate some coordination approaches that originate from various disciplines.", "authors": ["Tibor Bosse", "Mark Hoogendoorn", "Jan Treur"], "n_citation": 50, "title": "Automated evaluation of coordination approaches", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a5fce6bc-ce0a-4cff-a45e-71a728935a1f"}
{"abstract": "E-action rules, introduced in [8], represent actionability knowledge hidden in a decision system. They enhance action rules [3] and extended action rules [4]. [6], [7] by assuming that data can be either symbolic or nominal. Several efficient strategies for mining e-action rules have been developed [6], [7], [5], and [8]. All of them assume that data are complete. Clearly, this constraint has to be relaxed since information about attribute values for some objects can be missing or represented as multi-values. To solve this problem, we present DEAR_3 which is an e-action rule generating algorithm. It has three major improvements in comparison to DEAR-2: handling data with missing attribute values and uncertain attribute values, and pruning outliers at its earlier stage.", "authors": ["Li-Shiang Tsay", "Zbigniew W. Ras"], "n_citation": 50, "title": "Action Rules Discovery System DEAR_3", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a6c97ae7-f63c-4c54-83b2-ef78b18876d8"}
{"abstract": "This paper describes the Ephyra question answering engine, a modular and extensible framework that allows to integrate multiple approaches to question answer-ing in one system. Our framework can be adapted to languages other than English by replacing language-specific components. It supports the two major approaches to question answering, knowledge annotation and knowledge mining. Ephyra uses the web as a data resource, but could also work with smaller corpora. In addition, we propose a novel approach to question interpretation which abstracts from the original formulation of the question. Text patterns are used to interpret a question and to extract answers from text snippets. Our system automatically learns the patterns for answer extraction, using question-answer pairs as training data. Experimental results revealed the potential of this approach.", "authors": ["Nico Schlaefer", "Petra Gieselmann", "Thomas Schaaf", "Alex Waibel"], "n_citation": 0, "title": "A pattern learning approach to question answering within the ephyra framework", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a7208a75-1527-4a31-9f01-e3e56e1c561e"}
{"abstract": "The paper reviews two basic time-frequency distributions, spectrogram and cone-shaped kernel distribution. We study, analyze and compare properties and performance of these quadratic representations on speech signals. Cone-shaped kernel distribution was successfully applied to speech features extraction due to several useful properties in time-frequency analysis of speech signals.", "authors": ["Janez Zibert", "France Mihelic", "Nikola Pavesic"], "n_citation": 0, "title": "Speech features extraction using cone-shaped kernel distribution", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a73fb40f-0ca8-433d-846b-c46cbe85604e"}
{"abstract": "Principle Component Analysis (PCA) technique is an important and well-developed area of image recognition and to date many linear discrimination methods have been put forward. Despite these efforts, there persist in the traditional PCA some weaknesses. In this paper, we propose a new PCA-based method that can overcome one drawback existed in the traditional PCA method. In face recognition where the training data are labeled, a projection is often required to emphasize the discrimination between the clusters. PCA may fail to accomplish this, no matter how easy the task is, as they are unsupervised techniques. The directions that maximize the scatter of the data might not be as adequate to discriminate between clusters. So we proposed a new PCA-based scheme which can straightforwardly take into consideration data labeling, and makes the performance of recognition system better. Experiment results show our method achieves better performance in comparison with the traditional PCA method.", "authors": ["Vo Dinh Minh Nhat", "Sungyoung Lee"], "n_citation": 50, "title": "An improvement on PCA algorithm for face recognition", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "a7527ac4-c320-4a7e-91d1-a3f2834160ef"}
{"abstract": "We propose a relaxation of zero-knowledge, by allowing the simulator to run in quasi-polynomial time. We show that protocols satisfying this notion can be constructed in settings where the standard definition is too restrictive. Specifically, we construct constant-round straight-line concurrent quasi-polynomial time simulatable arguments and show that such arguments can be used in advanced composition operations without any set-up assumptions. Our protocols rely on slightly strong, but standard type assumptions (namely the existence of one-to-one one-way functions secure against subexponential circuits).", "authors": ["Rafael Pass"], "n_citation": 144, "title": "Simulation in quasi-polynomial time, and its application to protocol composition", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "a7da5b11-1303-4390-8e2f-0b132962c993"}
{"abstract": "In this paper competitive learning cluster are used for molecular data of large size sets. The competitive learning network can cluster the input data, it only adapts to the node of winner, the winning node is more likely to win the competition again when a similar input is presented, thus similar inputs are clustered into same a class and dissimilar inputs are clustered into different classes. The experimental results show that the competitive learning network has a good clustering reproducible, indicates the effectiveness of clusters for molecular data, the conscience learning algorithm can effectively cancel the dead nodes when the output nodes increasing, the kinds of network indicates the effectiveness of clusters for molecular data of large size sets.", "authors": ["Lin Wang", "Minghu Jiang", "Yinghua Lu", "Frank No\u00e9", "Jeremy C. Smith"], "n_citation": 0, "title": "Clustering Analysis of Competitive Learning Network for Molecular Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a82a7ed5-cb8d-4cb4-920d-e08e7db3bc7c"}
{"abstract": "This paper describes the combination of a well-known TSP heuristic, GENIUS, with a constraint programming model for routing problems. The result, GENIUS-CP, is an efficient heuristic single-vehicle routing algorithm which is generic in the sense that it can solve problems from many different contexts, each with its particular type(s) of constraints. The heuristic quickly constructs high-quality solutions while the constraint model provides great flexibility as to the nature of the problem constraints involved by relieving that heuristic of all constraint satisfaction concerns. We show how those two components are integrated in a clean way with a well-defined, minimal interface. We also describe different routing problems on which this algorithm can be applied and evaluate its performance.", "authors": ["Gilles Pesant", "Michel Gendreau", "Jean-Marc Rousseau"], "n_citation": 0, "title": "GENIUS-CP : A generic single-vehicle routing algorithm", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "a8324b25-2a95-4121-925c-236c1987fb8e"}
{"abstract": "Several researchers have proposed the use of threshold cryptographic model to enable secure communication in ad hoc networks without the need of a trusted center. In this model, the system remains secure even in the presence of a certain threshold t of corrupted/malicious nodes. In this paper, we show how to perform necessary public key operations without node-specific certificates in ad hoc networks. These operations include pair-wise key establishment, signing, and encryption. We achieve this by using Feldman's verifiable polynomial secret sharing (VSS) as a key distribution scheme and treating the secret shares as the private keys. Unlike in the standard public key cryptography, where entities have independent private/public key pairs, in the proposed scheme the private keys are related (they are points on a polynomial of degree t) and each public key can be computed from the public VSS information and node identifier. We show that such related keys can still be securely used for standard signature and encryption operations (using resp. Schnorr signatures and ElGamal encryption) and for pairwise key establishment, as long as there are no more that t collusions/corruptions in the system. The proposed usage of shares as private keys can also be viewed as a threshold-tolerant identity-based cryptosystem under standard (discrete logarithm based) assumptions.", "authors": ["Nitesh Saxena"], "n_citation": 50, "title": "Public key cryptography sans certificates in ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a9346a9c-43f2-4d32-b1a4-1ffdbb135b9b"}
{"abstract": "The hybridisation of systematic and stochastic search is an active research area with potential benefits for real-world combinatorial problems. This paper shows that randomising the backtracking component of a systematic backtracker can improve its scalability to equal that of stochastic local search. The hybrid may be viewed as stochastic local search in a constrained space, cleanly combining local search with constraint programming techniques. The approach is applied to two very different problems. Firstly a hybrid of local search and constraint propagation is applied to hard random 3-SAT problems, and is the first constructive search algorithm to solve very large instances. Secondly a hybrid of local search and branch-and-bound is applied to low-autocorrelation binary sequences (a notoriously difficult communications engineering problem), and is the first stochastic search algorithm to find optimal solutions. These results show that the approach is a promising one for both constraint satisfaction and optimisation problems.", "authors": ["Steven David Prestwich"], "n_citation": 0, "title": "A hybrid search architecture applied to hard random 3-SAT and low-autocorrelation binary sequences", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "a94c131c-ea3a-4119-9de8-7dda24d2e66b"}
{"authors": ["Chris K\u00f6cher", "Dietrich Kuske", "Pascal Weil"], "n_citation": 0, "references": ["20a69de0-12ec-40bd-a76a-4a6cffbcba10", "2b7f46ea-f44a-445e-8804-648c8b6f47ba", "2d9d022e-5ac4-4965-b4ff-ae80b6ab38be", "47be856d-1f7e-4d97-affd-b11ea78b4a9f", "51a61a03-62bb-418e-a1a9-fa7208431664", "5c012fda-c4f3-4878-9873-cc80caf71c84", "6281101c-bbd2-4510-8454-fd7757f593cd", "7914a8d8-005e-458a-ac26-9b919d99da8c", "9bcb0b2f-5f78-43ea-ac47-12162deee73a", "f8e56d69-4589-4965-9b2f-36765fa2bed2"], "title": "The Transformation Monoid of a Partially Lossy Queue", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "a967a1a5-23a5-4d6f-9df1-c9fa3d7a6749"}
{"abstract": "This work is a contribution to realizing prioritized reasoning in logic programming in the presence of preference relations involving atoms. In more details, the case of dynamic preferences is investigated and a semantics interpreting each preference rule as a tool for representing a choice over alternative options is proposed. The technique, providing a new interpretation for prioritized logic programs, is inspired by the one proposed by Sakama and Inoue in [19] and enriched with the use of structural information of preference rules as proposed by Brewka et al. in [6]. Specifically, the analysis of the logic program is carried out together with the analysis of preferences in order to determine the choice order and the sets of comparable models. The proposed approach is compared with those in [6,19]. Complexity analysis is also performed showing that the use of additional information does not increase the complexity of computing preferred stable models.", "authors": ["Sergio Greco", "Irina Trubitsyna", "Ester Zumpano"], "n_citation": 0, "title": "On the semantics of logic programs with preferences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa2bd9e7-0ec2-41d6-a035-4e4639a8f5f6"}
{"abstract": "MEDLINE is a widely used very large database of abstracts of research papers in medical domain. Abstracts in it are manually supplied with keywords from a controlled vocabulary called MeSH. The MeSH keywords assigned to a specific document are subdivided into MeSH major headings, which express the main topic of the document, and MeSH minor headings, which express additional information about the document's topic. The search engine supplied with MEDLINE uses Boolean retrieval model with only MeSH keywords used for indexing. We show that (I) vector space retrieval model with the full text of the abstracts indexed gives much better results; (2) assigning greater weights to the MeSH keywords than to the terms appearing in the text of the abstracts gives slightly better results, and (3) assigning slightly greater weight to major MeSH terms than to minor MeSH terms further improves the results.", "authors": ["Kwangcheol Shin", "Sangyong Han", "Alexander F. Gelbukh"], "n_citation": 0, "title": "Balancing manual and automatic indexing for retrieval of paper abstracts", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "aa30d812-50a2-4b67-8e0b-4d75c1970728"}
{"abstract": "The Bluetooth wireless technology realizes a low-cost short-range wireless voice- and data-connection through radio propagation. Bluetooth also has a security architecture. In this paper, we focus on the key agreement protocol, which is the most critical part of this security architecture. Several security flaws have been identified within the Bluetooth protocols: an attacker can track users by monitoring the Bluetooth hardware address, all keys depend on a low-entropy shared secret (the PIN), there are some very easy to perform Denial of Service attacks. We propose a new initialization mechanism for the key agreement protocol of Bluetooth. This improved pairing protocol can be easily extended so that it will not only solve the dependency of the keys on the PIN, but also the location privacy problem and an important Denial of Service attack. Our solution is user friendly and energy-efficient, two essential features for Wireless Personal Area Networks (WPAN).", "authors": ["Dave Singel\u00e9e", "Bart Preneel"], "n_citation": 0, "title": "Improved Pairing Protocol for Bluetooth", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa45c3cb-50f0-4f9c-b1e4-10883fbd94ec"}
{"abstract": "Unwinding conditions are helpful to prove that deterministic systems fulfill non-interference. In order to generalize non-interference to non-deterministic systems various possibilistic security properties have been proposed. In this paper, we present generic unwinding conditions which are applicable to a large class of such security properties. That these conditions are sufficient to ensure security is demonstrated by unwinding theorems. In certain cases they are also necessary. The practical usefulness of our results is illustrated by instantiating the generic unwinding conditions for well-known security properties. Furthermore, similarities of proving security with proving refinement are identified which results in proof techniques which are correct as well as complete.", "authors": ["Heiko Mantel"], "n_citation": 0, "title": "Unwinding possibilistic security properties", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "aa6a3a86-45b8-4652-b984-bd4a70c8ebcf"}
{"abstract": "This paper presents a hybrid method developed at France Telecom R&D to solve a difficult network problem. It takes place in an ATM network administration context and consists in planning connection demands over a period of one year. We introduce a new framework for solving this problem within the allowed computing time. This framework is based on two major elements: first a hybrid method which mixes shortest path algorithms, constraint propagation and repairing principles, then a model for the time dimension which is a critical issue in this ATM network administration problem. We compare our method with a greedy method (without rerouting) presently used in FTR&D upon realistic problems. The results of our experiments show that the difficult problem of rerouting can be solved with our method. Moreover, rerouting leads to accept of 46% of connections that are rejected with the greedy algorithm. This paper is a revised version of [14].", "authors": ["Muriel Lauvergne", "Philippe David", "Patrice Boizumault"], "n_citation": 0, "title": "Connections reservation with rerouting for ATM networks: A hybrid approach with constraints", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "aa779fcb-6de1-4087-ad27-5c91653ace58"}
{"authors": ["P. H. S. Torr", "Andrew Zisserman"], "n_citation": 0, "title": "Feature based methods for structure and motion estimation", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "aad75d24-36c5-4155-8e23-9f6dc9ca48c2"}
{"abstract": "Non-linear real constraint systems with universally and/or existentially quantified variables often need be solved in such contexts as s control design or sensor planning. To date, these systems are mostly handled by computing a quantifier-free equivalent form by means of Cylindrical Algebraic Decomposition (CAD). However, CAD restricts its input to be conjunctions and disjunctions of polynomial constraints with rational coefficients, while some applications such as camera control involve systems with arbitrary forms where time is the only universally quantified variable. In this paper, the handling of universally quantified variables is first related to the computation of inner-approximation of real relations. Algorithms for solving non-linear real constraint systems with universally quantified variables are then presented along with the theoretical framework on inner-approximation of relations supporting them. These algorithms are based on the computation of outer-approximations of the solution set of the negation of involved constraints. An application to the devising of a declarative modeller for expressing camera motion using a cinematic language is sketched, and results from a prototype are presented.", "authors": ["Fr\u00e9d\u00e9ric Benhamou", "Fr\u00e9d\u00e9ric Goualard"], "n_citation": 113, "title": "Universally quantified interval constraints", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "ab50fab9-a871-496a-9c23-47c6937a1172"}
{"authors": ["Alex Ferguson", "Barry O'Sullivan"], "n_citation": 0, "title": "Relaxations and explanations for quantified constraint satisfaction problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "abd9b8cc-38e0-4d90-967a-3a61a879507a"}
{"abstract": "Speech/non-speech (S/NS) detection plays the important role for automatic speech recognition (ASR) system, especially in the case of isolated words or commands recognition. Even in continuous speech a S/NS decision can be made at the beginning and at the end of a sequence resulting in a sleep mode of the speech recognizer during the silence and in a reduction of computation demands. It is very difficult, however, to precisely locate the endpoints of the input utterance because of unpredictable background noise. In the proposed method in this paper, we make use of the advantages of two approaches (i.e. to try to find the best set of heuristic features and apply a statistical induction method) for the best S/NS decision.", "authors": ["M. Prc\u00edn", "L. M\u00fcller"], "n_citation": 0, "title": "Heuristic and statistical methods for speech/non-speech detector design", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "abf7fea2-7ebd-4b25-bec9-1c8c1f7a49ce"}
{"abstract": "The Dendritic Cell algorithm (DCA) is inspired by recent work in innate immunity. In this paper a formal description of the DCA is given. The DCA is described in detail, and its use as an anomaly detector is illustrated within the context of computer security. A port scan detection task is performed to substantiate the influence of signal selection on the behaviour of the algorithm. Experimental results provide a comparison of differing input signal mappings.", "authors": ["Julie Greensmith", "Uwe Aickelin", "Jamie Twycross"], "n_citation": 0, "title": "Articulation and clarification of the dendritic cell algorithm", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ac3ae1a2-6c97-4898-a70a-2133b806fca6"}
{"abstract": "We propose a general scheme for the cooperation of different constraint solvers. On top of a uniform interface for constraint we stepwise develop reduction systems which describe the behaviour of an overall combined system. The modularity of our definitions of reduction relations at different levels allows the definition of cooperation strategies for the solvers according to the current requirements such that our overall system forms a general framework for cooperating solvers.", "authors": ["Petra Hofstedt"], "n_citation": 0, "title": "Cooperating constraint solvers", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "ace2aa8d-9fc5-4697-9162-aa4c8f01d860"}
{"abstract": "Traditional semantic models are based on entity notations provided by several links. Links are established to capture semantic detail about relationships among concepts. The ability to describe a process in a clear and sufficiently rich way is acknowledged as crucial to conceptual modelling. Current workflow models used in business process re-engineering offer limited analytical capabilities. Entity-Relationship models and Data Flow Diagrams are closer to the technical system development stage and, therefore, they do not capture organisational aspects. Although object-oriented models are quite comprehensible for users, they are not provided by rules of reasoning and complete integration between static and dynamic diagrams. The ultimate objective of this paper is to introduce principles of integration for different classes of semantic and pragmatic representations.", "authors": ["Remigijus Gustas"], "n_citation": 0, "title": "Integrated approach for modelling of semantic and pragmatic dependencies of information systems", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "ad516665-88f2-49f0-bdf5-d04dcb60cc9c"}
{"abstract": "This paper analyzes the behavior of Hopfield networks as a method of solving the travelling salesman problem (TSP) with an enhanced formulation of energy function, which is more available than the Hopfield-Tank (H-T) one. The analysis is based on the geometry of the subspace set up by the degenerate eigenvalues of the connection matrix. A set of criterion for parameter settings is derived. The new parameters performed well in the simulations.", "authors": ["Hong Qu", "Zhang Yi", "Xiaolin Xiang"], "n_citation": 0, "title": "Theoretical analysis and parameter setting of hopfield neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ae047889-70fe-491c-8a6b-aff223513ddb"}
{"abstract": "A novel sweep-based transiently chaotic neural network (STCNN) algorithm for Capacity Vehicle Routing Problem (CVRP) is proposed. CVRP can be partitioned into two kinds of decisions: the selection of vehicles among the available vehicles and the routing of the selected fleet. The proposed algorithm first generates a number of routes that are served by vehicles and then solves the routes to optimality. Simulations show that the proposed algorithm produces excellent solutions in short computing times.", "authors": ["Huali Sun", "Jianying Xie", "Yaofeng Xue"], "n_citation": 50, "title": "A sweep-based TCNN algorithm for capacity vehicle routing problem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ae12f30f-85e3-4c5b-bf32-fb0c2b5f0286"}
{"abstract": "The task of pattern recognition is a task of division of a feature space into regions separating the training examples belonging to different classes. Support Vector Machines (SVM) identify the most borderline examples called support vectors and use them to determine discrimination hyperplanes (hyper curves). In this paper a pattern recognition method is proposed which represents an alternative to SVM algorithm. Support vectors are identified using selected methods of computational geometry in the original space of features i.e. not in the transformed space determined partially by the kernel function of SVM. The proposed algorithm enables usage of kernel functions. The separation task is reduced to a search for an optimal separating hyperplane or a Winner Takes All (WTA) principle is applied.", "authors": ["Marek Bundzel", "Tomas Kasanicky", "Baltaz\u00e1r Frankovic"], "n_citation": 0, "title": "Building Support Vector Machine Alternative Using Algorithms of Computational Geometry", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ae253a00-4df0-4516-b7c3-5450dc233a3c"}
{"abstract": "We describe implementations for solving the discrete logarithm problem in the class group of an imaginary quadratic field and in the infrastructure of a real quadratic field. The algorithms used incorporate improvements over previously-used algorithms, and extensive numerical results are presented demonstrating their efficiency. This data is used as the basis for extrapolations, used to provide recommendations for parameter sizes providing approximately the same level of security as block ciphers with $80,$ $112,$ $128,$ $192,$ and $256$-bit symmetric keys.", "authors": ["Jean-Fran\u00e7ois Biasse", "Jacobson Michael", "Silverster Alan"], "n_citation": 50, "title": "Security Estimates for Quadratic Field Based Cryptosystems", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "ae2e1f4c-dec9-4cd9-806b-aa415d31a418"}
{"abstract": "Biometric data offer a potential source of high-entropy, secret information that can be used in cryptographic protocols provided two issues are addressed: (1) biometric data are not uniformly distributed; and (2) they are not exactly reproducible. Recent work, most notably that of Dodis, Reyzin, and Smith, has shown how these obstacles may be overcome by allowing some auxiliary public information to be reliably sent from a server to the human user. Subsequent work of Boyen has shown how to extend these techniques, in the random oracle model, to enable unidirectional authentication from the user to the server without the assumption of a reliable communication channel. We show two efficient techniques enabling the use of biometric data to achieve mutual authentication or authenticated key exchange over a completely insecure (i.e., adversarially controlled) channel. In addition to achieving stronger security guarantees than the work of Boyen, we improve upon his solution in a number of other respects: we tolerate a broader class of errors and, in one case, improve upon the parameters of his solution and give a proof of security in the standard model.", "authors": ["Xavier Boyen", "Yevgeniy Dodis", "Jonathan Katz", "Rafail Ostrovsky", "Adam D. Smith"], "n_citation": 0, "title": "Secure remote authentication using biometric data", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "aec62a0b-7fcb-4662-ad54-407eabfd7fef"}
{"abstract": "Intelligent behavior is characterized by flexible and creative pursuit of endogenously defined goals. Intentionality is a key concept by which to link brain dynamics to goal-directed behavior, and to design mechanisms for intentional adaptations by machines. Evidence from vertebrate brain evolution and clinical neurology points to the limbic system as the key forebrain structure that creates the neural activity which formulate goals as images of desired future states. The behavior patterns created by the mesoscopic dynamics of the forebrain take the form of hypothesis testing. Predicted information is sought by use of sense organs. Synaptic connectivity of the brain changes by learning from the consequences of actions taken. Software and hardware systems using coupled nonlinear differential equations with chaotic attractor landscapes simulate these functions in free-roving machines learning to operate in unstructured environments.", "authors": ["Walter J. Freeman"], "n_citation": 0, "title": "Dynamic models for intention (goal-directedness) are required by truly intelligent robots", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "aed583a0-e40c-4138-9719-ce854e379396"}
{"abstract": "We study the influence of collision-finding attacks on the security of time-stamping schemes. We distinguish between client-side hash functions used to shorten the documents before sending them to time-stamping servers and server-side hash functions used for establishing one way causal relations between time stamps. We derive necessary and sufficient conditions for client side hash functions and show by using explicit separation techniques that neither collision-resistance nor 2nd preimage resistance is necessary for secure time-stamping. Moreover, we show that server side hash functions can even be not one-way. Hence, it is impossible by using black-box techniques to transform collision-finders into wrappers that break the corresponding time-stamping schemes. Each such wrapper should analyze the structure of the hash function. However, these separations do not necessarily hold for more specific classes of hash functions. Considering this, we take a more detailed look at the structure of practical hash functions by studying the Merkle-Damgard (MD) hash functions. We show that attacks, which are able to find collisions for MD hash functions with respect to randomly chosen initial states, also violate the necessary security conditions for client-side hash functions. This does not contradict the black-box separations results because the MD structure is already a deviation from the black-box setting. As a practical consequence, MD5, SHA-0, and RIPEMD are no more recommended to use as client-side hash functions in time-stamping. However, there is still no evidence against using MD5 (or even MD4) as server-side hash functions.", "authors": ["Ahto Buldas", "Sven Laur"], "n_citation": 50, "title": "Do broken hash functions affect the security of time-stamping schemes?", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aedd8f9f-ee9d-4d33-9593-f8606d74337d"}
{"abstract": "Motivated by the representation of biometric and multimedia objects, we consider the problem of hiding noisy point-sets using a secure sketch. A point-set X consists of s points from a d-dimensional discrete domain [0,N - 1] d . Under permissible noises, for every point (x 1 ,...,x d ) \u2208 X, each x i  may be perturbed by a value of at most 6. In addition, at most t points in X may be replaced by other points in [0, N - 1] d . Given an original X, we want to compute a secure sketch P. A known method constructs the sketch by adding a set of random points R, and the description of (X U R) serves as part of the sketch. However, the dependencies among the random points are difficult to analyze, and there is no known non-trivial bound on the entropy loss. In this paper, we first give a general method to generate R and show that the entropy loss of (X U R) is at most s(d log \u0394 + d + 0.443), where \u0394 = 26 + 1. We next give improved schemes for d = 1, and special cases for d = 2. Such improvements are achieved by pre-rounding, and careful partition of the domains into cells. It is possible to make our sketch short, and avoid using randomness during construction. We also give a method in d = 1 to demonstrate that, using the size of R as the security measure would be misleading.", "authors": ["Ee-Chien Chang", "Qiming Li"], "n_citation": 0, "title": "Hiding secret points amidst chaff", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "af2b4527-f3ba-4609-b5d4-920ba13e4abb"}
{"abstract": "Mobile ad hoc networks offer convenient infrastructureless communications over the shared wireless channel. However, the nature of mobile ad hoc networks makes them vulnerable to security attacks, such as passive eavesdropping over the wireless channel and denial of service attacks by malicious nodes. To ensure the security, several cryptography protocols are implemented. Due to the resource scarcity in mobile ad hoc networks, the protocols must be communication efficient and need as less computational power as possible. Broadcast communication is an important operation for many application in mobile ad hoc networks. To securely broadcast a message, all the members in the network need share a group key so that they can use efficient symmetric encryption, such as DES and AES. Several group key management protocols have been proposed. However, not all of them are communication efficient when applied to mobile ad hoc networks. In this paper, we propose a hierarchical key agreement protocol that is communication efficient to mobile ad hoc networks. We also show how to manage the group efficiently in a mobile environment.", "authors": ["Gang Yao", "Kui Ren", "Feng Bao", "Robert H. Deng", "Dengguo Feng"], "n_citation": 0, "title": "Making the key agreement protocol in mobile ad hoc network more efficient", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "af3a1364-3117-403f-9914-0859e424b444"}
{"abstract": "This paper describes a distributed security infrastructure for mobile agents. The first property of the infrastructure is believability; this means that mechanisms are provided for authenticating information furnished by an agent. A second security property is survivability. This means that an agent computation can be programmed to survive attacks by malicious hosts on individual agents; this is achieved through encryption as well as agent replication and voting. The main feature of the infrastructure is that mobile agents are themselves used to enforce the security properties.", "authors": ["Ciar\u00e1n Bryce"], "n_citation": 50, "title": "A security framework for a mobile agent system", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "af7e9db3-8426-407b-9fa3-fcfa1c749005"}
{"abstract": "This paper investigates the problem of confidentiality violations via illegal data inferences that occur when arithmetic constraints are combined with non-confidential numeric data to infer confidential information. The database is represented as a point in an (n + k)-dimensional constraint space, where n is the number of numerical data items stored in the database (extensional database) and k is the number of derivable attributes (intensional database). Database constraints over both extensional and intensional databases form an (n + k)-dimensional constraint object. A query answer over a data item x is an interval I of values along the x axis of the database such that I is correct (i.e., the actual data value is within I) and safe (i.e., users cannot infer which point within I is the actual data value). The security requirements are expressed by the accuracy with which users are allowed to disclose data items. More specifically, we develop two classification methods: (1) volume-based classification, where the entire volume of the disclosed constraint object that contains the data item is considered and (2) interval based classification, where the length of the interval that contains the data item is considered. We develop correct and safe inference algorithms for both cases.", "authors": ["Alexander Brodsky", "Csilla Farkas", "Duminda Wijesekera", "X. Sean Wang"], "n_citation": 50, "title": "Constraints, inference channels and secure databases", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "af83f053-9957-4f46-97fe-b876ca4d6a2a"}
{"abstract": "Propagation based finite domain solvers provide a general mechanism for solving combinatorial problems. Different propagation methods can be used in conjunction by communicating through the domains of shared variables. The flexibility that this entails has been an important factor in the success of propagation based solving for solving hard combinatorial problems. In this paper we investigate how linear integer constraints should be represented in order that propagation can determine strong domain information. We identify two kinds of substitution which can improve propagation solvers, and can never weaken the domain information. This leads us to an alternate approach to propagation based solving where the form of constraints is modified by substitution as computation progresses. We compare and contrast a solver using substitution against an indexical based solver, the current method of choice for implementing propagation based constraint solvers, identifying the the relative advantages and disadvantages of the two approaches.", "authors": ["Warwick Harvey", "Peter J. Stuckey"], "n_citation": 50, "title": "Constraint representation for propagation", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "b0085f3b-b431-4502-ba10-3578b56333c9"}
{"authors": ["D.P. Gavidia", "Elena V. Zudilova-Seinstra", "Peter M. A. Sloot"], "n_citation": 0, "title": "A Client-Server Engine for Parallel Computation of High-Resolution Planes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b01ce1ec-548b-42e6-86f0-5968e58f0050"}
{"abstract": "XML data can be modeled as node-labeled graphs and XML queries can be expressed by structural relationships between labeled elements. XML query evaluation has been addressed using mainly database, and in some cases graph search, techniques. We propose an alternative method that models and solves such queries as constraint satisfaction problems (CSPs). We describe common constraint types occurring in XML queries and show how query evaluation can benefit from methods for preprocessing and solving CSPs. We identify an important non-binary constraint that is a common module of XML queries and describe a generalized arc consistency algorithm with low cost that can ensure polynomial query evaluation. Finally, we demonstrate that maintaining the consistency of such non-binary constraints can greatly accelerate search in intractable queries that include referential relationships.", "authors": ["Nikos Mamoulis", "Kostas Stergiou"], "n_citation": 0, "title": "Constraint satisfaction in semi-structured data graphs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b02ac6ef-4b66-4a37-b896-2c259ecf490a"}
{"abstract": "Computationally adequate representation of models is a topic arising in various forms in logic and AI. Two fundamental decision problems in this area are: (1) to check whether a given clause is true in a represented model, and (2) to decide whether two representations of the same type represent the same model. ARMs, contexts and DIGs are three important examples of model representation formalisms. The complexity of the mentioned decision problems has been studied for ARMs only for finite signatures, and for contexts and DIGs only for infinite signatures, so far. We settle the remaining cases. Moreover we show that, similarly to the case for infinite signatures, contexts and DIGs allow one to represent the same classes of models also over finite signatures; however DIGs may be exponentially more succinct than all equivalent contexts.", "authors": ["Christian G. Ferm\u00fcller", "Reinhard Pichler"], "n_citation": 0, "title": "Model representation over finite and infinite signatures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b0ac42da-2b0f-4514-832b-69697ca37a29"}
{"abstract": "This paper describes a general approach for optimized live heap space and live heap space-bound analyses for garbage-collected languages. The approach is based on program analysis and transformations and is fully automatic. In our experience, the space-bound analysis generally produces accurate (tight) upper bounds in the presence of partially known input structures. The optimization drastically improves the analysis efficiency. The analyses have been implemented and experimental results confirm their accuracy and efficiency.", "authors": ["Leena Unnikrishnan", "Scott D. Stoller", "Yanhong A. Liu"], "n_citation": 0, "title": "Optimized live heap bound analysis", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b0e333fc-f038-4cc1-bd47-b59351f2fc84"}
{"abstract": "In this paper, a new kind of neural network is proposed by combining ridgelet with feed-forward neural network (FNN). The network adopts ridgelet as the activation function in hidden layer of a three-layer FNN. Ridgelet is a good basis for describing the directional information in high dimension and it proves to be optimal in representing the functions with hyperplane singularity. So the network can approximate quite a wide range of multivariate functions in a more stable and efficient way, especially those with certain kinds of spatial inhomogeneities. Both theoretical analysis and experimental results of function approximation prove its superiority to wavelet neural network.", "authors": ["Shuyuan Yang", "Min Wang", "Licheng Jiao"], "n_citation": 50, "title": "A new adaptive ridgelet neural network", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b14d1ea2-80f9-42a0-9b05-e2a76068d9f0"}
{"abstract": "In this paper the authors analyze problems of existence and global asymptotic stability of the equilibrium for the neural networks with mixed delays. Some new sufficient conditions ensuring the existence, uniqueness, and global asymptotic stability of the equilibrium are established by means of Leray-Schauder principle, arithmetic-mean-geometricmean inequality and vector delay differential inequality technique. These conditions are less restrictive than previously known criteria.", "authors": ["Shuyong Li", "Yumei Huang", "Daoyi Xu"], "n_citation": 0, "title": "On Equilibrium and Stability of a Class of Neural Networks with Mixed Delays", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b1769b06-4ca0-4ff3-aafb-c64c40d24fad"}
{"abstract": "In this paper, we extend the principle of symmetry to dominance in Not-Equals Constraint Networks and show how dominated values are detected and eliminated efficiently at each node of the search tree.", "authors": ["Belaid Benhamou", "Mohamed R\u00e9da Sa\u00efdi"], "n_citation": 0, "title": "Reasoning by dominance in not-equals binary constraint networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b19da99d-e0d5-4265-a28a-33e52fc01883"}
{"abstract": "This article deals with global constraints for which the set of solutions can be recognized by an extended finite automaton whose size is bounded by a polynomial in n, where n is the number of variables of the corresponding global constraint. By reformulating the automaton as a conjunction of signature and transition constraints we show how to systematically obtain a filtering algorithm. Under some restrictions on the signature and transition constraints this filtering algorithm achieves arc-consistency. An implementation based on some constraints as well as on the metaprogramming facilities of SICStus Prolog is available. For a restricted class of automata we provide a filtering algorithm for the relaxed case, where the violation cost is the minimum number of variables to unassign in order to get back to a solution.", "authors": ["Nicolas Beldiceanu", "Mats Carlsson", "Thierry Petit"], "n_citation": 0, "title": "Deriving filtering algorithms from constraint checkers", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b1ed392e-4c1f-48dc-a988-b5c9e439e50a"}
{"authors": ["Ryszard Kutner", "Filip \u015awita\u0142a"], "n_citation": 0, "title": "Stochastic Simulation of Time Series by Using the Spatial-Temporal Weierstrass Function", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b274b545-fd06-4b42-b9ac-bc9889019ec3"}
{"abstract": "In this paper, we model the adversary (eavesdropper) present in the wireless communication medium using probabilistic models. We precisely formulate the security-throughput optimization and derive analytical solutions. The effect of different adversary models, and single and multi-rate modulation schemes (BPSK and MQAM) are studied. Simulation results are given to show that significant throughput gain can be achieved by using link (channel) adaptive and adversary adaptive encryption techniques compared to fixed block length encryption.", "authors": ["Mohamed A. Haleem", "Chetan Nanjunda Mathur", "Rajarathnam Chandramouli", "K. P. Subbalakshmi"], "n_citation": 0, "title": "On optimizing the security-throughput trade-off in wireless networks with adversaries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b277705b-53ca-4533-a3c6-54e84c6702dd"}
{"abstract": "The purpose of this article is the introduction of a novel stochastic Hebb-like learning rule for neural networks which combines features of unsupervised (Hebbian) and supervised (reinforcement) learning. This learning rule is stochastic with respect to the selection of the time points when a synaptic modification is induced by simultantious activation of the pre- and postsynaptic neuron. Moreover, the learning rule does not only affect the synapse between pre- and postsynaptic neuron which is called homosynaptic plasticity but effects also further remote synapses of the pre- and postsynaptic neuron. This more complex form of plasticity has recently come into the light of interest of experimental investigations in neurobiology and is called heterosynaptic plasticity. Our learning rule is motivated by these experimental findings and gives a qualitative explanation of this kind of synaptic plasticity. Additionally. we give some numerical results that demonstrate that our learning rule works well in training neural networks, even in the presence of noise.", "authors": ["Frank Emmert-Streib"], "n_citation": 0, "title": "A Novel Stochastic Learning Rule for Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b335045c-e6f1-44f2-a38f-e4d0b93e7789"}
{"abstract": "This paper addresses the interaction between randomization, with restart strategies, andlearning, an often crucial technique for prov- ing unsatisfiability. We use instances of SAT from the hardware veri- fication domain to provide evidence that randomization can indeed be essential in solving real-worldsatisfiable instances of SAT. More inter- estingly, our results indicate that randomized restarts and learning may cooperate in proving both satisfiability andunsatisfiability. Finally, we utilize and expand the idea of algorithm portfolio design to propose an alternative approach for solving hardunsatisfiable instances of SAT.", "authors": ["Luis Filipe Baptista", "Joao Marques-Silva"], "n_citation": 95, "references": ["2b8bae9d-40c1-4def-85c9-858688cac846", "3e5d18f4-a087-4eda-9d3f-96ae80cfd094", "4b4cd2a0-e4ff-4911-9087-4b4f42f36ab3", "4ec23a89-145e-4575-910a-b8c6c395c570", "9d1ed045-51bd-44ee-8e2e-a370b02b4aae"], "title": "Using Randomization and Learning to Solve Hard Real-World Instances of Satisfiability", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "b404f701-5c40-4d66-8ab0-4b74af40d9eb"}
{"abstract": "The IP security protocols (IPsEC) may be used via security gateways that apply cryptographic operations to provide security services to datagrams, and this mode of use is supported by an increasing number of commercial products. In this paper, we formalize the types of authentication and confidentiality goal that IPSEC is capable of achieving, and we provide criteria that entail that a network with particular IPsEC processing achieves its security goals. This requires us to formalize the structure of networks using IPSEC, and the state of packets relevant to IPSEC processing. We can then prove confidentiality goals as invariants of the formalized systems. Authentication goals are formalized in the manner of [9], and a simple proof method using unwinding sets is introduced. We end the paper by explaining the network threats that are prevented by correct IPsEC processing.", "authors": ["Joshua D. Guttman", "Amy L. Herzog", "F. Javier Thayer"], "n_citation": 50, "title": "Authentication and confidentiality via IPsec", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "b4a1a42b-96ff-4684-aea5-4afb35a4503a"}
{"abstract": "Symmetries in constraint satisfaction problems (CSPs) are one of the difficulties that practitioners have to deal with. We present in this paper a new method based on the symmetries of decisions taken from the root of the search tree. This method can be seen as an improvement of the nogood recording presented by Focacci and Milano[5] and Fahle, Schamberger and Sellmann[4]. We present a simple formalization of our method for which we prove correctness and completeness results. We also show that our method is theoretically more efficient as the number of dominance checks, the number of nogoods and the size of each nogood are smaller. This is confirmed by an experimental evaluation on the social golfer problem, a very difficult and highly symmetrical real world problem. We are able to break all symmetries for problems with more than 10 36  symmetries. We report both new results, and a comparison with previous work.", "authors": ["Jean-Francois Puget"], "n_citation": 0, "title": "Symmetry breaking revisited", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b4b624f6-ccdd-4155-9275-43f72f3d4a13"}
{"authors": ["Meiqin Wang", "Yue Sun", "Nicky Mouha", "Bart Preneel"], "n_citation": 50, "title": "Algebraic Techniques in Differential Cryptanalysis Revisited", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "b4cb9ecd-6713-47a9-9d47-6dfd26b9bca7"}
{"abstract": "Enforcing rich policies in open environments will increasingly require the ability to dynamically identify external sources of information necessary to enforce different policies (e.g. finding an appropriate source of location information to enforce a location-sensitive access control policy). In this paper, we introduce a semantic web framework and a meta-control model for dynamically interleaving policy reasoning and external service discovery and access. Within this framework, external sources of information are wrapped as web services with rich semantic profiles allowing for the dynamic discovery and comparison of relevant sources of information. Each entity (e.g. user, sensor, application, or organization) relies on one or more Policy Enforcing Agents responsible for enforcing relevant privacy and security policies in response to incoming requests. These agents implement meta-control strategies to dynamically interleave semantic web reasoning and service discovery and access. The paper also presents preliminary empirical results. This research has been conducted in the context of myCampus, a pervasive computing environment aimed at enhancing everyday campus life at Carnegie Mellon University.", "authors": ["Jinghai Rao", "Norman M. Sadeh"], "n_citation": 50, "title": "A Semantic Web framework for interleaving policy reasoning and external service discovery", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b4cd6b2f-abb4-432c-b1b7-3e2c63171ffb"}
{"abstract": "This paper presents a syntactic method to check so-called assignable clauses of annotated JAVA programs. Assignable clauses describe which variables may be assigned by a method. Their correctness is crucial for reasoning about class specifications. The method that we propose is incomplete, as it only makes a syntactic check and it does not take aliasing or expression evaluation into account, but it provides efficient means to find the most common errors in assignable clauses. This is demonstrated by applying the method to the specification of an industrial case study.", "authors": ["N\u00e9stor Cata\u00f1o", "Marieke Huisman"], "n_citation": 53, "title": "CHASE: A static checker for JML's Assignable clause", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b4dbd1ea-5171-402b-a4ff-cf08c8af4335"}
{"abstract": "Visopt ShopFloor is a complete system for solving real-life scheduling problems in complex industries. In particular, the system is intended to problem areas where traditional scheduling methods failed. In the paper we describe the heart of the Visopt system, a generic scheduling engine. This engine goes beyond traditional scheduling by offering some planning capabilities. We achieved this integrated behaviour by applying Constraint Logic Programming in a less standard way - the definition of a constraint model is dynamic and introduction of constraints interleaves with search.", "authors": ["Roman Bart\u00e1k"], "n_citation": 0, "title": "Visopt ShopFloor: On the edge of planning and scheduling", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b5183338-0d40-4e80-816b-2b96e8dba62f"}
{"abstract": "In this paper, we address the problem of file replica placement in Data Grids given a certain traffic pattern. We propose a new file replica placement algorithm and compare its performance with a standard replica placement algorithm using simulation. The results show that file replication improve the performance of the data access but the gains depend on several factors including where the file replicas are located, burstness of the request arrival, packet loses and file sizes.", "authors": ["Jemal H. Abawajy"], "n_citation": 50, "title": "Placement of file replicas in data grid environments", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "b527b156-788e-4458-89f6-eedafe488275"}
{"abstract": "Most techniques for probabilistic reasoning focus on reasoning about conditional probability constraints. However, human experts are accustomed to representing uncertain knowledge in the form of expectation rather than probability distribution directly in many cases. It is necessary to provide a logic for encoding hybrid probabilistic knowledge bases that contain expectation knowledge as well as the purely probabilistic knowledge in the form of conditional probability. This paper constructs a nonmonotonic logic for reasoning about hybrid probabilistic knowledge bases. We extend the propositional logic for reasoning about expectation to encoding hybrid probabilistic knowledge by introducing the conditional expectation constraint formula. Then we provide an approach to nonmonotonic reasoning about hybrid probabilistic knowledge bases. Finally,we compare this logic with related works.", "authors": ["Kedian Mu", "Zuoquan Lin", "Zhi Jin", "Ruqian Lu"], "n_citation": 0, "title": "Reasoning about hybrid probabilistic knowledge bases", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b572c147-fe9d-48c2-a8e1-a3de22b352dd"}
{"authors": ["Tieju Ma", "Hong-Bin Yan", "Yoshiteru Nakamori"], "n_citation": 0, "title": "Roadmapping and i -Systems", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "b6039b16-4058-4665-b742-7f59eba6d25a"}
{"abstract": "Activity recognition is one of the most challenging problems in the high-level computer vision field. In this paper, we present a novel approach to interacting activity recognition based on dynamic Bayesian network (DBN). In this approach the features representing the human activities are divided into two classes: global features and local features, which are on two different spatial scales. To model and recognize human interacting activities, we propose a hierarchical durational-state DBN model (HDS-DBN). HDS-DBN combines the global features with local ones organically and reveals structure of interacting activities well. The effectiveness of this approach is demonstrated by experiments.", "authors": ["Youtian Du", "Feng Chen", "Wenli Xu", "Weidong Zhang"], "n_citation": 0, "title": "Interacting Activity Recognition Using Hierarchical Durational-State Dynamic Bayesian Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b67725c9-473b-4a76-886f-7109aed9d0da"}
{"abstract": "Musical knowledge discovery, an important issue of digital network processing, is also a crucial question for music. Indeed, music may be considered as a kind of network. A new approach for Musical Pattern Discovery is proposed, which tries to consider musical discourse in a general polyphonic framework. We suggest a new vision of automated pattern analysis that generalizes the multiple viewpoint approach. Sharing the idea that pattern emerges from repetition, analogy-based modeling of music understanding adds the idea of a permanent induction of global hypotheses from local perception. Through a chronological scanning of the score, analogies are inferred between local relationships -namely, notes and intervals - and global structures - namely, patterns -whose paradigms are stored inside an abstract pattern trie. Basic mechanisms for inference of new patterns are described. Such an elastic vision of music enables a generalized understanding of its plastic expression.", "authors": ["Olivier Lartillot"], "n_citation": 0, "title": "Generalized Musical pattern Discovery by analogy from local viewpoints", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b67fc303-4dd8-43f5-b529-1f8fda242e80"}
{"abstract": "Multilayer perceptrons (MLP) has been proven to be very successful in many applications including classification. The activation function is the source of the MLP power. Careful selection of the activation function has a huge impact on the network performance. This paper gives a quantitative comparison of the four most commonly used activation functions, including the Gaussian RBF network, over ten real different datasets. Results show that the sigmoid activation function substantially outperforms the other activation functions. Also, using only the needed number of hidden units in the MLP, we improved its conversion time to be competitive with the RBF networks most of the time.", "authors": ["Emad A. M. Andrews Shenouda"], "n_citation": 0, "title": "A Quantitative Comparison of Different MLP Activation Functions in Classification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b77d8e27-2c9a-4699-a1c2-d135bbc55f9c"}
{"abstract": "CNF propositional satisfiability (SAT) is a special kind of the more general Constraint Satisfaction Problem (CSP). While look-back techniques appear to be of little use to solve hard random SAT problems, it is supposed that they are necessary to solve hard structured SAT problems. In this paper, we propose a very simple DPL procedure called Satz which only employs some look-ahead techniques: a variable ordering heuristic, a forward consistency checking (Unit Propagation) and a limited resolution before the search, where the heuristic is itself based on unit propagation. Satz is favorably compared on random 3-SAT problems with three DPL procedures among the best in the literature for these problems. Furthermore on a great number of problems in 4 well-known SAT benchmarks Satz reaches or outspeeds the performance of three other DPL procedures among the best in the literature for structured SAT problems. The comparative results suggest that a suitable exploitation of look-ahead techniques, while very simple and efficient for random SAT problems, may allow to do without sophisticated look-back techniques in a DPL procedure.", "authors": ["Chu Min Li", "Anbulagan"], "n_citation": 0, "title": "Look-ahead versus look-back for satisfiability problems", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "b799eb08-b113-48e4-b713-0d05de2f4551"}
{"abstract": "A metering scheme is a method by which an audit agency is able to measure the interaction between servers and clients during a certain number of time frames. Naor and Pinkas [9] considered schemes in which any server is able to construct a proof if and only if it has been visited by at least a number, say h, of clients in a given time frame. In this paper we construct metering schemes for more general access structures, which include multilevel and compartmented access structures. Metering schemes realizing these access structures have useful practical applications: for example, they can be used to measure the interaction of a web site with a specific audience which is of special interest. We also prove lower bounds on the communication complexity of metering schemes realizing general access structures.", "authors": ["Barbara Masucci", "Douglas R. Stinson"], "n_citation": 0, "title": "Metering schemes for general access structures", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "b801d19e-4b9d-4404-8a7e-d4814eb0d0c8"}
{"abstract": "The first phase of the SET protocol, namely Cardholder Registration, has been modelled inductively. This phase is presented in outline and its formal model is described. A number of basic lemmas have been proved about the protocol using Isabelle/HOL, along with a theorem stating that a certification authority will certify a given key at most once. Many ambiguities, contradictions and omissions were noted while formalizing the protocol.", "authors": ["Giampaolo Bella", "Fabio Massacci", "Lawrence C. Paulson", "Piero Tramontano"], "n_citation": 0, "title": "Formal verification of Cardholder registration in SET", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "b819b9c2-21f7-42d4-b8e8-8b2593c74c49"}
{"abstract": "Discovering, disclosing, and patching vulnerabilities in computer systems play a key role in the security area, but now vulnerability information from different sources is usually ambiguous text-based description that can't be efficiently shared and used in automated process. After explaining a model of vulnerability life cycle, this paper presents an XML-based common vulnerability markup language (CVML) describing vulnerabilities in a more structural way. Besides regular information contained in most of current vulnerability databases, information about classification, evaluation, checking existence and attack generation is also given in CVML. So it supports automated vulnerability assessment and remedy. A prototype of automated vulnerability management architecture based on CVML has been implemented. More manageable vulnerability databases will be built; promulgating and sharing of vulnerability knowledge will be easier; comparison and fusion of vulnerability information from different sources will be more efficient; moreover automated scanning and patching of vulnerabilities will lead to self-managing systems.", "authors": ["Haitao Tian", "Liusheng Huang", "Zhi Zhou", "Hui Zhang"], "n_citation": 0, "title": "Common vulnerability markup language", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b880a78d-d573-4bbe-89b7-e5b1923a27b9"}
{"authors": ["Annamaria Mazzia", "Giorgio Pini", "Maria Caterina Putti", "Flavio Sartoretto"], "n_citation": 0, "title": "Comparison of 3d flow fields arising in mixed and standard unstructured finite elements", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b8921dca-0e79-47b4-ad6d-f2acf823461e"}
{"abstract": "In this paper hierarchical clustering and self-organizing maps (SOM) clustering are compared by using molecular data of large size sets. The hierarchical clustering can represent a multi-level hierarchy which show the tree relation of cluster distance. SOM can adapt the winner node and its neighborhood nodes, it can learn topology and represent roughly equal distributive regions of the input space, and similar inputs are mapped to neighboring neurons. By calculating distances between neighboring units and Davies-Boulding clustering index, the cluster boundaries of SOM are decided by the best Davies-Boulding clustering index. The experimental results show the effectiveness of clustering for molecular data, between-cluster distance of low energy samples from transition networks is far bigger than that of local sampling samples, the former has a better cluster result, local sampling data nevertheless exhibit some clusters.", "authors": ["Lin Wang", "Minghu Jiang", "Yinghua Lu", "Frank No\u00e9", "Jeremy C. Smith"], "n_citation": 50, "title": "Self-Organizing Map Clustering Analysis for Molecular Data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b90d1261-1cfc-43ac-844e-b6a6fa3b0930"}
{"authors": ["A. A. Frolov", "Pavel Bobrov", "Olesya Mokienko", "Dusan Husek", "Alexey Korshakov", "Chernikova La", "Konovalov Rn"], "n_citation": 0, "title": "LOCALIZING SOURCES OF BRAIN ACTIVITY RELEVANT TO MOTOR IMAGERY BRAIN-COMPUTER INTERFACE PERFORMANCE, USING INDIVIDUAL HEAD GEOMETRY", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "b96e9551-82ac-4b6c-84c1-afa8e472cbbb"}
{"authors": ["Evan A. Sultanik", "Pragnesh Jay Modi", "William C. Regli"], "n_citation": 0, "title": "Constraint propagation for domain bounding in distributed task scheduling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "b9ee5b2c-785c-4aea-a19b-785b17434cf5"}
{"abstract": "In the paper, the orthogonal function neural network is utilized to realize the synchronization of chaotic system. The orthogonal function neural network is Legendred orthogonal neural network. First of all, the orthogonal function neural network is trained to learn the uncertain information. The parameters of Legendred orthogonal neural network are adjusted to accomplish the synchronization of two chaotic systems with the perturbation by Lyapunov steady theorem. At last, the result of numerical example is shown to illustrate the validity of the proposed method.", "authors": ["Hongwei Wang", "Hong Gu"], "n_citation": 0, "title": "Synchronization of Chaotic System with the Perturbation Via Orthogonal Function Neural Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ba092ef9-888d-4b94-817d-4a7fd6541079"}
{"abstract": "Based on chaotic communication and independent component analysis, an image protection and authentication technique is proposed. We modulate a watermark by chaotic signal and embed it in watermark host vector which is established in terms of the coefficients in DWT domain. When the author and the legal user want to authenticate digital image, they can detect and extract the watermark by using ICA and private key. Experimental results show that robustness of the watermarking satisfies both of their demands.", "authors": ["Linhua Zhang", "Shaojiang Deng", "Xuebing Wang"], "n_citation": 0, "title": "A new image protection and authentication technique based on ICA", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ba3e0903-8d88-4df5-9810-11df747f4dde"}
{"abstract": "Ad-hoc networks are an emerging networking technology, in which the nodes form a network with no fixed infrastructure: each node forwards messages to the others by using the wireless links induced by their power levels. Generally, energy-efficient protocols heavily rely on cooperation. In this paper, we analyze from a game-theoretic point of view the problem of performing a broadcast operation from a given station s. We show both theoretical and experimental results on how the existence of (good) Nash equilibria is determined by factors such as the transmission power of the stations or the payment policy that stations can use to enforce their reciprocal cooperation.", "authors": ["Pilu Crescenzi", "Miriam Di Ianni", "Alessandro Lazzoni", "Paolo Penna", "Gianluca Rossi", "Paola Vocca"], "n_citation": 50, "title": "Equilibria for broadcast range assignment games in ad-hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bac0c1a4-64ae-4a74-996e-673d2f65353f"}
{"abstract": "Feature trees have been used to accommodate records in constraint programming and record like structures in computational linguistics. Feature trees model records, and feature constraints yield extensible and modular record descriptions. We introduce the constraint system FT< of ordering constraints interpreted over feature trees. Under the view that feature trees represent symbolic information, the relation < corresponds to the information ordering (carries less information than). We present a polynomial algorithm that decides the satisfiability of conjunctions of positive and negative information ordering constraints over feature trees. Our results include algorithms for the satisfiability problem and the entailment problem of FT\u2264 in time O(n 3 ). We also show that FT\u2264 has the independence property and are thus able to handle negative conjuncts via entailment. Furthermore, we reduce the satisfiability problem of Dorre's weaksubsumption constraints to the satisfiability problem of FT\u2264. This improves the complexity bound for solving weak subsumption constraints from O( n  5 ) to O( n  3 ).", "authors": ["M. M\u00fcller", "Joachim Niehren", "Andreas Podelski"], "n_citation": 0, "title": "Ordering constraints over feature trees", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "bc063896-f573-4e65-a068-ceb7990c0e83"}
{"abstract": "We present a logical approach of spoken language understanding for a human-machine dialogue system. The aim of the analysis is to provide a logical formula, or a conceptual graph, by assembling concepts related to a delimited application domain. This flexible structure is gradually built during an incremental parsing, which is meant to combine syntactic and semantic criteria. Then, a contextual understanding step leads to completing this structure. The evaluations of the current system are encouraging. This approach is a preliminary for a logical dialogue that uses the form of the semantic representations.", "authors": ["Jeanne Villaneau", "Jean-Yves Antoine", "Olivier Ridoux"], "n_citation": 0, "title": "Logical approach to natural language understanding in a spoken dialogue system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "bc1266d9-b672-454a-a7f6-ca862b152810"}
{"abstract": "A mathematical model for learning a nonlinear line of attractions is presented in this paper. This model encapsulates attractive fixed points scattered in the state space representing patterns with similar characteristics as an attractive line. The dynamics of this nonlinear line attractor network is designed to operate between stable and unstable states. These criteria can be used to circumvent the plasticity-stability dilemma by using the unstable state as an indicator to create a new line for an unfamiliar pattern. This novel learning strategy utilized stability (convergence) and instability (divergence) criteria of the designed dynamics to induce self-organizing behavior. The self-organizing behavior of the nonlinear line attractor model can helps to create complex dynamics in an unsupervised manner. Experiments performed on CMU face expression database shows that the proposed model can perform pattern association and pattern classification tasks with few iterations and great accuracy.", "authors": ["Ming-Jung Seow", "Vijayan K. Asari"], "n_citation": 0, "title": "Robust Learning by Self-organization of Nonlinear Lines of Attractions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bc7d2560-c753-4705-9617-9b60d40844ac"}
{"abstract": "In this paper we propose a new data model called periodic spatiotemporal objects (PSOs) databases. We show that relational algebra can be extended to PSO databases and any fixed relational algebra queries can be evaluated in PTIME in the size of any input database. We also describe a database system implementation of the PSO model and several sample queries.", "authors": ["Peter Revesz", "Mengchu Cai"], "n_citation": 0, "title": "Efficient querying of periodic spatiotemporal objects", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "bc82c4fc-1436-4480-8942-fa43f3d071c3"}
{"abstract": "We present a facial deformation system that adapts a generic facial rig into different face models. The deformation is based on labels and allows transferring specific facial features between the generic rig and face models. High quality physics-based animation is achieved by combining different deformation methods with our labeling system, which adapts muscles and skeletons from a generic rig to individual face models. We describe how to find the correspondence of the main attributes of the generic rig, transfer them to different 3D face models and generate a sophisticated facial rig based on human anatomy. We show how to apply the same deformation parameters to different face models and obtain unique expressions. Our goal is to ease the character setup process and provide digital artists with a tool that allows manipulating models as if they were using a puppet. We end with different examples that show the strength of our proposal.", "authors": ["Ver\u00f3nica Costa Teixeira Orvalho", "Ernesto Zacur", "Antonio Sus\u00edn"], "n_citation": 0, "title": "Transferring a labeled generic rig to animate face models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bc9b584f-5c43-4bb2-bc3d-aa180028386f"}
{"authors": ["George Baciu", "Rico K. W. Tsang"], "n_citation": 0, "title": "Advancing Front Meshing for Radiosity Solutions", "venue": "Lecture Notes in Computer Science", "year": 1995, "id": "bcc4108f-4a38-465f-804e-117e501b50ef"}
{"abstract": "This paper proposes a double layer speech recognition and utterance verification system based on the analysis of the temporal evolution of HMM's state scores. For the lower layer, it uses standard HMM-based acoustic modeling, followed by a Viterbi grammar-free decoding step which provides us with the state scores of the acoustic models. In the second layer, these state scores are added to the regular set of acoustic parameters, building a new set of expanded HMMs. This new paremeter models the acoustic HMM's temporal evolution. Using the expanded set of HMMs for speech recognition a significant improvement in performance is achieved. Next, we will use this new architecture for utterance verification in a second opinion framework. We will consign to the second layer evaluating the reliability of decoding using the acoustic models from the first layer. An outstanding improvement in performance versus a baseline verification system has been achieved with this new approach.", "authors": ["Marta Casar", "Jos\u00e9 A. R. Fonollosa"], "n_citation": 0, "title": "Analysis of HMM temporal evolution for automatic speech recognition and verification", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bcf72d44-7a0e-4940-a919-19733f88ce96"}
{"abstract": "Rijndael-like structure is a special case of SPN structure. The linear transformation of Rijndael-like structures consists of linear transformations of two types, the one is byte permutation \u03c0 and the other is linear transformation \u03b8 = (\u03b8 1 ,\u03b8 2 ,\u03b8 3 ,\u03b8 4 ), where each of \u03b8 i  separately operates on each of the four columns of a state. Furthermore, \u03c0 and 0 have some interesting properties. In this paper, we present a new method for upper bounding the maximum differential probability and the maximum linear hull probability for Rijndael-like structures. By applying our method to Rijndael, we obtain that the maximum differential probability and the maximum linear hull probability for 4 rounds of Rijndael are bounded by 1.06 \u00d7 2 -96 .", "authors": ["Sangwoo Park", "Soo Hak Sung", "Seongtaek Chee", "E-Joong Yoon", "Jongin Lim"], "n_citation": 0, "title": "On the security of Rijndael-like structures against differential and linear cryptanalysis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "bd663459-33cc-4012-b417-505deedfe2f6"}
{"abstract": "This paper proposes a swarm optimization model for energy minimization problem of early vision, which is based on a multi-colony ant scheme. Swarm optimization is a new artificial intelligence field, which has been proved suitable to solve various combinatorial optimization problems. Compared with general optimization problems, energy minimization of early vision has its unique characteristics, such as higher dimensions, more complicate structure of solution space, and dynamic constrain conditions. In this paper, the vision energy functions are optimized by repeatedly minimizing a certain number of sub-problems according to divide-and-conquer principle, and each colony is allocated to optimize one sub-problem independently. Then an appropriate information exchange strategy between neighboring colonies, and an adaptive method for dynamic problem are applied to implement global optimization. As a typical example, stereo correspondence will be solved using the proposed swarm optimization model. Experiments show this method can achieve good results.", "authors": ["Wenhui Zhou", "Lili Lin", "Weikang Gu"], "n_citation": 0, "title": "A Swarm Optimization Model for Energy Minimization Problem of Early Vision", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "be8a8858-f222-4dbd-816d-bab511cb843f"}
{"abstract": "In this paper we develop a new method for solving queries on semistructured data. The main idea is to see a database as a Kripke Transition System (a model) and a query as a formula of the temporal logic CTL. In this way, the retrieval of data fulfilling a query is reduced to the problem of finding out the states of the model which satisfy the formula (the model-checking problem) that can be done in linear time.", "authors": ["Agostino Dovier", "Elisa Quintarelli"], "n_citation": 50, "title": "Model-checking based data retrieval", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "be9d65f0-dec9-4905-aaed-3c2ed04b1fb0"}
{"abstract": "MD4 is a hash function developed by Rivest in 1990. It serves as the basis for most of the dedicated hash functions such as MD5, SHAx, RIPEMD, and HAVAL. In 1996, Dobbertin showed how to find collisions of MD4 with complexity equivalent to 2 20  MD4 hash computations. In this paper, we present a new attack on MD4 which can find a collision with probability 2 -2  to 2 -6 , and the complexity of finding a collision doesn't exceed 2 8  MD4 hash operations. Built upon the collision search attack, we present a chosen-message pre-image attack on MD4 with complexity below 2 8 . Furthermore, we show that for a weak message, we can find another message that produces the same hash value. The complexity is only a single MD4 computation, and a random message is a weak message with probability 2 -122 . The attack on MD4 can be directly applied to RIPEMD which has two parallel copies of MD4, and the complexity of finding a collision is about 2 18  RIPEMD hash operations.", "authors": ["Xiaoyun Wang", "Xuejia Lai", "Dengguo Feng", "Hui Chen", "Xiuyuan Yu"], "n_citation": 511, "title": "Cryptanalysis of the hash functions MD4 and RIPEMD", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bf19518a-5619-4cd3-b0c2-5fcd1c11800d"}
{"abstract": "Stochastic resonance phenomenon in a biological sensory system has been studied through signal detection theories and psychophysical experiments. There is a conflict between the real experiments and the traditional signal detection theory for stochastic resonance because the latter treats the receiver as linear model. This paper presents a two-layer summing network of Hodgkin-Huxley (HH) neurons and a summing network of threshold devices to model the receiver, respectively. The simulation results indicate that the relevant index of signal detectability exhibit the stochastic resonance characteristics.", "authors": ["Jun Liu", "Jian Wu", "Zhengguo Lou", "Guang Li"], "n_citation": 0, "title": "Stochastic Resonance Enhancing Detectability of Weak Signal by Neuronal Networks Model for Receiver", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bf220a92-ad81-4511-9674-374cf9bb7a75"}
{"authors": ["Oskar Skibski"], "n_citation": 0, "title": "Steady Marginality: A Uniform Approach to Shapley Value for Games with Externalities", "venue": "Lecture Notes in Computer Science", "year": 2011, "id": "bf28b8bf-d365-4f21-bfd3-9011740e88fc"}
{"abstract": "Much recent work has gone into adapting techniques that were originally developed for SAT solving to QBF solving. In particular, QBF solvers are often based on SAT solvers. Most competitive QBF solvers are search-based. In this work we explore an alternative approach to QBF solving, based on symbolic quantifier elimination. We extend some recent symbolic approaches for SAT solving to symbolic QBF solving, using various decision-diagram formalisms such as OBDDs and ZDDs. In both approaches, QBF formulas are solved by eliminating all their quantifiers. Our first solver, QMRES, maintains a set of clauses represented by a ZDD and eliminates quantifiers via multi-resolution. Our second solver, QBDD, maintains a set of OBDDs, and eliminate quantifier by applying them to the underlying OBDDs. We compare our symbolic solvers to several competitive search-based solvers. We show that QBDD is not competitive, but QMRES compares favorably with search-based solvers on various benchmarks consisting of non-random formulas.", "authors": ["Guoqiang Pan", "Moshe Y. Vardi"], "n_citation": 0, "title": "Symbolic decision procedures for QBF", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "bf30da52-b78b-43e1-9d56-3edb91c75316"}
{"abstract": "Given a set S of points (stations) located in the d-dim. Euclidean space and a root b \u2208 S, the h-HOPS CONVERGECAST problem asks to find for a minimal energy-cost range assignment which allows to perform the converge-cast primitive (i.e. node accumulation) towards b in at most h hops. For this problem no polynomial time algorithm is known even for h = 2. The main goal of this work is the design of an efficient distributed heuristic (i.e. protocol) and the analysis (both theoretical and experimental) of its expected solution cost. In particular, we introduce an efficient parameterized randomized protocol for h-HOPS CONVERGECAST and we analyze it on random instances created by placing n points uniformly at random in a d-cube of side length L. We prove that for h = 2, its expected approximation ratio is bounded by some constant factor. Finally, for h = 3,..., 8, we provide a wide experimental study showing that our protocol has very good performances when compared with previously introduced (centralized) heuristics.", "authors": ["Andrea E. F. Clementi", "Miriam Di Ianni", "Massimo Lauria", "Angelo Monti", "Gianluca Rossi", "Riccardo Silvestri"], "n_citation": 50, "title": "A distributed protocol for the bounded-hops converge-cast in ad-hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bfbbffa6-a893-4810-8bbd-79a43d979ee4"}
{"abstract": "We show that lambda calculus is a computation model which can step by step simulate any sequential deterministic algorithm for any computable function over integers or words or any datatype. More formally, given an algorithm above a family of computable functions (taken as primitive tools, i.e., kind of oracle functions for the algorithm), for every constant K big enough, each computation step of the algorithm can be simulated by exactly K successive reductions in a natural extension of lambda calculus with constants for functions in the above considered family. The proof is based on a fixed point technique in lambda calculus and on Gurevich sequential Thesis which allows to identify sequential deterministic algorithms with Abstract State Machines. This extends to algorithms for partial computable functions in such a way that finite computations ending with exceptions are associated to finite reductions leading to terms with a particular very simple feature.", "authors": ["Marie Ferbus-Zanda", "Serge Grigorieff"], "n_citation": 0, "title": "ASMs and Operational Algorithmic Completeness of Lambda Calculus", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "c0694017-bb48-48be-adef-016390363f3e"}
{"abstract": "We study the problem of determining whether a query is contained in another when queries can carry along annotations from source data. We say that a query is annotation-contained in another if the annotated output of the former is contained in the latter on every possible annotated input databases. We study the relationship between query containment and annotation-containment and show that annotation-containment is a more refined notion in general. As a consequence, the usual equivalences used by a typical query optimizer may no longer hold when queries can carry along annotations from the source to the output. Despite this, we show that the same annotated result is obtained whether intermediate constructs of 4 query are evaluated with set or bag semantics. We also give a necessary and sufficient condition, via homomorphisms, that checks whether a query is annotation-contained in another. Even though our characterization suggests that annotation-containment is more complex than query containment, we show that the annotation-containment problem is NP-complete, thus putting it in the same complexity class as query containment. In addition, we show that the annotation placement problem, which was first shown to be NP-hard in [7], is in fact DP-hard and the exact complexity of this problem still remains open.", "authors": ["Wang-Chiew Tan"], "n_citation": 0, "title": "Containment of relational queries with annotation propagation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c0de1360-e531-47d0-94b1-2686ad953948"}
{"abstract": "Substitutability and interchangeability in constraint satisfaction problems (CSPs) have been used as a basis for search heuristics, solution adaptation and abstraction techniques. In this paper, we consider how the same concepts can be extended to soft constraint satisfaction problems (SCSPs). We introduce two notions: threshold a and degradation 6 for substitutability and interchangeability, ( \u03b1 substitutability/interchangeability and  \u03b4 substitutability/interchangeability respectively). We show that they satisfy analogous theorems to the ones already known for hard constraints. In  \u03b1 interchangeability, values are interchangeable in any solution that is better than a threshold \u03b1, thus allowing to disregard differences among solutions that are not sufficiently good anyway. In  \u03b4 interchangeability, values are interchangeable if their exchange could not degrade the solution by more than a factor of \u03b4. We give efficient algorithms to compute ( \u03b4 /  \u03b1 )interchangeable sets of values for a large class of SCSPs.", "authors": ["Stefano Bistarelli", "Boi Faltings", "Nicoleta Neagu"], "n_citation": 50, "title": "Interchangeability in soft CSPs", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c1763f54-6599-4773-8b61-4e36108344f2"}
{"abstract": "The development of a dialogue system for any task implies the acquisition of a dialogue corpus in order to study the structure of the dialogues used in that task. This structure is reflected in the dialogue system behaviour, which can be rule-based or corpus-based. In the case of corpus-based dialogue systems, the behaviour is defined by statistical models which are inferred from an annotated corpus of dialogues. This annotation task is usually difficult and expensive, and therefore, automatic dialogue annotation tools are necessary to reduce the annotation effort. An automatic dialogue labeller technique that is based on n-grams is presented in this work. Its different variants are evaluated with respect to manual human annotations of a dialogue corpus devoted to train queries.", "authors": ["Carlos D. Mart\u00ednez-Hinarejos"], "n_citation": 0, "title": "Automatic annotation of dialogues using n-grams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c1a7200d-748f-4b17-8e26-536ab7987b98"}
{"authors": ["William Aiello", "Steven Michael Bellovin", "Matt Blaze", "Ran Canetti", "John Ioannidis", "Angelos D. Keromytis", "Omer Reingold"], "n_citation": 50, "title": "Efficient, DoS-resistant, secure key exchange for Internet protocols", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c1d1dfb5-8be8-45e5-af21-554135b4fde5"}
{"abstract": "This architecture/infrastructure of parallel optical networks couples data exploration, visualization, and collaboration technologies through IP at multi-gigabit speeds.", "authors": ["Larry Smarr", "Andrew A. Chien", "Thomas A. DeFanti", "Jason Leigh", "Philip M. Papadopoulos"], "n_citation": 197, "references": ["24ac4217-50ba-4681-9979-9290c300e528", "32ca8369-423d-49be-b4a6-6a19e0945039", "f4eaec62-ea96-4862-a2a5-67602988c4fd", "fdfd25a5-fb35-4218-a454-ee8d7fbf29e5"], "title": "The OptIPuter", "venue": "Communications of The ACM", "year": 2003, "id": "c263bb7a-71ee-4836-873d-457f0fd15535"}
{"abstract": "Electronic information grows rapidly as the Internet is widely used in our daily life. In order to identify the exact information for the user query, information extraction is widely researched and investigated. The template, which pertains to events or situations, and contains slots that denote who did what to whom, when, and where, is predefined by a template builder. Therefore, fixed templates are the main obstacles for the information extraction system out of the laboratory. In this paper, a method to automatically discover the event pattern in Chinese from stock market bulletin is introduced. It is based on the tagged corpus and the domain model. The pattern discovery process is independent of the domain model by introducing a link table. The table is the connection between text surface structure and semantic deep structure represented by a domain model. The method can be easily adapted to other domains by changing the link table.", "authors": ["Fang Li", "Huanye Sheng", "Dongmo Zhang"], "n_citation": 50, "title": "Event pattern discovery from the stock market bulletin", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c3cbf3df-7613-47b6-8381-df2cb20ded0a"}
{"authors": ["Piotr Hoffman", "Miko\u0142aj Boja\u0144czyk"], "n_citation": 0, "title": "Reachability in Unions of Commutative Rewriting Systems Is Decidable", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "c41f5116-d175-4a6f-9df0-53917685ed25"}
{"abstract": "Information visualization is gaining importance in data mining and transactional data has long been an important target for data miners. We propose a novel approach for visualizing transactional data using multiple clustering results for knowledge discovery. This scheme necessitates us to relate different clustering results in a comprehensive manner. Thus we have invented a method for attributing colors to clusters of different clustering results based on minimal transversals. The effectiveness of our method VISUMCLUST has been confirmed with experiments using artificial and real-world data sets.", "authors": ["Nicolas Durand", "Bruno Cr\u00e9milleux", "Einoshin Suzuki"], "n_citation": 0, "title": "Visualizing Transactional Data with Multiple Clusterings for Knowledge Discovery", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c42300f1-5bfa-4556-8190-473a9b7267a9"}
{"abstract": "In recent years, brain-computer interface (BCI) technology has emerged very rapidly. Brain-computer interfaces (BCIs) bring us a new communication interface technology which can translate brain activities into control signals of devices like computers, robots. The pre-processing of electroencephalographic (EEG) signal and translation algorithms play an important role in EEG-based BCIs. In this study, we employed an independent component analysis (ICA)-based preprocessing method and a committee machine-based translation algorithm for the offline analysis of a cursor control experiment. The results show that ICA is an efficient preprocessing method and the committee machine is a good choice for translation algorithm.", "authors": ["Jianzhao Qin", "Yuanqing Li", "Andrzej Cichocki"], "n_citation": 50, "title": "ICA and committee machine-based algorithm for cursor control in a BCI system", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c44c71bb-67e8-47e3-9920-936ebc5aa6cd"}
{"abstract": "Planning of local wireless communication networks is about installing base stations (small radio transmitters) to provide wireless devices with strong enough signals. POPULAR is an advanced industrial prototype that allows to compute the minimal number of base stations and their location given a blue-print of the installation site and information about the materials used for walls and ceilings. It does so by simulating the propagation of radio-waves using ray tracing and by subsequent optimization of the number of base stations needed to cover the whole building. Taking advantage of state-of-the-art techniques for programmable application-oriented constraint solving, POPULAR is among the first practical tools that can optimally plan wireless communication networks.", "authors": ["Thom W. Fr\u00fchwirth", "Pascal Brisset"], "n_citation": 0, "title": "Optimal placement of base stations in wireless indoor telecommunication", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "c4a3290d-9c9e-4947-b519-9dc98daddabc"}
{"abstract": "Recently media streaming applications via Peer-to-Peer (P2P) overlay networks are getting more and more significant. However, before these applications can be successfully deployed, it is very important to develop efficient access control mechanisms to ensure that only legitimate members can access the media content. Existing schemes of key management and distribution often fail in facing a large-scale group accessing. In this paper, we propose an efficient key management scheme (EKM) for large-scale P2P media streaming applications. It employs the Distributed Hash Table (DHT) technique to build a key distribution overlay network and incorporates a periodical global rekeying mechanism, which is highly scalable and efficient, and is robust against frequently joining/leaving of members. EKM can cut down the overhead of storage and communication on the server side, which can eliminate potential bottleneck of the server. We demonstrate its scalability, efficiency and robustness properties through simulation. Its performance can be examined under real environments by combining EKM with the existing P2P media streaming protocols.", "authors": ["Feng Qiu", "Chuang Lin", "Hao Yin"], "n_citation": 0, "title": "EKM : An Efficient Key Management Scheme for Large-Scale Peer-to-Peer Media Streaming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c4f90c80-45c2-4ef6-9a77-5f9a12c18e39"}
{"abstract": "In this work we address the question of whether and how parallel local search exhibits the criticality and parallelism phenomenon when performed on structured instances. We experimentally show that also for structured instances there exists an optimal value of parallelism which enables the algorithm to reach the optimal performance and, by analyzing the frequency of node degree of the graphs associated with the SAT instances, we observe that an asymmetric and not regular distribution strongly affects the algorithm performance with respect to the parallelism.", "authors": ["Andrea Roli"], "n_citation": 50, "title": "Criticality and parallelism in structured SAT instances", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c52bc8e0-179d-48a5-a6b3-3342fab9d946"}
{"abstract": "Microarrays allow the monitoring of thousands of genes simultaneously. Before a measure of gene activity of an organism is obtained, however, many stages in the error-prone manual and automated process have to be performed. Without quality control, the resulting measures may, instead of being estimates of gene activity, be due to noise or systematic variation. We address the problem of detecting spots of low quality from the microarray images to prevent them to enter the subsequent analysis. We extract features describing spatial characteristics of the spots on the microarray image and train a classifier using a set of labeled spots. We assess the results for classification of individual spots using ROC analysis and for a compound classification using a non-symmetric cost structure for misclassifications.", "authors": ["Salla Ruosaari", "Jaakko Hollm\u00e9n"], "n_citation": 0, "title": "Image analysis for detecting faulty spots from microarray images", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c5ca2665-7ef0-4966-a2ab-0f6029a0fbe3"}
{"abstract": "Many problems and applications can be naturally modelled and solved using constraints with more than two variables. Such n-ary constraints, in particular, arithmetic constraints are provided by many finite domain constraint programming systems. The best known worst case time complexity of existing algorithms (GAC-schema) for enforcing arc consistency on general CSPs is O(ed n ) where d is the size of domain, e is the number of constraints and n is the maximum number of variables in a single constraint. We address the question of efficient consistency enforcing for n-ary constraints. An observation here is that even with a restriction of n-ary constraints to linear constraints, arc consistency enforcing is NP-complete. We identify a general class of monotonic n-ary constraints (which includes linear inequalities as a special case). Such monotonic constraints can be made arc consistent in time O(en 3 d). The special case of linear inequalities can be made arc consistent in time O(en 2 d) using bounds-consistency which exploits special properties of the projection function.", "authors": ["Zhang Yuanlin", "Roland H. C. Yap"], "n_citation": 50, "title": "Arc consistency on n-ary monotonic and linear constraints", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "c5e70d54-94d9-4da4-b0c4-8ebce77d8f3e"}
{"authors": ["Khaireel A. Mohamed", "Amitava Datta", "Ryszard Kozera"], "n_citation": 0, "title": "A Knowledge-Based Technique for Constraints Satisfaction in Manpower Allocation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c68e0492-6fc9-410a-87bf-0af932e77fd6"}
{"abstract": "We present a method for generating linear invariants for large systems. The method performs forward propagation in an abstract domain consisting of arbitrary polyhedra of a predefined fixed shape. The basic operations on the domain like abstraction, intersection, join and inclusion tests are all posed as linear optimization queries, which can be solved efficiently by existing LP solvers. The number and dimensionality of the LP queries are polynomial in the program dimensionality, size and the number of target invariants. The method generalizes similar analyses in the interval, octagon, and octahedra domains, without resorting to polyhedral manipulations. We demonstrate the performance of our method on some benchmark programs.", "authors": ["Sriram Sankaranarayanan", "Henny B. Sipma", "Zohar Manna"], "n_citation": 0, "title": "Scalable analysis of linear systems using mathematical programming", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c6aec000-6db1-419b-8e6f-5fd4a9ab7b33"}
{"abstract": "In this paper, we address an optimization problem that arises in context of cache placement in sensor networks. In particular, we consider the cache placement problem where the goal is to determine a set of nodes in the network to cache/store the given data item, such that the overall communication cost incurred in accessing the item is minimized, under the constraint that the total communication cost in updating the selected caches is less than a given constant. In our network model, there is a single server (containing the original copy of the data item) and multiple client nodes (that wish to access the data item). For various settings of the problem, we design optimal, near-optimal, heuristic-based, and distributed algorithms, and evaluate their performance through simulations on randomly generated sensor networks.", "authors": ["Bin Tang", "Samir R. Das", "Himanshu Gupta"], "n_citation": 0, "title": "Cache placement in sensor networks under update cost constraint", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c6d7b907-e346-4089-8890-aef9a2163d2a"}
{"abstract": "This paper describes the architecture and implementation of a constraint-based framework for rapid prototyping of distributed applications such as virtual simulations, collaborations and games. Our framework integrates three components based on (concurrent) constraint programming ideas: (1) Hybrid cc, a (concurrent) constraint modeling language for hybrid systems, (2) Sisl, a (discrete) timed constraint language for describing interactive services with flexible user interfaces and (3) Triveni, a process-algebraic language for concurrent programming. The framework is realized as a collection of tools implemented in Java. The utility of the ideas are illustrated by sketching the implementations of simple distributed applications.", "authors": ["Vineet Gupta", "Lalita Jategaonkar Jagadeesan", "Radha Jagadeesan", "Xiaowei Jiang", "Konstantin L\u00e4ufer"], "n_citation": 0, "title": "A constraint-based framework for prototyping distributed virtual applications", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "c71a16ff-bb74-4e07-9e52-42163e7a977b"}
{"abstract": "Network Intrusion Detection Systems (NIDS) monitor a network with the aim of discerning malicious from benign activity on that network. While a wide range of approaches have met varying levels of success, most IDS's rely on having access to a database of known attack signatures which are written by security experts. Nowadays, in order to solve problems with false positive alerts, correlation algorithms are used to add additional structure to sequences of IDS alerts. However, such techniques are of no help in discovering novel attacks or variations of known attacks, something the human immune system (HIS) is capable of doing in its own specialised domain. This paper presents a novel immune algorithm for application to an intrusion detection problem. The goal is to discover packets containing novel variations of attacks covered by an existing signature base.", "authors": ["Gianni Tedesco", "Jamie Twycross", "Uwe Aickelin"], "n_citation": 50, "title": "Integrating innate and adaptive immunity for intrusion detection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c76696b5-f2c7-4375-866f-56a5bed58480"}
{"abstract": "We present a natural deduction calculus for the propositional linear-time temporal logic and prove its correctness. The system extends the natural deduction construction of the classical propositional logic. This will open the prospect to apply our technique as an automatic reasoning tool in a deliberative decision making framework across various AI applications.", "authors": ["Alexander Bolotov", "Artie Basukoski", "Oleg Grigoriev", "Vasilyi Shangin"], "n_citation": 50, "title": "Natural deduction calculus for linear-time temporal logic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c78fbb3b-51b0-4e93-849f-f692375c84d6"}
{"abstract": "In this paper, we propose a novel holistic methodology for keyword search in historical typewritten documents combining synthetic data and user's feedback. The holistic approach treats the word as a single entity and entails the recognition of the whole word rather than of individual characters. Our aim is to search for keywords typed by the user in a large collection of digitized typewritten historical documents. The proposed method is based on: (i) creation of synthetic image words; (ii) word segmentation using dynamic parameters; (iii) efficient hybrid feature extraction for each image word and (iv) a retrieval procedure that is optimized by user's feedback. Experimental results prove the efficiency of the proposed approach.", "authors": ["Basilis Gatos", "Thomas Konidaris", "Ioannis Pratikakis", "Stavros J. Perantonis"], "n_citation": 0, "title": "A holistic methodology for keyword search in historical typewritten documents", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c7f253e2-eb43-4ab9-b287-26dd3e54b12f"}
{"abstract": "The eligibility trace is one of the basic mechanisms in reinforcement learning to handle delayed reward. In this paper, we have used meta-heuristic method to solve hard combinatorial optimization problems. Our proposed solution introduce Ant-Q learning method to solve Traveling Salesman Problem (TSP). The approach is based on population that use positive feedback as well as greedy search and suggest ant reinforcement learning algorithms using eligibility traces which is called replace-trace methods(Ant-TD(\u03bb)). Although replacing traces are only slightly, they can produce a significant improvement in learning rate. We could know through an experiment that proposed reinforcement learning method converges faster to optimal solution than ACS and Ant-Q.", "authors": ["Seunggwan Lee"], "n_citation": 0, "title": "On the Efficient Implementation Biologic Reinforcement Learning Using Eligibility Traces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c7f4f7d2-4e02-471d-a204-22679cd5805a"}
{"abstract": "TV broadcast video stream consists of various kinds of programs such as sitcoms, news, sports, commercials, weather, etc. In this paper, we propose a semantic image category, named as Program Oriented Informative Images (POIM), to facilitate the segmentation, indexing and retrieval of different programs. The assumption is that most stations tend to insert lead-in/-out video shots for explicitly introducing the current program and indicating the transitions between consecutive programs within TV streams. Such shots often utilize the overlapping of text, graphics, and storytelling images to create an image sequence of POIM as a visual representation for the current program. With the advance of post-editing effects, POIM is becoming an effective indicator to structure TV streams, and also is a fairly common prop in program content production. We have attempted to develop a POIM recognizer involving a set of global/local visual features and supervised/unsupervised learning. Comparison experiments have been carried out. A promising result, F1 = 90.2%, has been achieved on a part of TRECVID 2005 video corpus. The recognition of POIM, together with other audiovisual features, can be used to further determine program boundaries.", "authors": ["Jinqiao Wang", "Ling-Yu Duan", "Hanqing Lu", "Jesse S. Jin"], "n_citation": 50, "title": "A Semantic Image Category for Structuring TV Broadcast Video Streams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c8119c3b-07c5-4e31-bc94-0ea6d88a2712"}
{"abstract": "Directed model checking has proved itself to be a useful technique in reducing the state space of the problem graph. But still, its potential is limited by the available memory. This problem can be circumvented by the use of secondary storage devices to store the state space. This paper discusses directed best-first search to enhance error detection capabilities of model checkers like SPIN by using a streamed access to secondary storage. We explain, how to extend SPIN to allow external state access, and how to adapt heuristic search algorithms to ease error detection for this case. We call our derivate IO-HSF-SPIN. In the theoretical part of the paper, we extend the heuristic-based external searching algorithm to general weighted and directed graphs. We conduct experiments with some challenging protocols in Promela syntax like GIOP and dining philosophers and have succeeded in solving some hard instances externally.", "authors": ["Shahid Jabbar", "Stefan Edelkamp"], "n_citation": 0, "title": "I/O efficient directed model checking", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c8576484-7848-48a0-a610-d75c471c0478"}
{"abstract": "Making use of the ubiquitous kernel notion, we present a new nonlinear supervised feature extraction technique called Kernel Springy Discriminant Analysis. We demonstrate that this method can efficiently reduce the number of features and increase classification performance. The improvements obtained admittedly arise from the nonlinear nature of the extraction technique developed here. Since phonological awareness is a great importance in learning to read, a computer-aided training system could be most beneficial in teaching young learners. Naturally, our system employs an effective automatic phoneme recognizer based on the proposed feature extraction technique.", "authors": ["Andr\u00e1s Kocsor", "Korn\u00e9l Kov\u00e1cs"], "n_citation": 0, "title": "Kernel Springy Discriminant Analysis and its application to a phonological awareness teaching system", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c8b07300-24a8-4ac0-b08b-3fc72ea4b655"}
{"abstract": "Constraint programming is rapidly becoming the technology of choice for modeling and solving complex combinatorial problems. However, users of constraint programming technology need significant expertise in order to model their problem appropriately. The lack of availability of such expertise can be a significant bottleneck to the broader uptake of constraint technology in the real world. In this paper we are concerned with automating the formulation of constraint satisfaction problems from examples of solutions and non-solutions. We combine techniques from the fields of machine learning and constraint programming. In particular we present a portfolio of approaches to exploiting the semantics of the constraints that we acquire to improve the efficiency of the acquisition process. We demonstrate how inference and search can be used to extract useful information that would otherwise be hidden in the set of examples from which we learn the target constraint satisfaction problem. We demonstrate the utility of the approaches in a case-study domain.", "authors": ["Christian Bessiere", "Remi Coletta", "Eugene C. Freuder", "Barry O'Sullivan"], "n_citation": 0, "title": "Leveraging the learning power of examples in automated constraint acquisition", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c943a320-780c-4d2b-8485-a4f01963706f"}
{"abstract": "Intruders usually log in through a chain of multiple computer systems to hide their origins before breaking into their targets, which makes tracing difficult. In this paper we present a method to find the connection chain of an intruder for tracing back to the origin. We focus on telnet and rlogin as interactive applications intruders use to log in through hosts. The method involves setting up packet monitors at as many traffic points as possible on the Internet to record the activities of intruders at the packet level. When a host is compromised and used as a step-through host to access another host, we compare the packet logs of the intruder at that host to logs we have recorded all over the Internet to find the closest match. We define the 'deviation' for one packet stream on a connection from another, and implement a system to compute deviations. If a deviation is small, the two connections must be in the same connection chain. We present some experimental results showing that the deviation for two unrelated packet streams is large enough to be distinguished from the deviation for packet streams on connections in the same chain.", "authors": ["Kunikazu Yoda", "Hiroaki Etoh"], "n_citation": 0, "title": "Finding a connection chain for tracing intruders", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "c9454ab4-d658-4d8c-89d2-f9c101a85b2f"}
{"abstract": "Set constraints is a suitable formalism for static analysis of programs. However, it is known that the complexity of set constraint problems in the most general cases is very high (NEXPTIME-completeness of the satisfiability test). Lots of works are involved in finding more tractable subclasses. In this paper, we investigate two classes of set constraints shown to be useful for program analysis: the first one is an extension of definite set constraints including the main feature of quantified set expressions. We will show that the satisfiability problem for this class is EXPTIME-complete. The second one concerns constraints of the form X \u2286 exp, where exp is built with function symbols, the intersection and union connectives and projection operators. The dual aspects of those two classes allows to find a common approach for solving both of them. This approach uses as basic tool tree automata, which are suitable both for computation and representing the solution of those solving problems. It leads also to simple algorithms and an easy characterization of complexity.", "authors": ["Philippe Devienne", "Jean-Marc Talbot", "Sophie Tison"], "n_citation": 0, "title": "Solving classes of set constraints with tree automata", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "c9adc97c-daa7-4343-9373-8c9f230fa382"}
{"abstract": "Experimental results are given on the scaling of the Pure Random Walk version (PRWSAT) of WalkSAT. PRWSAT is very simple because of the absence of heuristics: not only the clause is selected at random, but also the literal within that clause. The main result is that, despite the simplicity and absence of heuristics, it has nontrivial behavior on Random 3-SAT. There appears to be a threshold at a clause/variable ratio of about 2.65. Below the threshold, problems are solved in a tightly-distributed and linear number of flips. Above the threshold scaling appears to be non-polynomial. The simplicity and the nontrivial threshold make it a good candidate for theoretical analysis.", "authors": ["Andrew J. Parkes"], "n_citation": 0, "title": "Scaling properties of Pure Random Walk on Random 3-SAT", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c9be0601-178a-4a3b-b920-51c13b7ca1e8"}
{"abstract": "Recently, identity-based cryptographic (IBC) schemes have been considered to secure mobile ad hoc networks (MANETs) due to their efficient key management properties. However, proposed schemes do not provide mechanisms for key revocation and key renewal. In this paper, we propose the first key revocation and key renewal mechanisms for IBC schemes that are especially designed for MANETs. In our fully self-organized revocation scheme, each node monitors nodes in communication range and securely propagates its observations. The public key of a node is revoked if a minimum number of nodes accused the node. To enable key renewal, we introduce a modified format for ID-based public keys, such that new keys can be issued for the same identity. The introduced revocation scheme is efficient because it uses pre-shared keys from the Weil pairing and messages are sent to an m-hop neighborhood instead to the entire network.", "authors": ["Katrin Hoeper", "Guang Gong"], "n_citation": 0, "title": "Key revocation for identity-based schemes in mobile ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c9d84870-bb51-42b0-8731-c7cc5e897b4c"}
{"abstract": "In this paper we examine the notion of plaintext awareness as it applies to hybrid encryption schemes. We apply this theory to the Cramer-Shoup hybrid scheme acting on fixed length messages and deduce that the Cramer-Shoup scheme is plaintext-aware in the standard model. This answers a previously open conjecture of Bellare and Palacio on the existence of fully plaintext-aware encryption schemes.", "authors": ["Alexander W. Dent"], "n_citation": 0, "title": "The cramer-shoup encryption scheme is plaintext aware in the standard model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ca50fa7d-5d49-4e68-b36d-c07e7eb6d67a"}
{"abstract": "This paper presents the latest version of the SKiPPER skeleton-based parallel programming environment dedicated to fast prototyping of vision applications. Compared to the previous version, its main innovative feature is the ability to handle arbitrary skeleton nesting.", "authors": ["R\u00e9mi Coudarcher", "Jocelyn S", "Rot", "Jean-Pierre Derutin"], "n_citation": 50, "title": "Implementation of a skeleton-based parallel programming environment supporting arbitrary nesting", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "cb28b23a-9fc8-497e-b74c-2aca0bf3343c"}
{"abstract": "Since small low-powered sensor nodes are constrained in their computation, communication, and storage capabilities, it is not easy to achieve secure key establishment in a wireless sensor network where a number of such sensor nodes are spread over. There are many previous studies in the area of secure key establishment without public key cryptography for the wireless sensor networks. Among them, location-aware key management is a considerable approach for easy management and security enhancement. In this paper, we propose a new key establishment scheme by utilizing both the rough sensor location information and the multi-layer grids. As for the multi-layer grids, we devise an extended grid group which covers all nodes deployed in two adjacent basic grids and overlaps each other. With regard to communication and power consumption overhead, our approach shows better performance than the previously proposed schemes without losing its security.", "authors": ["JongHyup Lee", "Taekyoung Kwon", "JooSeok Song"], "n_citation": 0, "title": "Location-aware key management using multi-layer grids for wireless sensor networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cbe91046-a092-4b0c-ac18-e5761efde4d7"}
{"abstract": "A (t, n) threshold signature scheme allows t or more group members to generate signatures on behalf of a group with n members, while any t - 1 or less members cannot do the same thing. In 2001, based on a variant of ElGamal digital signature scheme, Li et al. proposed two (t,n) threshold signature schemes with traceable signers. One of their schemes needs the assistance of a mutually trusted center, while the other does not. In this paper, we present a security analysis on their schemes. We first point out that in fact signers in their schemes are untraceable, since anybody can convert a valid threshold signature into a new one such that another subset of group members will be wrongly considered as the signers of the new threshold signature for the same message. Furthermore, we demonstrate an attack to show that their second threshold signature scheme is insecure. In our attack, (n - t + 1) colluding members can control the group secret key. Therefore, they can generate valid threshold signature for any message without the help of other members. Furthermore, honest members cannot detect this security flaw in the system, since any t members can generate threshold signatures according to the prescribed protocols.", "authors": ["Guilin Wang", "Xiaoxi Han", "Bo Zhu"], "n_citation": 0, "title": "On the security of two threshold signature schemes with traceable signers", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "cc594e8b-04f3-4d04-990c-30f0c90d18be"}
{"abstract": "Accessibility of digital libraries and other web-based repositories has caused the illusion of accessibility of the full texts of scientific papers. However, in the majority of cases such an access (at least free access) is limited only to abstracts having no more then 50-100 words. Traditional keyword-based approach for clustering this type of documents gives unstable and imprecise results. We show that they can be easy improved with more adequate keyword selection and document similarity evaluation. We suggest simple procedures for this. We evaluate our approach on the data from two international conferences. One of our conclusions is the suggestion for the digital libraries and other repositories to provide document images of full texts of the papers along with their abstracts for open access via Internet.", "authors": ["Pavel Makagonov", "Mikhail Alexandrov", "Alexander F. Gelbukh"], "n_citation": 0, "title": "Clustering abstracts instead of full texts", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cd163f78-a00a-4d9e-b2e4-02d395a0a6ff"}
{"abstract": "This paper is concerned with the neighbourhood-based supervised learning of a continuous class. It deals with identifying and handling outliers. We first explain why and how to use the neighbourhood graph issued from predictors in the prediction of a continuous class. Global quality of the representation is evaluated by a neighbourhood autocorrelation coefficient derived from the Moran spatial autocorrelation coefficient. Extending the analogy with spatial analysis, we suggest to identify outliers by using the scattering diagram associated to the Moran coefficient. Several experiments realized on classical benchmarks show the interest of removing the outliers with this new method.", "authors": ["Fabrice Muhlenbach", "St\u00e9phane Lallich", "Djamel Abdelkader Zighed"], "n_citation": 0, "title": "Outlier handling in the neighbourhood-based learning of a continuous class", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cd179829-0e0d-477b-a8c3-a75c2c0e52a6"}
{"abstract": "When agent chooses some action and does state transition in present state in reinforcement learning, it is important subject to decide how will reward for conduct that agent chooses. In this paper, by new meta heuristic method to solve hard combinatorial optimization problems, we introduce Ant-Q learning method that has been proposed to solve Traveling Salesman Problem (TSP) to approach that is based for population that use positive feedback as well as greedy search, and suggest ant reinforcement learning model using TD-error(ARLM-TDE). We could know through an experiment that proposed reinforcement learning method converges faster to optimal solution than original ACS and Ant-Q.", "authors": ["SeungGwan Lee"], "n_citation": 0, "title": "Multiagent reinforcement learning algorithm using temporal difference error", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cd8688be-cae8-40c8-a236-38499490962d"}
{"abstract": "We construct a fully collusion resistant tracing traitors system with sublinear size ciphertexts and constant size private keys. More precisely, let N be the total number of users. Our system generates ciphertexts of size O(\u221aN) and private keys of size O(1). We first introduce a simpler primitive we call private linear broadcast encryption (PLBE) and show that any PLBE gives a tracing traitors system with the same parameters. We then show how to build a PLBE system with O(\u221aN) size ciphertexts. Our system uses bilinear maps in groups of composite order.", "authors": ["Dan Boneh", "Amit Sahai", "Brent Waters"], "n_citation": 0, "title": "Fully collusion resistant traitor tracing with short ciphertexts and private keys", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cde2fd53-9446-45d6-b46a-c81ba94ccad6"}
{"abstract": "The principle of solving multiobjective optimization problems with fuzzy sets theory is studied. Membership function is the key to introduce the fuzzy sets theory to multiobjective optimization. However, it is difficult to determine membership functions in engineering applications. On the basis of rapid quadratic optimization in the learning of weights, simplification in hardware as well as in computational procedures of functional-link net, discrete membership functions are used as sample training data. When the network converges, the continuous membership functions implemented with the network. Membership functions based on functional-link net have been used in multiobjective optimization. An example is given to illustrate the method.", "authors": ["Ping Wang", "Hong-Zhong Huang", "Ming J. Zuo", "Weidong Wu", "Chunsheng Liu"], "n_citation": 0, "title": "Functional-link net based multiobjective fuzzy optimization", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ce1c3fe4-fc7c-414e-9a4d-a42c5ed63d1e"}
{"authors": ["J. Coulson", "H. Kosch", "Odej Kao", "Frank J. Seinstra"], "n_citation": 0, "title": "Topic 11: Distributed and High-Performance Multimedia", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ce2ab282-61e6-4330-82ed-c11f38cf3491"}
{"abstract": "Perfectly secret message transmission can be realized with only partially secret and weakly correlated information shared by the parties as soon as this information allows for the extraction of information-theoretically secret bits. The best known upper bound on the rate S at which such key bits can be generated has been the intrinsic information of the distribution modeling the parties', including the adversary's, knowledge. Based on a new property of the secret-key rate S, we introduce a conditional mutual information measure which is a stronger upper bound on S. Having thus seen that the intrinsic information of a distribution P is not always suitable for determining the number of secret bits extractable from P, we prove a different significance of it in the same context: It is a lower bound on the number of key bits required to generate P by public communication. Taken together, these two results imply that sometimes, (a possibly arbitrarily large fraction of) the correlation contained in distributed information cannot be extracted in the form of secret keys by any protocol.", "authors": ["Renato Renner", "Stefan Wolf"], "n_citation": 111, "title": "New bounds in secret-key agreement: The gap between formation and secrecy extraction", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "ce8438b2-222c-4a65-875c-6c33464d9f96"}
{"abstract": "This paper deals with head-driven chart parsing on large natural-language grammars. We present a procedure that optimizes positions of heads in the grammar rules based on the number of edges in the resulting chat. New performance evaluation tool PACE is briefly introduced first. The head-optimizing procedure is described and the results are given. The last part compares the head positions obtained automatically with those set according to traditional linguistics.", "authors": ["Vladim\u00edr Kadlec", "Pavel Smrz"], "n_citation": 0, "title": "Grammatical heads optimized for parsing and their comparison with linguistic intuition", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "cecac804-3c83-443c-8366-746ec8f71b96"}
{"abstract": "In this paper, we introduce a high-level modeling approach to camera control. The aim is to determine the path of a camera that verifies given declarative properties on the desired image, e.g., location or orientation of objects on the screen at a given time. The path is composed of elementary movements called hypertubes, based on established cinematographic techniques. Hypertubes are connected by relations that guarantee smooth transitions. Interval consistency techniques and quantified constraint solving algorithms are used to compute and propagate solutions between consecutive hypertubes. Preliminary experimental results from a prototype show a great improvement in time and quality of animations with respect to former approaches.", "authors": ["Marc Christie", "Eric Langu\u00e9nou", "Laurent Granvilliers"], "n_citation": 0, "title": "Modeling camera control with constrained hypertubes", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ceeaa7d7-b505-4532-b61c-6b7bc05a0b95"}
{"abstract": "In this paper, an energy-efficient data disemination protocol designed for mobile sink applications in sensor networks, is proposed. The dissemination scheme exploits the Distributed Dynamic Tree (DDT), which is able to identify current location sinks locally and dynamically transform the tree shape according to sink movement. In addition, the Dynamic Shared Tree(DST), which is an extension of the DDT, is presented. The DST is able to accomocate multiple mobile sinks. The DST, based on the DDT, creates a two-tired network composed of a sensor data dissemination level and communication level between sinks. The simulation results demonstrate that the DST performs considerably energy-efficient data dissemination with relatively low delay, compared to other dissemination protocols.", "authors": ["Kwang-Il Hwang", "Doo-Seop Eom"], "n_citation": 0, "title": "Energy-efficient data dissemination in sensor networks using distributed dynamic tree management", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cef923d6-3e2a-4529-b627-75b2bb34351f"}
{"abstract": "This paper gives a survey of the current state of ARTIC - the modern Czech concatenative corpus-based text-to-speech system. All stages of the system design are described in the paper, including the acoustic unit inventory building process, text processing and speech production issues. Two versions of the system are presented: the single unit instance system with the moderate output speech quality, suitable for low-resource devices, and the multiple unit instance system with a dynamic unit instance selection scheme, yielding the output speech of a high quality. Both versions make use of the automatically designed acoustic unit inventories. In order to assure the desired prosodic characteristics of the output speech, system-version-specific prosody generation issues are discussed here too. Although the system was primarily designed for synthesis of Czech speech, ARTIC can now speak three languages: Czech (both female and male voices are available), Slovak and German.", "authors": ["Jindrich Matousek", "Daniel Tihelka", "Jan Romportl"], "n_citation": 0, "title": "Current state of czech text-to-speech system ARTIC", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cf45d5ce-b9ba-43ea-b950-c1e4b779bb8e"}
{"abstract": "We expand a previous result of Dean [Dea99] to provide a second preimage attack on all n-bit iterated hash functions with Damgard-Merkle strengthening and n-bit intermediate states, allowing a second preimage to be found for a 2 k -message-block message with about k \u00d7 2 n/2+1 +2 n-k+1  work. Using RIPEMD-160 as an example, our attack can find a second preimage for a 2 60  byte message in about 2 106  work, rather than the previously expected 2 160  work. We also provide slightly cheaper ways to find multicollisions than the method of Joux [Jou04]. Both of these results are based on expandable messages-patterns for producing messages of varying length, which all collide on the intermediate hash result immediately after processing the message. We provide an algorithm for finding expandable messages for any n-bit hash function built using the Damgard-Merkle construction, which requires only a small multiple of the work done to find a single collision in the hash function.", "authors": ["John Kelsey", "Bruce Schneier"], "n_citation": 410, "title": "Second preimages on n-bit hash functions for much less than 2n work", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "cfeb05b9-193c-4734-b5f0-da6a27ce0fea"}
{"abstract": "After the years of hesitation the conservative Slovak telecommunication market seems to become conscious of the need of voice driven services. In the last year, all the three telecommunication operators have adopted our text to speech system Kempelen in their interactive voice response systems. The diphone concatenative synthesis has probably reached the frontier of its abilities and so the next step is to check for a synthesis method giving more intelligible and more natural synthesized speech with better prosody modelling. Therefore we have decided to build a one speaker speech database in Slovak for experiments and application building in unit-selection speech synthesis. To build such a database, we tried to exploit as much of the existing speech resources in Slovak as possible, to utilize the knowledge from previous projects and to use the existing routines developed at our department. The paper describes the structure, recording and annotation of this database as well as first experiments with unit-selection speech synthesizer.", "authors": ["Milan Rusko", "Mari\u00e1n Trnka", "Sachia Darzagin", "Milos Cernak"], "n_citation": 0, "title": "Slovak speech database for experiments and application building in unit-selection speech synthesis", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d03800cb-d86d-4a58-83a7-f33a3a0509fe"}
{"abstract": "In recent years, digital watermarking has become a popular technique for hiding information in digital images to help protect against copyright infringement. In this paper we develop a high quality and robust watermarking algorithm that combines the advantages of block-based permutation with that of neighboring coefficient embedding. The proposed approach uses the relationship between the coefficients of neighboring blocks to hide more information into high frequency blocks without causing serious distortion to the watermarked image. In addition, an extraction method for improving robustness to mid-frequency filter attacks is proposed. Our experimental results show that the proposed approach is very effective in achieving perceptual invisibility with an increase in the peak signal to noise ratio (PSNR). Moreover, the proposed approach is robust to a variety of signal processing operations, such as compression (JPEG), image cropping, sharpening, blurring, and brightness adjustments. In those experimentation, the robustness is especially evident under the attack of blurring.", "authors": ["Yu-Ting Pai", "Shanq-Jang Ruan", "Jiirgen Gbtze"], "n_citation": 0, "title": "A High Quality Robust Watermarking Scheme", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d067a8f9-16c1-4012-b280-c140bb8c1f36"}
{"abstract": "Spam messages are continually filling email boxes of practically every Web user. To deal with this growing problem, the development of high-performance filters to block those unsolicited messages is strongly required. An Antibody Network, more precisely SRABNET (Supervised Real-Valued Antibody Network), is proposed as an alternative filter to detect spam. The model of the antibody network is generated automatically from the training dataset and evaluated on unseen messages. We validate this approach using a public corpus, called PU1, which has a large collection of encrypted personal e-mail messages containing legitimate messages and spam. Finally, we compared the performance with the well known naive Bayes filter using some performances indexes that will be presented.", "authors": ["George Barreto Bezerra", "Tiago V. Barra", "Hamilton M. Ferreira", "Helder Knidel", "Leandro Nunes de Castro", "Fernando J. Von Zuben"], "n_citation": 0, "title": "An immunological filter for spam", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d06b57b8-2ad2-420d-a15e-199e21a181c8"}
{"abstract": "We present a method to extract principal deformation modes from a set of articulated models describing the human spine. The spine was expressed as a set of rigid transforms that superpose local coordinates systems of neighbouring vertebrae. To take into account the fact that rigid transforms belong to a Riemannian manifold, the Frechet mean and a generalized covariance computed in the exponential chart of the Frechet mean were used to construct a statistical shape model. The principal deformation modes were then extracted by performing a principal component analysis (PCA) on the generalized covariance matrix. Principal deformations modes were computed for a large database of untreated scoliotic patients and the obtained results indicate that combining rotation and translation into a unified framework leads to an effective and meaningful method of dimensionality reduction for articulated anatomical structures. The computed deformation modes also revealed clinically relevant information. For instance, the first mode of deformation appeared to be associated with patients' growth, the second is a double thoraco-lumbar curve and the third is a thoracic curve.", "authors": ["Jonathan Boisvert", "Xavier Pennec", "Hubert Labelle", "Farida Cheriet", "Nicholas Ayache"], "n_citation": 0, "title": "Principal spine shape deformation modes using riemannian geometry and articulated models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d0a5f04e-0708-4ffd-861b-6ec91e448a6e"}
{"abstract": "We present a technique to prove termination of multipath polynomial programs, an expressive class of loops that enables practical code abstraction and analysis. The technique is based on finite differences of expressions over transition systems. Although no complete method exists for determining termination for this class of loops, we show that our technique is useful in practice. We demonstrate that our prototype implementation for C source code readily scales to large software projects, proving termination for a high percentage of targeted loops.", "authors": ["Aaron R. Bradley", "Zohar Manna", "Henny B. Sipma"], "n_citation": 143, "title": "Termination of polynomial programs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d0c04d71-ecc8-48ae-aa6b-58d40bcfbb44"}
{"abstract": "In this paper we combine the error correction and encryption functionality into one block cipher, which we call High Diffusion (HD) cipher. The error correcting property of this cipher is due to the novel error correction code which we call High Diffusion code used in its diffusion layer. Theoretical bounds on the performance of the HD cipher in terms of security and error correction are derived. We show that the proposed HD cipher provides security equivalent to Rijndael cipher against linear and differential cryptanalysis. Experiments based on a four round HD cipher reveal that traditional concatenated systems using the Rijndael cipher followed by Reed Solomon codes require 89% more expansion to match the performance of HD cipher.", "authors": ["Chetan Nanjunda Mathur", "Karthik Narayan", "K. P. Subbalakshmi"], "n_citation": 0, "title": "High diffusion cipher : Encryption and error correction in a single cryptographic primitive", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d0d8f622-b2ab-4856-9172-fceb12154f6d"}
{"abstract": "Almost resilient function is the generalization of resilient function and has important applications in multiple authenticate codes and almost security cryptographic Boolean functions. In this paper, some constructions are provided. In particular, the Theorem 3 in [7] is improved. As e-almost (n, 1, k)-resilient functions play an important role in the secondary constructions, we concluded some properties and constructions. Specially we presented a spectral characterization of almost (n, 1, k)-resilient functions, which can be used to identify an almost (n, 1, k)-resilient function by computing its walsh spectra.", "authors": ["Pinhui Ke", "Jie Zhang", "Qiaoyan Wen"], "n_citation": 0, "title": "Results on almost resilient functions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d0e3dc60-e217-40f5-b231-feded2e935f8"}
{"abstract": "More and more coordination of health care relies on the electronic transmission of confidential information about patients between different health care services. Since the patient data is confidential, patients should be able to delegate, give or withhold e-consent to those who wish to access their electronic health information. Therefore the problem of how to represent and evaluate e-consent becomes quite important in secure health information processing. This paper presents an authorization model for e-consent requirement in a health care application. The model supports well controlled consent delegation, both explicit and implicit consent and denial, individual based or role based consent model, and consent inheritance and exception. A system architecture for e-consent is also presented.", "authors": ["Chun Ruan", "Vijay Varadharajan"], "n_citation": 0, "title": "An authorization model for E-consent requirement in a health care application", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d1555e71-e282-4c06-8405-027580ce926f"}
{"abstract": "This PhD thesis addresses the shop-oor control level in manufacturing and industrial production at some important points for the future. The main contributions are principles and software relating to the control of production systems and hereunder production devices. Specifically challenging at the production device level is the real-time, applicationoriented and sensor-integrated motion control of industrial robots. At the higher level, the production control system for a shop-oor, considered as an ensemble of independently controllable production devices, is addressed. The challenge at this higher level regards the representation of all production devices of a shop-oor by suitable control interfaces for a distributed deployment of the control system. The control of industrial robots and their integration into a production system is proposed to be troubled by each robot manufacturer developing their own native application platform. Each native platform oers some pre-chosen paradigm for programming and its own restricted set of technologies available for application development. For migrating the real-time motion control of industrial robots onto application platforms oering higher exibility, or simply the right technologies, than that found in the native controllers, several software frameworks and platforms exist. The work underlying this PhD thesis has produced such a software framework, dubbed PyMoCo. It is entirely implemented in the high-level, interpreted programming language Python, and allows fast and highly exible real-time motion control and application integration of industrial robots. At the shop-floor control level, it is proposed by a segment of the research community dealing with production control, that holonic and multi-agent control architectures are suitable paradigms for bringing highly intelligent, computationally powerful resources into the direct control loops in production systems. The use of autonomous holonic or agent-based systems for distributed real-time shop-oor control presents some inherent difficulties regarding formal validation and verification. Hence, in the research community it has been suggested to use real-time, realistic emulators of the production systems as platforms for experimenting, developing, and validating holonic and agent-based production control systems. This PhD thesis presents principles for using the Blender Game Engine for implementing a physically realistic real-time emulator for the collection of production devices of a production shop-floor. An emulator has been designed and implemented for an extended version of a prototype shop-floor system set up in a laboratory, and an experiment control system has been implemented to validate the emulator. The presented emulator is developed to a state of being suitable for use as a realistic platform for development of a distributed, autonomous multi-agent system for control of the extended prototype shop-floor system.", "authors": ["Morten Lind", "Olivier Roulet-Dubonnet", "Per \u00c5ge Nyen", "Lars Tore Gellein", "Terje K. Lien", "Amund Skavhaug"], "n_citation": 50, "title": "Holonic Manufacturing Paint Shop", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "d1556f4a-2525-4576-9e73-bf50dbb81a4f"}
{"abstract": "In this paper, we present a new method for supporting abstraction refinement in path-sensitive dataflow analysis. We show how an adjustable merge criterion can be used as an interface to control the degree of abstraction. In particular, we partition the merge criterion with two sets of predicates - one related to the dataflow facts being propagated and the other related to path feasibility. These tracked predicates are then used to guide merge operations and path feasibility analysis, so that expensive computations are performed only at the right places. Refinement amounts to lazily growing the path predicate set to recover lost precision. We have implemented our refinement technique in ESP, a software validation tool for C/C++ programs. We apply ESP to validate a future version of Windows against critical security properties. Our experience suggests that applying iterative refinement to path-sensitive dataflow analysis is both effective in cutting down spurious errors and scalable enough for solving real world problems.", "authors": ["Dinakar Dhurjati", "Manuvir Das", "Yue Yang"], "n_citation": 0, "title": "Path-sensitive dataflow analysis with iterative refinement", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d164c986-eaee-4d81-9106-bf31ecd025ec"}
{"abstract": "Quality fluctuation has a major negative effect on perceptive video quality. Many recent video quality smoothing works target on constant distortion (i.e., constant PSNR) throughout the whole coded video sequence. In [1], a target distortion was set up for each frame based on a hypothesis that maintaining constant distortion over frames would boast video quality smoothing and extensive experiments showed the constant-distortion bit allocation (CDBA) scheme significantly outperforms the popular constant bit allocation (CBA) scheme and Xie et al's recent work [2, 3] in terms of delivered video quality. But during the scene changes, it has been observed that the picture energy often dramatically changes. Maintaining constant PSNR would result in dramatically different SNR performance and translate into dramatically different perceptive effects. Although computationally more complex, SNR represents a more objective measure than PSNR in assessing video quality. In this paper, a single-pass frame-level constant-SNR bit allocation scheme (CSNRBA) is developed for video quality smoothing throughout the video sequence. To achieve constant-SNR, a power series weighted actual SNR average of previous coded frames is adopted as the target SNR for the current frame. From the target SNR, the target distortion for the current frame is calculated. Then according to the analytic close-form D-Q model and the linear rate control algorithm, the bit budget for the current frame can be estimated. Experimental results show that the proposed CSNRBA scheme provides much smoother video quality and achieve much better subjective video quality in terms of natural color, sharp objects and silhouette significantly on all testing video sequences than both CBA and CDBA schemes.", "authors": ["Xiangui Kang", "Junqiang Lan", "Li Liu", "Xinhua Zhuang"], "n_citation": 0, "title": "SNR-Based Bit Allocation in Video Quality Smoothing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d1a64502-8cd4-4f71-a681-a4a1c0c06db0"}
{"abstract": "Given a finite-state abstraction of a sequential program with potentially recursive procedures and input from the environment, we wish to check statically whether there are input sequences that can drive the system into bad/good executions. Pushdown games have been used in recent years for such analyses and there is by now a very rich literature on the subject. (See, e.g., [BS92,Tho95,Wal96,BEM97,Cac02a,CDT02].) In this paper we use recursive game graphs to model such interprocedural control flow in an open system. These models are intimately related to pushdown systems and pushdown games , but more directly capture the control flow graphs of recursive programs ([AEY01,BGR01,ATM03b]). We describe alternative algorithms for the well-studied problems of determining both reachability and Buchi winning strategies in such games. Our algorithms are based on solutions to second-order data flow equations, generalizing the Datalog rules used in [AEY01] for analysis of recursive state machines. This offers what we feel is a conceptually simpler view of these well-studied problems and provides another example of the close links between the techniques used in program analysis and those of model checking. There are also some technical advantages to the equational approach. Like the approach of Cachat [Cac02a], our solution avoids the necessarily exponential-space blow-up incurred by Walukiewicz's algorithms for pushdown games. However, unlike [Cac02a], our approach does not rely on a representation of the space of winning configurations of a pushdown graph by (alternating) automata. Only minimal sets of exits that can be forced need to be maintained, and this provides the potential for greater space efficiency. In a sense, our algorithms can be viewed as an automaton-free version of the algorithms of [Cac02a].", "authors": ["Kousha Etessami"], "n_citation": 0, "title": "Analysis of recursive game graphs using data flow equations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d2b43670-ffab-4ee4-98f6-09ac06297fdc"}
{"abstract": "We address a fundamental question concerning spatio-temporal database systems: What are exactly spatio-temporal queries? We define spatio-temporal queries to be computable mappings that are also generic, meaning that the result of a query may only depend to a limited extent on the actual internal representation of the spatio-temporal data. Genericity is defined as invariance under transformations that preserve certain characteristics of spatio-temporal data (e.g., collinearity, distance, velocity, acceleration,...) that are relevant to a database user. These transformations also respect the monotone nature of time. We investigate different genericity classes relative to the constraint database model for spatio-temporal databases and we identify sound and complete languages for the first-order, respectively the computable, queries in these genericity classes.", "authors": ["Floris Geerts", "Sofie Haesevoets", "Bart Kuijpers"], "n_citation": 0, "title": "A theory of spatio-temporal database queries", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d36b3a23-bb96-40d2-ba89-a8c76cbaef17"}
{"abstract": "This paper proposes a novel algorithm based on minimizing mutual information for a special case of nonlinear blind source separation: post-nonlinear blind source separation. A network composed of a set of radial basis function (RBF) networks, a set of multilayer perceptron and a linear network is used as a demixing system to separate sources in post-nonlinear mixtures. The experimental results show that our proposed method is effective, and they also show that the local character of the RBF network's units allows a significant speedup in the training of the system.", "authors": ["Chun-Hou Zheng", "Zhi-Kai Huang", "Michael R. Lyu", "Tat-Ming Lok"], "n_citation": 0, "title": "Nonlinear Blind Source Separation Using Hybrid Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d36d7111-8125-44a5-a2b9-3bcb7503cdd6"}
{"abstract": "In this paper, a ridgelet kernel regression model is proposed for approximation of multivariate functions, especially those with certain kinds of spatial inhomogeneities. It is based on ridgelet theory, kernel and regularization technology from which we can deduce a regularized kernel regression form. Using the objective function solved by quadratic programming to define a fitness function, we adopt particle swarm optimization algorithm to optimize the directions of ridgelets. Theoretical analysis proves the superiority of ridgelet kernel regression for multivariate functions. Experiments in regression indicate that it not only outperforms support vector machine for a wide range of multivariate functions, but also is robust and quite competitive on training of time.", "authors": ["Shuyuan Yang", "Min Wang", "Licheng Jiao", "Qing Li"], "n_citation": 0, "title": "A novel ridgelet kernel regression method", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d38df5f3-221c-429d-8219-e848ab62e4f2"}
{"abstract": "This paper presents a new method to simulate wrinkles on individual face model, applying convolution surface to face modeling. A generic face mesh is deformed and the texture image is computed using image-based modeling technique. The deformed mesh is then convolved with a kernel function to generate a convolution surface, and wrinkles are generated by modulating the surface with a designed profile function. The pre-computed texture is mapped onto the convolution surface to enhance the realism. Experimental results show that our method can generate wrinkles with different patterns by regulating some parameters of the profile function.", "authors": ["Qing He", "Minglei Tong", "Yuncai Liu"], "n_citation": 0, "title": "Face modeling and wrinkle simulation using convolution surface", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d4df9a24-bb53-453f-addc-079af2fbb8fb"}
{"abstract": "This paper presents two steganographic methods for JPEG2000 still images which preserve histograms of discrete wavelet transform (DWT) coefficients. The first one is a histogram quasi-preserving method using quantization index modulation (QIM) with a dead zone in DWT domain. The second one is a histogram preserving method based on histogram matching using two quantizers with a dead zone. Comparing with a conventional JPEG2000 steganography. the two methods show better histogram preservation. The proposed methods are promising candidates for secure JPEG2000 steganography against histogram-based attack.", "authors": ["Hideki Noda", "Yohsuke Tsukamizu", "Michiharu Niimi"], "n_citation": 0, "title": "JPEG2000 Steganography Possibly Secure Against Histogram-Based Attack", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d511cf7c-0e0c-408b-8acd-abbcc320cd55"}
{"abstract": "This paper addresses the problem of blind source separation (BSS) based on nonlinear principal component analysis (NPCA), and presents a new Kalman filtering algorithm, which applies a different state-space representation from the one proposed recently by Lv et al. It is shown that the new Kalman filtering algorithm can be simplified greatly under certain conditions, and it includes the existing Kalman-type NPCA algorithm as a special case. Comparisons are made with several related algorithms and computer simulations on BSS are reported to demonstrate the validity.", "authors": ["Xiao-Long Zhu", "Xian-Da Zhang", "Ying Jia"], "n_citation": 50, "title": "A new kalman filtering algorithm for nonlinear principal component analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d54789d5-21be-4384-bfcb-1a84da363645"}
{"abstract": "This paper presents an analysis of the global physical properties of an idiotypic network, using a growth model with complete dynamics. Detailed studies of the properties of idiotypic networks are valuable as one the one hand they offer a potential explanation for immunological memory, and on the other have been used by engineers in application of AIS to a range of diverse applications. The properties of both homogeneous and heterogeneous networks resulting from the model in an integer-valued shape-space are analysed and compared. In addition, the results are contrasted to those obtained using other generic growth models found in the literature which have been proposed to explain the structure and growth of biological networks, and also make a useful addition to previous published results obtained in alternative shape-spaces. We find a number of both similarities and differences with other growth models that are worthy of further study.", "authors": ["E. Hart"], "n_citation": 0, "title": "Analysis of a growth model for idiotypic networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d55c522c-c9ec-44e3-9c98-a02b8b1c85c3"}
{"abstract": "In mobile ad hoc wireless networks, energy consumption is an important issue as most mobile nodes operate on limited battery resources. Existing models for evaluating the energy consumption in a mobile ad hoc network have shown that the various components of energy related costs include transmission power as well as the reception power. In this paper, we extend the model for calculating the energy spent at a node due to a flow in the network. We include the transmission and reception costs if the node belongs to a flow, and reception costs if it is near a flow. This model gives the energy costs of nodes in ideal conditions where interferences are absent. It is then extended to evaluate the interference effect on energy consumption in more realistic conditions. The collisions due to concurrent flows are also measured. We then show how the extra energy spent due to collisions can be calculated by predicting the collisions in the nodes of the network. This prediction is shown to be capable of accurate calculation of the extra energy consumption.", "authors": ["G\u00e9raud Allard", "Pascale Minet", "Dang-Quan Nguyen", "Nirisha Shrestha"], "n_citation": 50, "title": "Evaluation of the energy consumption in MANET", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d5759875-7e40-4df7-902e-c7f07385739c"}
{"abstract": "The purpose of this paper is to describe the design and implementation choices that were made during the development of an Ada binding to the popular Gtk+ graphical toolkit. We concentrate on the methods used to interface between C and Ada, but many topics described in this paper are not tied to Ada and can be applied to other high level languages that need to interface with existing libraries. We also describe the various mechanisms developed to provide a powerful GUI builder with GtkAda. This paper emphasizes the added value that Ada brings to the task of writing a high level binding over an existing library.", "authors": ["Emmanuel Briot", "Jo\u00ebl Brobecker", "Arnaud Charlet"], "n_citation": 0, "title": "GtkAda: Design and implementation of a high level binding in Ada", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "d578b413-2efb-45ce-a922-d9b49f35cbdc"}
{"abstract": "We consider the framework of secure n-party computation based on threshold homomorphic cryptosystems as put forth by Cranier, Damgard, and Nielsen at Eurocrypt 2001. When used with Paillier's cryptosystem, this framework allows for efficient secure evaluation of any arithmetic circuit defined over Z N , where N is the RSA modulus of the underlying Paillier cryptosystem. In this paper, we extend the scope of the framework by considering the problem of converting a given Paillier encryption of a value x \u2208Z N  into Paillier encryptions of the bits of x. We present solutions for the general case in which x can be any integer in {0, 1,..., N - 1}, and for the restricted case in which x   least significant bits of x (in encrypted form) in time proportional to  , typically saving a factor of log 2  N/i compared to the general case. Thus, intermediate computations that rely in an essential way on the binary representations of their input values can be handled without enforcing that the entire computation is done bitwise. Typical examples involve the relational operators such as < and =. As a specific scenario we will consider the setting for (approximate) matching of biometric templates, given as bit strings.", "authors": ["Berry Schoenmakers", "Pim Tuyls"], "n_citation": 102, "title": "Efficient binary conversion for paillier encrypted values", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d5c93e9e-dca3-4a30-8e71-726748ca17da"}
{"abstract": "Spectra is usually shown as a two-dimensional graph where colors are directly related to signal levels. A great deal of speech recognition work and research takes this type of parameter directly. In this paper we propose to combine typical signal level values with the vectorial components of a Slope matrix containing orientation information on spectra surfaces. This additional information will enable us to obtain an enhanced speech signal spectra as well as formant evolution detection and a matching method to compare speech spectra sections. The mathematical formalization is based on vector analysis and matrix operations, where the basic components are the normal vectors to a set of triangular surfaces covering the spectral values. This formalism enables the use of mathematical tools (Matlab or similar) in a very easy way; and from here it is possible to program algorithms and visualize the results efficiently.", "authors": ["Jesus Bobadilla"], "n_citation": 0, "title": "New speech enhancement approach for formant evolution detection", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d5cd560e-0c2f-4a91-9adc-0692e7056403"}
{"abstract": "Predicting the virulence of new Influenza strains is an important problem. The solution to this problem will likely require a combination of in vitro and in silico tools that are used iteratively. We describe the agent-based modeling component of this program and report preliminary results from both the in vitro and in silico experiments.", "authors": ["Catherine A. A. Beauchemin", "Stephanie Forrest", "Frederick Koster"], "n_citation": 0, "title": "Modeling influenza viral dynamics in tissue", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d636cb40-9ea3-4fd7-a5f5-a47ecd4e491f"}
{"abstract": "We present a cooperation technique using an accurate management of nogoods to solve a hard real-time problem which consists in assigning periodic tasks to processors in the context of fixed priorities preemptive scheduling. The problem is to be solved off-line and our solving strategy is related to the logic based Benders decomposition. A master problem is solved using constraint programming whereas subproblems are solved with schedulability analysis techniques coupled with an ad hoc nogood computation algorithm. Constraints and nogoods are learnt during the process and play a role close to Benders cuts.", "authors": ["Hadrien Cambazard", "Pierre-Emmanuel Hladik", "Anne-Marie D\u00e9planche", "Narendra Jussien", "Yvon Trinquet"], "n_citation": 0, "title": "Decomposition and learning for a hard real time task allocation problem", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d637bbd4-2928-4335-88b7-b867ca189650"}
{"abstract": "In view of the character of saturation linearity of output functions of neurons of the cellular neural networks, the method decomposing the state space to sub-regions is adopted to study almost sure exponential stability on delayed cellular neural networks which are in the noised environment. When perturbed terms in the model of the neural network satisfy Lipschitz condition, some algebraic criteria are obtained. The results obtained in this paper show that if an equilibrium of the neural network is the interior point of a sub-region, and an appropriate matrix related to this equilibrium has some stable degree to stabilize the perturbation, then the equilibrium of the delayed cellular neural network can still remain the property of exponential stability. All results in the paper is only to compute eigenvalues of matrices.", "authors": ["Wudai Liao", "Yulin Xu", "Xiaoxin Liao"], "n_citation": 0, "title": "Exponential Stability of Delayed Stochastic Cellular Neural Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d6820ea1-e785-47ec-b3de-4c73294067c3"}
{"abstract": "We apply the theory of abstract interpretation to validate a Reed Solomon error correcting code. We design and implement an simulator for VHDL descriptions. This tool computes an over-approximation of all the states that would be reached during any run of a conventional simulator. It collects linear constraints that hold between signals in the design. It is used to check the RTL implementations of the Reed Solomon encoder and decoder against correct high-level specifications. We explain how to express the correctness property so as to defeat the state explosion incurred by the deep pipeline in the decoder. Benchmarks show the abstract simulator is very frugal in both memory and time. Comparisons with VIS confirm that specialized tools outperform general purpose algorithms. Abstract simulation also competes advantageously with simulation. In less time than what was allocated for simulation by the designers of the components, it achieves full coverage.", "authors": ["Charles Hymans"], "n_citation": 0, "title": "Verification of an error correcting code by abstract interpretation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d701aa97-c03f-410d-a5a3-34a1c87685f4"}
{"abstract": "Saving internal program data for further use is one of the most useful ideas in programming. Developing general features to provide such data saving/ restoring is a very active research area. There are two application areas for such features we believe to be crucial: system fault tolerance and data persistence. Our analysis shows that the features used in these areas have a lot in common: they are to flatten data of different types and save them in a store which can be used later on. The recent revision of the Ada language standard, Ada 95, introduces a new mechanism called streams that allows structured data to be flattened. Streams are sequences of elements comprising values from possibly different types. Ada 95 allows programmers to develop their streams following the standard abstract class interface. In this paper we show how to use the stream concept for developing new features to provide internal program data saving suitable for fault tolerance and persistence. A hierarchy of different storage types, useful in different application domains, is introduced. The standard stream interface is extended, making it possible for programmers to have a better control of the way streams work by separating storage medium control from the actual stream type using the design patterns. The convenience of this new interface is demonstrated by developing a generic package allowing any non-limited object to be written into a storage device. It can be used for providing data persistence and as a state restoration feature in schemes used for tolerating software design faults.", "authors": ["Joerg Kienzle", "Alexander Romanovsky"], "n_citation": 0, "title": "On persistent and reliable streaming in Ada", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "d7e6514a-6fba-401e-9235-f102f9ff0f6f"}
{"abstract": "The multi-agent systems in Artificial Intelligence very often include task allocation problems. These problems are arduous to be modelled; therefore it is also difficult to end up with an optimal allocation plan. AgentAllocator is an easy to use, platform independent application, which implements a multi-criteria method to support the decision of Task Allocation. The decision maker is able to model the problem (according to his policy) through its inputs dialogs and employ the final solution proposed by the system. In this paper, both the theoretical background of AgentAllocator and the DSS itself are presented.", "authors": ["Nikolaos F. Matsatsinis", "Pavlos Delias"], "n_citation": 0, "title": "AgentAllocator: An agent-based multi-criteria decision support system for task allocation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "d80c7f65-51dd-4d9a-9b1d-dcc17751415c"}
{"abstract": "Predicate abstraction provides a powerful tool for verifying properties of infinite-state systems using a combination of a decision procedure for a subset of first-order logic and symbolic methods originally developed for finite-state model checking. We consider models where the system state contains mutable function and predicate state variables. Such a model can describe systems containing arbitrarily large memories, buffers, and arrays of identical processes. We describe a form of predicate abstraction that constructs a formula over a set of universally quantified variables to describe invariant properties of the function state variables. We provide a formal justification of the soundness of our approach and describe how it has been used to verify several hardware and software designs, including a directory-based cache coherence protocol with unbounded FIFO channels.", "authors": ["Shuvendu K. Lahiri", "Randal E. Bryant"], "n_citation": 0, "title": "Constructing quantified invariants via predicate abstraction", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d815cef2-12be-475d-ad01-a1b038a534f3"}
{"abstract": "Support Vector Data Description (SVDD) concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a data set can be used to detect outliers. SVDD is affected by noises during being trained. In this paper, Grid-based Fuzzy Support Vector Data Description (G-FSVDD) is presented to deal with the problem. G-FSVDD reduces the effects of noises by a new fuzzy membership model, which is based on grids. Each grid is a hypercube in data set. After obtaining enough grids, Apriori algorithm is used to find grids with high density. In G-FSVDD. different training data make different contributions to the domain description according to their density. The advantage of G-FSVDD is shown in the experiment.", "authors": ["Yugang Fan", "Ping Li", "Zhihuan Song"], "n_citation": 0, "title": "Grid-Based Fuzzy Support Vector Data Description", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d8b32d3b-bd86-4615-aac7-d8e6f3ce5e6b"}
{"abstract": "Blind signatures allow a signer to digitally sign a document without being able to glean any information about the document. In this paper, we investigate the symmetric analog of blind signatures, namely blind message authentication codes (blind MACs). One may hope to get the same efficiency gain from blind MAC constructions as is usually obtained when moving from asymmetric to symmetric cryptosystems. Our main result is a negative one however: we show that the natural symmetric analogs of the unforgeability and blindness requirements cannot be simultaneously satisfied. Faced with this impossibility, we show that blind MACs do exist (under the one-more RSA assumption in the random oracle model) in a more restrictive setting where users can share common state information. Our construction, however, is only meant to demonstrate the existence; it uses an underlying blind signature scheme, and hence does not achieve the desired performance benefits. The construction of an efficient blind MAC scheme in this restrictive setting is left as an open problem.", "authors": ["Michel Abdalla", "Chanathip Namprempre", "Gregory Nevenl"], "n_citation": 50, "title": "On the (im)possibility of blind message authentication codes", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d8ed3f6c-5b9e-438a-94b6-4f070d5ce8ee"}
{"abstract": "Because of VLSI realization of artificial neural networks and measuring the elements of the circuits, noises coming from the circuits and the errors of the parameters of the network systems are therefore unavoidable. Making use of the stochastic version of Razumikhin theorem of stochastic functional differential equation, Lyapunov direct methods and matrix analysis,almost sure exponential stability on interval neural networks perturbed by white noises with time varying delays is examined, and some sufficient algebraic criteria which only depend on the systems' parameters are given. For well designed deterministic neural networks, the results obtained in the paper also imply that how much tolerance against perturbation they have.", "authors": ["Wudai Liao", "Zhongsheng Wang", "Xiaoxin Liao"], "n_citation": 50, "title": "Almost Sure Exponential Stability on Interval Stochastic Neural Networks with Time-Varying Delays", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d9a841d0-e7a1-4031-9b62-61741cb0060f"}
{"abstract": "Refutation proofs can be viewed as a special case of constraint propagation, which is a fundamental technique in solving constraint-satisfaction problems. The generalization lifts, in a uniform way, the concept of refutation from Boolean satisfiability problems to general constraint-satisfaction problems. On the one hand, this enables us to study and characterize basic concepts, such as refutation width, using tools from finite-model theory. On the other hand, this enables us to introduce new proof systems, based on representation classes, that have not been considered up to this point. We consider ordered binary decision diagrams (OBDDs) as a case study of a representation class for refutations, and compare their strength to well-known proof systems, such as resolution, the Gaussian calculus, cutting planes, and Frege systems of bounded alternation-depth. In particular, we show that refutations by ODBBs polynomially simulate resolution and can be exponentially stronger.", "authors": ["Albert Atserias", "Phokion G. Kolaitis", "Moshe Y. Vardi"], "n_citation": 0, "title": "Constraint propagation as a proof system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "d9be6395-02a8-4fc4-9a58-385b552043bc"}
{"abstract": "This paper presents a database designed to extract prosodic models corresponding to emotional speech to be used in speech synthesis for standard Basque. A database of acted speech, which uses a corpus containing both neutral texts and texts semantically related with emotion has been recorded for the six basic emotions: anger, disgust, fear, joy, sadness and surprise. Subjective evaluation of the database shows that emotions are accurately identified, so it can be used to study prosodic models of emotion in Basque.", "authors": ["Eva Navas", "Inmaculada Hern\u00e1ez", "Amaia Castelruiz", "Iker Luengo"], "n_citation": 0, "title": "Obtaining and evaluating an emotional database for prosody modelling in standard Basque", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "db59d4a3-3c60-4527-91a8-43b348ba37d5"}
{"abstract": "In this paper we ask the question what happens if we replace all the constants in Rijndael, including the replacement of the irreducible polynomial, the coefficients of the MixColumn operation, the affine transformation in the S box, etc. We show that such replacements can create new dual ciphers, which are equivalent to the original in all aspects. We present several such dual ciphers of Rijndael, such as the square of Rijndael, and dual ciphers with the irreducible polynomial replaced by primitive polynomials. We also describe another family of dual ciphers consisting of the logarithms of Rijndael. We then discuss self-dual ciphers, and extend our results to other ciphers.", "authors": ["Elad Barkan", "Eli Biham"], "n_citation": 0, "title": "In how many ways can you write Rijndael", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "dbad2b59-277d-4941-b206-b8fbaeb916ff"}
{"abstract": "Sparse least squares support vector machine (SLS-SVM) for regression is proposed to solve the problem of regression for large sample data. The samples are mapped into Reproducing Kernel Hilbert Space (RKHS) and span a subspace there. Then we could find the basis of the subspace. The basis can represent all the samples linearly. So we can get the least squares support vector machine by solving a small equations set. A numerical example is used to illustrate that this approach can be used to fit nonlinear models for large data set. Being compared with common least squares support vector machine, this method can find sparse solution without any pruning or surgeon, and the computing speed is much faster because the final result is found by solving a small-scale equations set.", "authors": ["Liangzhi Gan", "Haikuan Liu", "Youxian Sun"], "n_citation": 0, "title": "Sparse Least Squares Support Vector Machine for Function Estimation", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dc0da5f5-fe00-4503-b66b-bfb252a1aaf8"}
{"abstract": "This paper evaluates the power of a new scheme that generates search heuristics mechanically. This approach was presented and evaluated first in the context of optimization in belief networks. In this paper we extend this work to Max-CSP. The approach involves extracting heuristics from a parameterized approximation scheme called Mini-Bucket elimination that allows controlled trade-off between computation and accuracy. The heuristics are used to guide Branch-and-Bound and Best-First search, whose performance are compared on a number of constraint problems. Our results demonstrate that both search schemes exploit the heuristics effectively, permitting controlled trade-off between preprocessing (for heuristic generation) and search. These algorithms are compared with a state of the art complete algorithm as well as with the stochastic local search anytime approach, demonstrating superiority in some problem cases.", "authors": ["Kalev Kask"], "n_citation": 50, "title": "New search heuristics for Max-CSP", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "dce29ab8-4996-4764-bbc2-9e27db4c2225"}
{"abstract": "In this paper, we propose a watermarking algorithm working directly on JPEG bit-stream. The algorithm embeds watermark bits by modifying de-quantized DC coefficients. By improving an existing embedding method for watermark bit, the quality of the watermarked image can be improved greatly while keeping the same robustness of the original method. Further more, we analyze the performance of the watermarking algorithm against re-quantization and recompression. We give the relationship among the watermarking strength, the quality factor of JPEG compression and the BER (Bit Error Rate) of the watermark. Experiment results support the analysis. Compared with several JPEG-based algorithms in literature, the robustness to JPEG recompression of the proposed algorithm is better than most of them when recompression quality factor is above 30.", "authors": ["Hongmei Liu", "Huiying Fu", "Jiwu Huang"], "n_citation": 0, "title": "A Watermarking Algorithm for JPEG File", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "dd3d84ac-51eb-4579-80c0-e6c7a3ff5d71"}
{"abstract": "A dynamical system is proposed for generalized eigen-decomposition problem. The stable points of the dynamical system are proved to be the eigenvectors corresponding to the largest generalized eigenvalue.", "authors": ["Lijun Liu", "Wei Wu"], "n_citation": 0, "title": "Dynamical System for Computing Largest Generalized Eigenvalue", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ddf97f1f-b0d2-4c83-ab2c-db7d311d1513"}
{"abstract": "Motion estimation is one of the most important steps in super-resolution algorithms for a video sequence, which require estimating motion from a noisy, blurred, and down-sampled sequence; therefore the motion estimation has to be robust. In this paper, we propose a robust sub-pixel motion estimation algorithm based on region matching. Non-rectangular regions are first extracted by using a so-called water-shed transform. For each region, the best matching region in a previous frame is found to get the integer-pixel motion vector. Then in order to refine the accuracy of the estimated motion vector, we search the eight sub-pixels around the estimated motion vector for a sub-pixel motion vector. Performance of our proposed algorithm is compared with the well known full search with both integer-pixel and sup-pixel accuracy. Also it is compared with the integer-pixel region matching algorithm for several noisy video sequences with various noise variances. The results show that our proposed algorithm is the most suitable for noisy, blurred, and down-sampled sequences among these conventional algorithms.", "authors": ["Osama A. Omer", "Toshihisa Tanaka"], "n_citation": 50, "title": "Region-Based Sub-pixel Motion Estimation from Noisy, Blurred, and Down-Sampled Sequences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "decb51fb-3209-43ba-8bad-ad20fbf2c68d"}
{"authors": ["David Canright", "Lejla Batina"], "n_citation": 0, "title": "A Very Compact \"Perfectly Masked\" S-Box for AES", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "df23720d-87fb-4dc6-89db-124ab8700f51"}
{"abstract": "Association rule engines typically output a very large set of rules. Despite the fact that association rules are regarded as highly comprehensible and useful for data mining and decision support in fields such as marketing, retail, demographics, among others, lengthy outputs may discourage users from using the technique. In this paper we propose a post-processing methodology and tool for browsing/visualizing large sets of association rules. The method is based on a set of operators that transform sets of rules into sets of rules, allowing focusing on interesting regions of the rule space. Each set of rules can be then seen with different graphical representations. The tool is web-based and uses SVG. Association rules are given in PMML.", "authors": ["Al\u00edpio M\u00e1rio Jorge", "Jo\u00e3o Po\u00e7as", "Paulo J. Azevedo"], "n_citation": 0, "title": "Post-processing operators for browsing large sets of association rules", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "df284bde-78c6-454b-b19b-f9375bcc7c7f"}
{"abstract": "A fast verification algorithm of calculating guaranteed error bounds for all approximate eigenvalues of a real symmetric matrix is proposed. In the proposed algorithm, Rump's and Wilkinson's bounds are combined. By introducing Wilkinson's bound, it is possible to improve the error bound obtained by the verification algorithm based on Rump's bound with a small additional cost. Finally, this paper includes some numerical examples to show the efficiency of the proposed algorithm.", "authors": ["Shinya Miyajima", "Takeshi Ogita", "Shin'ichi Oishi"], "n_citation": 0, "title": "Fast verification for respective eigenvalues of symmetric matrix", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "df5693fe-4ad2-4617-bf92-e3c5bc1260eb"}
{"abstract": "We present a new chaotic neural network whose neuron activation function is hysteretic, called hysteretic transiently chaotic neural network (HTCNN) and with this network, a combinatorial optimization problem is solved. By using hysteretic activation function which is multi-valued, has memory, is adaptive, HTCNN has higher ability of overcoming drawbacks that suffered from the local minimum. We proceed to prove Lyapunov stability for this new model, and then solve a combinatorial optimization problem-Assignment problems. Numerical simulations show that HTCNN has higher ability to search for globally optimal and has higher searching efficiency.", "authors": ["Xiuhong Wang", "Qingli Qiao", "Zhengqu Wang"], "n_citation": 0, "title": "Solving optimization problems based on chaotic neural network with hysteretic activation function", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "df5762cb-9af4-414c-b03c-3603bc816748"}
{"abstract": "This paper presents new measures, based on the induced decision tree, to characterise datasets for meta-learning in order to select appropriate learning algorithms. The main idea is to capture the characteristics of dataset from the structural shape and size of decision tree induced from the dataset. Totally 15 measures are proposed to describe the structure of a decision tree. Their effectiveness is illustrated through extensive experiments, by comparing to the results obtained by the existing data characteristics techniques, including data characteristics tool (DCT) that is the most wide used technique in meta-learning, and Landmarking that is the most recently developed method.", "authors": ["Yonghong Peng", "Peter A. Flach", "Carlos Soares", "Pavel Brazdil"], "n_citation": 0, "title": "Improved dataset characterisation for meta-learning", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "df70b5bc-56cb-4d3c-a4eb-6289f5b9d142"}
{"abstract": "A modified principal component analysis (PCA) neural network (NN) based on signal eigen-analysis is proposed to blind estimation of the pseudo noise (PN) sequence in lower signal to noise ratios (SNR) direct sequence spread spectrum (DS-SS) signals. The received signal is firstly sampled and divided into non-overlapping signal vectors according to a temporal window, which duration is two periods of PN sequence. Then an autocorrelation matrix is computed and accumulated by these signal vectors. The PN sequence can be estimated by the principal eigenvector of autocorrelation matrix in the end. Since the duration of temporal window is two periods of PN sequence, the PN sequence can be reconstructed by the first principal eigenvector only. Additionally, the eigen-analysis method becomes inefficiency when the estimated PN sequence becomes longer. We can use a PCA NN to realize the PN sequence estimation from lower SNR input DS-SS signals effectively.", "authors": ["Tianqi Zhang", "Xiaokang Lin", "Zhengzhong Zhou", "Aiping Mu"], "n_citation": 0, "title": "A modified PCA neural network to blind estimation of the PN sequence in lower SNR DS-SS signals", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "df7e4e1d-091d-45fb-a4ac-42243c9ec7d2"}
{"abstract": "This paper describes a selective root finding method for polynomials based in Complex Analysis results. It can find the poles of the speech signal LPC model that are close to the unit circle, without wasting computations with the others, lesser significant ones. This feature makes our method faster than the standard ones for speech analysis. These poles are in better correspondence with the formants than the local maxima of the spectral envelope. Experimental results are showed.", "authors": ["Juan-Luis Garcia Zapata", "Juan Carlos Diaz Martin", "Pedro G\u00f3mez Vilda"], "n_citation": 0, "title": "Parallel root-finding method for LPC analysis of speech", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "dfd4044b-c110-4d9d-8c6d-6c56544be416"}
{"abstract": "Web search engines are optimized for early precision, which makes it difficult to perform recall-oriented tasks using these search engines. In this article, we present our tool Needle Custom Search. This tool exploits semantic annotations of Web search results and, thereby, increase the efficiency of recall-oriented search tasks. Semantic annotations, such as temporal annotations, named entities, and part-of-speech tags are used to rerank and cluster search result sets.", "authors": ["Rianne Kaptein", "Gijs Koot", "Mirjam A.A. Huis in 't Veld", "Egon L. van den Broek"], "n_citation": 0, "title": "Needle Custom Search : Recall-oriented search on the web using semantic annotations", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "dff07a65-428e-414c-b315-34d01dfa93c2"}
{"abstract": "Reconstruction of a three-dimensional scene using images taken from two views is possible if the relative pose of the cameras is known. A traditional approach to estimating the pose of the cameras uses eight pairs of corresponding points and involves the solution of a set of homogeneous equations. We propose a multi-layered feedforward network solution. Empirical results demonstrate the feasibility of using the network to recover the relative pose of the cameras in the three-dimensional world.", "authors": ["Ryan G. Benton", "Chee-Hung Henry Chu"], "n_citation": 0, "title": "Camera Pose Estimation by an Artificial Neural Network", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e0331fee-d1ab-4753-9724-efab12b949f8"}
{"abstract": "This paper deals with methods exploiting tree-decomposition approaches for solving constraint networks. We consider here the practical efficiency of these approaches by defining five classes of variable orders more and more dynamic which guarantee time complexity bounds from O(exp(\u03c9 + 1)) to O(exp{2(\u03c9 + k))), with \u03c9 the tree-width of a CSP and k a constant. Finally, we assess practically their relevance.", "authors": ["Philippe J\u00e9gou", "Samba Ndojh Ndiaye", "Cyril Terrioux"], "n_citation": 50, "title": "An extension of complexity bounds and dynamic heuristics for tree-decompositions of CSP", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e06abcc2-4b98-4c35-9536-f50eb34ff831"}
{"abstract": "To improve the generalization ability of neural network ensemble, a selective method based on clustering is proposed. The method follows the overproduce and choose paradigm. It first produces a large number of individual networks, and then clusters these networks according to their diversity. Networks with the highest classification accuracies in each cluster are selected for the final integration. Experiments on ten UCI data sets showed the superiority of the proposed algorithm to the other two similiar ensemble learning algorithms.", "authors": ["Haixia Chen", "Senmiao Yuan", "Kai Jiang"], "n_citation": 0, "title": "Selective Neural Network Ensemble Based on Clustering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e0cd092f-5a24-4ef4-974b-d04add84de76"}
{"abstract": "In practice, constraint satisfaction problems are often structured. By exploiting this structure, solving algorithms can make important gains in performance. In this paper, we focus on structured continuous CSPs defined by systems of equations. We use graph decomposition techniques to decompose the constraint graph into a directed acyclic graph of small blocks. We present new algorithms to solve decomposed problems which solve the blocks in partial order and perform intelligent backtracking when a block has no solution. For under-constrained problems, the solution space can be explored by choosing some variables as input parameters. However, in this case, the decomposition is no longer unique and some choices lead to decompositions with smaller blocks than others. We present an algorithm for selecting the input parameters that lead to good decompositions. First experimental results indicate that, even on small problems, significant speedups can be obtained using these algorithms.", "authors": ["Christian Bliek", "Bertrand Neveu", "Gilles Trombettoni"], "n_citation": 50, "title": "Using graph decomposition for solving continuous CSPs", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "e0d1736f-4fad-43c8-8ca0-f26aa1ca2c94"}
{"abstract": "We introduce new method for discriminating speech and non-speech segments in audio signals based on the transcriptions produced by phoneme recognizers. Four measures based on consonant-vowels and voiced-unvoiced pairs obtained from different phonemes speech recognizers were proposed. They were constructed in a way to be recognizer and language independent and could be applied in different segmentation-classification frameworks. The segmentation systems were evaluated on different broadcast news datasets consisted of more than 60 hours of multilingual BN shows. The results of these evaluations illustrate the robustness of the proposed features in comparison to MFCC and posterior probability based features. The overall frame accuracies of the proposed approaches varied in range from 95% to 98% and remained stable through different test conditions and different phoneme recognizers.", "authors": ["France Mihelic", "Janez Zibert"], "n_citation": 0, "title": "Robust speech detection based on phoneme recognition features", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e1060bef-0d56-41f2-b47e-9bbe94d4e29d"}
{"abstract": "In this paper, new architectures and design methodologies of Rule based Neurofuzzy Networks (RNFN) are introduced and the dynamic search-based GAs is introduced to lead to rapidly optimal convergence over a limited region or a boundary condition. The proposed RNFN is based on the fuzzy set based neurofuzzy networks (NFN) with the extended structure of fuzzy rules being formed within the networks. In the consequence part of the fuzzy rules, three different forms of the regression polynomials such as constant, linear and modified quadratic are taken into consideration. The dynamic search-based GAs optimizes the structure and parameters of the RNFN.", "authors": ["Byoung-Jun Park", "Sungkwun Oh"], "n_citation": 0, "title": "Design of rule-based neurofuzzy networks by means of genetic fuzzy set-based granulation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e1c8b250-7006-4fcd-9868-7b14043fbc08"}
{"abstract": "My dissertation will make the case that answer set semantics can form the basis of powerful, logical NP problem solving tools. The argument is made in two steps. Firstly a logical programming language, AnsProlog CE  is proposed and an efficient, parallel implementation is described. This language is then used to solve a variety of diverse real world problems, demonstrating the power and flexibility of this approach.", "authors": ["Martin Brain"], "n_citation": 0, "title": "Declarative problem solving using answer set semantics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e21eec5c-b475-4225-a2d4-8cc183f81a14"}
{"abstract": "The limited display size of the mobile devices has been imposing significant barriers for mobile device users to enjoy browsing high-resolution videos. In this paper, we present a novel video adaptation scheme based on attention area detection for users to enrich browsing experience on mobile devices. During video compression, the attention information which refers to as attention objects in frames will be detected and embedded into bitstreams using the supplement enhanced information (SEI) tool. In this research, we design a special SEI structure for embedding the attention information. Furthermore, we also develop a scheme to adjust adaptive quantization parameters in order to improve the quality on encoding the attention areas. When the high-resolution bitstream is transmitted to mobile users, a fast transcoding algorithm we developed earlier will be applied to generate a new bitstream for attention areas in frames. The new low-resolution bitstream containing mostly attention information, instead of the high-resolution one, will be sent to users for display on the mobile devices. Experimental results show that the proposed spatial adaptation scheme is able to improve both subjective and objective video Qualities.", "authors": ["Yi Wang", "Houqiang Li", "Zhengkai Liu", "Chang Wen Chen"], "n_citation": 0, "title": "Attention Information Based Spatial Adaptation Framework for Browsing Videos Via Mobile Devices", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e28d6c4e-bf43-426b-8977-0b72887f5181"}
{"abstract": "In this paper, we define a general architecture that integrates micromobility protocols and dynamic management of 802.11 QoS MAC mechanisms and evaluate it through simulation in a scenario where a specific micromobility protocol, Cellular IP, is integrated with a specific QoS mechanism, ACKnowledgement Skipping. Our results demonstrate that seamless, QoS aware connectivity can be provided to mobile users, and identify directions to fully integrate these functionalities in a comprehensive WLAN management system.", "authors": ["Simona Acanfora", "Filippo Cacace", "Giulio Iannello", "Luca Vollero"], "n_citation": 50, "title": "Dynamic configuration of MAC QoS mechanisms in 802.11 access networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e30d7545-41c2-44bd-94b3-07ecf8775b05"}
{"abstract": "The Polynomial Reconstruction problem (PR) has been introduced in 1999 as a new hard problem. Several cryptographic primitives established on this problem have been constructed, for instance Naor and Pinkas have proposed a protocol for oblivious polynomial evaluation. Then it has been studied from the point of view of robustness, and several important properties have been discovered and proved by Kiayias and Yung. Furthermore the same authors constructed a symmetric cipher based on the PR problem. In the present paper, we use the published security results and construct a new public key encryption scheme based on the hardness of the problem of Polynomial Reconstruction. The scheme presented is the first public key encryption scheme based on this Polynomial Reconstruction problem. We also present some attacks, discuss their performances and state the size of the parameters required to reach the desired security level. In conclusion, this leads to a cryptosystem where the cost of encryption and decryption per bit is low, and where the public key is kept relatively small.", "authors": ["Daniel Augot", "Matthieu Finiasz"], "n_citation": 50, "title": "A public key encryption scheme based on the Polynomial Reconstruction problem", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e3521854-89fd-4270-ab89-a28e83a728d3"}
{"abstract": "In this paper, we present the rainbow attack on stream ciphers filtered by Maiorana-McFarland functions. This can be considered as a generalization of the time-memory-data trade-off attack of Mihaljevic and Imai on Toyocrypt. First, we substitute the filter function in Toyocrypt (which has the same size as the LFSR) with a general Maiorana-McFarland function. This allows us to apply the attack to a wider class of stream ciphers. Moreover, our description replaces the time-memory-data trade-off attack with the rainbow attack of Oeshlin, which offers better performance and implementation advantages. Second, we highlight how the choice of different Maiorana-McFarland functions can affect the effectiveness of our attack. Third, we show that the attack can be modified to apply on filter functions which are smaller than the LFSR or on filter-combiner stream ciphers. This allows us to crypt-analyze other configurations commonly found in practice. Finally, filter functions with vector output are sometimes used in stream ciphers to improve the throughput. Therefore the case when the Maiorana-McFarland functions have vector output is investigated. We found that the extra speed comes at the price of additional weaknesses which make the attacks easier.", "authors": ["Khoongming Khoo", "Guang Gong", "Hian-Kiat Lee"], "n_citation": 50, "title": "The rainbow attack on stream ciphers based on maiorana-McFarland functions", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e3eaf60c-bff6-426e-be9c-2d828fbf9637"}
{"abstract": "Motion capture is an important application in different areas such as biomechanics, computer animation, and human-computer interaction. Current motion capture methods typically use human body models in order to guide pose estimation and tracking. We model the human body as a set of tapered super-quadrics connected in an articulated structure and propose an algorithm to automatically estimate the parameters of the model using video sequences obtained from multiple calibrated cameras. Our method is based on the fact that the human body is constructed of several articulated chains that can be visualised as essentially 1-D segments embedded in 3-D space and connected at specific joint locations. The proposed method first computes a voxel representation from the images and maps the voxels to a high dimensional space in order to extract the 1-D structure. A bottom-up approach is then suggested in order to build a parametric (spline-based) representation of a general articulated body in the high dimensional space followed by a top-down probabilistic approach that registers the segments to the known human body model. We then present an algorithm to estimate the parameters of our model using the segmented and registered voxels.", "authors": ["Aravind Sundaresan", "Rama Chellappa"], "n_citation": 0, "title": "Acquisition of articulated human body models using multiple cameras", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e47284ef-4be0-4d22-b42f-5d9665485c94"}
{"abstract": "Mobile ad-hoc networks (MANETs) are dynamic computing environments where it is hard to make predictions about service provision. To ensure a level of predictability -and thus make the services more dependable-, it has been argued that the hosts must exchange information that allows guessing how the network is set up at a given moment, and how it will be in the near future. This paper introduces an approach to handling that information, which has been explicitly devised to deal with incomplete and changeable knowledge. As a contribution to the current state of the art, this approach enables a practical scheme where the different hosts in a MANET can collaborate to make up the network that best satisfies their service requirements.", "authors": ["Mart\u00edn L\u00f3pez-Nores", "Jorge Garcfa-Duque", "Jos\u00e9 J. Pazos-Arias"], "n_citation": 0, "title": "Managing ad-hoc networks through the formal specification of service requirements", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e48e3ae2-fe61-4886-8ddb-bc64b4058519"}
{"abstract": "We introduce a new flavor of commitment schemes, which we call mercurial commitments. Informally, mercurial commitments are standard commitments that have been extended to allow for soft decommitment. Soft decommitments, on the one hand, are not binding but, on the other hand, cannot be in conflict with true decommitments. We then demonstrate that a particular instantiation of mercurial commitments has been implicitly used by Micali, Rabin and Kilian to construct zero-knowledge sets. (A zero-knowledge set scheme allows a Prover to (1) commit to a set S in a way that reveals nothing about S and (2) prove to a Verifier, in zero-knowledge, statements of the form x \u2208 S and x \u00ac\u2208 S.) The rather complicated construction of Micali et al. becomes easy to understand when viewed as a more general construction with mercurial commitments as an underlying building block. By providing mercurial commitments based on various assumptions, we obtain several different new zero-knowledge set constructions.", "authors": ["Melissa Chase", "Alexander Healy", "Anna Lysyanskaya", "Tal Malkin", "Leonid Reyzin"], "n_citation": 0, "title": "Mercurial commitments with applications to zero-knowledge sets", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "e59a063a-b290-40b8-bc87-7b7f3cae8630"}
{"abstract": "Independent component analysis (ICA) is a widely applicable and effective approach in blind source separation (BSS) for basic ICA model, but with limitations that sources should be statistically independent, while more common situation is BSS for non-negative linear (NNL) model where observations are linear combinations of non-negative sources with non-negative coefficients and sources may be statistically dependent. By recognizing the fact that BSS for basic ICA model corresponds to matrix factorization problem, in this paper, a novel idea of BSS for NNL model is proposed that the BSS for NNL come'[ sponds to a non-negative matrix factorization problem and the non-negative matrix factorization (NMF) technique is utilized. For better expression of the patterns of the sources, the NMF is further extended to pattern expression NMF (PE-NMF) and its algorithm is presented. Finally, the experimental results are presented which show the effectiveness and efficiency of the PE-NMF to BSS for a variety of applications which follow NNL model.", "authors": ["Junying Zhang", "Zhang Hongyi", "Le Wei", "Yue Joseph Wang"], "n_citation": 0, "title": "Blind Source Separation with Pattern Expression NMF", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e5aefca3-555a-456c-9718-5ade17621bc8"}
{"abstract": "Negative selection algorithm is one of the most important algorithms inspired by biological immune system. In this paper, a heuristic detector generation algorithm for negative selection algorithm is proposed when the partial matching rule is Hamming distance. Experimental results show that this novel detector generation algorithm has a better performance than traditional detector generation algorithm.", "authors": ["Wenjian Luo", "Zeming Zhang", "Xufa Wang"], "n_citation": 0, "title": "A heuristic detector generation algorithm for negative selection algorithm with hamming distance partial matching rule", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e5eea817-cbcc-4629-9363-8b9cd9b454ed"}
{"abstract": "We present an approach to revising qualitative causal models of gene regulation with DNA microarray data. The method combines search through a space of variable orderings with search through a space of parameters on causal links, with weight decay driving the model toward integer values. We illustrate the technique on a model of photosynthesis regulation and associated microarray data. Experiments with synthetic data that varied distance from the target model, noise, and number of training cases suggest the method is robust with respect to these factors. In closing, we suggest directions for future research and discuss related work on inducing causal regulatory models.", "authors": ["Kazumi Saito", "Stephen D. Bay", "Pat Langley"], "n_citation": 0, "title": "Revising qualitative models of gene regulation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "e62b72f0-c39e-4859-bf1d-c361e26c61cf"}
{"abstract": "Yaron Sella recently proposed a scalable version of Jakobsson's algorithm to traverse a hash chain of size n. Given the hash chain and a computation limit m (k = m + 1 and b =  k \u221an), Sella's algorithm traverses the hash chain using a total of kb memory. We improve the memory usage to k(b - 1). Because efficient hash chain traversal algorithms are aimed at devices with severely restricted computation and memory requirements, a reduction by a factor of (b - 1)/b is considered to be important. Further, our algorithm matches the memory requirements of Jakobsson's algorithm while still remaining scalable. Sella's algorithm, when scaled to the case of Jakobsson's algorithm, has a memory requirement of about twice that of Jakobsson's.", "authors": ["Sung-Ryul Kim"], "n_citation": 0, "title": "Improved scalable hash chain traversal", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e6cec9d8-6e81-4ac3-a98f-1c7146f8d367"}
{"abstract": "The designer of an information filtering system based on user preferences formulated as user models has to decide what method to use to provide summaries of the available documents without losing information that may be significant to a particular user even if it would not be considered as such in general terms. In this paper we describe a personalised summarization facility to maximise the density of relevance of information sent by the system. The selection uses a relevance feedback mechanism that captures short term interests as indicated by a user's acceptance or rejection of the news items received. Controlled experiments were carried out with a group of users and satisfactory and insightful results were obtained, providing material for further development. The experimental results suggest that personalised summaries perform better than generic summaries at least in terms of identifying documents that satisfy user preferences.", "authors": ["Alberto D\u00edaz", "Pablo Gerv\u00e1s"], "n_citation": 0, "title": "Item summarization in personalisation of news delivery systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "e6f44c39-b92d-44bc-987d-1b7daacafe67"}
{"abstract": "We study bisimulation-based information flow security properties which are persistent, in the sense that if a system is secure, then all states reachable from it are secure too. We show that such properties can be characterized in terms of bisimulation-like equivalence relations between the system and the system itself prevented from performing confidential actions. Moreover, we provide a characterization of such properties in terms of unwinding conditions which demand properties of individual actions. These two different characterizations naturally lead to efficient methods for the verification and construction of secure systems. We also prove several compositionality results and discuss a sufficient condition to define refinement operators preserving security.", "authors": ["Annalisa Bossi", "Riccardo Focardi", "Carla Piazza", "Sabina Rossi"], "n_citation": 50, "title": "Bisimulation and unwinding for verifying possibilistic security properties", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "e7ade63c-2b65-41c5-95bd-4c0e7b852fe6"}
{"abstract": "In many industrial robotic applications there is a need to#R##N#track periodic reference signals and/or reject periodic disturbances. This#R##N#paper presents a novel repetitive control design for systems with constant#R##N#time-delays in both forward and feedback control channels. An additional#R##N#delay is introduced together with plant delays to construct an internal#R##N#model for periodic signals, and a simple proportional control is utilized#R##N#to stabilize the closed-loop system. Sufficient stability conditions of the#R##N#closed-loop system and the robustness analysis under modeling uncertainties#R##N#are studied. Experimental results are included to evaluate the#R##N#validity and effectiveness of the proposed method.", "authors": ["Jing Na", "Xuemei Ren", "Ramon Costa Castell\u00f3", "Robert Gri\u00f1\u00f3 Cubero", "Yu Guo"], "n_citation": 50, "title": "Repetitive control for systems with time-delays and application to robotic servo motor", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "e81440b3-b90d-4173-a498-9920a505dd34"}
{"abstract": "Adaptive information filtering is a challenging research problem. It requires the adaptation of a representation of a user's multiple interests to various changes in them. We investigate the application of an immune-inspired approach to this problem. Nootropia, is a user profiling model that has many properties in common with computational models of the immune system that have been based on Franscisco Varela's work. In this paper we concentrate on Nootropia's evaluation. We define an evaluation methodology that uses virtual user's to simulate various interest changes. The results show that Nootropia exhibits the desirable adaptive behaviour.", "authors": ["Nikolaos Nanas", "Anne N. De Roeck", "Victoria S. Uren"], "n_citation": 0, "title": "Immune-inspired adaptive information filtering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e8c065f6-cf9b-4c49-992f-dcd9f96d34ed"}
{"abstract": "We extend the common depth-first backtrack search for constraint satisfaction problems with randomized variable and value selection. The resulting methods are applied to real-world instances of the tail assignment problem, a certain kind of airline planning problem. We analyze the performance impact of these extensions and, in order to exploit the improvements, add restarts to the search procedure. Finally computational results of the complete approach are discussed.", "authors": ["Lars Otten", "Mattias Gr\u00f6nkvist", "Devdatt P. Dubhashi"], "n_citation": 50, "title": "Randomization in constraint programming for airline planning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "e96cea52-c726-4aa4-859f-386de687e0e5"}
{"abstract": "This paper introduces a classification scheme for global constraints. This classification is based on a small number of basic ingredients from which one can generate almost all existing global constraints and come up with new interesting constraints. Global constraints are defined in a very concise way, in terms of graph properties that have to hold, where the graph is a structured network of same elementary constraints.", "authors": ["Nicolas Beldiceanu"], "n_citation": 134, "title": "Global constraints as graph properties on a structured network of elementary constraints of the same type", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "e9a7057b-1a03-4c73-b618-47ae72b788be"}
{"abstract": "An environment for the Common Algebraic Specification Language CASL consists of several independent tools. A number of CASL tools have been built using the algebraic specification formalism ASF+SDF and the ASF+SDF Meta-Environment. CASL supports user-defined syntax which is non-trivial to process: ASF+SDF offers a powerful parsing technology (Generalized LR). Its interactive development environment facilitates rapid prototyping complemented by early detection and correction of errors. A number of core technologies developed for the ASF+SDF Meta-Environment can be reused in the context of CASL. Furthermore, an instantiation of a generic format developed for the representation of ASF+SDF specifications and terms provides a CASL-specific exchange format.", "authors": ["Mark van den Brand", "Jeroen Scheerder"], "n_citation": 0, "title": "Development of parsing tools for CASL using generic language technology", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "ea221962-673f-4673-96cd-59723a075e65"}
{"abstract": "Many applications of CSPs require partial solutions to be found before all the information about the problem is available. We examine the case where the future is partially known, and where it is important to make decisions in the present that will be robust in the light of future events. We introduce the branching CSP to model these situations, incorporating some elements of decision theory, and describe an algorithm for its solution that combines forward checking with branch and bound search. We also examine a simple thresholding method which can be used in conjunction with the forward checking algorithm, and we show the trade-off between time and solution quality.", "authors": ["David W. Fowler", "Kenneth N. Brown"], "n_citation": 50, "title": "Branching constraint satisfaction problems for solutions robust under likely changes", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "ea44a1ba-a31a-4e95-a7f7-36b8712eb1ac"}
{"abstract": "Multicast and Broadcast Service (MBS) is a novel application supported by the currently released IEEE 802.16e. This service can increase the efficiency of WiMAX networks. The key management and distribution is crucial to evolve MBS efficiently and safely. In this paper, we present a group-based key distribution algorithm GKDA to provide a more scalable solution to reduce the key updating overhead, therefore a base station (BS) can support and maintain more MBS users. Theoretical analyses and simulation results have proven the performance and advantage of our algorithm. In addition, GKDA can strengthen the network security.", "authors": ["Huijie Li", "Guangbin Fan", "Jigang Qiu", "Xiaokang Lin"], "n_citation": 0, "title": "GKDA : A Group-Based Key Distribution Algorithm for WiMAX MBS Security", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ea6f3831-d63e-48df-9c82-896af17d39d3"}
{"abstract": "Protein mass spectrometry (MS) pattern recognition has recently emerged as a new method for cancer diagnosis. Unfortunately, classification performance may degrade owing to the enormously high dimensionality of the data. This paper investigates the use of Random Projection in protein MS data dimensionality reduction. The effectiveness of Random Projection (RP) is analyzed and compared against Principal Component Analysis (PCA) by using three classification algorithms, namely Support Vector Machine, Feed-forward Neural Networks and K-Nearest Neighbour. Three real-world cancer data sets are employed to evaluate the performances of RP and PCA. Through the investigations, RP method demonstrated better or at least comparable classification performance as PCA if the dimensionality of the projection matrix is sufficiently large. This paper also explores the use of RP as a pre-processing step prior to PCA. The results show that without sacrificing classification accuracy, performing RP prior to PCA significantly improves the computational time.", "authors": ["Chen Change Loy", "Weng Kin Lai", "Chee Peng Lim"], "n_citation": 0, "title": "Dimensionality Reduction of Protein Mass Spectrometry Data Using Random Projection", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "eaafd8f9-a267-49d1-9720-3827e31c2a98"}
{"abstract": "The paper describes developments and results of the work being carried out during the European research project CATCH-2004 (Converse in AThens Cologne and Helsinki) 3 . The objective of the project is multi-modal, multi-lingual conversational access to information systems. This paper concentrates on issues of the multilingual telephony-based speech and natural language understanding components.", "authors": ["Marion Mast", "Thomas Ross", "Henrik Schulz", "Heli Harrikari"], "n_citation": 50, "title": "Different approaches to build multilingual conversational systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "eaeef4e0-8547-446f-b202-c862b4992d4f"}
{"abstract": "In this paper, we present tools to help understanding the dynamics of cognitive processes involved in handwriting and in text composition. Three computer systems or programs used for this analysis are explained, the results obtained by their mean are exposed and their potential meaning discussed.", "authors": ["Gilles Caporossi", "Denis Alamargot", "David Chesnet"], "n_citation": 0, "title": "Using the computer to study the dynamics of the handwriting processes", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "eaf05bc0-80f1-47b4-a3ff-b22cbba32680"}
{"abstract": "Montgomery multiplication normally spends over 90% of its execution time in inner loops executing some kind of multiply-and-add operations. The performance of these critical code sections can be greatly improved by customizing the processor's instruction set for low-level arithmetic functions. In this paper, we investigate the potential of architectural enhancements for multiple-precision Montgomery multiplication according to the so-called Finely Integrated Product Scanning (FIPS) method. We present instruction set extensions to accelerate the FIPS inner loop operation based on the availability of a multiply/accumulate (MAC) unit with a wide accumulator. Finally, we estimate the execution time of a 1024-bit Montgomery multiplication on an extended MIPS32 core and discuss the impact of the multiplier latency.", "authors": ["J. Grossschadl", "Guy-Armand Kamendje"], "n_citation": 0, "title": "Architectural enhancements for Montgomery multiplication on embedded risc processors", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "eb6789c9-2119-48cd-bcf6-ec1ce27ffaee"}
{"abstract": "Lowe [1] proposed that the kernel parameters of a radial basis function (RBF) neural network may first be fixed and the weights of the output layer can then be determined by pseudo-inverse. Jang, Sun, and Mizutani (p.342 [2]) pointed out that this type of two-step training methods can also be used in fuzzy neural networks (FNNs). By extensive computer simulations, we [3] demonstrated that an FNN with randomly fixed membership function parameters (FNN-RM) has faster training and better generalization in comparison to the classical FNN. To provide a theoretical basis for the FNN-RM, we present an intuitive proof of the universal approximation ability of the FNN-RM in this paper, based on the orthogonal set theory proposed by Kaminski and Strumillo for RBF neural networks [4].", "authors": ["Lipo Wang", "Bing Liu", "Chunru Wan"], "n_citation": 50, "title": "On the universal approximation theorem of fuzzy neural networks with random membership function parameters", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ec728f79-8a19-499a-8b32-520dea9936ac"}
{"abstract": "This paper is devoted to letter-to-sound (LTS) conversion for the German language. The system for automatic LTS conversion is an essential module in speech recognition and text-to-speech (TTS) systems. The LTS rules are generated by the decision tree based method. Although this method usually uses the entropy criterion we proposed and used a new original criterion described in the paper. The both criterions are investigated and their results are compared. Our training lexicon does not contain any alignment information. Therefore, three different automatic alignment methods were used and tested. A chunk-based alignment instead of usual one-to-one character correspondence is employed.", "authors": ["Jan Zelinka", "L. M\u00fcller"], "n_citation": 0, "title": "Automatic general letter-to-sound rules generation for German text-to-speech system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "ecfe8cec-888f-4cf1-be4b-3b8600ce4939"}
{"abstract": "Geographic routing protocols are one of the most common routing schemes for sensor networks. These protocols consist of two different modes of operation: greedy routing to forward data to the destination using neighbors which are closer to the destination than current node and face routing to avoid voids in the network. Face routing requires the graph to be planar, which usually means that some crossing links of the original network cannot be considered when routing in face mode. In this paper we introduce a new localized scheme to build a virtual spanner which is planar by construction and is guaranteed to be connected if the underlying network is connected as well. Unlike previous works, by performing face routing over this spanner we can reduce energy consumption in face mode because the elimination of any of the original links in the network is not required. Thus, the most energy-efficient paths can be selected when the protocol enters face mode. The virtual spanner is easy-to-build and uses only local information, making it scalable to large-scale networks. Routing is always performed in real nodes: virtual nodes are used only as routing anchors when the agent is in face mode. In addition, our simulation results show that the proposed scheme outperforms the best energy-efficient geographic routing protocol for different network densities and energy models.", "authors": ["H\u00e9ctor Tejeda", "Edgar Ch\u00e1vez", "Juan Antonio Pastor S\u00e1nchez", "Pedro M. Ruiz"], "n_citation": 0, "title": "Energy-efficient face routing on the virtual spanner", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ed027b1a-2ab3-40b3-85f9-854474dc470e"}
{"abstract": "Human action recognition is a popular research area while it is changeling when facing various conditions related to viewpoint, subject, background, illumination and so on. Among all the variances, viewpoint variant is one of the most urgent problems to deal with. To this end, some view invariance approaches have been proposed, but they suffered from some weaknesses, such as lack of abundant information for recognition, dependency on robust meaningful feature detection or point correspondence. We propose a novel representation named Envelop Shape. We prove it from both theory and experiments that such representation is viewpoint insensitive. Envelop Shape is easy to acquire. It conveys abundant information enough for supporting action recognition directly. It also gets ride of the burdens such as feature detection and point correspondence, which are often difficult and error prone. In order to validate our proposed approach, we also present some experiments. With the help of Envelop Shape, our system achieves an impressive distinguishable result under different viewpoints.", "authors": ["Feiyue Huang", "Huijun Di", "Guangyou Xu"], "n_citation": 0, "title": "Viewpoint insensitive posture representation for action recognition", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ed525f1d-b52e-436d-818d-fc0ab51ef5aa"}
{"abstract": "This paper presents an exploratory study of jihadi extremist groups' videos using content analysis and a multimedia coding tool to explore the types of videos, groups' modus operandi, and production features. The videos convey messages powerful enough to mobilize members, sympathizers, and even new recruits to launch attacks that will once again be captured and disseminated via the Internet. The content collection and analysis of the groups' videos can help policy makers, intelligence analysts, and researchers better understand the groups' terror campaigns and modus operandi, and help suggest counterintelligence strategies and tactics for troop training.", "authors": ["Arab Salem", "Edna Reid", "Hsinchun Chen"], "n_citation": 0, "title": "Content analysis of jihadi extremist groups' videos", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "00388a30-3c9f-487c-aaa9-512165877708"}
{"abstract": "Let f : {0,1} n  \u2192 {0,1} l  be a one-way function. A function h: {0,1} n  \u2192 {0,1} m  is called a hard-core function for f if, when given f(x) for a (secret) x drawn uniformly from {0,1} n , it is computationally infeasible to distinguish h(x) from a uniformly random m-bit string. A (randomized) function h: {0,1} x {0,1} k  \u2192 {0,1} m  is a general hard-core function if it is hard-core for every one-way function f: {0,1} n  \u2192 {0,1} l , where the second input to h is a k-bit uniform random string r. Hard-core functions are a crucial tool in cryptography, in particular for the construction of pseudo-random generators and pseudo-random functions from any one-way function. The first general hard-core predicate, proposed by Goldreich and Levin, and several subsequently proposed hard-core functions, are bilinear functions in the two arguments x and r. In this paper we introduce a parameter of bilinear functions h: {0,1} n  \u00d7 {0,1} k  \u2192 {0,1} m , called exponential rank loss, and prove that it characterizes exactly whether or not h is a general hard-core function. The security proofs for the previously proposed bilinear hard-core functions follow as simple consequences. Our results are obtained by extending the class of list-decodable codes and by generalizing Hast's list-decoding algorithm from the Reed-Muller code to general codes.", "authors": ["Thomas Holenstein", "Ueli Maurer", "Johan Sj\u00f6din"], "n_citation": 0, "title": "Complete classification of bilinear hard-core functions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "011cc6fe-36a5-4152-9ab7-4d0cbcc05312"}
{"abstract": "This contribution introduces a class of Galois field used to achieve fast finite field arithmetic which we call an Optimal Extension Field (OEF). This approach is well suited for implementation of public-key cryptosystems based on elliptic and hyperelliptic curves. Whereas previous reported optimizations focus on finite fields of the form GF(p) and GF(2 m ), an OEF is the class of fields GF(p m ), for p a prime of special form and m a positive integer. Modern RISC workstation processors are optimized to perform integer arithmetic on integers of size up to the word size of the processor. Our construction employs well-known techniques for fast finite field arithmetic which fully exploit the fast integer arithmetic found on these processors. In this paper, we describe our methods to perform the arithmetic in an OEF and the methods to construct OEFs. We provide a list of OEFs tailored for processors with 8, 16, 32, and 64 bit word sizes. We report on our application of this approach to construction of elliptic curve cryptosystems and demonstrate a substantial performance improvement over all previous reported software implementations of Galois field arithmetic for elliptic curves.", "authors": ["Daniel V. Bailey", "Christof Paar"], "n_citation": 0, "title": "Optimal extension fields for fast arithmetic in public-key algorithms", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "026b3cf8-a235-47ea-aefb-38827169ed5e"}
{"authors": ["Matthias Baaz"], "n_citation": 0, "title": "Proof analysis by resolution", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "027000ea-f70d-4005-b509-413bb0bab652"}
{"abstract": "Cryptosystem designers frequently assume that secrets will be manipulated in closed, reliable computing environments. Unfortunately, actual computers and microchips leak information about the operations they process. This paper examines specific methods for analyzing power consumption measurements to find secret keys from tamper resistant devices. We also discuss approaches for building cryptosystems that can operate securely in existing hardware that leaks information.", "authors": ["Paul C. Kocher", "Joshua M. Jaffe", "Benjamin C. Jun"], "n_citation": 7091, "title": "Differential power analysis", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "033ebb72-0119-4cc9-89b8-f011c55ddd79"}
{"abstract": "Bilingual collocation correspondence is helpful to machine translation and second language learning. Existing techniques for identifying Chinese-English collocation correspondence suffer from two major problems. They are sensitive to the coverage of the bilingual dictionary and the insensitive to semantic and contextual information. This paper presents the ICT (Improved Collocation Translation) method to overcome these problems. For a given Chinese collocation, the word translation candidates extracted from a bilingual dictionary are expanded to improve the coverage. A new translation model, which incorporates statistics extracted from monolingual corpora, word semantic similarities from monolingual thesaurus and bilingual context similarities, is employed to estimate and rank the probabilities of the collocation correspondence candidates. Experiments show that ICT is robust to the coverage of bilingual dictionary. It achieves 50.1% accuracy for the first candidate and 73.1% accuracy for the top-3 candidates.", "authors": ["Ruifeng Xu", "Kam-Fai Wong", "Qin Lu", "Wenjie Li"], "n_citation": 0, "title": "An Improved Method for Finding Bilingual Collocation Correspondences from Monolingual Corpora", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "048bdd81-2094-4e08-ab95-0e130b8afdcf"}
{"authors": ["Susan Landau"], "n_citation": 0, "title": "Security, liberty, and electronic communications", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "04c71929-6951-4442-9035-a8b7d8fd09a5"}
{"abstract": "In general music composed by recurrent neural networks (RNNs) suffers from a lack of global structure. Though networks can learn note-by-note transition probabilities and even reproduce phrases, they have been unable to learn an entire musical form and use that knowledge to guide composition. In this study, we describe model details and present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and some listeners believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.", "authors": ["Douglas Eck", "J\u00fcrgen Schmidhuber"], "n_citation": 50, "title": "Learning the long-term structure of the blues", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "04d15b25-bc10-4887-8b47-aa54afce5c43"}
{"abstract": "Most guidelines for implementation of the RC4 stream cipher recommend discarding the first 256 bytes of its output. This recommendation is based on the empirical fact that known attacks can either cryptanalyze RC4 starting at any point, or become harmless after these initial bytes are dumped. The motivation for this paper is to find a conservative estimate for the number of bytes that should be discarded in order to be safe. To this end we propose an idealized model of RC4 and analyze it applying the theory of random shuffles. Based on our analysis of the model we recommend dumping at least 512 bytes.", "authors": ["Ilya Mironov"], "n_citation": 0, "title": "not so) random shuffles of RC4", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "04ec8ee1-313a-4e51-9d3a-232403aec95f"}
{"abstract": "We investigate, in the Shannon model, the security of constructions corresponding to double and (two-key) triple DES. That is, we consider F k1 (F k2 (.)) and F k1  (F -1  k2 (F k1  (.))) with the component functions being ideal ciphers. This models the resistance of these constructions to generic attacks like meet in the middle attacks. We obtain the first proof that composition actually increases the security in some meaningful sense. We compute a bound on the probability of breaking the double cipher as a function of the number of computations of the base cipher made, and the number of examples of the composed cipher seen, and show that the success probability is the square of that for a single key cipher. The same bound holds for the two-key triple cipher. The first bound is tight and shows that meet in the middle is the best possible generic attack against the double cipher.", "authors": ["William Aiello", "Mihir Bellare", "G. Di Crescenzo", "Ramarathnam Venkatesan"], "n_citation": 50, "title": "Security amplification by composition : The case of doubly-iterated, ideal ciphers", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "0609e047-26d1-49cb-8d29-9fa8d78aeac4"}
{"abstract": "We study the problem of broadcasting confidential information to a collection of n devices while providing the ability to revoke an arbitrary subset of those devices (and tolerating collusion among the revoked devices). In this paper, we restrict our attention to low-memory devices, that is, devices that can store at most O(log n) keys. We consider solutions for both zero-state and low-state cases, where such devices are organized in a tree structure T. We allow the group controller to encrypt broadcasts to any subtree of T, even if the tree is based on an multi-way organizational chart or a severely unbalanced multicast tree.", "authors": ["Michael T. Goodrich", "Jonathan Z. Sun", "Roberto Tamassia"], "n_citation": 0, "title": "Efficient tree-based revocation in groups of low-state devices", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "072b8c24-1bcf-4870-bbd1-ccd34e2565a9"}
{"abstract": "This study sought to address the problem of novices not being able to select the appropriate diagrams to suit given tasks. It investigated the usefulness of providing teaching sessions that involved active comparison of diagrams and review of lessons learnt following problem solving. Fifty-eight 8th grade participants were assigned to one of two instruction conditions. In both, traditional math classes were provided in which diagrams were used to explain how to solve math word problems. However, participants in the experimental group were additionally provided with sessions that required them to actively compare diagrams used, and consider and articulate the lessons they learnt from the problem solving exercises. The results showed that participants in the experimental condition subsequently constructed more appropriate diagrams in solving math word problems. In an assessment of conditional knowledge, these participants also provided more abstract and detailed descriptions about the uses of diagrams in problem solving.", "authors": ["Yuri Uesaka", "Emmanuel Manalo"], "n_citation": 50, "title": "Active comparison as a means of promoting the development of abstract conditional knowledge and appropriate choice of diagrams in math word problem solving", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "08174ec3-cb54-42cd-a637-0ca0d5ed9b54"}
{"abstract": "In this paper we extend the ideas for differential fault attacks on the RSA cryptosystem (see [4]) to schemes using elliptic curves. We present three different types of attacks that can be used to derive information about the secret key if bit errors can be inserted into the elliptic curve computations in a tamper-proof device. The effectiveness of the attacks was proven in a software simulation of the described ideas.", "authors": ["Ingrid Biehl", "Bernd Meyer", "Volker M\u00fcller"], "n_citation": 0, "title": "Differential fault attacks on elliptic curve cryptosystems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "08bdc7b8-d0e2-4a96-9e2d-9baf6c8550c4"}
{"abstract": "VoIP service over WLAN networks is a promising alternative to provide mobile voice communications to compete with cellular systems. However, several performance problems appear due to i) heavy protocol overheads, ii) unfairness and asymmetry between the uplink and downlink flows and iii ) the coexistence with other traffic flows. This paper addresses the performance of VoIP communications with simultaneous presence of bidirectional TCP traffic, showing how the presence of elastic flows drastically reduces the capacity of the system. To solve this limitation we propose a simple solution using an adaptive Admission and Rate Control algorithm which tunes the BEB (Binary Exponential Backoff) parameters as is defined in the IEEE 802. lie standard. The results show the improvement achieved on the overall system performance (in terms of number of simultaneous voice calls with QoS guarantees).", "authors": ["Boris Bellalta", "Michela Meo", "Miquel Oliver"], "n_citation": 0, "title": "A BEB-based admission control for VoIP calls in WLAN with coexisting elastic TCP flows", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "08edc8d1-43f5-4163-a2a8-cb727abdb940"}
{"abstract": "Current proposals for languages to encode terminological knowledge in intelligent systems support logical reasoning for answering user queries about objects and classes. An application of these languages on the World Wide Web, however, is hampered by the limitations of logical reasoning in terms of efficiency and flexibility. In this paper we describe, how techniques from approximate reasoning can be used to overcome these problems. We discuss terminological knowledge and approximate reasoning in general and show the benefits of approximate reasoning using the example of building and maintaining semantic catalogues that can be used to query resource locations based on object classes.", "authors": ["Heiner Stuckenschmidt", "Frank van Harmelen"], "n_citation": 0, "title": "Approximating terminological queries", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "09749a2d-db65-405e-ac2c-72814c42f2c8"}
{"abstract": "An important part of the software engineering process in today's component technologies is the integration of business logic and infrastructure services. In this paper, we investigate the current situation regarding transaction management services and discuss existing problems. We then present a conceptual framework for a model-based transaction service configuration approach and explain its constituent parts. The framework is based on metamodelling and thus can directly be used for efficient development of tool support.", "authors": ["Sten Loecher"], "n_citation": 0, "title": "Model-based transaction service configuration for component-based development", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "099e8a48-c224-4891-ad4b-326f270456aa"}
{"abstract": "Recently the braid groups were introduced as a new source for cryptography. The group operations are performed efficiently and the features are quite different from those of other cryptographically popular groups. As the first step to put the braid groups into the area of pseudorandomness, this article presents some cryptographic primitives under two related assumptions in braid groups. First, assuming that the conjugacy problem is a one-way function, say f, we show which particular bit of the argument x is pseudorandom given f(x). Next, under the decision Ko-Lee assumption, we construct two provably secure pseudorandom schemes: a pseudorandom generator and a pseudorandom synthesizer.", "authors": ["Eon-Kyung Lee", "Sang Jin Lee", "Sang Geun Hahn"], "n_citation": 50, "title": "Pseudorandomness from braid groups", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "0a2af95e-e306-4959-9913-f5e9e0aafe86"}
{"abstract": "This paper introduces a framework for the matching of 3D shapes represented by topological graphs. The method proposes as comparison algorithm an error tolerant graph isomorphism that includes a structured process for identifying matched areas on the input objects. Finally, we provide a series of experiments showing its capability to automatically compare complex objects starting from different skeletal representations used in Shape Modeling.", "authors": ["Silvia Biasotti", "Simone Marini", "Michela Mortara", "Giuseppe Patan\u00e8", "Michela Spagnuolo", "Bianca Falcidieno"], "n_citation": 0, "title": "3D Shape matching through topological structures", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "0bbfa5f3-5c14-49b7-8651-a57b66527a25"}
{"authors": ["Mario Lamberger", "Florian Mendel", "Vincent Rijmen"], "n_citation": 0, "title": "Collision Attack on the Hamsi-256 Compression Function", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "0c51e856-8667-428a-9134-4293c1e0eb2b"}
{"authors": ["Jong-An Park", "Nishat Ahmad", "Gwangwon Kang", "Jun Hyung Jo", "Pankoo Kim", "Seung-Jin Park"], "n_citation": 50, "title": "Defining a set of features using histogram analysis for content based image retrieval", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "0cf798f4-10fb-4d06-ad05-3ba50af85f55"}
{"authors": ["Pierpaolo Degano", "Gian Luigi Ferrari", "Letterio Galletta"], "n_citation": 0, "title": "A Two-Phase Static Analysis for Reliable Adaptation", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "0f13742d-aed8-423a-a47e-f2f4a9369815"}
{"abstract": "Connectionist semantic modeling in natural language processing (a typical symbolic domain) is still a challenging problem. This paper introduces a novel technique, combining Discourse Representation Theory (DRT) with Recursive Neural Networks (RNN) in order to yield a neural model capable to discover properties and relationships among constituents of a knowledge-base expressed by natural language sentences. DRT transforms sequences of sentences into directed ordered acyclic graphs, while RNNs are trained to deal with such structured data. The acquired information allows the network to reply on questions, the answers of which are not directly expressed into the knowledge-base. A simple experimental demonstration, drawn from the context of a fairy tales is presented. Finally, on-going research direction are pointed-out.", "authors": ["A. Bua", "Marco Gori", "Fabrizio Santini"], "n_citation": 0, "title": "Recursive Neural Networks applied to Discourse Representation Theory", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "112c4331-c41d-4d67-a414-35d68b3efda2"}
{"abstract": "Many cryptographic primitives begin with parameter generation, which picks a primitive from a family. Such generation can use public coins (e.g., in the discrete-logarithm-based case) or secret coins (e.g., in the factoring-based case). We study the relationship between public-coin and secret-coin collision-resistant hash function families (CRHFs). Specifically, we demonstrate that: - there is a lack of attention to the distinction between secret-coin and public-coin definitions in the literature, which has led to some problems in the case of CRHFs; - in some cases, public-coin CRHFs can be built out of secret-coin CRHFs; - the distinction between the two notions is meaningful, because in general secret-coin CRHFs are unlikely to imply public-coin CRHFs. The last statement above is our main result, which states that there is no black-box reduction from public-coin CRHFs to secret-coin CRHFs. Our proof for this result, while employing oracle separations, uses a novel approach, which demonstrates that there is no black-box reduction without demonstrating that there is no relativizing reduction.", "authors": ["Chun-Yuan Hsiao", "Leonid Reyzin"], "n_citation": 0, "title": "Finding collisions on a public road, or do secure hash functions need secret coins?", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "120efa76-cad2-4815-be8d-b691f71ec374"}
{"abstract": "While the educational effectiveness of static diagrams is underpinned by a long heritage of manipulating visuospatial characteristics of the depicted content to improve explanatory power, no corresponding evolution has occurred regarding temporal manipulation of animated diagrams. When complex dynamic subject matter is presented using animations that depict temporal information in a behaviorally realistic manner, the perceptual properties of the display may be poorly matched to the learner's information processing capacities. An exploration of the effect on information extraction of manipulating the speed and presentation frequency of an animation depicting dynamically-complex subject matter suggests that such manipulations may be used to improve perception of thematically relevant information.", "authors": ["Richard Lowe"], "n_citation": 50, "title": "Changing perceptions of animated diagrams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "133b56f1-573b-433d-b78c-2e4535fb4a23"}
{"abstract": "Previously, we proposed techniques to detect the misuse of search systems using predominantly relevance feedback based techniques. Although the approaches developed achieved high detection rate, they did so with a relatively high rate of false alarm. We now present a clustering query results based approach. This approach supports a higher precision, i.e., lower false alarm rate, with only a modest compromise on detection rate, namely recall.", "authors": ["Nazli Goharian", "Alana Platt"], "n_citation": 0, "title": "Detection using clustering query results", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "13dd5f03-8e54-4e22-9031-0d8ac6b191c5"}
{"abstract": "The method of conjugate gradients provides a very effective way to optimize large, deterministic systems by gradient descent. In its standard form, however, it is not amenable to stochastic approximation of the gradient. Here we explore ideas from conjugate gradient in the stochastic (online) setting, using fast Hessian-gradient products to set up low-dimensional Krylov subspaces within individual mini-batches. In our benchmark experiments the resulting online learning algorithms converge orders of magnitude faster than ordinary stochastic gradient descent.", "authors": ["Nicol N. Schraudolph", "Thore Graepel"], "n_citation": 0, "title": "Conjugate directions for stochastic gradient descent", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "150a70c4-3723-404e-b371-4645ac6a8544"}
{"abstract": "There are many situations in GiScience where it would be useful to be able to assign a region to characterize the space occupied by a set of points. Such a region should represent the location or configuration of the points as an aggregate, abstracting away from the individual points themselves. In this paper, we call such a region a 'footprint' for the points. We investigate and compare a number of methods for producing such footprints, with respect to nine general criteria. The discussion identifies a number of potential choices and avenues for further research. Finally, we contrast the related research already conducted in this area, highlighting differences between these existing constructs and our footprints'.", "authors": ["Antony Galton", "Matt Duckham"], "n_citation": 0, "title": "What Is the Region Occupied by a Set of Points", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1526dea9-d3ec-4012-a237-7a9d87afa530"}
{"abstract": "In this paper, we propose a novel and efficient protocol for proving the correctness of a shuffle, without leaking how the shuffle was performed. Using this protocol, we can prove the correctness of a shuffle of n data with roughly 18n exponentiations, where as the protocol of Sako-Kilian[SK95] required 642n and that of Abe[Ab99] required 22nlogn. The length of proof will be only 211n bits in our protocol, opposed to 2 18 n bits and 2 14 n log n bits required by Sako-Kilian and Abe, respectively. The proposed protocol will be a building block of an efficient, universally verifiable mix-net, whose application to voting system is prominent.", "authors": ["Jun Furukawa", "Kazue Sako"], "n_citation": 278, "title": "An efficient scheme for proving a shuffle", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "160834cc-caef-4a3a-9c07-22d6b531623b"}
{"abstract": "We describe a family of three replication protocols, each of which can operate in three different modes of consistency. The protocols are tailored to satisfy the availability demands of interconnected databases that have a high degree of data locality. The protocols accomplish a grade of transaction completion which does not compromise availability, and ensure the consistency of replicas also if a transaction needs to be aborted. Flexibility of query answering is understood as optimizing the tradeoff between consistency and availability, i.e., between correctness and timeliness of query answering. This is achieved by choosing an appropriate protocol alternative, and changing the consistency mode of operation during the session, as appropriate for a given transaction.", "authors": ["Francesc D. Mu\u00f1oz-Esco\u00ed", "Luis Ir\u00fan-Briz", "Pablo Gald\u00e1mez", "Jos\u00e9 M. Bernab\u00e9u-Aub\u00e1n", "Jordi Bataller", "M. Carmen Ba\u00f1uls", "Hendrik Decker"], "n_citation": 0, "title": "Flexible management of consistency and availability of networked data replications", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "16b2172f-2e61-4cec-b948-861511bf9c1f"}
{"abstract": "We present a PCA-LVQ model and a balanced-training method for efficient intrusion alert analysis. For the connection records in the 1999 DARPA intrusion dataset, we firstly get a dimension-reduced dataset through Principal Component Analysis (PCA). Then, we use the Learning Vector Quantization (LVQ) neural network to perform intrusion alert clustering on the purified intrusion dataset. The experiment results show that the PCA-LVQ model and the balanced-training method are effective: the time costs can be shortened about by three times, and the accuracy of detection can be elevated to a higher level, especially for the U2R and R2L alerts.", "authors": ["Jing-Xin Wang", "Zhiying Wang", "Kui Dai"], "n_citation": 0, "title": "A PCA-LVQ model for intrusion alert analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "170ac330-5851-4aff-b5db-302696179a91"}
{"abstract": "In 1980 Martin Hellman described a cryptanalytic time-memory trade-off which reduces the time of cryptanalysis by using precalculated data stored in memory. This technique was improved by Rivest before 1982 with the introduction of distinguished points which drastically reduces the number of memory lookups during cryptanalysis. This improved technique has been studied extensively but no new optimisations have been published ever since. We propose a new way of precalculating the data which reduces by two the number of calculations needed during cryptanalysis. Moreover, since the method does not make use of distinguished points, it reduces the overhead due to the variable chain length, which again significantly reduces the number of calculations. As an example we have implemented an attack on MS-Windows password hashes. Using 1.4GB of data (two CD-ROMs) we can crack 99.9% of all alphanumerical passwords hashes (2 37 ) in 13.6 seconds whereas it takes 101 seconds with the current approach using distinguished points. We show that the gain could be even much higher depending on the parameters used.", "authors": ["Philippe Oechslin"], "n_citation": 0, "title": "Making a faster cryptanalytic time-memory trade-off", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "174a0811-a5be-4100-897b-14af531e20da"}
{"abstract": "Fuzzy relational databases have been extensively studied in recent years, resulting in several models and representation techniques, some of which have been implemented as software layers on top of diverse existing database systems. Fuzzy extensions to query languages and end-user query interfaces have also been developed, but the design of programming interfaces has not been properly addressed yet. In this paper, we describe a software framework called fJDBC that extends the Java Database Connectivity API by enabling fuzzy queries on existing relational databases, using externally-stored metadata. Since the main design objective of this extension is usability for existing database programmers, only a restricted subset of extensions (supported also by an extended object modelling notation) has been included. The overall design of the framework and some empirical results are also described.", "authors": ["Miguel-Angel Sicilia", "E. Garcia", "Paloma D\u00edaz", "Ignacio Aedo"], "n_citation": 50, "title": "Extending relational data access programming libraries for fuzziness: The fJDBC framework", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "179da144-2438-43a6-b0d3-9c66e761e0d1"}
{"abstract": "Constraint programming offers a variety of modeling objects such as logical and global constraints, that lead to concise and clear models for expressing combinatorial optimization problems. We propose a way to provide a linear formulation of such a model and detail, in particular, the transformation of some global constraints. An automatic procedure for producing and updating formulations has been implemented and we illustrate it on combinatorial optimization problems.", "authors": ["Philippe Refalo"], "n_citation": 83, "title": "Linear formulation of constraint programming models and hybrid solvers", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "ef112ff9-5762-491e-a59b-a0395bae4804"}
{"abstract": "A machine learning technique called Graph-Based Induction (GBI) extracts typical patterns from graph data by stepwise pair expansion (pairwise chunking). Because of its greedy search strategy, it is very efficient but suffers from incompleteness of search. Improvement is made on its search capability without imposing much computational complexity by 1) incorporating a beam search, 2) using a different evaluation function to extract patterns that are more discriminatory than those simply occurring frequently, and 3) adopting canonical labeling to enumerate identical patterns accurately. This new algorithm, now called Beam-wise GBI, B-GBI for short, was tested against a small DNA dataset from UCI repository and shown successful in extracting discriminatory substructures.", "authors": ["Takashi Matsuda", "Hiroshi Motoda", "Tetsuya Yoshida", "Takashi Washio"], "n_citation": 0, "title": "Mining patterns from structured data by beam-wise Graph-Based induction", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ef27182c-74a2-4461-b372-5db42f1223bc"}
{"authors": ["Atsuko Miyaji", "Masahiro Sukegawa"], "n_citation": 0, "title": "New Correlations of RC4 PRGA Using Nonzero-Bit Differences", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "ef7944ae-40a9-495c-9dde-3f97f06a2729"}
{"abstract": "Certificateless public-key cryptosystem is a new and attractive paradigm, which avoids the inherent key escrow property in identity-based public-key cryptosystems, and does not need expensive certificates as in the public key infrastructure. A strong security model for certificateless public key encryption was established by Al-Riyami and Paterson in 2003. In this paper, we first present a security model for certificateless public-key signature schemes, and then propose an efficient construction based on bilinear pairings. The security of the proposed scheme can be proved to be equivalent to the computational Diffie-Hellman problem in the random oracle model with a tight reduction.", "authors": ["Zhenfeng Zhang", "Duncan S. Wong", "Jing Xu", "Dengguo Feng"], "n_citation": 248, "title": "Certificateless public-key signature : Security model and efficient construction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "efcc7ee5-6034-4f9b-86e6-ec7583a3ead4"}
{"abstract": "We present a robust recursive total least squares (RRTLS) algorithm for multilayer feed-forward neural networks. So far, recursive least squares (RLS) has been successfully applied to training multilayer feed-forward neural networks. However, if input data has additive noise, the results from RLS could be biased. Theoretically, such biased results can be avoided by using the recursive total least squares (RTLS) algorithm based on Power Method. In this approach, Power Method uses rank-1 update and thus is apt to be in ill condition. In this paper, therefore, we propose a robust RTLS algorithm using regularized UDU factorization 1 . This method gives better performance than RLS based training over a wide range of SNRs.", "authors": ["Jun-Seok Lim", "Nakjin Choi", "Koeng-Mo Sung"], "n_citation": 0, "title": "Robust recursive TLS (total least square) method using regularized UDU decomposed for FNN (feedforward neural network) training", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f06171c5-7a1f-493d-875a-589575d3727b"}
{"abstract": "All existing architectures and learning algorithms for Generalized Congruence Neural Network (GCNN) seem to have some shortages or lack rigorous theoretical foundation. In this paper, a novel GCNN architecture (BPGCNN) is proposed. A new error back-propagation learning algorithm is also developed for the BPGCNN. Experimental results on some benchmark problems show that the proposed BPGCNN performs better than standard sigmoidal BPNN and some improved versions of BPNN in convergence speed and learning capability, and can overcome the drawbacks of other existing GCNNs.", "authors": ["Yong Chen", "Guoyin Wang", "Fan Jin", "Tianyun Yan"], "n_citation": 50, "title": "A novel generalized congruence neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f0e821e4-154d-4291-8af0-b75f970082b9"}
{"abstract": "Modeling and describing temporal structure in multimedia signals, which are captured simultaneously by multiple sensors, is important for realizing human machine interaction and motion generation. This paper proposes a method for modeling temporal structure in multimedia signals based on temporal intervals of primitive signal patterns. Using temporal difference between beginning points and the difference between ending points of the intervals, we can explicitly express timing structure; that is, synchronization and mutual dependency among media signals. We applied the model to video signal generation from an audio signal to verify the effectiveness.", "authors": ["Hiroaki Kawashima", "Kimitaka Tsutsumi", "Takashi Matsuyama"], "n_citation": 50, "title": "Modeling timing structure in multimedia signals", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f129ca23-0ce0-4c39-85e4-015b5ae7494f"}
{"abstract": "The IEEE 802.11 MAC protocol is vulnerable to selfish backoff attacks exploiting the constituent CSMA/CA mechanism. Administrative prevention of such attacks fails in wireless ad-hoc LANs which cannot mandate stations' behavior. We take a game-theoretic approach whereby stations are allowed to maximize their payoffs (success rates). Using a fairly accurate performance model we show that a noncooperative CSMA/CA game then arises with a payoff structure characteristic of a Prisoners' Dilemma. For a repeated CSMA/CA game, a novel SPELL strategy is proposed. If the stations are rational players and wish to maximize a long-term utility, SPELL deters a single attacker by providing a disincentive to deviate from SPELL.", "authors": ["Jerzy Konorski"], "n_citation": 0, "title": "Playing CSMA/CA game to deter backoff attacks in ad hoc wireless LANs", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f13dac68-5a61-4dea-841f-902885f7bce0"}
{"abstract": "SAT based Bounded Model Checking (BMC) is an efficient method for detecting logical errors in finite-state transition systems. Given a transition system, an LTL property, and a user defined bound k, a bounded model checker generates a propositional formula that is satisfiable if and only if a counterexample to the property of length up to k exists. Standard SAT checkers can be used to check this formula. BMC is complete if k is larger than some pre-computed threshold. It is still unknown how to compute this threshold for general properties. We show that the longest initialized loop-free path in the state graph, also known as the recurrence diameter, is sufficient for Fp properties. The recurrence diameter is also a known over-approximation for the threshold of simple safety properties (Gp). We discuss various techniques to compute the recurrence diameter efficiently and provide experimental results that demonstrate the benefits of using the new annroach.", "authors": ["Daniel Kroening", "Ofer Strichman"], "n_citation": 143, "title": "Efficient computation of recurrence diameters", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f1c46cfb-80c6-4c55-a71b-f63cb4649535"}
{"abstract": "In this paper we evolve a rule based approach to SLA representation and management which allows separating the contractual business logic from the application logic and enables automated execution and monitoring of SLA specifications. We make use of a set of knowledge representation (KR) concepts and combine adequate logical formalisms in one expressive formal framework called ContractLog.", "authors": ["Adrian Paschke", "Martin Bichler", "Jens Dietrich"], "n_citation": 0, "title": "ContractLog : An approach to rule based monitoring and execution of service level agreements", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f214db68-daef-4552-b7eb-057ce4590939"}
{"abstract": "The independent component analysis (ICA) problem originates from many practical areas, but there has not been any mathematical theory to solve it completely. In this paper, we establish a mathematical theory to solve it under the condition that the number of super-Gaussian sources is known. According to this theory, a step by step optimization algorithm is proposed and demonstrated well on solving the ICA problem with both the super- and sub-Gaussian sources.", "authors": ["Dengpan Gao", "Jinwen Ma", "Qiansheng Cheng"], "n_citation": 0, "title": "A step by step optimization approach to independent component analysis", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f244764f-83e0-4918-994f-6406cdd0d4ef"}
{"abstract": "The primary objective of this paper is to put forward a general framework under which clear definitions of immune operators and their roles are provided. To this aim, a novel Population Adaptive Based Immune Algorithm (PAIA) inspired by Clonal Selection and Immune Network theories for solving multi-objective optimization problems (MOP) is proposed. The algorithm is shown to be insensitive to the initial population size; the population and clone size are adaptive with respect to the search process and the problem at hand. It is argued that the algorithm can largely reduce the number of evaluation times and is more consistent with the vertebrate immune system than the previously proposed algorithms. Preliminary results suggest that the algorithm is a valuable alternative to already established evolutionary based optimization algorithms, such as NSGA II, SPEA and VIS.", "authors": ["Jun Chen", "Mahdi Mahfouf"], "n_citation": 0, "title": "A Population adaptive based immune algorithm for solving multi-objective optimization problems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f30a7dfc-4cd5-41b3-9039-d607e5788299"}
{"abstract": "We present new algorithms for concurrent reading and updating of B*-trees and binary search trees. Our algorithms are based on the well-known link technique, and improve previously proposed solutions in several respects. We prove formally that our algorithms are correct. We show that they satisfy a view serializability criterion, which fails for previous solutions. This stronger serializability criterion is central to the proof that several subtle (but essential) optimizations incorporated in our algorithms are correct.", "authors": ["Stavros S. Cosmadakis", "Kleoni Ioannidou", "Stergios Stergiou"], "n_citation": 0, "title": "View serializable updates of concurrent index structures", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f3397358-d6a3-492f-9f13-7af3c86a7a35"}
{"abstract": "Certificateless cryptography involves a Key Generation Center (KGC) which issues a partial key to a user and the user also independently generates an additional public/secret key pair in such a way that the KGC who knows only the partial key but not the additional secret key is not able to do any cryptographic operation on behalf of the user; and a third party who replaces the public/secret key pair but does not know the partial key cannot do any cryptographic operation as the user either. We call this attack launched by the third party as the key replacement attack. In ACISP 2004, Yum and Lee proposed a generic construction of digital signature schemes under the framework of certificateless cryptography. In this paper, we show that their generic construction is insecure against key replacement attack. In particular, we show that the security requirements of their generic building blocks are insufficient to support some security claim stated in their paper. We then propose a modification of their scheme and show its security in a new and simplified security model. We show that our simplified definition and adversarial model not only capture all the distinct features of certificateless signature but are also more versatile when compared with all the comparable ones. We believe that the model itself is of independent interest.", "authors": ["Bessie C. Hu", "Duncan S. Wong", "Zhenfeng Zhang", "Xiaotie Deng"], "n_citation": 185, "title": "Key replacement attack against a generic construction of certificateless signature", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f34ef91b-7222-4516-95b4-19dd5f383be1"}
{"abstract": "3D video, which is composed of a sequence of mesh models and can provide the user with interactivity, is attracting increasing attention in many research groups. However, it is time-consuming and expensive to generate 3D video sequences. In this paper, a motion composition method is proposed to edit 3D video based on the user's requirements so that 3D video can be re-used. By analyzing the feature vectors, the hierarchical motion structure is parsed and then a motion database is set up by selecting the representative motions. A motion graph is constructed to organize the motion database by finding the possible motion transitions. Then, the best path is searched based on a proposed cost function by a modified Dijkstra algorithm after the user selects the desired motions in the motion database, which are called key motions in this paper. Our experimental results show the edited 3D video sequence looks natural and realistic.", "authors": ["Jianfeng Xu", "Toshihiko Yamasaki", "Kiyoharu Aizawa"], "n_citation": 0, "title": "Motion Composition of 3D Video", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f354ccf6-231d-4fa9-9cd0-6d7de89c2fd4"}
{"abstract": "We present a new numerical abstract domain. This domain automatically detects and proves bounds on the values of program variables. For that purpose, it relates variable values to a clock counter. More precisely, it bounds these values with the i-th iterate of the function [X \u03b1\u00d7X+\u03b2] applied on M, where i denotes the clock counter and the floating-point numbers a, \u03b2, and M are discovered by the analysis. Such properties are especially useful to analyze loops in which a variable is iteratively assigned with a barycentric mean of the values that were associated with the same variable at some previous iterations. Because of rounding errors, the computation of this barycenter may diverge when the loop is iterated forever. Our domain provides a bound that depends on the execution time of the program.", "authors": ["J\u00e9r\u00f4me Feret"], "n_citation": 52, "title": "The arithmetic-geometric progression abstract domain", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f37e8ee7-2302-4cbc-8495-a863740a09a7"}
{"abstract": "This article describes a new approach to estimate F 0  curves using B-spline and Spline models characterized by a knot sequence and associated control points. The free parameters of the model are the number of knots and their location. The free-knot placement, which is a NP-hard problem, is done using a global MLE (Maximum Likelihood Estimation) within a simulated-annealing strategy. Experiments are conducted in a speech processing context on a 7000 syllables french corpus. We estimate the two challenging models for increasing values of the number of free parameters. We show that a B-spline model provides a slightly better improvement than the Spline model in terms of RMS error.", "authors": ["Damien Lolive", "Nelly Barbot", "Olivier Bo\u00ebffard"], "n_citation": 0, "title": "Comparing B-spline and spline models for F0 modelling", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f43f8261-207b-4c58-bd55-a44137785dee"}
{"abstract": "The Java platform has many characteristics which make it very desirable for integrated continuous media processing. Unfortunately, it lacks the necessary CPU resource management facility to support Quality of Service guarantees for soft real-time multimedia tasks. In this paper, we present our new Java Virtual Machine, Q-JVM, which brings CPU resource management to the Java platform. Q-JVM is based on Sun's reference implementation. It incorporates an enhanced version of the MTR-LS algorithm in its thread scheduler. Combined with an optional admission control mechanism, this algorithm is able to support QoS parameters such as fairness, bandwidth partitioning and delay bound guarantees, as well as the cumulative service guarantee. Preliminary experimental results show that Q-JVM is backward compatible with the standard version from Sun, has low scheduling overhead, and is able to provide QoS guarantees as specified.", "authors": ["James C. Pang", "Gholamali C. Shoja", "Eric G. Manning"], "n_citation": 0, "title": "Supporting soft real-time tasks and QoS on the Java platform", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "f4685503-a5f2-4f9f-a740-171a4fcf7d28"}
{"abstract": "This paper presents a Tabu Search (TS) algorithm for solving maximal constraint satisfaction problems. The algorithm was tested on a wide range of random instances (up to 500 variables and 30 values). Comparisons were carried out with a min-conflicts+random-walk (MCRW) algorithm. Empirical evidence shows that the TS algorithm finds results which are better than that of the MCRW algorithm.the TS algorithm is 3 to 5 times faster than the MCRW algorithm to find solutions of the same quality.", "authors": ["Philippe Galinier", "Jin Kao Hao"], "n_citation": 0, "title": "Tabu Search for maximal constraint satisfaction problems", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "f5042495-9a9d-4a89-a7a2-5c1e0695bce9"}
{"abstract": "Combinatorial problems can be efficiently tackled with constraint programming systems. The main tasks of the development of a constraint-based application are modeling the problem at hand and subsequently implementing that model. Typically, erroneous behavior of a constraint-based application is caused by either the model or the implementation (or both of them). Current constraint programming systems provide limited debugging support for modeling and implementing a problem. This paper proposes the Constraint Investigator, an interactive tool for debugging the model and the implementation of a constraint-based application. In particular, the Investigator is targeted at problems like wrong, void, or partial solutions. A graph metaphor is used to reflect the constraints in the solver and to present them to the user. The paper shows that this metaphor is intuitive and proposes appraoches to deal with real-life problem sizes. The Investigator has been implemented in Mozart Oz and complements other constraint programming tools as an interactive visual search engine, forming the base for an integrated constraint debugging environment.", "authors": ["Tobias M\u00fcller"], "n_citation": 0, "title": "Practical investigation of constraints with graph views", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "f5bfc2e8-f751-4c06-b271-29703435a210"}
{"abstract": "Research in the field of dialogue systems often involves building a dialogue system used for evaluation of algorithms, collection of data and various experiments. A significant amount of time is needed to create such a system. In order to facilitate this task, we created a flexible, extensible and easy to use framework which can be used as a base for experimenting with dialogue systems. Major features of the framework are introduced in the paper together with possible ways of their practical use.", "authors": ["Pavel Cenek"], "n_citation": 0, "title": "A flexible framework for evaluation of new algorithms for dialogue systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "f663c7b2-3f12-4518-9c7d-c94edd43342f"}
{"abstract": "Vocal Tract Length Normalization (VTLN) is a well established and successful technique for speaker normalization. It can be applied in the recognition stage, but the improvements are roughly doubled if the same algorithm is applied to the training data before building the acoustic model as well. The most common implementation uses a few minutes of speech or more per speaker and the final result, even if the recognition was faster than real time has significant latency. In this work we address the following constraints: reduced amount of data per speaker in training and testing; reduced latency, with no latency as the ultimate goal. The experiments show that although these restrictions impact the performance improvements possible with VTLN, real-time implementation of VTLN is not only practical but highly desirable.", "authors": ["Andrej Ljolje", "Vincent Goffin", "Murat Saraclar"], "n_citation": 0, "title": "Low latency real-time vocal Tract Length Normalization", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f677a6e5-f908-42c0-b5b3-1c0a618b36b2"}
{"abstract": "Clusters of shared-memory multiprocessors (SMPs) have become the most promising parallel computing platforms for scientific computing. However, SMP clusters significantly increase the complexity of user application development when using the low-level application programming interfaces MPI and OpenMP, forcing users to deal with both distributed-memory and shared-memory parallelization details. In this paper we present extensions of High Performance Fortran for SMP clusters which enable the compiler to adopt a hybrid parallelization strategy, efficiently combining distributed-memory with shared-memory parallelism. By means of a small set of new language features, the hierarchical structure of SMP clusters may be specified. This information is utilized by the compiler to derive inter-node data mappings for controlling distributed-memory parallelization across the nodes of a cluster, and intra-node data mappings for extracting shared-memory parallelism within nodes. Additional mechanisms are proposed for specifying interand intra-node data mappings explicitly, for controlling specific SM parallelization issues, and for integrating OpenMP routines in HPF applications. The proposed features are being realized within the ADAPTOR and VFC compiler. The parallelization strategy for clusters of SMPs adopted by these compilers is discussed as well as a hybrid-parallel execution model based on a combination of MPI and OpenMP. Early experimental results indicate the effectiveness of the proposed features.", "authors": ["Siegfried Benkner", "Thomas Brandes"], "n_citation": 50, "title": "High-level data mapping for clusters of SMPs", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "f6c084d0-cf81-464f-b4b2-2b920af863a4"}
{"abstract": "We use hidden algebra as a formal framework for object paradigm. We introduce a labeled transition system for each object specification model, and then define a suitable notion of bisimulation over these models. The labeled transition systems are used to define CTL models of object specifications. Given two hidden algebra models of an object specification, the bisimilar states satisfy the same set of CTL formulas. We build a canonical CTL model directly from the object specification. Using this CTL model, we can verify the temporal properties using a software tool allowing SMV model checking.", "authors": ["Dorel Lucanu", "Gabriel Ciobanu"], "n_citation": 0, "title": "Model checking for object specifications in hidden algebra", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f712a76a-8d7b-4556-b05b-32384c668ef5"}
{"abstract": "In this paper, we present a distributed computing method, namely Sequential Bayesian Learning for modular neural networks. The method is based on the idea of sequential Bayesian decision analysis to gradually improving the decision accuracy by collecting more information derived from a series of experiments and determine the combination weights of each sub-network. One of the advantages of this method is it emulates humans' problems processing mode effectively and makes uses of old information while new data information is acquired at each stage. The results of experiments on eight regression problems show that the method is superior to simple averaging on those hard-to-learn problems.", "authors": ["Pan Wang", "Zhun Fan", "Youfeng Li", "Shan Feng"], "n_citation": 50, "title": "Sequential bayesian learning for modular neural networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "f714acec-7fb2-4318-b70d-237b790d876d"}
{"authors": ["Maciej Cytowski", "Maciej Remiszewski", "I. Soszynski"], "n_citation": 0, "title": "Astronomical Period Searching on the Cell Broadband Engine", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "f75b7822-2d3b-485d-a147-72343870e7e1"}
{"abstract": "In this paper, a supervised learning method of semantic role labeling is presented. It is based on maximum entropy conditional probability models. This method acquires the linguistic knowledge from an annotated corpus and this knowledge is represented in the form of features. Several types of features have been analyzed for a few words selected from sections of the Wall Street Journal part of the Penn Treebank corpus.", "authors": ["Paloma Moreda", "Manuel L\u00f3pez Fern\u00e1ndez", "Manuel Palomar", "Armando Su\u00e1rez"], "n_citation": 0, "title": "Identifying semantic roles using maximum entropy models", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "f7d604b6-e42c-4f59-b54f-68c43aa7dd78"}
{"abstract": "This paper presents a simple method of determining the voice similarity by analyzing a set of very short sounds. A large number of pitch-length sounds were extracted from natural voice signals from different realizations of open vowels 'a' and 'o'. The voice similarity was defined as the sum of single elementary similarities of short sound pairs. This method is oriented to the microphonemic speech synthesis based on waveform concatenation, and it could help to limit the time needed for database collection. This simple and low computational load speech synthesis method can be applied in small portable devices and used for the rehabilitation of speech disabled people.", "authors": ["K Lukaszewicz", "Matti Karjalainen"], "n_citation": 0, "title": "Simple method of determining the voice similarity and stability by analyzing a set of very short sounds", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f7d67d2f-4d5e-43c7-a96b-154047f41f74"}
{"abstract": "To improve coding efficiency, the H.264/AVC video coding standard uses new coding tools, such as variable block size, quarter-pixel-accuracy motion estimation, multiple reference frames, intra prediction and a loop filter. Using these coding tools, H.264/AVC achieves significant improvement in coding efficiency compared with existing standards. However, the encoder complexity also increases tremendously. Among the tools, macroblock mode decision and motion estimation contribute most to total encoder complexity. This paper focuses on complexity reduction in macroblock mode decision. Of the macroblock modes which can be selected, inter8x8 and intra4x4 have the highest complexity. We propose three methods for complexity reduction, one for intra4x4 in infra-frames, one for inter8x8 in inter-frames, and one for intra4x4 in inter-frames. Simulation results show that the proposed methods save about 56.5% of total encoding time compared with the H.264/AVC reference implementation.", "authors": ["Donghyung Kim", "Joohyun Lee", "Kicheol Jeon", "Jechang Jeong"], "n_citation": 0, "title": "Macroblock Mode Decision Scheme for Fast Encoding in H.264/AVC", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f847f732-01da-4981-a362-30ed5bced2f4"}
{"abstract": "We consider semantics of infinite-state programs, both probabilistic and nondeterministic, as expectation functions: for any set of states A, we associate to each program point a function mapping each state to its expectation of starting a trace reaching A. We then compute a safe upper approximation of these functions using abstract interpretation. This computation takes place in an abstract domain of extended Gaussian (normal) distributions.", "authors": ["David Monniaux"], "n_citation": 0, "title": "Abstraction of expectation functions using Gaussian distributions", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "f8ccab88-4eb3-4092-bed1-046fcfe1fa8d"}
{"abstract": "We propose the notion of plaintext awareness in the two-key setting, called PATK. We also prove that if a public-key encryption scheme is secure in the sense of PATK, then it is also secure in the sense of IK-CCA. Since it looks much easier to prove that a public-key encryption scheme is secure in the sense of PATK than to prove directly that it is secure in the sense of IK-CCA, the notion of PATK is useful to prove the anonymity property of public-key encryption schemes. We also propose the first generic conversion for the anonymity, that is, we prove that the public-key encryption scheme derived from the Fujisaki-Okamoto conversion scheme, where the basic public-key encryption scheme is secure in the sense of IK-CPA, is secure in the sense of IK-CCA in the random oracle model.", "authors": ["Ryotaro Hayashi", "Keisuke Tanaka"], "n_citation": 0, "title": "PA in the two-key setting and a generic conversion for encryption with anonymity", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "f94bc97e-d19b-44e7-8b27-58e017e3ec72"}
{"abstract": "Seeking to use software, hardware, and algorithmic ingenuity to create unique domain-independent instruments.", "authors": ["William Regli"], "n_citation": 0, "references": ["a662a4e7-415e-417e-8a8f-fe085d7e487f"], "title": "Wanted: toolsmiths", "venue": "Communications of The ACM", "year": 2017, "id": "f9593524-54fe-4928-8dce-d2c773f1782e"}
{"abstract": "Mobile ad hoc networks (MANETs) define a challenging computing scenario where access to resources is restrained by connectivity among hosts. Replication offers an opportunity to increase data availability beyond the span of transient connections. Unfortunately, standard replication techniques for wired environments mostly target improvements to fault-tolerance and access time, and in general are not well-suited to the dynamic environment defined by MANETs. In this paper we explore replication for mobility in the context of a veneer for LIME, a Linda-based middleware for MANETs. This veneer puts into the hands of the application programmer control over what to replicate as well as a set of novel replication and consistency modes meaningful in mobile ad hoc networks. The entire replication veneer is built on top of the existing LIME model and implementation, confirming their versatility.", "authors": ["Amy L. Murphy", "Gian Pietro Picco"], "n_citation": 50, "title": "Using LIME to support replication for availability in mobile ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fa0916ad-5dd5-4735-b8d7-a6fa8c102d03"}
{"abstract": "Cascades of boosted ensembles have become popular in the object detection community following their highly successful introduction in the face detector of Viola and Jones [1]. In this paper, we explore several aspects of this architecture that have not yet received adequate attention: decision points of cascade stages, faster ensemble learning, and stronger weak hypotheses. We present a novel strategy to determine the appropriate balance between false positive and detection rates in the individual stages of the cascade based on a probablistic model of the overall cascade's performance. To improve the training time of individual stages, we explore the use of feature filtering before the application of Adaboost. Finally, we show that the use of stronger weak hypotheses based on CART can significantly improve upon the standard face detection results on the CMU-MIT data set.", "authors": ["S. Charles Brubaker", "Matthew D. Mullin", "James M. Rehg"], "n_citation": 62, "title": "Towards Optimal Training of Cascaded Detectors", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fa5ac403-3694-49e0-ab32-cfe7a5361408"}
{"abstract": "Virtual assistants, also called Avatars, are virtual characters making the communication between the user and the machine more natural and interactive. In this research we have given avatars the capacity of having and expressing emotions by means of a computational emotional model based on the cognitive perspective. Once the system knows the emotional expressiveness that the virtual character will show to the user, we also have worked in how to express it through facial animation techniques.", "authors": ["Amalia Ortiz", "David Oyarzun", "Mar\u00eda del Puy Carretero", "Nestor Garay-Vitoria"], "n_citation": 0, "title": "Virtual characters as emotional interaction element in the user interfaces", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "faced112-9ee1-4328-a4fa-adb0a34f79d5"}
{"abstract": "One technique that uses Wang's Recurrent Neural Networks with the Winner Takes All principle is presented to solve the Assignment problem. With proper choices for the parameters of the Recurrent Neural Network, this technique reveals to be efficient solving the Assignment problem in real time. In cases of multiple optimal solutions or very closer optimal solutions, the Wang's Neural Network does not converge. The proposed technique solves these types of problem. Comparisons between some traditional ways to adjust the RNN's parameters are made, and some proposals concerning to parameters with dispersion measures of the problem's cost matrix' coefficients are show.", "authors": ["Paulo Henrique Siqueira", "S\u00e9rgio Scheer", "Maria Teresinha Arns Steiner"], "n_citation": 50, "title": "Application of the winner takes all principle in wang's recurrent neural network for the assignment problem", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "fafe2700-f42e-44e9-a421-650f6bfbee37"}
{"abstract": "Self-organizing neural networks are typically associated with unsupervised learning. This paper presents a self-organizing neural architecture, known as TD-FALCON, that learns cognitive codes across multi-modal pattern spaces, involving states, actions, and rewards, and is capable of adapting and functioning in a dynamic environment with external evaluative feedback signals. We present a case study of TD-FALCON on a mine avoidance and navigation cognitive task, and illustrate its performance by comparing with a State-of-the-art reinforcement learning approach based on gradient descent backpropagation algorithm.", "authors": ["Ah-Hwee Tan"], "n_citation": 50, "title": "Self-organizing Neural Architecture for Reinforcement Learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fb61c886-162e-4607-9091-7d5cc07bd447"}
{"abstract": "Sentence compression is a method of text summarisation, where each sentence in a text is shortened in such a way as to retain the original information and grammatical correctness as much as possible. In a previous paper, we formulated the problem of sentence compression as an optimisation problem of extracting a subsequence of phrases from the original sentence that maximises the sum of topical importance and grammatical correctness. Based on this formulation an efficient sentence compression algorithm was derived. This paper reports a result of subjective evaluation for the quality of sentences compressed by using the algorithm.", "authors": ["Rei Oguro", "Hiromi Sekiya", "Yuhei Morooka", "Kazuyuki Takagi", "Kazuhiko Ozeki"], "n_citation": 50, "title": "Evaluation of a Japanese sentence compression method based on phrase significance and inter-phrase dependency", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fc657db5-1a9c-4239-a3c4-2ee654aab432"}
{"authors": ["Florian Mendel", "Tomislav Nad", "Martin Schl\u00e4ffer"], "n_citation": 50, "title": "Collision Attack on Boole", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "fc99e1ed-3ac5-470f-8dd0-1962134d8b0f"}
{"abstract": "Let g be an element of prime order p in an abelian group and a \u2208 Zp. We show that if g,g\u03b1, and g \u03b1d  are given for a positive divisor d of p - 1, we can compute the secret a in O(log p. (\u221ap/d + \u221ad)) group operations using O(max{\u221ap/d, \u221ad}) memory. If g \u03b1i  (i = 0,1, 2,..., d) are provided for a positive divisor d of p + 1, a can be computed in O(log p . (\u221ap/d + d)) group operations using O(max{\u221ap/d, \u221ad}) memory. This implies that the strong Diffie-Hellman problem and its related problems have computational complexity reduced by O(\u221a2) from that of the discrete logarithm problem for such primes. Further we apply this algorithm to the schemes based on the Diffie-Hellman problem on an abelian group of prime order p. As a result, we reduce the complexity of recovering the secret key from O(\u221ap) to O( \u221ap/d) for Boldyreva's blind signature and the original ElGamal scheme when p - 1 (resp. p + 1) has a divisor d < p 1/2  (resp. d < p 1/3 ) and d signature or decryption queries are allowed.", "authors": ["Jung Hee Cheon"], "n_citation": 0, "title": "Security analysis of the strong diffie-hellman problem", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fcdc4103-414e-4d94-adb1-df559d80ba28"}
{"authors": ["Jordan Demeulenare", "Renaud Hartert", "Christophe Lecoutre", "Guillaume Perrez", "Laurrent Perron", "Jean-Charles R\u00e9gin", "Pierre Schaus"], "n_citation": 0, "title": "Compact-Table: Efficiently Filtering Table Constraints with Reversible Sparse Bit-Sets", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "fd1a4753-60d0-49f1-bd35-70b25ab70430"}
{"abstract": "The present authors share the characteristic of a significant participation in R&D projects for designing and implementing interactive systems in the field of digital music: tools for computer-based assistance to browsing sound and music data, assisted analysis and commenting systems for musical works, or assisted interactive environments for the creation of interactive virtual works. In spite of the diversity of their experience and practice, the authors are aiming at renewing the theoretical framework of musicology, which is working in the background of their creation or engineering activities, since a massive digital inscription of music allows its objects to be handled by programs, or even to be transformed into programs executable by other programs. Such a framework is only a model, which functions only if it allows us to interpret artefacts and phenomena, producing at the same moment a consistent musicological language, a space for the categorisation and evaluation of realisations, as well as a number of objectives for technological deployment and theoretical evolution. But, within a research teams, we also require that these models could provide capitalisation structures for experience, know-how and acquired knowledge.", "authors": ["Francis Rousseaux", "Alain Bonardi"], "n_citation": 0, "title": "Knowledge discovery as applied to music: Will music web retrieval revolutionize musicology?", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "fd426003-2026-4419-b4f0-5906f99c9941"}
{"authors": ["Josep Balasch", "Sebastian Faust", "Benedikt Gierlichs", "Ingrid Verbauwhede"], "n_citation": 0, "title": "Theory and Practice of a Leakage Resilient Masking Scheme", "venue": "Lecture Notes in Computer Science", "year": 2012, "id": "fe35b30a-e2f5-4b06-b397-c5b0066dea2d"}
{"abstract": ", In this paper, a simple but efficient texture synthesis algorithm is presented. New image is synthesized by a patch-based approach. Motivated by energy equation, the method can manipulate the overlap region perfectly. After the most reasonable cut path through overlap regions is found, satisfying resultant images whose size specified by user can be produced. As a general method, our algorithm is also applied to image composition and texture transfer-rendering a target image with given source texture image. Experiments show that our algorithm is very efficient and easy to implement....", "authors": ["Shuchang Xu", "Xiuzi Ye", "Yin Zhang", "Sanyuan Zhang"], "n_citation": 0, "title": "Texture Synthesis Based on Minimum Energy Cut and Its Applications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "feb120f6-97c8-4d16-8f07-07f96d8ae3c9"}
{"abstract": "In the last decades, the Satisfiability and Constraint Satisfaction Problem frameworks were extended to integrate aspects such as uncertainties, partial observabilities, or uncontrollabilities. The resulting formalisms, including Quantified Boolean Formulas (QBF), Quantified CSP (QCSP), Stochastic SAT (SSAT), or Stochastic CSP (SCSP), still rely on networks of local functions defining specific graphical models, but they involve queries defined by sequences of distinct elimination operators (3 and V for QBF and QCSP, max and + for SSAT and SCSP) preventing variables from being considered in an arbitrary order when the problem is solved (be it by tree search or by variable elimination). In this paper, we show that it is possible to take advantage of the actual structure of such multi-operator queries to bring to light new ordering freedoms. This leads to an improved constrained induced-width and doing so to possible exponential gains in complexity. This analysis is performed in a generic semiring-based algebraic framework that makes it applicable to various formalisms. It is related with the quantifier tree approach recently proposed for QBF but it is much more general and gives theoretical bases to observed experimental gains.", "authors": ["C\u00e9dric Pralet", "Thomas Schiex", "G\u00e9rard Verfaillie"], "n_citation": 0, "title": "Decomposition of multi-operator queries on semiring-based graphical models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "feeedc1c-b30d-4b4c-9038-2ae0ebfea8de"}
{"abstract": "We analyze the distribution of computational effort required by backtracking algorithms on unsatisfiable CSPs, using analogies with reliability models, where lifetime of a specimen before failure corresponds to the runtime of backtracking on unsatisfiable CSPs. We extend the results of [7] by showing empirically that the lognormal distribution is a good approximation of the backtracking effort on unsolvable CSPs not only at the 50% satisfiable point, but in a relatively wide region. We also show how the law of proportionate effect [9] commonly used to derive the lognormal distribution can be applied to modeling the number of nodes expanded in a search tree. Moreover, for certain intervals of C/N, where N is the number of variables, and C is the number of constraints, the parameters of the corresponding lognormal distribution can be approximated by the linear lognormal model [11] where mean log(deadends) is linear in C/N, and variance of log(deadends) is close to constant. The linear lognormal model allows us to extrapolate the results from a relatively easy overconstrained region to the hard critically constrained region and, in particular, to use more efficient strategies for testing backtracking algorithms.", "authors": ["Irina Rish", "Daniel Frost"], "n_citation": 0, "title": "Statistical analysis of backtracking on inconsistent CSPs", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "ff29d198-43eb-4a73-bdf2-86747083ffde"}
{"abstract": "This paper presents the open source reference implementation of RuleML based on modular XML Schema definitions and bidirectional OO jDREW interpreters written in Java. For the family of RuleML sublanguages, schema modularization and RDF rules are discussed. The central bidirectional interpreters are introduced via jDREW principles, and explained w.r.t. OO jDREW slots, types, OIDs, and extensions.", "authors": ["Marcel Ball", "Harold Boley", "David Hirtle", "Jing Mei", "Bruce Spencer"], "n_citation": 0, "title": "The OO jDREW reference implementation of RuleML", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ff3ffa2d-c92c-4c39-97bd-3a5d8b9fa788"}
{"abstract": "Querying XML has been the subject of much recent investigation. A formal bulk algebra is essential for applying database-style optimization to XML queries. We develop such an algebra, called TAX (Tree Algebra for XML), for manipulating XML data, modeled as forests of labeled ordered trees. Motivated both by aesthetic considerations of intuitiveness, and by efficient computability and amenability to optimization, we develop TAX as a natural extension of relational algebra, with a small set of operators. TAX is complete for relational algebra extended with aggregation, and can express most queries expressible in popular XML query languages. It forms the basis for the TIMBER XML database system currently under development by us.", "authors": ["H. V. Jagadish", "Laks V. S. Lakshmanan", "Divesh Srivastava", "Keith Thompson"], "n_citation": 0, "title": "TAX: A tree Algebra for XML", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ff7ce51a-6acf-4890-889d-3f44c0aa16de"}
{"abstract": "A method to embed P multi-valued patterns x s  \u2208 R N  into a memory of a recurrent neural network is introduced. The method represents memory as a nonlinear line of attraction as opposed to the conventional model that stores memory in attractive fixed points at discrete locations in the state space. The activation function of the network is defined by the statistical characteristics of the training data. The stability of the proposed nonlinear line attractor network is investigated by mathematical analysis and extensive computer simulation. The performance of the network is benchmarked by reconstructing noisy gray-scale images.", "authors": ["Ming-Jung Seow", "Vijayan K. Asari"], "n_citation": 50, "title": "Associative memory using nonlinear line attractor network for multi-valued pattern association", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ff92e883-b013-46c1-99a4-34927b7f2451"}
{"abstract": "Membership queries are basic predicate operations that apply to datasets. Quantifications of such queries express global properties between datasets, including subset inclusion and disjointness. These operations are basic tools in set-theoretic data-mining procedures such as frequent-itemset-mining. In this work we formalize a family of such queries syntactically and we consider how they can be evaluated in a privacy-preserving fashion. We present a syntax-driven compiler that produces a protocol for each query and we show that semantically such queries correspond to basic set operation predicates between datasets. Using our compiler and based on the fact that it is syntax-driven, two parties can generate various privacy-preserving protocols with different complexity behavior that allow them to efficiently and securely evaluate the predicate of interest without sharing information about the datasets they possess. Our compiler sheds new light on the complexity of privacy-preserving evaluation of predicates such as disjointness and subset-inclusion and achieves substantial complexity improvements compared to previous works in terms of round as well as communication complexity. In particular, among others, we present protocols for both predicates that require one-round of interaction and have communication less than the size of the universe, while previously the only one round protocols known had communication proportional to the size of the universe.", "authors": ["Aggelos Kiayias", "Antonina Mitrofanova"], "n_citation": 0, "title": "Syntax-driven private evaluation of quantified membership queries", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ffceec16-a586-4545-aa9d-fd09088e9a08"}
{"abstract": "We demonstrate a transformation of Yao's protocol for secure two-party computation to a fair protocol in which neither party gains any substantial advantage by terminating the protocol prematurely. The transformation adds additional steps before and after the execution of the original protocol, but does not change it otherwise, and does not use a trusted third party. It is based on the use of gradual release timed commitments, which are a new variant of timed commitments, and on a novel use of blind signatures for verifying that the committed values are correct.", "authors": ["Benny Pinkas"], "n_citation": 0, "title": "Fair secure two-party computation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "fff276af-3cac-478a-9d01-fb5e382d1bb7"}
{"abstract": "In this paper, a clustering algorithm based on Gradient Based Fuzzy C-Means with a Mercer Kernel, called GBFCM (MK), is proposed. The kernel method adopted in this paper implicitly performs nonlinear mapping of the input data into a high-dimensional feature space. The proposed GBFCM(MK) algorithm is capable of dealing with nonlinear separation boundaries among clusters. Experiments on a synthetic data set and several real MPEG data sets show that the proposed algorithm gives better classification accuracies than both the conventional k-means algorithm and the GBFCM.", "authors": ["Dong-Chul Park", "Chung Nguyen Tran", "Sancho Park"], "n_citation": 0, "title": "Gradient Based Fuzzy C-Means Algorithm with a Mercer Kernel", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "fff5d1b3-2a5d-4897-8410-648fc4c87fe4"}
{"abstract": "Finding appropriate values for the parameters of an algorithm is a challenging, important, and time consuming task. While typically parameters are tuned by hand, recent studies have shown that automatic tuning procedures can effectively handle this task and often find better parameter settings. F-Race has been proposed specifically for this purpose and it has proven to be very effective in a number of cases. F-Race is a racing algorithm that starts by considering a number of candidate parameter settings and eliminates inferior ones as soon as enough statistical evidence arises against them. In this paper, we propose two modifications to the usual way of applying F-Race that on the one hand, make it suitable for tuning tasks with a very large number of initial candidate parameter settings and, on the other hand, allow a significant reduction of the number of function evaluations without any major loss in solution quality. We evaluate the proposed modifications on a number of stochastic local search algorithms and we show their effectiveness. \u00a9 Springer-Verlag Berlin Heidelberg 200", "authors": ["Prasanna Balaprakash", "Mauro Birattari", "Thomas St\u00fctzle"], "n_citation": 0, "title": "Improvement strategies for the F-Race algorithm: sampling design and iterative refinement", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "1889bae3-0b64-4904-997e-71dcbd59824a"}
{"abstract": "Stochastic Diffusion Search is an efficient probabilistic best-fit search technique, capable of transformation invariant pattern matching. Although inherently parallel in operation it is difficult to implement efficiently in hardware as it requires full inter-agent connectivity. This paper describes a lattice implementation, which, while qualitatively retaining the properties of the original algorithm, restricts connectivity, enabling simpler implementation on parallel hardware. Diffusion times are examined for different network topologies, ranging from ordered lattices, over small-world networks to random graphs.", "authors": ["Kris De Meyer", "J. Mark Bishop", "Slawomir J. Nasuto"], "n_citation": 0, "title": "Small-world effects in lattice Stochastic Diffusion Search", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "194b2bcc-370f-495e-a504-4562f8ef4322"}
{"abstract": "Under the assumption that solving the discrete logarithm problem modulo an n-bit prime p is hard even when the exponent is a small c-bit number, we construct a new and improved pseudo-random bit generator. This new generator outputs n - c - 1 bits per exponentiation with a c-bit exponent. Using typical parameters, n = 1024 and c = 160, this yields roughly 860 pseudo-random bits per small exponentiations. Using an implementation with quite small precomputation tables, this yields a rate of more than 20 bits per modular multiplication, thus much faster than the the squaring (BBS) generator with similar parameters.", "authors": ["Rosario Gennaro"], "n_citation": 0, "title": "An improved pseudo-random generator based on discrete log", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "1992203d-b0f9-4d6c-863d-1951f7f80a91"}
{"abstract": "We analyze extensively the temporal properties of the train of spikes emitted by a simple model neuron as a function of the statistics of the synaptic input. In particular we focus on the asynchronous case, in which the synaptic inputs are random and uncorrelated. We show that the NMDA component acts as a non-stationary input that varies on longer time scales than the inter-spike intervals. In the sub-threshold regime, this can increase dramatically the coefficient of variability (bringing it beyond one). The analysis provides also simple guidelines for searching parameters that maximize irregularity.", "authors": ["Giancarlo La Camera", "Stefano Fusi", "Walter Senn", "Alexander Rauch", "Hans-R. L\u00fcscher"], "n_citation": 50, "title": "When NMDA receptor conductances increase inter-spike interval variability", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "19cd8716-09ae-490c-9fe6-1c9cddf1c66c"}
{"abstract": "This paper presents the application of program transformation to the development of marshaling code. Marshaling code amounts to about half of the signaling software in a subscriber radio, and as such constitutes about 20% of the total software in the radio. Development of marshaling code is considered to be a difficult, error prone, and laborious task. We have successfully developed significant portions of Motorola TETRA (Trans-European Trunked Radio) infrastructure and subscriber software through automated code generation techniques using the process outlined in this paper. This process, and the tools described in this paper, have subsequently been applied to the development of a number of communication applications and delivered tremendous cycle time and quality improvements.", "authors": ["Thomas Weigert", "Paul F. Dietz"], "n_citation": 0, "title": "Automated generation of marshaling code from high-level specifications", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1a5acd33-767a-462c-9945-b2608be22d9d"}
{"abstract": "Internet becomes more and more popular, and most companies and institutes use web services for e-business and many other purposes. As results. Internet and web services become core infrastructure for a company or an institute. With the explosion of Internet, the occurrence of cyber terrorism has grown very rapidly. It is difficult to find and close all security flaws in a computer system that is connected to a network. Internet worms take advantages of these security flaws, and attack a large number of hosts with self-propagating techniques. To study and analyze internet worms, one of the most feasible ways is to use simulations. It is quite challenging to simulate very large-scale worm attacks. In this paper, we propose a hybrid model for large-scale worm propagation simulations.", "authors": ["Eul Gyu Im", "Jung Taek Seo", "Dong-Soo Kim", "Yong Ho Song", "Yongsu Park"], "n_citation": 50, "title": "Hybrid modeling for large-scale worm propagation simulations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1a951677-ba53-4f6d-841e-e2b770c1f363"}
{"abstract": "Directed line segments are fundamental geometric elements used to model through their spatial relations such concepts as divergence, confluence, and interference. A new model is developed that captures spatial relations between pairs of directed line segments through the intersections of the segments' heads, bodies, and tails. This head-body-tail intersection identifies 68 classes of topological relations between two directed line segments highlighting two equal-sized subsets of corresponding relations that differ only by their empty and non-empty body-body intersections. The relations' conceptual neighborhood graph takes the shape of a torus inside a torus, one for each subset. Another 12 classes of topological relation classes are distinguished if the segments' exteriors are considered as well, lining up such that their conceptual neighborhood graph forms another torus that contains the other two tori. These conceptual neighborhoods as well as the relations' composition table enable spatial inferences and similarity assessments in a consistent and reasoned manner.", "authors": ["Yohei Kurata", "Max J. Egenhofer"], "n_citation": 0, "title": "The Head-Body-Tail Intersection for Spatial Relations Between Directed Line Segments", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1ab5237b-49eb-477f-a14c-723e30cd8879"}
{"abstract": "We present an approach to approximate reasoning by agents in distributed environments based on calculi of information granules. Approximate reasoning schemes are basic schemes of information granule construction. An important property of such schemes is their robustness with respect to input deviations. In distributed environments, such schemes are extended to rough neural networks that transform information granules into information granules rather than vectors of real numbers into (vectors of) real numbers. Problems of learning in rough neural networks from experimental data and background knowledge are outlined.", "authors": ["Andrzej Skowron"], "n_citation": 0, "title": "Approximate reasoning by agents", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1c008278-086d-4f26-8383-66319c335ca2"}
{"abstract": "Medial Axis (MA), also known as Centres of Maximal Disks, is a useful representation of a shape for image description and analysis. MA can be computed on a distance transform, where each point is labelled to its distance to the background. Recent algorithms allow to compute Squared Euclidean Distance Transform (SEDT) in linear time in any dimension. While these algorithms provide exact measures, the only known method to characterize MA on SEDT, using local tests and Look-Up Tables, is limited to 2D and small distance values [5]. We have proposed in [14] an algorithm which computes the look-up table and the neighbourhood to be tested in the case of chamfer distances. In this paper, we adapt our algorithm for SEDT in arbitrary dimension and show that results have completely different properties.", "authors": ["Eric Remy", "Edouard Thiel"], "n_citation": 0, "title": "Look-up tables for medial Axis on Squared Euclidean Distance Transform", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "1c219ddf-2db4-4fe3-a8ec-4c262dd741fb"}
{"authors": ["N. N. Nefedov"], "n_citation": 0, "title": "Towards Distributed Communications Systems: Relay-Based Wireless Networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1c4b22dd-7c40-4a45-bb72-1d4a7ab95d89"}
{"abstract": "Security administration is an uphill task to implement in an enterprise network providing secured corporate services. With the slew of patches being released by Microsoft, HP and other vendors, system administrators require a barrage of tools for analyzing the risk due to these vulnerabilities. In addition to this, criticalities in patching some end hosts (eg., in hospitals) raises serious security issues about the network to which the end hosts are connected. In this context, it would be imperative to know the risk level of all critical resources (e.g., Oracle Server in HR department) keeping in view the everyday emerging new vulnerabilities. We hypothesize that sequence of network actions by an attacker depends on the social behavior (e.g., skill level, tenacity, financial ability). We extended this and formulated a mechanism to estimate the risk level of critical resources that may be compromised based on attacker behavior. This estimation is accomplished using behavior based attack graphs. These graphs represent all the possible attack paths to all the critical resources. Based on these graphs, we calculate the risk level of a critical resource using Bayesian methodology and periodically update the subjective beliefs about the occurrence of an attack. Such a calculated risk level would be a measure of the vulnerability of the resource and it forms an effective basis for a system administrator to perform suitable changes to network configuration. Thus suitable vulnerability analysis and risk management strategies can be formulated to efficiently curtail the risk from different types of attackers (script kiddies, hackers, criminals and insiders).", "authors": ["Ram Dantu", "Prakash Kolan"], "n_citation": 50, "title": "Risk management using behavior based bayesian networks", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "1c4e3529-2197-40c4-838e-b69c9fa42e76"}
{"abstract": "Long running applications often need to adapt due to changing requirements or changing environment. Typically, such adaptation is performed by dynamically adding or removing components. In these types of adaptation, components are often added to or removed from multiple processes in the system. While techniques for such adaptations have been extensively discussed in the literature, there is a lack of systematic methods to ensure the correctness of dynamic adaptation. To redress this deficiency, in this paper, we propose a new method, based on the concept of proof lattice, for verifying correctness of dynamic adaptation in a distributed application. We use transitional-invariant lattice to verify correctness of adaptation. As an illustration of this method, we show how correctness of dynamic adaptation is obtained in the context of a message communication application.", "authors": ["Sandeep S. Kulkarni", "Karun N. Biyani"], "n_citation": 0, "title": "Correctness of component-based adaptation", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "1c82d6a9-4e8b-4199-9dad-37f9ac39dc48"}
{"abstract": "Side channel cryptanalysis techniques such as the analysis of instantaneous power consumption, have been extremely effective in attacking implementations on simple hardware platforms. There are several proposed solutions to resist these attacks, most of which are ad-hoc and can easily be rendered ineffective. A scientific approach is to create a model for the physical characteristics of the device, and then design implementations provably secure in that model, i.e, they resist generic attacks with an a priori bound on the number of experiments. We propose an abstract model which approximates power consumption in most devices and in particular small single-chip devices. Using this, we propose a generic technique to create provably resistant implementations for devices where the power model has reasonable properties, and a source of randomness exists. We prove a lower bound on the number of experiments required to mount statistical attacks on devices whose physical characteristics satisfy reasonable properties.", "authors": ["Suresh Chari", "Charanjit S. Jutla", "Josyula R. Rao", "Pankaj Rohatgi"], "n_citation": 763, "title": "Towards sound approaches to counteract power-analysis attacks", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "1d3e6e35-7838-46d1-87ce-5459e2edd319"}
{"abstract": "Today the Intelligence Community (IC) has faced increasing challenges of insider threats. It is generally accepted that the cost of insider threats exceeds that of outsider threats. Although the currently available access control approaches have a great potential for preventing insider threats, there are still critical obstacles to be solved, especially in large-scale computing environments. In this paper we discuss those requirements with respect to scalability, granularity, and context-awareness. For each requirement we discussed related problems, techniques, and basic approaches to the corresponding countermeasures. Detailed solutions and implementations are not described in this paper.", "authors": ["Joon S. Park", "Joseph Giordano"], "n_citation": 0, "title": "Access control requirements for preventing insider threats", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1d8ec525-fbb0-4691-9205-a476d444071f"}
{"abstract": "This paper reports on two case-studies of applying BAN logic to industrial strength security protocols. These studies demonstrate the flexibility of the BAN language, as it caters for the addition of appropriate constructs and rules. We argue that, although a semantical foundation of the formalism is lacking, BAN logic provides an intuitive and powerful technique for security analysis.", "authors": ["Nesria Agray", "Wiebe van der Hoek", "Erik P. de Vink"], "n_citation": 50, "title": "On BAN logics for industrial security protocols", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1d917e0c-f2bf-4469-b04f-d629abca62b4"}
{"abstract": "This paper shows that using direct properties of a zero-knowledge protocol itself, one may impose a honest behavior on the verifier (without additional cryptographic tools). The main technical contribution is showing that if a language L has an Arthur-Merlin (i.e. public coins) honest-verifier statistical SZK proof system then L has an (any-verifier) SZK proof system when we use a non-uniform simulation model of SZK (where the simulation view and protocol view can be made statistically closer than any given polynomial given as a parameter). Three basic questions regarding statistical zero-knowledge (SZK) are solved in this model: - If L has a honest-verifier SZK proof then L has an any-verifier non-uniform simulation SZK proof. - If L has an SZK proof then L has an non-uniform simulation SZK proof. - If L has a private-coin SZK proof then L has a public-coin non-uniform simulation SZK proof.", "authors": ["G. Di Crescenzo", "T. Okamoto", "Moti Yung"], "n_citation": 50, "title": "Keeping the SZK-verifier honest unconditionally", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "1e346a22-0f75-4713-9c68-5c8dd9cacdd2"}
{"abstract": "A model checking algorithm for proving correctness of causal knowledge protocols for multi-agent systems is given. Protocols are specified in an extension of the temporal logic of causal knowledge [18]. The temporal language is interpreted over labelled prime event structures. The epistemic operators correspond to knowledge and goals, whereas the temporal modalities correspond to the immediate causality and causality. The model checking algorithm is translated to the model checking problem for LTL or ACTL. This enables a further translation to the SAT-problem, using the technique of the bounded model checking.", "authors": ["Wojciech Penczek"], "n_citation": 0, "title": "Efficient model checking of causal-knowledge protocols", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "1e7b1ad3-d2b3-44b0-9299-f67818d0aed3"}
{"abstract": "Effective usage of multimedia digital libraries has to deal with the problem of building efficient content annotation and retrieval tools. In particular in video domain, different techniques for manual and automatic annotation and retrieval have been proposed. Despite the existence of well-defined and extensive standards for video content description, such as MPEG-7, these languages are not explicitly designed for automatic annotation and retrieval purpose. Usage of linguistic ontologies for video annotation and retrieval is a common practice to classify video elements by establishing relationships between video contents and linguistic terms that specify domain concepts at different abstraction levels. The main issue related to the use of description languages such as MPEG-7 or linguistic ontologies is due to the fact that linguistic terms are appropriate to distinguish event and object categories but they are inadequate when they must describe specific or complex patterns of events or video entities. In this paper we propose the usage of knowledge representation languages to define ontologies enriched with visual information that can be used effectively for video annotation and retrieval. Difference between content description languages and knowledge representation languages are shown, the advantages of using enriched ontologies both for the annotation and the retrieval process are presented in terms of enhanced user experience in browsing and querying video digital libraries.", "authors": ["Marco Bertini", "Gianpaolo D'Amico", "A. Del Bimbo", "Carlo Torniai"], "n_citation": 0, "title": "Using knowledge representation languages for video annotation and retrieval", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "1fc7c060-6727-49e9-935b-1c0eba2c5ea4"}
{"abstract": "Text authored by an unidentified assailant can offer valuable clues to the assailant's identity. In this paper, we show that stylistic text features can be exploited to determine an anonymous author's native language with high accuracy.", "authors": ["Moshe Koppel", "Jonathan Schler", "Kfir Zigdon"], "n_citation": 0, "title": "Automatically determining an anonymous Author's native language", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "201f0fcf-1218-45ae-a58e-144f69a5b7ed"}
{"abstract": "To balance demand and supply of information, we propose a framework called information supply chain (ISC). This framework is based on supply chain management (SCM), which has been used in business management science. Both ISC and SCM aim to satisfy demand with high responsiveness and efficiency. ISC uses an information requirement planning (IRP) algorithm to reason, plan, and satisfy needers with useful information. We believe that ISC can not only unify existing information-sharing methods, but also produce new solutions that enable the right information to be delivered to the right recipients in the right way and at the right time.", "authors": ["Shuang Sun", "John Yen"], "n_citation": 0, "title": "Information supply chain : A unified framework for information-sharing", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "202aefc5-6708-4a1e-8009-d577923590ef"}
{"abstract": "Metamodeling became in the last decade a widely accepted tool to describe the (abstract) syntax of modeling languages in a concise, but yet precise way. For the description of the language's semantics, the situation is less satisfactory and formal semantics definitions are still seen as a challenge. In this paper, we propose an approach to specify the semantics of modeling languages in a graphical way. As an example, we describe the evaluation semantics of OCL by transformation rules written in the graphical formalism QVT. We believe that the graphical format of our OCL semantics has natural advantages with respect to understandability compared to existing formalizations of OCL's semantics. Our semantics can also be seen as a reference implementation of an OCL evaluator, because the transformation rules can be executed by any QVT compliant transformation engine.", "authors": ["Slavi\u0161a Markovi\u0107", "Thomas Baar"], "n_citation": 0, "title": "An OCL semantics specified with QVT", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "207704e2-dc55-4d77-aa38-218298fea188"}
{"abstract": "This paper presents FRACTAL, a hierarchical and reflective component model with sharing. Components in this model can be endowed with arbitrary reflective capabilities, from black-boxes to components that allow a fine-grained manipulation of their internal structure. The paper describes JULIA, a Java implementation of the model, a small but efficient run-time framework, which relies on a combination of interceptors and mixins for the programming of reflective features of components. The paper presents a qualitative and quantitative evaluation of this implementation, showing that component-based programming in FRACTAL can be made very efficient.", "authors": ["Eric Bruneton", "Thierry Coupaye", "Matthieu Leclercq", "Vivien Qu\u00e9ma", "Jean-Bernard Stefani"], "n_citation": 0, "title": "An open component model and its support in Java", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "20974d47-315e-44b5-8bca-accd93bfa720"}
{"abstract": "Mesh-based terrain representations provide accurate descriptions of a terrain, but fail in capturing its morphological structure. The morphology of a terrain is defined by its critical points and by the critical lines joining them, which form a so-called surface network. Because of the large size of current terrain data sets, a multi-resolution representation of the terrain morphology is crucial. Here, we address the problem of representing the morphology of a terrain at different resolutions. The basis of the multi-resolution terrain model, that we call a Multi-resolution Surface Network (MSN), is a generalization operator on a surface network, which produces a simplified representation incrementally. An MSN is combined with a multi-resolution mesh-based terrain model, which encompasses the terrain morphology at different resolutions. We show how variable-resolution representations can be extracted from an MSN, and we present also an implementation of an MSN in a compact encoding data structure.", "authors": ["Emanuele Danovaro", "Leila De Floriani", "Laura Papaleo", "Maria Vitali"], "n_citation": 0, "title": "A Multi-resolution Representation for Terrain Morphology", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "211e99f5-1ed3-4e0a-a58d-38b74210d46e"}
{"abstract": "We consider the broadcast exclusion problem: how to transmit a message over a broadcast channel shared by N = 2 n  users so that all but some specified coalition of k excluded users can understand the contents of the message. Using error-correcting codes, and avoiding any computational assumptions in our constructions, we construct natural schemes that completely avoid any dependence on n in the transmission overhead. Specifically, we construct: (i) (for illustrative purposes,) a randomized scheme where the server's storage is exponential (in n), but the transmission overhead is O(k), and each user's storage is O(kn); (ii) a scheme based on polynomials here the transmission overhead is O(kn) and each user's storage is O(kn); and (iii) a scheme using algebraic-geometric codes where the transmission overhead is O(k 2 ) and each user is required to store O(kn) keys. In the process of proving these results, we show how to construct very good cover-free set systems and combinatorial designs based on algebraic-geometric codes, which may be of independent interest and application. Our approach also naturally extends to solve the problem in the case where the broadcast channel may introduce errors or lose information.", "authors": ["Ravi Kumar", "Sridhar Rajagopalan", "Amit Sahai"], "n_citation": 218, "title": "Coding constructions for blacklisting problems without computational assumptions", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "21efa435-e037-4d38-9e95-f4b8799ea787"}
{"abstract": "The study of minimal cryptographic primitives needed to implement secure computation among two or more players is a fundamental question in cryptography. The issue of complete primitives for the case of two players has been thoroughly studied. However, in the multi-party setting, when there are n > 2 players and t of them are corrupted, the question of what are the simplest complete primitives remained open for t > n/3. We consider this question, and introduce complete primitives of minimal cardinality for secure multi-party computation. The cardinality issue (number of players accessing the primitive) is essential in settings where the primitives are implemented by some other means, and the simpler the primitive the easier it is to realize it. We show that our primitives are complete and of minimal cardinality possible.", "authors": ["Matthias Fitzi", "Juan A. Garay", "Ueli Maurer", "Rafail Ostrovsky"], "n_citation": 50, "title": "Minimal complete primitives for secure multi-party computation", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "228914bd-0bd7-4044-9bc3-1e858c045d33"}
{"abstract": "The nonlinear feedforward generator is one of the commonly used building blocks of stream ciphers. This paper describes a novel known-plaintext attack for cryptanalyzing nonlinear feedforward generator. The plaintext requirement of the attack is only twice the length of the shift register. The implementation of this attack could identify the initial settings of the system for a 128 stage register and randomly chosen nonlinear feedforward function of 10 variables in few minutes on a P-II 300 MHz machine.", "authors": ["S. S. Bedi", "N. Rajesh Pillai"], "n_citation": 0, "title": "Cryptanalysis of the nonlinear feedforward generator", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "239bd8bb-f67d-44d1-89fd-19adcfdb3029"}
{"abstract": "In this paper we investigate the 'local' definitions of length of digital curves in the digital space rZ 2  where r is the resolution of the discrete space. We prove that if \u03bc r  is any local definition of the length of digital curves in rZ 2 , then for almost all segments S of R 2 , the measure \u03bc r (S r ) does not converge to the length of S when the resolution r converges to 0, where S r  is the Bresenham discretization of the segment S in rZ 2 . Moreover, the average errors of classical local definitions are estimated, and we define a new one which minimizes this error.", "authors": ["Mohamed Tajine", "Alain Daurat"], "n_citation": 0, "title": "On local definitions of length of digital curves", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "24d3ab8c-3b61-4384-a3d0-e8840b5d1e67"}
{"abstract": "In this paper we report on a case study of correct automatic assembly of software components. We show the application of our tool (called Synthesis) for correct components assembly to a software system in the area of CSCW (Computer Supported Cooperative Work). More specifically we consider a product data management (PDM) cooperative system which has been developed by the company Think3 in Bologna, ITALY (www.think3.com). In the area of CSCW, the automatic enforcing of desired interactions among the components forming the system requires the ability to properly manage the dynamic interactions of the components. Moreover once a customer acquires a CSCW system, the vendor of the CSCW system has to spend many further resources in order to integrate the CSCW system with the client applications used by the customer organization. Thus the full automation of the phase of integration code development has a great influence for a good setting of a CSCW system on the market. We present the application of our approach and we describe our experience in automatic derivation of the code which integrates the components forming the PDM cooperative system above mentioned. The case study we treat in this paper represent the first attempt to, successfully, apply Synthesis in real-scale contexts.", "authors": ["Massimo Tivoli", "Paola Inverardi", "Valentina Presutti", "Alessandro Forghieri", "Maurizio Sebastianis"], "n_citation": 0, "title": "Correct components assembly for a product data management Cooperative system", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "24e0f98a-bb08-47e0-9a40-b608d136c643"}
{"abstract": "This paper proposes a method of ranking XML documents with respect to an Information Retrieval query by means of fuzzy logic. The proposed method allows imprecise queries to be evaluated against an XML document collection and it provides a model of ranking XML documents. In addition the proposed method enables sophisticated ranking of documents by employing proximity measures and the concept of editing (Levenshtein) distance between terms or XML paths.", "authors": ["Evangelos Kotsakis"], "n_citation": 0, "title": "XML fuzzy ranking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2621e336-13a2-4e8a-9565-a58e5bfb74a1"}
{"abstract": "Substantial efforts have been spent on characterizing the round complexity of various cryptographic tasks. In this work we study the round complexity of secure multiparty computation in the presence of an active (Byzantine) adversary, assuming the availability of secure point-to-point channels and a broadcast primitive. It was recently shown that in this setting three rounds are sufficient for arbitrary secure computation tasks, with a linear security threshold, and two rounds are sufficient for certain nontrivial tasks. This leaves open the question whether every function can be securely computed in two rounds. We show that the answer to this question is no: even some very simple functions do not admit secure 2-round protocols (independently of their communication and time complexity) and thus 3 is the exact round complexity of general secure multiparty computation. Yet, we also present some positive results by identifying a useful class of functions which can be securely computed in two rounds. Our results apply both to the information-theoretic and to the computational notions of security.", "authors": ["Rosario Gennaro", "Yuval Ishai", "Eyal Kushilevitz", "Tal Rabin"], "n_citation": 0, "title": "On 2-round secure multiparty computation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2748c338-a9b6-4a89-98d2-66f8f4a767b5"}
{"abstract": "One approach to secure mobile agent execution is restricting the agent route to trusted environments. A necessary condition for this approach to be practical is that the agent route be protected. Previous proposals for agent route protection either offer low security or suffer from high computational costs due to cryptographic operations. We present two fast, hash-based mechanisms for agent route protection. The first solution relies on hash collisions and focuses on minimizing the computational cost of route verification by hosts along the route; the cost is shifted to the stage of route protection by the agent owner. The second solution uses Merkle trees and minimizes the cost of route protection by the agent owner, so that a single digital signature suffices to protect the whole route; for hosts along the route, the verification cost is similar to the cost of previous schemes in the literature, namely one digital signature verification per route step. The first solution is especially suitable for agent routes which go through heavily loaded hosts (to avoid denial of service or long delay). The second solution is more adapted to mitigating the bottleneck at agent owners who are expected to launch a great deal of agents. Both solutions provide independent protection for each route step and can be extended to handle flexible itineraries.", "authors": ["Josep Domingo-Ferrer"], "n_citation": 0, "title": "Mobile agent route protection through hash-based mechanisms", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "27807bc9-ef0c-48b9-b412-f706cc2225cb"}
{"abstract": "Intelligent information retrieval tools can help intelligence and security agencies to retrieve and exploit relevant information from unstructured information sources and give them insight into the criminal behavior and networks, in order to fight crime more efficiently and effectively. This article aims at analysing off-the-shelf information extraction tools on their applicability and competency for such applications.", "authors": ["Nishant Kumar", "Jan De Beer", "Jan Vanthienen", "Marie-Francine Moens"], "n_citation": 0, "title": "Intelligent information retrieval tools for police", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "27ba1ee5-c02f-4855-beb7-98fda6e037d4"}
{"abstract": "A robust image watermarking technique is proposed in this paper. The watermarked image is obtained by modifying the maximum singular value in each image block. The robustness of the proposed algorithm is achieved from two aspects: the stability of the maximum singular values and preprocessing before watermark extraction. Zemike moments are used to estimate the rotation angle, and the translation and scaling distortions are corrected by geometric moment methods. Experimental results show that this algorithm makes a trade-off among the imperceptibility, robustness and capacity.", "authors": ["Haifeng Li", "Shuxun Wang", "Weiwei Song", "Quan Wen"], "n_citation": 50, "title": "A novel watermarking algorithm based on SVD and zernike moments", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "27f7bf93-4af3-4d26-9e8a-6dbfa6380d3f"}
{"abstract": "We present a new general multiparty computation protocol for the cryptographic scenario which is universally composable - in particular, it is secure against an active and adaptive adversary, corrupting any minority of the parties. The protocol is as efficient as the best known statically secure solutions, in particular the number of bits broadcast (which dominates the complexity) is \u03a9(nk|C|), where n is the number of parties, k is a security parameter, and |C| is the size of a circuit doing the desired computation. Unlike previous adaptively secure protocols for the cryptographic model, our protocol does not use non-committing encryption, instead it is based on homomorphic threshold encryption, in particular the Paillier cryptosystem.", "authors": ["Ivan Damg\u00e5rd", "Jesper Buus Nielsen"], "n_citation": 0, "title": "Universally composable efficient multiparty computation from threshold homomorphic encryption", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "285e5d49-fcbb-4962-b125-7906588636ad"}
{"abstract": "In 1999, Poupard and Stern proposed on the fly signature scheme (PS-scheme), which aims at minimizing the on-line computational work for a signer. In this paper, we propose more efficient on the fly signature schemes by improving the PS-scheme. In PS-scheme, the size of secret-key is fixed by modulus n, so that this feature leads to some drawbacks in terms of both the computational work and the communication load. The main idea of our schemes is to reduce the size of secret-key in PS-scheme by using a public element g which has a specific structure. Consequently, our schemes are improved with respect to the computational work (which means the computational cost for precomputation, (on-line) signature generation and verification) and the data size such as a secret-key and a signature.", "authors": ["Takeshi Okamoto", "Mitsuru Tada", "Atsuko Miyaji"], "n_citation": 0, "title": "Efficient \"on the fly\" signature schemes based on integer factoring", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "28d562b6-89b6-4c88-b9b7-ea12b4598cba"}
{"abstract": "When designing applications with Enterprise JavaBeans (EJBs) and more specifically with Stateful Session Beans, a major difficulty (or even an impossibility) is being able to properly transform business models and more precisely UML 2 models, into such component types, while including the expression of their mutual compositions. This contradicts with the spirit of the emerging Model-Driven Architecture (MDA) software engineering paradigm based on the definition of seamless model transformations. In this scope, this paper proposes and describes an appropriate Java library in order to increase the composition power of EJBs. The proposition includes a support for a broadcast communication mode (assimilated to horizontal composition in the paper) which is, a priori, incompatible with non reentrance, a key characteristic of EJBs. Besides, vertical composition is the counterpart of horizontal composition. Vertical composition enables the consistent hierarchical combination of composite behaviors and compound behaviors, both being specified and implemented by means of UML 2 State Machine Diagrams.", "authors": ["Franck Barbier"], "n_citation": 0, "title": "An Enhanced Composition Model for Conversational Enterprise JavaBeans", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "28fe0e6e-1f90-4985-a3ae-644f15972ba7"}
{"abstract": "Consider a complex, convoluted three dimensional object that has been digitized and is available as a set of voxels. We describe a fast, practical scheme for delineating a region of interest on the surface of the object and estimating its original area. The voxel representation is maintained and no triangulation is carried out. The methods presented rely on a theoretical result of Mullikin and Verbeek, and bridge the gap between their idealized setting and the harsh reality of 3D medical data. Performance evaluation results are provided, and operation on segmented white matter MR brain data is demonstrated.", "authors": ["Guy Windreich", "Nahum Kiryati", "Gabriele Lohmann"], "n_citation": 0, "title": "Surface area estimation in practice", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "29d3a8f0-a8c7-4ef3-873d-ce0ba23bde24"}
{"abstract": "The mobile Internet will be a dominating access information technology in the near future. This paper presents basic aspects of mobile and pervasive computing with particular attention to SRW. It focuses on evaluation of existing and forthcoming wireless technology, middleware and service provision architectures for mobile applications and illustration of the existing possibilities by practical examples.", "authors": ["Krzysztof Zielinski"], "n_citation": 0, "title": "Network services in context of pervasive mobile Internet", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2b96ad36-a5ee-45e5-a364-8c430fd89a7c"}
{"abstract": "This paper introduces a topological map dedicated to an automatic classification categorical data. Usually, topological maps uses a numerical (or binary) coding of the categorical data during the learning process. In the present paper, we propose a probabilistic formalism where the neurons now represent probability tables. Two examples using actual and synthetic data allow to validate the approach. The results show the good quality of the topological order obtained as well as its performances in classification.", "authors": ["Mustapha Lebbah", "C. Chabanon", "Fouad Badran", "Sylvie Thiria"], "n_citation": 0, "title": "Categorical topological map", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "2bcfb176-0b4c-4aea-835d-5dff7f5dfd93"}
{"abstract": "From several decades, non-adjacent form (NAF) representations for integers have been extensively studied as an alternative to the usual binary number system where digits are in {0, 1}. In cryptography, the non-adjacent digit set (NADS) {-1, 0, 1} is used for optimization of arithmetic operations in elliptic curves. At SAC 2003, Muir and Stinson published new results on alternative digit sets: they proposed infinite families of integers x such that {0, 1, x} is a NADS as well as infinite families of integers x such that {0,1, x} is not a NADS, so called a NON-NADS. Muir and Stinson also provided an algorithm that determines whether x leads to a NADS by checking if every integer n E [0, [-x/3]] has a {0, 1, x}-NAF. In this paper, we extend these results by providing generators of NON-NADS infinite families. Furthermore, we reduce the search bound from [-x/3] to [-x/12]. We introduce the notion of worst NON-NADS and give the complete characterization of such sets. Beyond the theoretical results, our contribution also aims at exploring some algorithmic aspects. We supply a much more efficient algorithm than those proposed by Muir and Stinson, which takes only 343 seconds to compute all x's from 0 to -10 7  such that {0, 1, x} is a NADS.", "authors": ["Gildas Avoine", "Jean Monnerat", "Thomas Peyrin"], "n_citation": 0, "title": "Advances in alternative non-adjacent form representations", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "2c0bc2d7-0d64-4459-bd03-b4f4cc5703d3"}
{"abstract": "Topological map is a mathematical model of labeled image representation which contains both topological and geometrical information. In this work, we use this model to improve a Markovian segmentation algorithm. Image segmentation methods based on Markovian assumption consist in optimizing a Gibbs energy function. This energy function can be given by a sum of potentials which could be based on the shape or the size of a region, the number of adjacencies,...and can be computed by using topological map. In this work we propose the integration of a new potential: the global linearity of the boundaries, and show how this potential can be extracted from the topological map. Moreover, to decrease the complexity of our algorithm, we propose a local modification of the topological map in order to avoid the reconstruction of the entire structure.", "authors": ["Guillaume Damiand", "Olivier Alata", "Camille Bihoreau"], "n_citation": 0, "title": "Using 2D topological map information in a Markovian image segmentation", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "2c2a7840-16bf-485e-87eb-dd36582f1e78"}
{"abstract": "Researchers in all areas of science recognize the value of graphical displays and research on graphs has focused on determining which graphical elements enhance readability. To date, no research has examined the physiological processing of graphs. The purpose of this project was to examine the event-related potentials (ERPs) associated with the processing of bivariate scatterplots. Participants viewed scatterplots depicting different linear relationships (positive and negative; strong and weak) and their ERPs were analyzed. Results indicate interesting differences in how scatterplots are processed. Overall, there was differential processing in posterior, medial, and anterior brain sites. Sites on the left and right sides of the brain showed different patterns of activity in response to the scatterplots. In addition, results suggest that different relationships are processed differently in the brain (confirming previous research that has suggested that the perception of covariation is dependent upon the type of relationship depicted on a scatterplot).", "authors": ["Lisa A. Best", "Aren C. Hunter", "Brandie M. Stewart"], "n_citation": 0, "title": "Perceiving relationships : A physiological examination of the perception of scatterplots", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "2d3dc806-aa81-4a2e-b02d-ac3c23fbbf7d"}
{"abstract": "In this paper, we investigate one-class and clustering problems by using statistical learning theory. To establish a universal framework, a unsupervised learning problem with predefined threshold \u03b7 is formally described and the intuitive margin is introduced. Then, one-class and clustering problems are formulated as two specific \u03b7-unsupervised problems. By defining a specific hypothesis space in \u03b7-one-class problems, the crucial minimal sphere algorithm for regular one-class problems is proved to be a maximum margin algorithm. Furthermore, some new one-class and clustering marginal algorithms can be achieved in terms of different hypothesis spaces. Since the nature in SVMs is employed successfully, the proposed algorithms have robustness, flexibility and high performance. Since the parameters in SVMs are interpretable, our unsupervised learning framework is clear and natural. To verify the reasonability of our formulation, some synthetic and real experiments are conducted. They demonstrate that the proposed framework is not only of theoretical interest, but they also has a legitimate place in the family of practical unsupervised learning techniques.", "authors": ["Qing Tao", "Wu Gao-wei", "Fei-Yue Wang", "Jue Wang"], "n_citation": 0, "title": "Some marginal learning algorithms for unsupervised problems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "2e951155-669d-4e65-9491-fb4a3c5c3f74"}
{"abstract": "We investigate how elementary motion detectors (EMDs) can be used to control behavior. We have developed a model of the fly visual system which operates in real time under real world conditions and was tested in course and altitude stabilization tasks using a flying robot. While the robot could stabilize gaze i.e. orientation, we found that stabilizing translational movements requires more elaborate preprocessing of the visual input and fine tuning of the EMDs. Our results show that in order to control gaze and altitude EMD information needs to be computed in different processing streams.", "authors": ["Cyrill Planta", "J\u00f6rg Conradt", "Adrian Jencik", "Paul F. M. J. Verschure"], "n_citation": 50, "title": "A neural model of the fly visual system applied to navigational tasks", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "312a29de-336c-4bbc-b212-3e3dc8daf88d"}
{"abstract": "We propose a cryptosystem modulo p k q based on the RSA cryptosystem. We choose an appropriate modulus p k q which resists two of the fastest factoring algorithms, namely the number field sieve and the elliptic curve method. We also apply the fast decryption algorithm modulo p k  proposed in [22]. The decryption process of the proposed cryptosystems is faster than the RSA cryptosystem using Chinese remainder theorem, known as the Quisquater-Couvreur method [17]. For example, if we choose the 768-bit modulus p 2 q for 256-bit primes p and q, then the decryption process of the proposed cryptosystem is about 3 times faster than that of RSA cryptosystem using Quisquater-Couvreur method.", "authors": ["Tsuyoshi Takagi"], "n_citation": 0, "title": "Fast RSA-type cryptosystem modulo pkq", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "313181ca-5185-411f-bb6b-0cdd871806e0"}
{"abstract": "A system for modelling in three-dimensional discrete space is presented. Objects can be modelled combining simple shapes by set operations to obtain large and regular shapes. The system also supports aspects of free-form modelling to generate organic and more complex shapes. Techniques known from image processing are applied to transform and to smooth objects. The basic geometric transformations translation, rotation, scaling, and shearing are provided for discrete objects.", "authors": ["Andreas Emmerling", "Kristian Hildebrand", "J\u00f6rg Hoffmann", "Przemyslaw Musialski", "Grit Th\u00fcrmer"], "n_citation": 0, "title": "A system for modelling in three-dimensional discrete space", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "326cb136-abc6-43d5-ab50-a689b1cd57b4"}
{"abstract": "Fuzzy segmentation methods have been developed in order to reduce the negative effects of the unavoidable loss of data in the digitization process. These methods require the development of new image analysis methods, handling grey-level images. This paper describes the first step in our work on developing shape analysis methods for fuzzy images: the investigation of several measurements on digitized objects with fuzzy borders. The performance of perimeter, area, and the P 2 A measure estimators for digitized disks and digitized squares with fuzzy borders is analyzed. The method we suggest greatly improves the results obtained from crisp (hard) segmentation, especially in the case of low resolution images.", "authors": ["Natasa Sladoje", "Ingela Nystr\u00f6m", "Punam K. Saha"], "n_citation": 0, "title": "Perimeter and area estimations of digitized objects with fuzzy borders", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "342f3de4-6502-4d5b-8025-943def26eaa1"}
{"abstract": "We propose a new and efficient signature scheme that is provably secure in the plain model. The security of our scheme is based on a discrete-logarithm-based assumption put forth by Lysyanskaya, Rivest, Sahai, and Wolf (LRSW) who also showed that it holds for generic groups and is independent of the decisional Diffie-Hellman assumption. We prove security of our scheme under the LRSW assumption for groups with bi-linear maps. We then show how our scheme can be used to construct efficient anonymous credential systems as well as group signature and identity escrow schemes. To this end, we provide efficient protocols that allow one to prove in zero-knowledge the knowledge of a signature on a committed (or encrypted) message and to obtain a signature on a committed message.", "authors": ["Jan Camenisch", "Anna Lysyanskaya"], "n_citation": 0, "title": "Signature schemes and anonymous credentials from bilinear maps", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "34638ad1-ec52-48bc-8426-cffbb3c33206"}
{"abstract": "In this paper we present a novel Genetic Algorithm (GA) for feature selection in machine learning problems. We introduce a novel genetic operator which fixes the number of selected features. This operator, we will refer to it as m-features operator, reduces the size of the search space and improves the GA performance and convergence. Simulations on synthetic and real problems have shown very good performance of the m-features operator, improving the performance of other existing approaches over the feature selection problem.", "authors": ["Sancho Salcedo-Sanz", "Mario de Prado-Cumplido", "Fernando Perez-Cruz", "Carlos Bouso\u00f1o-Calz\u00f3n"], "n_citation": 50, "title": "Feature selection via genetic optimization", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "3508563a-a113-4279-b395-7c6278cd28e2"}
{"abstract": "Most morphological operators appear by pairs such as erosion/dilation, opening/closing, and thinning/thickening. These are pairs of dual operators with respect to set complementation. The output of a (dual) morphological operator applied to an object depends on whether it is a bright object over a dark background or a dark object over a bright background. When dealing with complex images such as earth observation data, there is no clear distinction between the background and the foreground because the image consists of a partition of the space into image objects of arbitrary intensity values. In this paper, we present an overview of existing approaches for tackling this problem and propose new techniques based on area filters applied first to the image extrema and then to all flat regions.", "authors": ["Pierre Soille"], "n_citation": 0, "title": "On the morphological processing of objects with varying local contrast", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "35c1a564-1f82-4b7d-aada-c7e3f80515b2"}
{"abstract": "The Internet which has enabled global businesses to flourish has become the very same channel for mushrooming 'terrorist news networks.' Terrorist organizations and their sympathizers have found a cost-effective resource to advance their courses by posting high-impact Websites with short shelf-lives. Because of their evanescent nature, terrorism research communities require unrestrained access to digitally archived Websites to mine their contents and pursue various types of analyses. However, organizations that specialize in capturing, archiving, and analyzing Jihad terrorist Websites employ different, manual-based analyses techniques that are inefficient and not scalable. This study proposes the development of automated or semi-automated procedures and systematic methodologies for capturing Jihad terrorist Website data and its subsequent analyses. By analyzing the content of hyperlinked terrorist Websites and constructing visual social network maps, our study is able to generate an integrated approach to the study of Jihad terrorism, their network structure, component clusters, and cluster affinity.", "authors": ["Edna Reid", "Jialun Qin", "Yilu Zhou", "Guanpi Lai", "Marc Sageman", "Gabriel Weimann", "Hsinchun Chen"], "n_citation": 0, "title": "Collecting and analyzing the presence of terrorists on the web : A case study of jihad websites", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "36b3174b-0abb-4b61-8cd7-017e255f9227"}
{"abstract": "Each user accesses a Website with certain interests. The interest can be manifested by the sequence of each Web user access. The access paths of all Web users can be clustered. The effectiveness and efficiency are two problems in clustering algorithms. This paper provides a clustering algorithm for personalized Web recommendation. It is path clustering based on competitive agglomeration (PCCA). The path similarity and the center of a cluster are defined for the proposed algorithm. The algorithm relies on competitive agglomeration to get best cluster numbers automatically. Recommending based on the algorithm doesn't disturb users and needn't any registration information. Experiments are performed to compare the proposed algorithm with two other algorithms and the results show that the improvement of recommending performance is significant.", "authors": ["Yijun Yu", "Huaizhong Lin", "Yimin Yu", "Chun Chen"], "n_citation": 0, "title": "Personalized web recommendation based on path clustering", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "36f654ca-3fad-4b33-9828-eb9ee350cd31"}
{"authors": ["W.X. Wilcke", "Mark Hoogendoorn", "J.J.M. Roessingh"], "n_citation": 0, "title": "Co-evolutionary Learning for Cognitive Computer Generated Entities", "venue": "Lecture Notes in Computer Science", "year": 2014, "id": "370ece4e-f3b7-4bf8-acb9-ec090c011660"}
{"abstract": "Privacy homomorphisms (PHs) are encryption functions mapping a set of operations on plaintext to another set of operations on ciphertext. In this paper we present new PH scheme that is based on the theory of the finite fields.", "authors": ["Mikhail Stepanov", "Sergey Bezzateev", "Tae-chul Jung"], "n_citation": 0, "title": "Privacy homomorphism for delegation of the computations", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "37f7f4c3-09fc-473a-b17d-a9064effd441"}
{"abstract": "Worms have been becoming a serious threat in web age because worms can cause huge loss due to the fast-spread property. To detect worms effectively, it is important to investigate the characteristics of worm traffic at individual source level. We model worm traffic with the multi-fractal process, and compare the multi-fractal property of worm and normal traffics at individual source level. The results show that the worm traffic possesses less multi-fractal property.", "authors": ["Yufeng Chen", "Yabo Doug", "Dongming Lu", "Yunhe Pan"], "n_citation": 0, "title": "The multi-fractal nature of worm and normal traffic at individual source level", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3801480a-ce35-42a4-9180-40c6de06546f"}
{"authors": ["Selwyn Russell"], "n_citation": 0, "title": "Issues in merging internet autonomous systems for emergency communications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "39769ad5-ac3f-49a0-8f0a-98f92a26c088"}
{"authors": ["Shigeki Matsubara", "Tomohiro Ohno", "Masashi Ito"], "n_citation": 0, "title": "Text Editing for Lecture Speech Archiving on the Web", "venue": "Lecture Notes in Computer Science", "year": 2009, "id": "39bcf5ce-b0e1-48d0-ad47-fc00f5b7e677"}
{"abstract": "Trusted software execution, prevention of code and data tampering, authentication, and providing a secure environment for software are some of the most important security challenges in the design of embedded systems. This short paper evaluates the performance of a hardware/software co-design methodology for embedded software protection. Secure software is created using a secure compiler that inserts hidden codes into the executable code which are then validated dynamically during execution by a reconfigurable hardware component constructed from Field Programmable Gate Array (FPGA) technology. While the overall approach has been described in other papers, this paper focuses on security-performance tradeoffs and the effect of using compiler optimizations in such an approach. Our results show that the approach provides software protection with modest performance penalty and hardware overhead.", "authors": ["Kripashankar Mohan", "Bhagirath Narahari", "Rahul Simha", "Paul Ott", "Alok N. Choudhary", "Joseph Zambreno"], "n_citation": 0, "title": "Performance study of a compiler/hardware approach to embedded systems security", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3aac3325-8efb-4281-8dbc-9e89ced08df9"}
{"abstract": "Relational classification in networked data plays an important role in many problems such as text categorization, classification of web pages, group finding in peer networks, etc. We have previously demonstrated that for a class of label propagating algorithms the underlying dynamics can be modeled as a two-state epidemic process on heterogeneous networks, where infected nodes correspond to classified data instances. We have also suggested a binary classification algorithm that utilizes non trivial characteristics of epidemic dynamics. In this paper we extend our previous work by considering a three-state epidemic model for label propagation. Specifically, we introduce a new, intermediate state that corresponds to susceptible data instances. The utility of the added state is that it allows to control the rates of epidemic spreading, hence making the algorithm more flexible. We show empirically that this extension improves significantly the performance of the algorithm. In particular, we demonstrate that the new algorithm achieves good classification accuracy even for relatively large overlap across the classes.", "authors": ["Aram Galstyan", "Paul R. Cohen"], "n_citation": 0, "title": "Iterative relational classification through three-state epidemic dynamics", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3adfd711-1505-4227-b46a-7e349aa39d38"}
{"abstract": "We will report evaluation of Automatic Named Entity Extraction feature of IR tools on Dutch, French, and English text. The aim is to analyze the competency of off-the-shelf information extraction tools in recognizing entity types including person, organization, location, vehicle, time, & currency from unstructured text. Within such an evaluation one can compare the effectiveness of different approaches for identifying named entities.", "authors": ["Nishant Kumar", "Jan De Beer", "Jan Vanthienen", "Marie-Francine Moens"], "n_citation": 50, "title": "Evaluation of information retrieval and text mining tools on automatic named entity extraction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3b332d70-71c8-4cf2-b3ad-c5b2a75dd4a4"}
{"authors": ["Mark Jyn-Huey Lim", "Michael Negnevitsky", "Js Hartnett"], "n_citation": 0, "title": "E-mail traffic analysis using visualisation and decision trees", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3eb28409-15cd-41d4-95f8-93ddd8dfd99f"}
{"authors": ["Christophe De Canni\u00e8re", "Christian Rechberger"], "n_citation": 0, "title": "Preimages for Reduced SHA-0 and SHA-1", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "3f024a5c-62d7-4cee-94d4-584f3251b2e4"}
{"abstract": "In this paper, we employed two machine learning algorithms -namely, a clustering and a neural network algorithm - to analyze the network traffic recorded from three sources. Of the three sources, two of the traffic sources were synthetic, which means the traffic was generated in a controlled environment for intrusion detection benchmarking. The main objective of the analysis is to determine the differences between synthetic and real-world traffic, however the analysis methodology detailed in this paper can be employed for general network analysis purposes. Moreover the framework, which we employed to generate one of the two synthetic traffic sources, is briefly discussed.", "authors": ["H. Gunes Kayacik", "Nur Zincir-Heywood"], "n_citation": 0, "title": "Analysis of three intrusion detection system benchmark datasets using machine learning algorithms", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "3f737266-99ee-4831-97a5-8722d16ec3b0"}
{"abstract": "In this paper, it is remarked that BZ lattice structures can recover several theoretical approaches to rough sets, englobing their individual richness in a unique structure. Rough sets based on a similarity relation are also considered, showing that the BZ lattice approach turns out to be even more useful, since enables one to define another rough approximation, which is better than the corresponding similarity one.", "authors": ["Gianpiero Cattaneo", "Davide Ciucci"], "n_citation": 50, "references": ["6932d64a-49b6-44fd-b45f-d5fc7b3d89eb"], "title": "Some methodological remarks about categorical equivalences in the abstract approach to roughness Part II", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "3fab9dd9-25c1-45a3-a935-d28b37a1fab4"}
{"abstract": "We address the problem of fairly distributing the cost of system-wide improvements to the security of a transportation infrastructure over the beneficiaries. We present a framework that models transportation links and the emergence (magnitude and frequency) and propagation of threats. The cost-distribution is based on a weighted sum that characterizes the expected reduction in tlie vulnerability of a site as a result of the security improvements.", "authors": ["Sudarshan S. Chawathe"], "n_citation": 0, "title": "Distributing the cost of securing a transportation infrastructure", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "408dfc54-4672-4482-b6e8-479283d5d1c5"}
{"abstract": "We consider a range of attacks on reduced-round variants of the block cipher Skipjack. In particular we concentrate on the role of truncated differentials anti consider what insight they give us into the design and long-term security of Skipjack. An attack on the full 32 rounds of Skipjack remains elusive. However we give attacks on the first 16 rounds of Skipjack that can efficiently recover the key with about 2 17  chosen plaintexts and an attack on the middle sixteen rounds of Skipjack which recovers the secret key using only two chosen plaintexts. Several high-probability truncated differentials are presented the existence of which might best be described as surprising. Most notably, we show that the techniques used by Biham et al, can be presented in terms of truncated differentials and that there exists a 24-round truncated differential that holds with probability one.", "authors": ["Lars R. Knudsen", "Matthew J. B. Robshaw", "David Wagner"], "n_citation": 0, "title": "Truncated differentials and Skipjack", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "40f05ffc-a425-4c9a-b3bb-eceafac3274f"}
{"abstract": "Specification of Quality of Service (QoS) for components can only be done in relation to the QoS the components themselves are given by imported components. Developers as well as users need support in order to derive valid data for specification respectively for checking whether a selected component complies with its specification. In this paper we introduce the architecture of a measurement framework for EJBs giving such support and discuss in detail the measurement of the well understood property of response time.", "authors": ["Marcus Meyerh\u00f6fer", "Christoph P. Neumann"], "n_citation": 0, "title": "TESTEJB - A measurement framework for EJBs", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "41264310-b6d7-4320-8595-14f57f5d6bf8"}
{"abstract": "We construct several new statistical zero-knowledge proofs with efficient provers, i.e. ones where the prover strategy runs in probabilistic polynomial time given an NP witness for the input string. Our first proof systems are for approximate versions of the SHORTEST VECTOR PROBLEM (SVP) and CLOSEST VECTOR PROBLEM (CVP), where the witness is simply a short vector in the lattice or a lattice vector close to the target, respectively. Our proof systems are in fact proofs of knowledge, and as a result, we immediately obtain efficient lattice-based identification schemes which can be implemented with arbitrary families of lattices in which the approximate SVP or CVP are hard. We then turn to the general question of whether all problems in SZKnNP admit statistical zero-knowledge proofs with efficient provers. Towards this end, we give a statistical zero-knowledge proof system with an efficient prover for a natural restriction of STATISTICAL DIFFERENCE, a complete problem for SZK. We also suggest a plausible approach to resolving the general question in the positive.", "authors": ["Daniele Micciancio", "Salil P. Vadhan"], "n_citation": 0, "title": "Statistical zero-knowledge proofs with efficient provers: Lattice problems and more", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "42f6aaf0-8330-4ef0-8e04-9e51dac1017c"}
{"authors": ["Kazuyoshi Murata", "Kouhei Shigematsu", "Yu Shibuya", "Masaaki Kurosu"], "n_citation": 0, "references": ["a71b6492-0d7d-435d-9e39-8a673785c2e5"], "title": "Notification System to Encourage a User to Refrain from Using Smartphone Before Going to Bed", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "434168dd-2008-43b7-aa9b-2ec7a985f4b3"}
{"abstract": "In this work we use cryptography to solve a game-theoretic problem which arises naturally in the area of two party strategic games. The standard game-theoretic solution concept for such games is that of an equilibrium, which is a pair of self-enforcing strategies making each player's strategy an optimal response to the other player's strategy. It is known that for many games the expected equilibrium payoffs can be much higher when a trusted third party (a mediator) assists the players in choosing their moves (correlated equilibria), than when each player has to choose its move on its own (Nash equilibria). It is natural to ask whether there exists a mechanism that eliminates the need for the mediator yet allows the players to maintain the high payoffs offered by mediator-assisted strategies. We answer this question affirmatively provided the players are computationally bounded and can have free communication (so-called cheap talk ) prior to playing the game. The main building block of our solution is an efficient cryptographic protocol to the following Correlated Element Selection problem, which is of independent interest. Both Alice and Bob know a list of pairs (a 1 ,b 1 )...(a n ,b n ) (possibly with repetitions), and they want to pick a random index i such that Alice learns only as and Bob learns only b i . Our solution to this problem has constant number of rounds, negligible error probability, and uses only very simple zero-knowledge proofs. We then show how to incorporate our cryptographic protocol back into a game-theoretic setting, which highlights some interesting parallels between cryptographic protocols and extensive form games.", "authors": ["Yevgeniy Dodis", "Shai Halevi", "Tal Rabin"], "n_citation": 0, "title": "A cryptographic solution to a game theoretic problem", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "4368e7b8-b9f9-4355-9661-3a8da1779428"}
{"abstract": "In this paper we propose a new key recovery attack on irregular clocked keystream generators where the stream is filtered by a nonlinear Boolean function. We show that the attack is much more efficient than expected from previous analytic methods, and we believe it improves all previous attacks on the cipher model.", "authors": ["Havard Molland", "Tor Helleseth"], "n_citation": 0, "title": "An improved correlation attack against irregular clocked and filtered keystream generators", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "43cda4bf-2ac8-425d-abae-79de79cac04a"}
{"abstract": "Simple password authentication is often used e.g. from an email software application to a remote IMAP server. This is frequently done in a protected peer-to-peer tunnel, e.g. by SSL/TLS. At Eurocrypt'02, Vaudenay presented vulnerabilities in padding schemes used for block ciphers in CBC mode. He used a side channel, namely error information in the padding verification. This attack was not possible against SSL/TLS due to both unavailability of the side channel (errors are encrypted) and premature abortion of the session in case of errors. In this paper we extend the attack and optimize it. We show it is actually applicable against latest and most popular implementations of SSL/TLS (at the time this paper was written) for password interception. We demonstrate that a password for an IMAP account can be intercepted when the attacker is not too far from the server in less than an hour in a typical setting. We conclude that these versions of the SSL/TLS implementations are not secure when used with block ciphers in CBC mode and propose ways to strengthen them. We also propose to update the standard protocol.", "authors": ["Brice Canvel", "Alain P. Hiltgen", "Serge Vaudenay", "Martin Vuagnoux"], "n_citation": 0, "title": "Password interception in a SSL/TLS channel", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "445480e1-31b3-4f73-a990-0789c1afd51a"}
{"authors": ["Laura R. Novick", "Kefyn M. Catley"], "n_citation": 0, "title": "Interpreting hierarchical structure : Evidence from cladograms in biology", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "446dfc4a-bda1-4cb4-866f-8e2f9d24a1ef"}
{"authors": ["Yongsu Park", "Yong Ho Song", "Eul Gyu Im"], "n_citation": 0, "title": "Design of a reliable hardware stack to defend against frame pointer overwrite attacks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "44ccfd31-69bc-4e00-a074-3a46ae3d9d59"}
{"abstract": "Those who want to conceal the content of their communications can do so by replacing words that might trigger attention by other words or locutions that seem more ordinary. We address the problem of discovering such substitutions when the original and substitute words have the same natural frequency. We construct a number of measures, all of which search for local discontinuities in properties such as string and bag-of-words frequency. Each of these measures individually is a weak detector. However, we show that combining them produces a detector that is reasonably effective.", "authors": ["SzeWang Fong", "David B. Skillicorn", "Dmitri Roussinov"], "n_citation": 50, "title": "Measures to detect word substitution in intercepted communication", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "45237f4c-290d-465e-9f42-5ec0514b73b0"}
{"abstract": "Embedded systems must be cost-effective. This imposes strict requirements on the resource consumption of their applications. It is therefore desirable to be able to determine the resource consumption of applications as early as possible in its development. Only then, a designer is able to guarantee that an application will fit on a target device. In this paper we will present a method for predicting run-time resource resource consumption in multi-task component based systems based on a design of an application. In [5] we describe a scenario based resource prediction technique and show that it can be applied to non-pre-emptive non-processing resources, like memory. In this paper we extend this technique, which enables us to handle pre-emptive processing resources and their scheduling policies. Examples of these class of resources are CPU and network. For component based software engineering the challenge is to express resource consumption characteristics per component, and to combine them to do predictions over compositions of components. To this end, we propose a model and tools, for combining individual resource estimations of components. These composed resource estimations are then used in scenarios (which model run-time behavior) to predict resource consumption.", "authors": ["Johan Muskens", "Michel R. V. Chaudron"], "n_citation": 0, "title": "Prediction of run-time resource consumption in multi-task component-based software systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4567b295-a28b-4b17-b995-5c4304ad41e5"}
{"abstract": "In recent years hierarchical concepts of temporal abstraction have been integrated in the reinforcement learning framework to improve scalability. However, existing approaches are limited to domains where a decomposition into subtasks is known a priori. In this paper we propose the concept of explicitly selecting time scale related actions if no subgoal-related abstract actions are available. This is realised with multi-step actions on different time scales that are combined in one single action set. The special structure of the action set is exploited in the MSA-Q-learning algorithm. By learning on different explicitly specified time scales simultaneously, a considerable improvement of learning speed can be achieved. This is demonstrated on two benchmark problems.", "authors": ["Ralf Schoknecht", "Martin A. Riedmiller"], "n_citation": 50, "title": "Speeding-up reinforcement learning with multi-step actions", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "4733d824-4760-4b71-9f5f-cc9c29d12337"}
{"abstract": "A new semiautomatic method for tissue reconstruction based on deformation of a dual simplex mesh was developed. The method is suitable for specifically-shaped objects. The method consists of three steps: the first step includes searching for object markers, i. e. the approximate centre of each object is localized. The searching procedure is based on careful analysis of object boundaries and on the assumption that the analyzed objects are sphere-like shaped. The first contribution of the method is the ability to find the markers without choosing the particular objects by hand. In the next step the surface of each object is reconstructed. The procedure is based on the method for spherical object reconstruction presented in [3]. The method was partially changed and was adapted to be more suitable for our purposes. The problem of getting stuck in local minima was solved. In addition, the deformation process was sped up. The final step concerns quality evaluation: both of the first two steps are nearly automatic, therefore the quality of their results should be measured.", "authors": ["David Svoboda", "Pavel Matula"], "n_citation": 0, "title": "Tissue reconstruction based on deformation of dual simplex meshes", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "48e68695-93c8-45a4-a21f-11d1aa10f874"}
{"abstract": "Designers of information visualization and user interfaces must take account of culture in the design of metaphors, mental models, navigation, interaction, and appearance. Culture models define dimensions of difference and similarity among groups of people regarding signs, rituals, heroes/heroines, and values. Examples on the Web reveal these dimensions. Developers will increasingly need to take into account culture and other factors in development to better ensure usability, usefulness, and appeal.", "authors": ["Aaron Marcus"], "n_citation": 50, "title": "Cross-cultural user-experience design", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "48ea21c7-86c4-444b-9ad3-1d49d07a48f3"}
{"abstract": "Symmetry detection through a net of coupled maps is proposed. Logistic maps are associated with each element of a pixel image where the symmetry is intended to be verified. The maps are locally and globally coupled and the reflection-symmetry structure can be embedded in local couplings. Computer simulations are performed by using random gray level images with different image sizes, asymmetry levels and noise intensity. The symmetry detection is also done under dynamic scene changing. Finally the extensions and the adherence of the present model to biological systems are discussed.", "authors": ["Rogerio Ribeiro de Oliveira", "Luiz Henrique Alves Monteiro"], "n_citation": 50, "title": "Symmetry detection using global-locally coupled maps", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "49956b76-5a17-45f7-a197-de3f4ae258de"}
{"abstract": "We present a protocol for two parties to generate an RSA key in a distributed manner. At the end of the protocol the public key: a modulus N = PQ, and an encryption exponent e are known to both parties. Individually, neither party obtains information about the decryption key d and the prime factors of N: P and Q. However, d is shared among the parties so that threshold decryption is possible.", "authors": ["Niv Gilboa"], "n_citation": 0, "title": "Two party RSA key generation", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "4ac67f1b-0c4c-4f84-8f03-82f9c984d4df"}
{"abstract": "This paper presents a new paradigm of cryptography, quantum public-key cryptosystems. In quantum public-key cryptosystems, all parties including senders, receivers and adversaries are modeled as quantum (probabilistic) poly-time Turing (QPT) machines and only classical channels (i.e., no quantum channels) are employed. A quantum trapdoor one-way function, f, plays an essential role in our system, in which a QPT machine can compute f with high probability, any QPT machine can invert f with negligible probability, and a QPT machine with trapdoor data can invert f. This paper proposes a concrete scheme for quantum public-key cryptosystems: a quantum public-key encryption scheme or quantum trapdoor one-way function. The security of our schemes is based on the computational assumption (over QPT machines) that a class of subset-sum problems is intractable against any QPT machine. Our scheme is very efficient and practical if Shor's discrete logarithm algorithm is efficiently realized on a quantum machine.", "authors": ["Tatsuaki Okamoto", "Katsunori Tanaka", "Shigenori Uchiyama"], "n_citation": 140, "title": "Quantum public-key cryptosystems", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "4b4b1543-ad2f-4a29-b587-01f6d626a5f1"}
{"abstract": "This study investigates key properties of self-similar processes of multiservice traffic in IP networks. On the basis of the analytical modeling the impact of self-similarity properties on the QoS (Quality of Service) parameters (delays and losses) is shown. The results of simulation are presented.", "authors": ["Anatoly M. Galkin", "Olga A. Simonina", "Gennady G. Yanovsky"], "n_citation": 50, "title": "Multiservice IP network QoS parameters estimation in presence of self-similar traffic", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4bce2a37-d833-4193-b97e-492fdbae2b0e"}
{"authors": ["Oscar Nierstrasz"], "n_citation": 0, "title": "Putting change at the center of the software process", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "4c47b140-68be-4f17-8e69-3c7ca797e4a5"}
{"abstract": "A visual query is based on pictorial representation of conceptual entities and operations. One of the most important features used in visual queries is the shape. Despite its intuitive writing, a shape-based visual query usually suffers of a complexity processing related to two major parameters: 1-the imprecise user request. 2-shapes may undergo several types of transformation. Several methods are provided in the literature to assist the user during query writing. On one hand, relevance feedback technique is widely used to rewrite the initial user query. On the other hand, shape transformations are considered by current shape-based retrieval approaches without any user intervention. In this paper, we present a new cooperative approach based on the shape neighborhood concept allowing the user to rewrite a shape-based visual query according to his preferences with high flexibility in terms of including (or excluding) only some shape transformations and of result sorting.", "authors": ["Georges Chalhoub", "Richard Chbeir", "Kokou Yetongnon"], "n_citation": 0, "title": "Flexible shape-based query rewriting", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "4d769e42-c905-4020-a96c-6f88eedcb874"}
{"abstract": "We propose a private-key cryptosystem and a protocol for key agreement by public discussion that are unconditionally secure based on the sole assumption that an adversary's memory capacity is limited. No assumption about her computing power is made. The scenario assumes that a random bit string of length slightly larger than the adversary's memory capacity can be received by all parties. The random bit string can for instance be broadcast by a satellite or over an optical network, or transmitted over an insecure channel between the communicating parties. The proposed schemes require very high bandwidth but can nevertheless be practical.", "authors": ["Christian Cachin", "Ueli Maurer"], "n_citation": 0, "title": "Unconditional security against memory-bounded adversaries", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "4e491710-e3a6-49d4-9f4a-e1c39d664634"}
{"abstract": "Theoretical models of Turing complete linear genetic programming (GP) programs suggest the fraction of halting programs is vanishingly small. Convergence results proved for an idealised machine, are tested on a small T7 computer with (finite) memory, conditional branches and jumps. Simulations confirm Turing complete fitness landscapes of this type hold at most a vanishingly small fraction of usable solutions.", "authors": ["Wb Langdon", "Riccardo Poli"], "n_citation": 0, "title": "The halting probability in von neumann architectures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "506bbe88-dc3f-418b-865e-e703aa5b2e7c"}
{"abstract": "In this paper a modelling approach to the dynamics within a multi-agent organisation is presented. A declarative, executable temporal modelling language for organisation dynamics is proposed as a basis for simulation. Moreover, to be able to specify and analyse dynamic properties, another temporal language is put forward, which is much more expressive than the executable language for simulations. Supporting tools have been implemented that consist of a software environment for simulation of multi-agent organisation models and a software environment for analysis of dynamic properties against traces organisation dynamics.", "authors": ["Catholijn M. Jonker", "Jan Treur", "Wouter C. A. Wijngaards"], "n_citation": 50, "title": "Temporal languages for simulation and analysis of the dynamics within an organisation.", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5181be22-0c65-4a55-8b3e-824ed851fec3"}
{"abstract": "Linear Feedback Shift Registers (LFSRs) are used as pseudorandom keystream generators in cryptographic schemes. Hardware implementation of LFSRs are simple and fast but their software implementation is not quite efficient. Here we present a fast software implementation strategy for the LFSRs. The output will be available as a block of bits after each operation. We discuss theoretical issues for such block oriented implementation and present necessary algorithms. We clearly identify the constraints in the choice of connection polynomials for block oriented implementation. Actual implementation results have been presented in support of our claims. The results emphasise the usability of LFSRs in software based stream cipher systems.", "authors": ["Sandeepan Chowdhury", "Subhamoy Maitra"], "n_citation": 50, "title": "Efficient software implementation of linear Feedback Shift Registers", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "5197bc21-9a01-4a4b-b328-b432eace4561"}
{"abstract": "Management of data systems must cope with changes, initiated by users or applications. Query answering in a frequently modified data system must be consistent with the updates. A natural expectation is that a fact that was successfully added, is retrievable as long as it was not intentionally removed. Active Databases do not meet this expectation, since contradicting rules may undo events that triggered them. In this paper, we associate database transactions with effects, and present a method that takes care of preserving the effects of updates. We introduce a compile-time effect preservation transformation that revises a transaction so to prevent contradictory updates, i.e., yields an effect preserving transaction. Our method yields an expressive and efficient transaction, since it is based on interleaving run-time sensitive analysis of effects within the compile-time transformation of a transaction. The interleaving and the compile-time reduction of effects account for the efficiency; the run-time sensitivity of effects accounts for the expressiveness. In the context of Active Databases this method can be used to statically revise a rule application plan, so to prevent contradictory updates. We claim that an effect preserving method should be integrated into every update and query processing system.", "authors": ["Mira Balaban", "Steffen Jurk"], "n_citation": 0, "title": "Effect preservation as a means for achieving update consistency", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "51b0d41d-edb4-437c-b696-185e66c18e48"}
{"abstract": "In this paper, we describe a methodology for the design and the development of component-based real-time systems. In our model, a component consists of a set of concurrent real-time threads that can communicate by means of synchronized operations. In addition, each component can specify its own local scheduling algorithm. We also discuss the support that must be provided at the operating system level, and present an implementation in the SHaRK operating system.", "authors": ["Giuseppe Lipari", "Paolo Gai", "Michael Trimarchi", "Giacomo Guidi", "Paolo Ancilotti"], "n_citation": 0, "title": "A hierarchical framework for component-based real-time systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "523035a2-b92f-4187-a5b7-30f63484cd09"}
{"abstract": "Machine- understandable data constitutes the foundation for the Semantic Web. This paper presents a viable way for authoring and annotating Semantic Documents on the desktop. In our approach, the PDF file format is the container for document semantics, being able to store both the content and the related metadata in a single file. To achieve this, we provide a framework (SALT - Semantically Annotated LATEX), that extends the LATEX writing environment and supports the creation of metadata for scientific publications. SALT allows the author to create metadata concurrently, i.e. while in the process of writing a document. We discuss some of the requirements which have to be met when developing such a support for creating semantic documents. In addition, we describe a usage scenario to show the feasability and benefit of our approach.", "authors": ["Knud Moeller", "Siegfried Handschuh", "Stefan Decker", "Tudor Groza"], "n_citation": 0, "title": "SALT - Semantically Annotated LaTeX for scientific publications", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "52b816e3-5e5c-4a79-9e5c-8c3961e9564d"}
{"abstract": "In a recent paper Dinur and Nissim considered a statistical database in which a trusted database administrator monitors queries and introduces noise to the responses with the goal of maintaining data privacy [5]. Under a rigorous definition of breach of privacy, Dinur and Nissim proved that unless the total number of queries is sub-linear in the size of the database, a substantial amount of noise is required to avoid a breach, rendering the database almost useless. As databases grow increasingly large, the possibility of being able to query only a sub-linear number of times becomes realistic. We further investigate this situation, generalizing the previous work in two important directions: multi-attribute databases (previous work dealt only with single-attribute databases) and vertically partitioned databases, in which different subsets of attributes are stored in different databases. In addition, we show how to use our techniques for datamining on published noisy statistics.", "authors": ["Cynthia Dwork", "Kobbi Nissim"], "n_citation": 0, "title": "Privacy-preserving datamining on vertically partitioned databases", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "53e3c17b-8ee4-4e6b-a1dd-d937ca38f27a"}
{"abstract": "SecureUML is a security modeling language for formalizing access control requirements in a declarative way. It is equipped with a UML notation in terms of a UML profile, and can be combined with arbitrary design modeling languages. We present a semantics for SecureUML in terms of a model transformation to standard UML/OCL. The transformation scheme is used as part of an implementation of a tool chain ranging from front-end visual modeling tools over code-generators to the interactive theorem proving environment HOL-OCL. The methodological consequences for an analysis of the generated OCL formulae are discussed.", "authors": ["Achim D. Brucker", "J\u00fcrgen Doser", "Burkhart Wolff"], "n_citation": 0, "title": "A model transformation semantics and analysis methodology for secureUML", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "540da332-24b8-4ae2-b9c9-350418651309"}
{"abstract": "This paper presents a formalisation of obligations, social commitments and roles for BDI agents. We present a formal analysis of general obligations and relativised-to-one obligations from a bearer to a single counterparty and we examine obligations and relativised-to-one obligations in the context of strong realism for BDI agents. We also discuss how relativised-to-one obligations arise as a result of social commitments and the adoption of roles. In our framework, if an agent adopts a role, then this role is associated with one or more social commitments. Social commitments give rise to relativised obligations and consequently, roles, social commitments and relativised obligations are interwoven.", "authors": ["Maria Fasli"], "n_citation": 0, "title": "On commitments, roles, and obligations", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "5436a91e-712d-4f52-967e-8ef6ed8d56ef"}
{"abstract": "Autonomous agents are given the authority to select which actions they will execute. If the agent behaves rationally, the actions it selects will be in its own best interests. When addressing multiple goals, the rational action may not be obvious. Equipping the agents with decision-theoretic methods allows the agent to mathematically evaluate the risks, uncertainty, and benefits of the various available courses of action. Using this evaluation, an agent can determine which goals are worth achieving, as well as the order in which to achieve those goals. When the goals of the agent changes, the agent must replan to maintain rational decision-making. This research uses macro actions to transform the state space for the agent's decision problem into the desire space of the agent. Reasoning in the desire space, the agent can efficiently maintain rationality in response to addition and removal of goals.", "authors": ["David C. Han", "K. Suzanne Barber"], "n_citation": 0, "title": "Desire-space analysis and action selection for multiple dynamic goals", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "5490eeba-cc9b-47ae-994f-2e6fefe64092"}
{"abstract": "For certain security applications, including identity based encryption and short signature schemes, it is useful to have abelian varieties with security parameters that are neither too small nor too large. Supersingular abelian varieties are natural candidates for these applications. This paper determines exactly which values can occur as the security parameters of supersingular abelian varieties (in terms of the dimension of the abelian variety and the size of the finite field), and gives constructions of supersingular abelian varieties that are optimal for use in cryptography.", "authors": ["Karl Rubin", "Alice Silverberg"], "n_citation": 135, "title": "Supersingular abelian varieties in cryptology", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "561501c7-e89b-4e43-a9fb-7d6b8eb468f1"}
{"abstract": "Verifiable Signature Sharing (V\u03a3S) enables the recipient of a digital signature, who is not necessarily the original signer, to share such signature among n proxies so that a subset of them can later reconstruct it. The original RSA and Rabin V\u03a3S protocols were subsequently broken and the original DSS V\u03a3S lacks a formal proof of security. We present new protocols for RSA, Rabin and DSS V\u03a3S. Our protocols are efficient and provably secure and can tolerate the malicious behavior of up to half of the proxies. Furthermore we believe that some of our techniques are of independent interest. Some of the by-products of our main result are: a new threshold cryptosystem, a new undeniable signature scheme and a way to create binding RSA cryptosystems.", "authors": ["Dario Catalano", "Rosario Gennaro"], "n_citation": 50, "title": "New efficient and secure protocols for verifiable signature sharing and other applications", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "56d3e319-4cee-4ee3-bc75-27e07b99c9db"}
{"authors": ["Bart Preneel", "Ren\u00e9 Govaerts", "Joos Vandewalle"], "n_citation": 50, "title": "Hash Functions Based on Block Ciphers: A Synthetic Approach", "venue": "Lecture Notes in Computer Science", "year": 1993, "id": "56dba1c8-3726-49c2-8b2e-f18cacf26f78"}
{"abstract": "A stepwise approach is proposed to predict the performance of component compositions. The approach considers the major factors influencing the performance of component compositions in sequence: component operations, activities, and composition of activities. During each step, various models - analytical, statistical, simulation - can be constructed to specify the contribution of each relevant factor to the performance of the composition. The architects can flexibly choose which model they use at each step in order to trade prediction accuracy against prediction effort. The approach is illustrated with an example about the performance prediction for an Automobile Navigation System.", "authors": ["Evgeni M. Eskenazi", "Alexandre V. Fioukov", "Dieter K. Hammer"], "n_citation": 0, "title": "Performance prediction for component compositions", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "56fe2846-2a37-46b8-8a78-e8d8716c1348"}
{"authors": ["Kerstin Fischer"], "n_citation": 50, "title": "The role of users concepts of the robot in human-robot spatial instruction", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "57dc44fa-72aa-41db-b46e-230d922c2780"}
{"authors": ["Giorgio Ghelli", "Dario Colazzo", "Carlo Sartiani"], "n_citation": 0, "title": "Efficient Inclusion for a Class of XML Types with Interleaving and Counting", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "5853af0e-7f99-43d7-bdf3-35c67d33da01"}
{"abstract": "Complex systems theory and Cellular Automata (CA) are widely used in geospatial modeling. However, existing models have been limited by challenges such as handling of multiple datasets, parameter definition and the calibration procedures in the modeling process. Bayesian network (BN) formalisms provide an alternative method to address the drawbacks of these existing models. This study proposes a hybrid model that integrates BNs, CA and Geographic Information Systems (GIS) to model land use change. The transition rules of the CA model are generated from a graphical formalism where the key land use drivers are represented by nodes and the dependencies between them are expressed by conditional probabilities extracted from historical spatial datasets. The results indicate that the proposed model is able to realistically simulate and forecast spatio-temporal process of land use change. Further, it forms the basis for new synergies in CA model design that can lead to improved model outcomes.", "authors": ["Verda Kocabas", "Suzana Dragicevic"], "n_citation": 0, "title": "Coupling Bayesian Networks with GIS-Based Cellular Automata for Modeling Land Use Change", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "58c5afab-158c-4430-a1b9-f234aac14912"}
{"authors": ["Kimmo J\u00e4rvinen", "Josep Balasch"], "n_citation": 0, "title": "Single-Trace Side-Channel Attacks on Scalar Multiplications with Precomputations", "venue": "Lecture Notes in Computer Science", "year": 2016, "id": "59be5ec8-d821-4eda-b9e4-2c6556331483"}
{"authors": ["Gautham Sekar", "Souradyuti Paul", "Bart Preneel"], "n_citation": 0, "title": "Related-Key Attacks on the Py-Family of Ciphers and an Approach to Repair the Weaknesses", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "59e585e2-d789-47a6-b9fe-063709de7ea6"}
{"abstract": "Component containers provide a deployment environment for components in a component-based system. Containers supply a variety of services to the components that are deployed in them, such as persistence, enforcement of security policies and transaction management. Recently, containers have shown a large amount of potential for aiding in the predictable assembly of component-based systems. This paper describes an augmentation to the component container, called the Container-Managed Exception Handling (CMEH) Framework, which provides an effective means for deploying exception handling mini-components into a component-based system. This framework promotes a more effective handling of exceptional events, as well as a better separation of concerns, yielding a more robust component assembly.", "authors": ["Kevin Simons", "Judith A. Stafford"], "n_citation": 0, "title": "CMEH: Container managed Exception Handling for increased assembly robustness", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "5a307c87-3a54-4506-ae3c-190e3ce51640"}
{"authors": ["Maxim A. Babenko", "Ignat I. Kolesnichenko", "Starikovskaya Tatiana"], "n_citation": 0, "title": "On Minimal and Maximal Suffixes of a Substring", "venue": "Lecture Notes in Computer Science", "year": 2013, "id": "5b024f5f-1fe2-44a9-bfc3-d11fa382e7ef"}
{"abstract": "Worm research depends on simulation to a large degree due to worm propagation characters. In worm simulation, worm traffic generation is the base to analyze influences of worm traffic on network. The popular Random Constant Spread (RCS) model ignores the burstiness of latency-limited worm traffic, which will cause underestimation of the influences. According to worm scan behaviors, the Periodic Burst Scanning (PBS) model is proposed to model latency-limited worm traffic. Simulation results show that network performance decreases much more with PBS model than that with RCS model.", "authors": ["Yufeng Chen", "Yabo Dong", "Dongming Lu", "Yunhe Pan", "Honglan Lao"], "n_citation": 0, "title": "Worm traffic modeling for network performance analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5b9189b8-610e-49cd-9286-91f5da5ea16d"}
{"abstract": "Imai and Matsumoto introduced alternative algebraic methods for constructing public key cryptosystems. An obvious advantage of theses public key cryptosystems is that the private side computations can be made very efficient with a simple hardware. Almost all of these proposals and variants of them were broken. However, scheme B in [3] is still unbroken. In this paper we show some statistical weaknesses of this scheme. In particular, we show that trying to minimize the size of the public key facilitates a cryptanalytic attack that enables the cryptanalyst to decrypt, with high probability of success, a given ciphertext by performing a very limited number of encryption operations using the public encryption function.", "authors": ["Amr M. Youssef", "Guang Gong"], "n_citation": 0, "title": "Cryptanalysis of imai and Matsumoto scheme B asymmetric cryptosystem", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "5eaa428a-8a56-4eea-9a9f-3a320d3dc97f"}
{"abstract": "In many organizations, it is common to control access to confidential information based on the need-to-know principle; The requests for access are authorized only if the content of the requested information is relevant to the requester's current information analysis project. We formulate such content-based authorization, i.e. whether to accept or reject access requests as a binary classification problem. In contrast to the conventional error-minimizing classification, we handle this problem in a cost-sensitive learning framework in which the cost caused by incorrect decision is different according to the relative importance of the requested information. In particular, the cost (i.e., damaging effect) for a false positive (i.e., accepting an illegitimate request) is more expensive than that of false negative (i.e., rejecting a valid request). The former is a serious security problem because confidential information, which should not be revealed, can be accessed. From the comparison of the cost-sensitive classifiers with error-minimizing classifiers, we found that the costing with a logistic regression showed the best performance, in terms of the smallest cost paid, the lowest false positive rate, and the relatively low false negative rate.", "authors": ["Young-Woo Seo", "Katia P. Sycara"], "n_citation": 50, "title": "Cost-sensitive access control for illegitimate confidential access by insiders", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "5eacc83f-c41c-4c88-8cc4-085f8aa8f1ed"}
{"abstract": "This paper will focus on a proposal to speed up Shape From Shading (SFS) approaches based on energy minimization. To this end, Graduated Non Convexity (GNC) algorithm has been adopted to minimize this strongly non convex energy. Achieved results are very promising and involve aspects both theoretical and practical. In fact, both a generalization of the original formulation of GNC and an effective discrete shape recovery characterize our approach. Finally, a drastic reduction of the computational time is reached in comparison with the other currently available approaches.", "authors": ["Daniele Gelli", "Domenico Vitulano"], "n_citation": 0, "title": "Speed up of Shape from shading using graduated non-convexity", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "60546a73-4203-4f24-945b-11a92fd8748d"}
{"abstract": "Animals show an abundance of different sensor morphologies, for example in insect compound eyes. However, the advantages of having highly specific sensor morphologies still remain unclear. In this paper we show that an appropriate sensor morphology can improve the learning performance of an agent's neural controller significantly. Using a sensor morphology that is optimised for a given task environment the agent is able to learn faster and to adapt more quickly to changes.", "authors": ["Lukas Lichtensteiger", "Rolf Pfeifer"], "n_citation": 50, "title": "An optimal sensor morphology improves adaptability of neural network controllers", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "61f73e6e-126e-4768-b671-7d38231388c8"}
{"abstract": "Classical logic predicts that everything (thus nothing useful at all) follows from inconsistency. A paraconsistent logic is a logic where inconsistency does not lead to such an explosion, and since in practice consistency is difficult to achieve there are many potential applications of paraconsistent logics in query answering systems. We compare the paraconsistent and the non-monotonic solutions to the problem of contradictions. We propose a many-valued paraconsistent logic based on a simple notion of indeterminacy. In particular we describe the semantics of the logic using key equalities for the logical operators. We relate our approach to works on bilattices. We also discuss and provide formalizations of two case studies, notably the well-known example involving penguins and a more interesting example in the domain of medicine.", "authors": ["J\u00f8rgen Villadsen"], "n_citation": 0, "title": "Paraconsistent query answering systems", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "6261bd86-62d1-4531-bd83-00d7a1dd37a9"}
{"abstract": "In this paper, we present optimal in time algorithms to solve the reverse Euclidean distance transformation and the reversible medial axis extraction problems for d-dimensional images. In comparison to previous technics, the proposed Euclidean medial axis may contain less points than the classical medial axis.", "authors": ["David Coeurjolly"], "n_citation": 0, "title": "d-Dimensional reverse Euclidean distance transformation and Euclidean medial axis extraction in optimal time", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "62d284ba-b6f6-4793-b710-5acd04a1d2f0"}
{"abstract": "While some researchers have exploited the similarity between cyber attacks and epidemics we believe there is also potential to leverage considerable experience gained in other biological domains: phylogenetics, ecological niche modeling, and biomonitoring. Here we describe some new ideas for threat detection from biomonitoring, and approximate graph searching and matching for cross network aggregation. Generic anomaly aggregation systems using these methods could detect and model the inheritance and evolution of vulnerability and threats across multiple domains and time scales.", "authors": ["David R. B. Stockwell", "Jason Tsong-li Wang"], "n_citation": 0, "title": "Biomonitoring, phylogenetics and anomaly aggregation systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "633de294-dadd-435c-8724-c7a7db9d13da"}
{"abstract": "In this paper, we discuss an approach for relaxing a failing query in the context of flexible querying. The approach relies on the notion of proximity which is defined in a relative way. We show how such proximity allows for transforming a given predicate into an enlarged one. The resulting predicate is semantically not far from the original one and it is obtained by a simple fuzzy arithmetic operation. We show also how the search for a non-failing relaxed query over the lattice of relaxed queries can be improved by exploiting the notions of MFSs (Minimal Failing Sub-queries) and MGQs (Maximally Generalized failing Queries) of the original query.", "authors": ["Patrick Bosc", "Allel Hadjali", "Olivier Pivert"], "n_citation": 0, "title": "Relaxation paradigm in a flexible querying context", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "63a40fc3-6f08-4c14-8448-783c25a7bd6f"}
{"abstract": "We introduce the notion of abuse-free distributed contract signing, that is, distributed contract signing in which no party ever can prove to a third party that he is capable of choosing whether to validate or invalidate the contract. Assume Alice and Bob are signing a contract. If the contract protocol they use is not abuse-free, then it is possible for one party, say Alice, at some point to convince a third party, Val, that Bob is committed to the contract, whereas she is not yet. Contract protocols with this property are therefore not favorable to Bob, as there is a risk that Alice does not really want to sign the contract with him, but only use his willingness to sign to get leverage for another contract. Most existing optimistic contract signing schemes are not abuse-free. (The only optimistic contract signing scheme to date that does not have this property is mefficient, and is only abuse-free against an off-line attacker.) We give an efficient abuse-free optimistic contract-signing protocol based on ideas introduced for designated verifier proofs (i.e., proofs for which only a designated verifier can be convinced). Our basic solution is for two parties. We show that straightforward extensions to n > 2 party contracts do not work, and then show how to construct a three-party abuse-free optimistic contract-signing protocol An important technique we introduce is a type of signature we call a private contract signature Roughly, these are designated verifier signatures that can be converted into universally-verifiable siguatures by either the signing party or a trnsted third party appointed by the signing party, whose identity and power to convert can be verified (without interaction) by the party who is the designated verifier.", "authors": ["Juan A. Garay", "Markus Jakobsson", "Philip D. MacKenzie"], "n_citation": 0, "title": "Abuse-free optimistic contract signing", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "6442b411-da82-4d13-b0ee-8dc581523a0d"}
{"abstract": "In this paper, we propose a novel approach towards integrating the ideas behind Aspect-Oriented and Component-Based Software Development. Our approach aims at achieving a symmetric, unified component architecture that treats aspects and components as uniform entities. To this end, a novel component model is introduced that does not employ specialized aspect constructs for modularizing crosscutting concerns. Instead, an expressive configuration language is provided that allows to describe both regular and aspect-oriented interactions amongst components. This paper presents the ongoing FuseJ research, a first experiment for realizing this symmetric and unified aspect/component architecture.", "authors": ["Davy Suv\u00e9e", "Bruno De Fraine", "Wim Vanderperren"], "n_citation": 50, "title": "A Symmetric and Unified Approach Towards Combining Aspect-Oriented and Component-Based Software Development", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "64876327-90fd-4589-bec2-0a87a169db17"}
{"abstract": "The concept of Personal Information Management (PIM) is currently a hot topic of research. Some of the ideas being discussed under this topic have a long history, and they are relevant to the work of intelligence analysts. Hence, the intelligence community should pay attention to the developments in this area. In this position paper we examine the concept of PIM, point out some issues that are relevant to intelligence work, and discuss some areas of research that PIMs should address in order to be even more relevant to the Intelligence Community.", "authors": ["Antonio Badia"], "n_citation": 0, "title": "Personal information management (PIM) for intelligence analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "65999fde-8225-4860-8138-3eca0a8f6e8f"}
{"abstract": "There is a conceptual gap between the way we currently articulate requirements and the reuse-driven paradigm embodied in component-based system development. The principal challenge in requirements engineering for component-based systems is to develop models and methods that allow us make the best use of the available component technology by balancing aspects of requirements and business concerns, with the architectural assumptions and capabilities embodied in blackbox software components. This paper proposes a method for requirements engineering based on the notion of viewpoints that provides an explicit framework for expressing component-based system requirements from initial formulation through to detailed specification.", "authors": ["Gerald Kotonya", "John Edward Hutchinson"], "n_citation": 0, "title": "Viewpoints for specifying component-based systems", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "65b2568c-f2be-42d1-a44a-1fb196dfb88a"}
{"abstract": "This paper provides an overview and comparison of traditional and advanced Link-to-System (L2S) interface mappings used in system level simulations to evaluate link performance in terms of packet error rate (PER). An Actual Value Interface (AVI) as traditional L2S interface is compared with an Effective SINR Mapping (ESM) as advanced L2S interface. This comparison is able to highlight the main differences in prediction of multi-state channel performance effecting on final data throughput, delay of data packets and required CPU time. Furthermore, this paper proposes an improved version of AVI method in order to improve the accuracy of system performance estimation. The comparison results show that in case of multi-state OFDM system, highest accuracy is achieved with Mutual Information ESM (MIESM) method and relatively good accuracy with the exponential ESM (EESM) method.", "authors": ["Martti Moisio", "Alexandra Oborina"], "n_citation": 0, "title": "Comparison of effective SINR mapping with traditional AVI approach for modeling packet error rate in multi-state channel", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6768cc73-d616-467a-9a20-3b4ea04a9175"}
{"abstract": "Low-rate TCP-targeted Denial-of-Service (DoS) attack (shrew) is a new kind of DoS attack which is based on TCP's Retransmission Timeout (RTO) mechanism and can severely reduce the throughput of TCP traffic on victim. The paper proposes a novel mechanism which consists of effective detection and response methods. Through analyzing sampled attack traffic, we find that there is a stable difference between attack and legitimate traffic in frequency field, especially in low frequency. We use Sum of Low Frequency Power spectrum (SLFP) for detection. In our algorithm the destination IP address is used as flow label and SLFP is applied to every flow traversing edge router. If shrew is found, all flows to the destination are processed by Aggregated Flows Balance (AFB) at a proper upstream router. Simulation shows that attack traffics are restrained and TCP traffics can obtain enough bandwidth. The result indicates that our mechanism is effective and deployable.", "authors": ["Wei Wei", "Yabo Dong", "Dongming Lu", "Guang Jin", "Honglan Lao"], "n_citation": 50, "title": "A novel mechanism to defend against low-rate denial-of-service attacks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "679ac982-5e74-4f18-9330-f60c1cf2f954"}
{"abstract": "Increased industrialization and new markets have led to an accumulation of used technical consumer goods, which results in greater exploitation of raw materials, energy and landfill sites. In order to reduce the use of natural resources conserve precious energy and limit the increase in waste volume. The linear progression from production through consummation and finally to landfill sites must be stopped either by the product reuse, the parts reuse or the material recycling. The application of disassembly techniques is the first step towards this prevention of waste. These techniques form a reliable and clean approach: noble or high-graded recycling. This paper presents a multi agent system for disassembly process, which is implemented in a computer-aided application for supervising of the disassembling system: the Interactive Intelligent Interface for Disassembling System.", "authors": ["Ales Pavliska", "Vilem Srovnal"], "n_citation": 0, "title": "Robot disassembly process using multi-agent system", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "68eb97ed-fc02-4d3e-af2e-12a6aa79edf6"}
{"abstract": "We study the security of partial-domain hash signature schemes, in which the output size of the hash function is only a fraction of the modulus size. We show that for e = 2 (Rabin), partial-domain hash signature schemes are provably secure in the random oracle model, if the output size of the hash function is larger than 2/3 of the modulus size. This provides a security proof for a variant of the signature standards ISO 9796-2 and PKCS#1 v1.5, in which a larger digest size is used.", "authors": ["Jean-S\u00e9bastien Coron"], "n_citation": 50, "title": "Security proof for partial-domain hash signature schemes", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "69ae3e7b-a2a1-427f-9934-f8701ce7938d"}
{"abstract": "In many situations conversations involve more than two parties. However, most research on communication modelling in e.g. multi-agent systems limits itself to conversations between two parties at a time. Very little research has been done yet on modelling multi-party dialogues. In this paper we first explore the differences between two party and multi-party dialogues and we indicate a number of issues that arise when considering dialogues between more than two parties. Then we take some steps towards creating a testbed in which these issues can be explored and theory on multi-party dialogues can be developed.", "authors": ["Frank Dignum", "Gerard Vreeswijk"], "n_citation": 50, "title": "Towards a testbed for multi-party dialogues", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "6d006e9f-537e-4d4f-bfea-cca2c27c659d"}
{"abstract": "In this paper, we consider a new class of unconditionally secure authentication codes, called linear authentication code (or linear A-code). We show that a linear A-code can be characterised by a family of subspaces of a vector space over a finite field. We then derive an upper bound on the size of source space when other parameters of the systems, that is the size of the key space and the authenticator space, and the deception probability, are fixed. We give constructions that are asymptotically close to the bound and show application of these codes in constructing distributed authentication systems.", "authors": ["Rei Safavi-Naini", "Huaxiong Wang", "Chaoping Xing"], "n_citation": 0, "title": "Linear authentication codes: Bounds and constructions", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "6f37426f-0f62-43a8-a0c4-5f7627e55a61"}
{"abstract": "While the study of terrorism has expanded dramatically since the 1970s, most analyses are limited to qualitative case studies or quantitative analyses of international incidents only-which comprise a very small proportion of all terrorist events. Until now, empirical data on both domestic and international terrorist events have not existed. We have compiled information from more than 69,000 terrorism global incidents from 1970 to 1997. Most of these data were originally collected from a private intelligence service agency using open-source data. Since we completed coding the original data in 2005, we have been continually updating and validating the data and we now call the Global Terrorism Database (GTD). We begin this paper by describing the data collection efforts and the strengths and weaknesses of relying on open-source data. We then summarize completed research projects and end by listing on-going efforts to better understand the dynamics of worldwide terrorism events.", "authors": ["Laura Dugan", "Gary LaFree", "Heather Fogg"], "n_citation": 0, "title": "A first look at domestic and international global terrorism events, 1970-1997", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "6fe103c9-8b53-4550-a55d-b9d2e691c945"}
{"abstract": "Given any weak pseudorandom function, we present a general and efficient technique transforming such a function to a new weak pseudorandom function with an arbitrary length output. This implies, among other things, an encryption mode for block ciphers. The mode is as efficient as known (and widely used) encryption modes as CBC mode and counter (CTR) mode, but is provably secure against chosen-plaintext attack (CPA) already if the underlying symmetric cipher is secure against known-plaintext attack (KPA). We prove that CBC, CTR and Jutla's integrity aware modes do not have this property. In particular, we prove that when using a KPA secure block cipher, then: CBC mode is KPA secure, but need not be CPA secure, Jutla's modes need not be CPA secure, and CTR mode need not be even KPA secure. The analysis is done in a concrete security framework.", "authors": ["Ivan Damg\u00e5rd", "Jesper Buus Nielsen"], "n_citation": 0, "title": "Expanding pseudorandom functions; or: From known-plaintext security to chosen-plaintext security", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "711ced22-c7b1-463c-8cab-d5b6c3e06fd0"}
{"authors": ["Lyle Noakes", "Ryszard Kozera", "Reinhard Klette"], "n_citation": 0, "title": "Length Estimation for Curves with epsilon-Uniform Sampling", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "71ca4db7-8558-486a-8f2d-0071e23305e5"}
{"abstract": "A unique signature scheme has the property that a signature \u03c3 PK (m) is a (hard-to-compute) function of the public key PK and message m, for all, even adversarially chosen, PK. Unique signatures, introduced by Goldwasser and Ostrovsky, have been shown to be a building block for constructing verifiable random functions. Another useful property of unique signatures is that they are stateless: the signer does not need to update his secret key after an invocation. The only previously known construction of a unique signature in the plain model was based on the RSA assumption. The only other previously known provably secure constructions of stateless signatures were based on the Strong RSA assumption. Here, we give a construction of a unique signature scheme based on a generalization of the Diffie-Hellman assumption in groups where decisional Diffie-Hellman is easy. Several recent results suggest plausibility of such groups. We also give a few related constructions of verifiable random functions (VRFs). VRFs, introduced by Micali, Rabin, and Vadhan, are objects that combine the properties of pseudorandom functions (i.e. indistinguishability from random even after querying) with the verifiability property. Prior to our work, VRFs were only known to exist under the RSA assumption.", "authors": ["Anna Lysyanskaya"], "n_citation": 0, "title": "Unique signatures and verifiable random functions from the DH-DDH separation", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "73ab6822-f487-45a3-906c-e1230a5cffb5"}
{"abstract": "In this paper we present a first-order cost model that describes the costs associated with developing products in a product line organization. The model addresses a number of issues that we present as a set of scenarios. The goal of this work is to develop models of varying granularity that support a manager's decision-making needs at a variety of levels. The basis of these models is the relationships among the artifacts of the product line.", "authors": ["G\u00fcnter B\u00f6ckle", "Paul C. Clements", "John D. McGregor", "Dirk Muthig", "Klaus Schmid"], "n_citation": 0, "title": "A cost model for software product lines", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "74051266-7c22-4494-80a7-1d6f42aac5ac"}
{"abstract": "Most humans cannot detect lies at a rate better than chance. Alternative methods of deception detection may increase accuracy, but are intrusive, do not offer immediate feedback, or may not be useful in all situations. Automated classification methods have been suggested as an alternative to address these issues, but few studies have tested their utility with real-world, high-stakes statements. The current paper reports preliminary results from classification of actual security police investigations collected under high stakes and proposes stages for conducting future analyses.", "authors": ["Christie M. Fuller", "David P. Biros", "Mark Adkins", "Judee K. Burgoon", "Jay F. Nunamaker", "Steven Coulon"], "n_citation": 0, "title": "Detecting deception in person-of-interest statements", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "745ed056-ec66-43fb-8812-c13d086c96a6"}
{"abstract": "Thanks to a new upper bound, we study more precisely the nonlinearities of Maiorana-McFarland's resilient functions. We characterize those functions with optimum nonlinearities and we give examples of functions with high nonlinearities. But these functions have a peculiarity which makes them potentially cryptographically weak. We study a natural super-class of Maiorana-McFarland's class whose elements do not have the same drawback and we give examples of such functions achieving high nonlinearities.", "authors": ["Claude Carlet"], "n_citation": 0, "title": "A larger class of cryptographic boolean functions via a study of the maiorana-mcfarland construction", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "758a582b-98a0-4aeb-997e-5c04aa6b8cb4"}
{"abstract": "Euler diagrams are an effective and intuitive way of representing relationships between sets. As the number of sets represented grows, Euler diagrams can become 'cluttered' and lose some of their intuitive appeal. In this paper we consider various measures of 'clutter' for Euler diagrams and show that they compare well with results obtained from an empirical study. We also show that all abstract Euler diagrams can be constructed inductively by inserting a contour at a time and we relate this inductive description to the clutter metrics.", "authors": ["Chris John", "Andrew Fish", "John Howse", "John Taylor"], "n_citation": 0, "title": "Exploring the notion of 'clutter' in euler diagrams", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7671fedc-ea9a-4723-9660-d387083b423a"}
{"abstract": "In island based genetic algorithms, the population is splitted into subpopulations which evolve independently and ocasionally communicate by sending some individuals. This way, several zones of the landscape are explored in parallel and solutions with different features can be discovered. The interchange of information is a key point for the performance of these algorithms, since the combination of those solutions usually produces better ones. In this work, it is proposed a method based in path relinking which makes the combination process more effective.", "authors": ["Luis delaOssa", "Jos\u00e9 A. G\u00e1mez", "Jos\u00e9 Miguel Puerta"], "n_citation": 0, "title": "Improvement in the performance of island based genetic algorithms through path relinking", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "774e9d55-e72b-4536-8b58-676c3f5ff074"}
{"authors": ["Bart Demoen", "Konstantinos F. Sagonas"], "n_citation": 50, "title": "Heap garbage collection in XSB: Practice and experience", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "77599abf-3e9a-4389-aa7d-286a097c5399"}
{"abstract": "In [1], Bernstein proposed a circuit-based implementation of the matrix step of the number field sieve factorization algorithm. These circuits offer an asymptotic cost reduction under the measure construction cost x run time. We evaluate the cost of these circuits, in agreement with [1], but argue that compared to previously known methods these circuits can factor integers that are 1.17 times larger, rather than 3.01 as claimed (and even this, only under the non-standard cost measure). We also propose an improved circuit design based on a new mesh routing algorithm, and show that for factorization of 1024-bit integers the matrix step can, under an optimistic assumption about the matrix size, be completed within a day by a device that costs a few thousand dollars. We conclude that from a practical standpoint, the security of RSA relies exclusively on the hardness of the relation collection step of the number field sieve.", "authors": ["Arjen K. Lenstra", "Adi Shamir", "Jim Tomlinson", "Eran Tromer"], "n_citation": 0, "title": "Analysis of Bernstein's factorization circuit", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7848aa78-a789-454d-8b78-dfba2b1f8ea7"}
{"abstract": "In the Internet era, enterprises want to use personal information of their own or other enterprises' subscribers, and even provide it to other enterprises for their profit. On the other hand, subscribers to Internet enterprises expect their privacy to be securely protected. Therefore, a conflict between enterprises and subscribers can arise in using personal information for the enterprises' benefits. In this paper, we introduce a privacy policy model and propose a policy-based privacy authorization system. The privacy policy model is used for authoring privacy policies and the privacy authorization system renders the authorization decision based on the privacy policies. In the proposed system, policies for enterprises and subscribers are described in XACML, an XML-based OASIS standard language for access control policies. In addition, we show the details of how the procedure of the privacy authorization and conflict resolution is processed in the proposed system.", "authors": ["Hyang-Chang Choi", "Seungyong Lee", "HyungHyo Lee"], "n_citation": 50, "title": "Design and implementation of a policy-based privacy authorization system", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "794d66ac-bf6b-4521-851e-3dc36e3b8573"}
{"abstract": "We initiate a study of on-line ciphers. These are ciphers that can take input plaintexts of large and varying lengths and will output the ith block of the ciphertext after having processed only the first i blocks of the plaintext. Such ciphers permit length-preserving encryption of a data stream with only a single pass through the data. We provide security definitions for this primitive and study its basic properties. We then provide attacks on some possible candidates, including CBC with fixed IV. Finally we provide a construction called HCBC which is based on a given block cipher E and a family of AXU functions. HCBC is proven secure against chosen-plaintext attacks assuming that E is a PRP secure against chosen-plaintext attacks.", "authors": ["Mihir Bellare", "Alexandra Boldyreva", "Lars R. Knudsen", "Chanathip Namprempre"], "n_citation": 0, "title": "Online ciphers and the hash-CBC construction", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "7a6bd2ad-d909-4970-8e49-6fb56ac43335"}
{"abstract": "We construct a supersingular implementation of the Elliptic Curve Digital Signature Algorithm (ECDSA) that is essentially equivalent to a finite field implementation of the Digital Signature Algorithm (DSA), and then we compare the efficiency of the two systems. The elliptic curve method is about 12 times faster. In the last section we use the same ideas to give a particularly efficient nonsupersingular implementation of elliptic curve cryptography in characteristic 7.", "authors": ["Neal Koblitz"], "n_citation": 0, "title": "An elliptic curve implementation of the finite field Digital Signature Algorithm", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "7ab22f77-98d2-4c77-8783-5a43e18d99f4"}
{"authors": ["Yong Ho Song", "Jung Min Park", "Yongsu Park", "Eul Gyu Im"], "n_citation": 0, "title": "A new methodology of analyzing security vulnerability for network services", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ad18ff0-4e6d-48e4-abad-b9234ce59768"}
{"abstract": "We present a generative model for constructing continuous word representations using mixtures of probabilistic PCAs. Applied to co-occurrence data, the model performs word clustering and allows the visualization of each cluster in a reduced space. In combination with a simple document model, it permits the definition of low-dimensional Fisher scores which are used as document features. We investigate the models' potential through kernel-based methods using the corresponding Fisher kernels.", "authors": ["George Siolas", "Florence d'Alch\u00e9-Buc"], "n_citation": 0, "title": "Mixtures of probabilistic PCAs and Fisher kernels for word and document modeling", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "7b26587a-44ce-428a-afd5-01ce84664b08"}
{"abstract": "We present a solution to both the robust threshold RSA and proactive RSA problems. Our solutions are conceptually simple, and allow for an easy design of the system. The signing key, in our solution, is shared at all times in additive form, which allows for simple signing and for a particularly efficient and straightforward refreshing process for proactivization. The key size is (up to a very small constant) the size of the RSA modulus, and the protocol runs in constant time, even when faults occur, unlike previous protocols where either the size of the key has a linear blow-up (at best) in the number of players or the run time of the protocol is linear in the number of faults. The protocol is optimal in its resilience as it can tolerate a minority of faulty players. Furthermore, unlike previous solutions, the existence and availability of the key throughout the lifetime of the system, is guaranteed without probability of error. These results are derived from a new general technique for transforming distributed computations for which there is a known n-out-n solution into threshold and robust computations.", "authors": ["Tal Rabin"], "n_citation": 0, "title": "A simplified approach to threshold and proactive RSA", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "7c3abedd-b3c4-434c-ae83-ab31e8d0a3be"}
{"abstract": "The charging predicament in communication networks is complex and multifaceted because it involves issues such as customer requirements whilst ensuring compatibility with existing and planned protocols and a diverse set of desired services. Therefore, it is essential to understand and evaluate the usefulness of available and emerging new techniques that provide improvement and extend the capability of charging schemes in order to reduce congestion, ensure and deliver quality of service to the users who need it most and hence achieve the most efficient consumption of resources available. This paper describes a charging simulation model based on Opnet UMTS model for the dimensioning of Charging Data Record (CDR) generation process by analysing the real network load, to estimate how many CDRs are generated at various points in the network and to realise optimisation techniques to assess the reasonable time duration for trigger settings and optimal rate of CDR generation.", "authors": ["Farah Khan", "Nigel Baker"], "n_citation": 50, "title": "Optimization of charging control in 3G mobile networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "7ce3c770-360d-42fb-ba0d-b18d1ed49498"}
{"abstract": "The Parameterized Model Checking Problem (PMCP) is to determine whether a temporal property is true for every size instance of a system comprised of many homogenous processes. Unfortunately, it is undecidable in general. We are able to establish, nonetheless, decidability of the PMCP in quite a broad framework. We consider asynchronous systems comprised of an arbitrary number of homogeneous copies of a generic process template. The process template is represented as a synchronization skeleton while correctness properties are expressed using Indexed CTL * \\X. We reduce model checking for systems of arbitrary size n to model checking for systems of size up to (of) a small cutoff size c. This establishes decidability of PMCP as it is only necessary to model check a finite number of relatively small systems. Efficient decidability can be obtained in some cases. The results generalize to systems comprised of multiple heterogeneous classes of processes, where each class is instantiated by many homogenous copies of the class template (e.g., m readers and n writers).", "authors": ["E.A. Emerson", "Vineet Kahlon"], "n_citation": 0, "title": "Reducing model checking of the many to the few", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "7db3eb40-6955-4e8a-be7f-cb2a1a841d9f"}
{"abstract": "XTR cryptosystem makes use of an irreducible polynomial F(c,x) = x 3  - cx 2  + c p x - 1 over a finite field F p2 . In this paper, we develop a new method to generate such an irreducible polynomial. Our method requires only computations of Jacobi symbols and thus improves those given [1], [2] and [3].", "authors": ["Jae Moon Kim", "Ikkwon Yie", "Seung Ik Oh", "Hyung Don Kim", "Jado Ryu"], "n_citation": 0, "title": "Fast generation of cubic irreducible polynomials for XTR", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "7f17a34c-d4f1-4d5f-a282-6bcfd9dc23ab"}
{"abstract": "In this paper, we propose an algorithm for detecting a face in a target image using sketch operators and vertical facial symmetry (VFS). The former are operators which effectively reflect perceptual characteristics of human visual system to compute sketchiness of pixels and the latter means the bilateral symmetry which a face shows about its central longitudinal axis. In the proposed algorithm, horizontal and vertical sketch images are first obtained from a target image by using a directional BDIP (block difference inverse probabilities) operator which is modified from the BDIP operator. The pair of sketch images is next transformed into a generalized symmetry magnitude (GSM) image by the generalized symmetry transform (GST). From the GSM image, face candidates are then extracted which are quadrangular regions enclosing the triangles that satisfy eyes-mouth triangle (EMT) conditions and VFS. The sketch image for each candidate is obtained by the BDIP operator and classified into a face or nonface by the Bayesian classifier. Among the face candidates classified into faces, one with the largest VFS becomes the output where the EMT gives the location of two eyes and a mouth of a target face. If the procedure detects no face, then it is executed again after illumination compensation on the target image. Experimental results for 1,000 320x240 target images of various backgrounds and circumstances show that the proposed method yields about 97% detection rate and takes a time less than 0.25 second per target image.", "authors": ["Hyun Joo So", "Mi Hye Kim", "Yun Su Chung", "Nam Chul Kim"], "n_citation": 0, "title": "Face detection using sketch operators and vertical symmetry", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "80a09826-e22c-4ee8-ab50-278051148f84"}
{"abstract": "Keyword filtering is a commonly used way to select, from a set of intercepted messages, those that need further scrutiny. An obvious countermeasure is to replace words that might be on a keyword list by others. We show that this strategy itself creates a signature in the altered messages that makes them readily detectable using several forms of matrix decomposition. Not only can unusual messages be detected, but sets of related messages can be detected as conversations, even when their endpoints have been obscured (by using transient email addresses, stolen cell phones and so on).", "authors": ["David B. Skillicorn"], "n_citation": 0, "title": "Beyond keyword filtering for message and conversation detection", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "80c63dd6-d548-4ed6-ba9e-3accc855428a"}
{"abstract": "The notion of on-line/off-line signature schemes was introduced in 1990 by Even, Goldreich and Micali. They presented a general method for converting any signature scheme into an on-line/off-line signature scheme, but their method is not very practical as it increases the length of each signature by a quadratic factor. In this paper we use the recently introduced notion of a trapdoor hash function to develop a new paradigm called hash-sign-switch, which can convert any signature scheme into a highly efficient on-line/off-line signature scheme: In its recommended implementation, the on-line complexity is equivalent to about 0.1 modular multiplications, and the size of each signature increases only by a factor of two. In addition, the new paradigm enhances the security of the original signature scheme since it is only used to sign random strings chosen off-line by the signer. This makes the converted scheme secure against adaptive chosen message attacks even if the original scheme is secure only against generic chosen message attacks or against random message attacks.", "authors": ["Adi Shamir", "Yael Tauman"], "n_citation": 306, "title": "Improved online/offline signature schemes", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "810fae06-1fb3-4eaa-b4a6-d9f8a977f0b1"}
{"abstract": "In 1998, Boneh, Durfee and Frankel [4] presented several attacks on RSA when an adversary knows a fraction of the secret key bits. The motivation for these so-called partial key exposure attacks mainly arises from the study of side-channel attacks on RSA. With side channel attacks an adversary gets either most significant or least significant bits of the secret key. The polynomial time algorithms given in [4] only work provided that the public key e is smaller than N. It was raised as an open question whether there are polynomial time attacks beyond this bound. We answer this open question in the present work both in the case of most and least significant bits. Our algorithms make use of Coppersmith's heuristic method for solving modular multivariate polynomial equations [8]. For known most significant bits, we provide an algorithm that works for public exponents e in the interval [N, N 0.725 ]. Surprisingly, we get an even stronger result for known least significant bits: An algorithm that works for all e < N. We also provide partial key exposure attacks on fast RSA-variants that use Chinese Remaindering in the decryption process (e.g. [20,21]). These fast variants are interesting for time-critical applications like smart-cards which in turn are highly vulnerable to side-channel attacks. The new attacks are provable. We show that for small public exponent RSA half of the bits of dp = d mod p- 1 suffice to find the factorization of N in polynomial time. This amount is only a quarter of the bits of N and therefore the method belongs to the strongest known partial key exposure attacks.", "authors": ["Johannes Bl\u00f6mer", "Alexander May"], "n_citation": 0, "title": "New partial key exposure attacks on RSA", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "821b16d8-7c18-4e92-847e-ec61bfea9b8b"}
{"abstract": "This work investigates the accuracy and efficiency tradeoffs between centralized and collective (distributed) algorithms for (i) sampling, and (ii) n-way data analysis techniques in multidimensional stream data, such as Internet chatroom communications. Its contributions are threefold. First, we use the Kolmogorov-Smirnov goodness-of-fit test to show that statistical differences between real data obtained by collective sampling in time dimension from multiple servers and that of obtained from a single server are insignificant. Second, we show using the real data that collective data analysis of 3-way data arrays (users x keywords x time) known as high order tensors is more efficient than centralized algorithms with respect to both space and computational cost. Furthermore, we show that this gain is obtained without loss of accuracy. Third, we examine the sensitivity of collective constructions and analysis of high order data tensors to the choice of server selection and sampling window size. We construct 4-way tensors (users x keywords x time x servers) and analyze them to show the impact of server and window size selections on the results.", "authors": ["Evrim Acar", "Seyit A. Camtepe", "B\u00fclent Yener"], "n_citation": 0, "title": "Collective sampling and analysis of high order tensors for chatroom communications", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "829f19d2-0597-48fe-8f2c-bae739352fa3"}
{"abstract": "Canetti and Fischlin have recently proposed the security notion universal composability for commitment schemes and provided two examples. This new notion is very strong. It guarantees that security is maintained even when an unbounded number of copies of the scheme are running concurrently, also it guarantees non-malleability and security against adaptive adversaries. Both proposed schemes use \u03b8(k) bits to commit to one bit and can be based on the existence of trapdoor commitments and non-malleable encryption. We present new universally composable commitment (UCC) schemes based on extractable q one-way homomorphisms. These in turn exist based on the Paillier cryptosystem, the Okamoto-Uchiyama cryptosystem, or the DDH assumption. The schemes are efficient: to commit to k bits, they use a constant number of modular exponentiations and communicates O(k) bits. Furthermore the scheme can be instantiated in either perfectly hiding or perfectly binding versions. These are the first schemes to show that constant expansion factor, perfect hiding, and perfect binding can be obtained for universally composable commitments. We also show how the schemes can be applied to do efficient zero-knowledge proofs of knowledge that are universally composable.", "authors": ["Ivan Damg\u00e5rd", "Jesper Buus Nielsen"], "n_citation": 0, "title": "Perfect hiding and perfect binding universally composable commitment schemes with constant expansion factor", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "83b617bc-7572-4a04-adb7-72f3792ae7c6"}
{"abstract": "This paper describes the experience of building component-oriented applications with a framework that supports run-time adaptation in response to the dynamic availability of functionality provided by constituent components. The framework's approach is to define a service-oriented component model, which is a component model that includes concepts from service orientation and an execution environment that provides automatic adaptation mechanisms. This paper focuses on an example scenario and two real-world examples where this framework has been used.", "authors": ["Humberto Cervantes", "Richard S. Hall"], "n_citation": 0, "title": "A framework for constructing adaptive component-based applications: Concepts and experiences", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "83b72b1d-74b5-418e-967b-bf45e686a520"}
{"abstract": "This article presents an algorithm for finding and visualizing the shortest route between two points on a gray-level height map. The route is computed using gray-level distance transforms, which are variations of the Distance Transform on Curved Space (DTOCS). The basic Route DTOCS uses the chessboard kernel for calculating the distances between neighboring pixels, but variations, which take into account the larger distance between diagonal pixels, produce more accurate results, particularly for smooth and simple image surfaces. The route opimization algorithm is implemented using the Weighted Distance Transform on Curved Space (WDTOCS), which computes the piecewise Euclidean distance along the image surface, and the results are compared to the original Route DTOCS. The implementation of the algorithm is very simple, regardless of which distance definition is used.", "authors": ["Leena Ikonen", "Pekka Toivanen"], "n_citation": 0, "title": "Shortest Route on height map using gray-level Distance transforms", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "83c0d672-95ab-4c85-8013-ff898d422002"}
{"abstract": "This paper deals with mobility protocols specification, validation and testing using a formal approach. A well suited SDL model is proposed to handle the component-based nature of Mobile systems. Two solutions are proposed to derive automatically TTCN-3 test cases from the SDL model.", "authors": ["Francine Ngani Noudem", "C\u00e9sar Viho"], "n_citation": 0, "title": "Modeling, verifying and testing mobility protocol from SDL language", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8521a258-9699-4887-ad6d-f0425b02cdf3"}
{"abstract": "A new method for spherical object reconstruction based on deformation of star-shaped simplex meshes has been developed in our laboratory and published recently. The method can handle volumetric as well as three-dimensional range data and is easy to use and relatively fast. The method, however, can yield wrong results for sparse data. The goal of this paper is to describe a modification of the method that is suitable also for sparse data. The performance of the proposed modification is demonstrated on real biomedical data.", "authors": ["Pavel Matula", "David Svoboda"], "n_citation": 0, "title": "Spherical object reconstruction using simplex meshes from sparse data", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "85ca5af7-deb8-4915-b4ef-909df4a68147"}
{"abstract": "In this paper we will compare two signature schemes proposed by different sets of authors. One is the XTR-Nyberg-Rueppel signature proposed by A.K. Lenstra and E.R. Verheul in [3] and the other is the signature scheme proposed by C.H. Tan, X. Yi and C.K. Siew (We will call it TYS signature.) in [9]. XTR-NR signature uses the third degree trace projection Tr: GF(p 6 ) \u2192 GF(p 2 ) and has been generalized in [8] by Lim et, al. as a scheme in GF(p 6m ) using Tr: GF(p 6m ) \u2192 GF(p 2m ). On the other hand, TYS signature is based on a third order LFSR. Tan et, al. claimed that TYS signature is as secure as Schnorr signature scheme. We will explain why these two schemes are essentially the same. In addition, we will point out that TYS signature as it is has some flaws in their arguments. We will show that in order to cure the flaws of TYS signature, one should bring in exactly the same security and efficiency consideration of XTR scheme as in [8].", "authors": ["Seongan Lim", "Seungjoo Kim", "Ikkwon Yie", "Jaemoon Kim"], "n_citation": 0, "title": "Comments on: A signature scheme based on the third order LFSR proposed at ACISP2001", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "864eda29-dc9d-44b4-b4e1-ed300c409ab3"}
{"abstract": "We are concerned with a problem of checking the structure freeness of S +  for a given set S of DNA sequences. It is still open whether or not there exists an efficient algorithm for this problem. In this paper, we will give an efficient algorithm to check the structure freeness of S +  under the constraint that every sequence may form only linear secondary structures, which partially solves the open problem.", "authors": ["Satoshi Kobayashi", "Takashi Yokomori", "Yasubumi Sakakibara"], "n_citation": 0, "references": ["1974cca8-cb08-48bf-8b65-b5e123c877c9", "23f36239-ce95-4656-af91-cf35a709e395", "488346f3-05d1-4844-a1c7-c7bea41ef837", "78ae5f10-5293-4dc8-8f09-e1491caabec8", "833eca3a-1e8c-4e76-9664-3f43fc252b64", "9e0fe93d-c653-4eba-b784-9b4803045ac2"], "title": "An algorithm for testing structure freeness of biomolecular sequences", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "86a841c9-e24c-4167-b1f8-204a2142f86d"}
{"abstract": "Scanning traffic is the majority of worm traffic. Gaining deep insight into worm traffic can do much help in detecting worm hosts. The distributions of vectors related with First Contact Connections (FCC) of legitimate hosts and worm hosts are analyzed. The vectors are arrival interval, request size, response size, duration and RTT. Distributions of these vectors of worm traffic show abnormalities of the lack of heavy-tailed character, which is hold by that of legitimate traffic. Besides high probability of failed FCC, arrival interval and request size can be used as additional vectors.", "authors": ["Zhengtao Xiang", "Yufeng Chen", "Yabo Dong", "Honglan Lao"], "n_citation": 0, "title": "Analysis of abnormalities of worm traffic for obtaining worm detection vectors", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8750adc3-8100-41cc-99c0-99a594018157"}
{"abstract": "We present the SIGMA family of key-exchange protocols and the SIGn-and-MAc approach to authenticated Diffie-Hellman underlying its design. The SIGMA protocols provide perfect forward secrecy via a Diffie-Hellman exchange authenticated with digital signatures, and are specifically designed to ensure sound cryptographic key exchange while providing a variety of features and trade-offs required in practical scenarios (such as optional identity protection and reduced number of protocol rounds). As a consequence, the SIGMA protocols are very well suited for use in actual applications and for standardized key exchange. In particular, SIGMA serves as the cryptographic basis for the signature-based modes of the standardized Internet Key Exchange (IKE) protocol (versions 1 and 2). This paper describes the design rationale behind the SIGMA approach and protocpls, and points out to many subtleties surrounding the design of secure key-exchange protocols in general, and identity-protecting protocols in particular. We motivate the design of SIGMA by comparing it to other protocols, most notable the STS protocol and its variants. In particular, it is shown how SIGMA solves some of the security shortcomings found in previous protocols.", "authors": ["Hugo Krawczyk"], "n_citation": 0, "title": "SIGMA: The 'SIGn-and-MAc' approach to authenticated Diffie-Hellman and its use in the IKE protocols", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "87598e62-8961-49b4-b83c-2c125ba59a4f"}
{"abstract": "Reservoir Computing (RC) is a recent research axea, in which a untrained recurrent network of nodes is used for the recognition of temporal patterns. Contrary to Recurrent Neural Networks (RNN), where the weights of the connections between the nodes are trained, only a linear output layer is trained. We will introduce three different time-scales and show that the performance and computational complexity are highly dependent on these time-scales. This is demonstrated on an isolated spoken digits task.", "authors": ["Benjamin Schrauwen", "Jeroen Defour", "David Verstraeten", "Jan Van Campenhout"], "n_citation": 53, "title": "The introduction of time-scales in Reservoir Computing, applied to isolated digits recognition", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "87814201-b93d-40b2-a60c-3cb61061adf0"}
{"abstract": "Problems of bias in intelligence analysis may be reduced by the use of web-based cognitive aids. We introduce a framework spanning the entire collaborative thought process using the Angler and SEAS (Structured Evidential Argumentation System) applications. Angler encourages creative brainstorming while SEAS demands analytical reasoning. The dual nature of this approach suggests substantial benefits from using computer-mediated collaborative and structured reasoning tools for intelligence analysis and policymaking. Computer-mediated communication (CMC) may be impacted by many factors, including group dynamics and cultural and individual differences between participants. Based on empirical research, potential enhancements to Angler and SEAS are outlined, along with experiments to evaluate their worth. The proposed methodology may also be applied to assess the value of the suggested features to other such CMC tools.", "authors": ["Douglas Yeung", "John D. Lowrance"], "n_citation": 50, "title": "Computer-mediated collaborative reasoning and intelligence analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "882c1fb3-86d4-41f4-a293-e439c0d56f11"}
{"abstract": "The development of the stochastic approach, based on queueing models and simulation, to optimize of resources allocation, in distributed production networks (DPN), is examined in the paper. The task of performance optimization of DPN is formulated as a non-linear integer programming problem with stochastic parameters. In our previous works, a branch and bound algorithm based on sequential use of queueing models and simulation is proposed. The analytical models are used for Lower Bound determination and choice of the optimal branching direction. The simulation is used for validation of analytical results and for computation of Upper Bound and stop condition of the algorithm. The developed methods have been used for optimization of corporate computer and telecommunication networks, High-Tech assembly manufacturing, and a printing and publishing company. In this paper, we show a new model and some additional results of simulation study for validation of models used.", "authors": ["Oleg Zaikin", "Alexandre Dolgui", "Przemyslaw Korytkowski"], "n_citation": 0, "title": "Optimization of resource allocation in distributed production networks", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8865a284-cf1b-4f6e-8f8c-1119b50870b6"}
{"abstract": "The relational data model does not offer query language solutions for recursive applications. A missing semantic update operation is the fundamental reason for this failure. It is shown that a semantic data model can solve these applications using simple linear constructs for definition and manipulation, without explicit recursion, nesting, iteration or navigation. The new implementation technique is reliable and efficient for application in end user environments. It is based on semantic query specification and metadata processing. Experiments with the semantic Xplain DBMS have confirmed the linear time complexity for product database applications.", "authors": ["J. H. ter Bekke", "J. A. Bakker"], "n_citation": 0, "title": "Recursive queries in product databases", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "890f8cf0-3373-4959-b722-0932ed478242"}
{"abstract": "This paper presents a basic and global approach to design a signalling that is able to guarantee Quality of Service (QoS) in heterogeneous networks. The signalling protocol handles the requests of connections for end-to-end QoS paths between heterogeneous domains. The proposed approach is based on the use of the minimum information needed and on performance figures existing for each domain. It defines the end-to-end QoS path only as a set of inter-domain requirements between border routers; then in particular, this means that all internal domain paths have no implementation constraints and so may be implemented in the most efficient way by all domain providers and their different technology. The architecture shows how to build the sequence of the needed path equipments and of the ingress and egress routers to construct the end-to-end admission-controlled path.", "authors": ["Christophe Chassot", "Andr\u00e9 Lozes", "Florin Racaru", "M. Diaz"], "n_citation": 0, "title": "Signalling concepts in heterogeneous IP multi-domains networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8b58ca29-1770-4b42-8694-9b63f171c9c1"}
{"abstract": "This paper presents formal semantics for behavioural substitutability of agent components, and argues that the traditional approaches used in software multi-agent systems are not rigorous and have some limitations. We propose various substitutability relations based upon the preorder relations which are considered in the study of concurrent systems. Examples of interaction protocols such as the Contract-Net-Protocol are given to illustrate our approach.", "authors": ["Nabil Hameurlain"], "n_citation": 50, "title": "Formal semantics for behavioural substitutability of agent components: Application to interaction protocols", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8be9bee1-43c8-4d43-8836-7cc77a7bb51f"}
{"abstract": "Unconditionally secure multi-party computations in general, and broadcast in particular, are impossible if any third of the players can be actively corrupted and if no additional information-theoretic primitive is given. In this paper, we relativize this pessimistic result by showing that such a primitive can be as simple as noisy communication channels between the players or weakly correlated pieces of information. We consider the scenario where three players have access to random variables X, Y, and Z, respectively, and give the exact condition on the joint distribution P XYZ  under which unconditional broadcast is possible. More precisely, we show that this condition characterizes the possibility of realizing so-called pseudo-signatures between the players. As a consequence of our results, we can give conditions for the possibility of achieving unconditional broadcast between n players and any minority of cheaters and, hence, general multi-party computation under the same condition.", "authors": ["Matthias Fitzi", "Stefan Wolf", "Jiirg Wullschleger"], "n_citation": 0, "title": "Pseudo-signatures, broadcast, and multi-party computation from correlated randomness", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "8cf035fa-73fa-4578-9b95-92f5a836be50"}
{"abstract": "The focus of this paper is approaches to measuring similarity for application in connection with query evaluation. Rather than only comparing at the level of words the issue here is to compare concepts that appear as compound expressions derived from list of words through brief natural language analysis. Concepts refers to and are compared with respect to an ontology describing the domain of the database. We discuss three different principles for measuring similarity between concepts. One in the form of subsumption expansion of concepts and two as different measures of distance in a graphical representation of an ontology.", "authors": ["Henrik Bulskov", "Rasmus Knappe", "Troels Andreasen"], "n_citation": 0, "title": "On measuring similarity for conceptual querying", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "8dd9d382-0269-4b4c-befa-97b8fbfea69a"}
{"abstract": "Over the past four years there has been a marginal increase in research on developing advanced information technologies that can be efficiently used for national and international security in our war against terrorism. The list of wanted persons who are still free is getting larger, however, in most cases there is a database containing their face images and this can be used in the development of face recognition systems. A human face is an extremely complex object with features that can vary over time, sometimes very rapidly. This paper presents a fast intelligent face recognition system that uses essential face features averaging and a neural network to identify multi-expression faces. A real life application using this method is implemented on 180 images of 30 persons. Experimental results suggest that this simple but efficient system performs well, thus providing a fast intelligent system for recognizing faces with different expressions.", "authors": ["Adnan Khashman", "Akram A. Garad"], "n_citation": 0, "title": "Intelligent face recognition using feature averaging", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "8e3f5f8c-bcc7-4bb6-b513-459122716d09"}
{"abstract": "Intelligence analysts are flooded with massive amounts of data from a multitude of sources and in many formats. From this raw data they attempt to gain insight that will provide decision makers with the right information at the right time. Data quality varies from very high quality data generated by reputable sources to misleading and very low quality data generated by malicious entities. Disparate organizations and databases, global collection networks and international language differences further hamper the analyst's job. We present a web based information firewall to help counter these problems. It allows analysts to collaboratively customize web content by the creation and sharing of dynamic knowledge-based user interfaces that greatly improve data quality, and hence analyst effectiveness, through filtering, fusion and dynamic transformation techniques. Our results indicate that this approach is not only effective, but will scale to support large entities within the Intelligence Community.", "authors": ["Gregory J. Conti", "Mustaque Ahamad", "Robert Norback"], "n_citation": 0, "title": "Filtering, fusion and dynamic information presentation : Towards a general information firewall", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "8e7ab434-1567-434c-aff1-892642ee59c5"}
{"abstract": "Password-Authenticated Key Exchange (PAKE) protocols allow parties to share secret keys in an authentic manner based on an easily memorizable password. Byun et al. first proposed a cross realm client-to-client (C2C) PAKE for clients of different realms (with different trusted servers) to establish a key. Subsequent work includes some attacks and a few other variants either to resist existing attacks or to improve the efficiency. However, all these variants were designed with heuristic security analysis despite that well founded provable security models already exist for PAKEs, e.g. the Bellare-Pointcheval-Rogaway model. Recently, the first provably secure cross-realm C2C-PAKE protocols were independently proposed by Byun et al. and Yin-Bao, respectively; i.e. security is proven rigorously within a formally defined security model and based on the hardness of some computationally intractable assumptions. In this paper, we show that both protocols fall to undetectable online dictionary attacks by any adversary. Further we show that malicious servers can launch successful man-in-the-middle attacks on the variant by Byun et al., while the Yin-Bao variant inherits a weakness against unknown key-share attacks. Designing provably secure protocols is indeed the right approach, but our results show that such proofs should be interpreted with care.", "authors": ["Raphael C.-W. Phan", "Bok-Min Goi"], "n_citation": 0, "title": "Cryptanalysis of two provably secure cross-realm C2C-PAKE protocols", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "901b2bc4-e897-4f33-b777-70ea2bb01d0a"}
{"abstract": "Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. In this paper we investigate the use of sequences of system calls for classifying intrusions and faults induced by privileged processes in Unix Operating system. In our work we applied sequence-data mining approach in the context of intrusion detection system (IDS). This paper introduces a new similarity measure that considers both sequence as well as set similarity among sessions. Considering both order of occurrences as well as content in a session enhances the capabilities of kNN classifier significantly, especially in the context of intrusion detection. From our experiments on DARPA 1998 IDS dataset we infer that the order of occurrences plays a major role in determining the nature of the session. The objective of this work is to construct concise and accurate classifiers to detect anomalies based on sequence as well as set similarity.", "authors": ["Pradeep Kumar", "M. Venkateswara Rao", "P. Radha Krishna", "Raju S. Bapi", "Arijit Laha"], "n_citation": 0, "title": "Intrusion detection system using sequence and set preserving metric", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "902125cd-d5f7-45f1-9ba1-435d05ff5c5c"}
{"abstract": "In a broadcast encryption scheme, digital content is encrypted to ensure that only privileged users can recover the content from the encrypted broadcast. Key material is usually held in a tamper-resistant, replaceable, smartcard. A coalition of users may attack such a system by breaking their smartcards open, extracting the keys, and building pirate decoders based on the decryption keys they extract. In this paper we suggest the notion of long-lived broadcast encryption as a way of adapting broadcast encryption to the presence of pirate decoders and maintaining the security of broadcasts to privileged users while rendering all pirate decoders useless. When a pirate decoder is detected in a long-lived encryption scheme, the keys it contains are viewed as compromised and are no longer used for encrypting content. We provide both empirical and theoretical evidence indicating that there is a long-lived broadcast encryption scheme that achieves a steady state in which only a small fraction of cards need to be replaced in each epoch. That is, for any fraction \u03b2, the parameter values may be chosen in such a way to ensure that eventually, at most \u03b2 of the cards must be replaced in each epoch. Long-lived broadcast encryption schemes are a more comprehensive solution to piracy than traitor-tracing schemes, because the latter only seek to identify the makers of pirate decoders and don't deal with how to maintain secure broadcasts once keys have been compromised. In addition, long-lived schemes are a more efficient long-term solution than revocation schemes, because their primary goal is to minimize the amount of recarding that must be done in the long term.", "authors": ["Juan A. Garay", "Jessica Staddon", "Avishai Wool"], "n_citation": 184, "title": "Long-lived broadcast encryption", "venue": "Lecture Notes in Computer Science", "year": 2000, "id": "905daad3-ccb6-470e-9bdf-e4753e9c0083"}
{"authors": ["Peter Hegarty", "Carmen Buechel", "Simon Ungar"], "n_citation": 0, "title": "Androcentric preferences for visuospatial representations of gender differences", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "908f92f5-3454-4bad-943f-86d88efdb7ec"}
{"abstract": "We analyze the Gaudry-Hess-Smart (GHS) Weil descent attack on the elliptic curve discrete logarithm problem (ECDLP) for elliptic curves defined over characteristic two finite fields of composite extension degree. For each such field F 2N , N \u2208 [160, 600], we identify elliptic curve parameters such that (i) there should exist a cryptographically interesting elliptic curve E over F 2N  with these parameters; and (ii) the GHS attack is more efficient for solving the ECDLP in E(F 2N ) than for any other cryptographically interesting elliptic curve over F 2N .", "authors": ["Markus Maurer", "Alfred Menezes", "Edlyn Teske"], "n_citation": 0, "title": "Analysis of the GHS Weil descent attack on the ECDLP over characteristic two finite fields of composite degree", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "90ba1a1a-b852-45e5-9471-3b47183fcb49"}
{"abstract": "This paper discusses two kinds of search strategies supported by the Flexible Annotation Service Tool (FAST), an annotation service that can be used by different Digital Library Management Systems (DLMSs). The first strategy concerns the search and retrieval of annotations, considered as stand-alone documents; while, the second one regards how to exploit annotations in order to search and retrieve annotated documents which are relevant for a user query. This paper describes the proposed search strategies in the light of the architectural design choices needed to support them.", "authors": ["Maristella Agosti", "Nicola Ferro"], "n_citation": 50, "title": "Search strategies for finding annotations and annotated documents : The FAST service", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "90dd6a8e-6967-4721-9242-eee3f24033c7"}
{"authors": ["Abubakar Hassan", "Ian Mackie", "Jorge Sousa Pinto"], "n_citation": 0, "title": "Visual programming with Interaction Nets", "venue": "Lecture Notes in Computer Science", "year": 2008, "id": "910fe0da-e15a-4f30-a1cd-14780205a198"}
{"authors": ["Nazli Goharian", "Alana Platt", "Ophir Frieder"], "n_citation": 0, "title": "On off-topic web browsing", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9110f726-5413-4471-82ee-1b75c6d6d7a1"}
{"abstract": "Optimal Asymmetric Encryption Padding (OAEP) is a technique for converting the RSA trapdoor permutation into a chosen ciphertext secure system in the random oracle model. OAEP padding can be viewed as two rounds of a Feistel network. We show that for the Rabin and RSA trapdoor functions a much simpler padding scheme is sufficient for chosen ciphertext security in the random oracle model. We show that only one round of a Feistel network is sufficient. The proof of security uses the algebraic properties of the RSA and Rabin functions.", "authors": ["Dan Boneh"], "n_citation": 0, "title": "Simplified OAEP for the RSA and Rabin functions", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "918cf45a-bd62-4b60-a55a-320f2f800a2d"}
{"abstract": "In this paper, we propose a reasoning model for extracting collective behaviours in Multi-Agent Systems (MAS) from regularities in interaction streams. Using reflexive structures to describe the actions and knowledge of the system, called views, we build chronicles that catch and register the actual situations related to events and actions occurrences. These chronicles are then intensionalised in order to extract regularities in a single autonomous agent's behaviour, thus defining a local behaviour. We then propose to extend this model for extracting collective behaviours in MAS using machine learning algorithms. Finally, we try to show that this dynamic analysis of an agent's runtime can lead to organisations in MAS.", "authors": ["Nicolas Sabouret", "Jean-Paul Sansonnet"], "n_citation": 0, "title": "Learning collective behaviour from local interactions", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "91ec87dc-6474-4500-856f-8eb210e4bb69"}
{"abstract": "Previously, an unbiased estimator of the generalization error called the subspace information criterion (SIC) was proposed for a finite dimensional reproducing kernel Hilbert space (RKHS). In this paper, we extend SIC so that it can be applied to any RKHSs including infinite dimensional ones. Computer simulations show that the extended SIC works well in ridge parameter selection.", "authors": ["Masashi Sugiyama", "Klaus-Robert M\u00fcller"], "n_citation": 0, "title": "Selecting ridge parameters in infinite dimensional hypothesis spaces", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "930c1895-e815-4f5c-a3c6-1dd743aacc4d"}
{"abstract": "Existing diagrams for tense use as presented in grammar surveys for foreign learners have been analyzed from a semantic theoretic point of view. The analysis reveals that the notational systems used in the diagrammatic representations of the tense rules dealt with are defective in that they can give rise to incorrect interpretations and ambiguities. Moreover, they do not allow the learner to get a coherent view of the tense system. The flaws of the existing diagrams are mainly caused by the lack of a clear delimitation and description of the critical information to be conveyed to the learner. In this paper, we try to avoid these defects by proposing a notational system that relies on a formal semantic approach of tensed discourse.", "authors": ["Leonie Bosveld-de Smet"], "n_citation": 0, "title": "Diagrams in second or foreign language learning", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "931739a7-1d7a-40a7-8e52-4fd2f3ae20a9"}
{"abstract": "This paper presents a structuring element decomposition method and a corresponding morphological erosion algorithm able to compute the binary erosion of an image using a single regular pass whatever the size of the convex structuring element. Similarly to classical dilation-based methods [1], the proposed decomposition is iterative and builds a growing set of structuring elements. The novelty consists in using the set union instead of the Minkowski sum as the elementary structuring element construction operator. At each step of the construction, already-built elements can be joined together in any combination of translations and set unions. There is no restrictions on the shape of the structuring element that can be built. Arbitrary shape decompositions can be obtained with existing genetic algorithms [2] with an homogeneous construction method. This paper, however, addresses the problem of convex shape decomposition with a deterministic method.", "authors": ["Nicolas Normand"], "n_citation": 0, "title": "Convex structuring element decomposition for single scan binary mathematical morphology", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "943f5e2f-46d3-46ba-8d33-7cf65fb36e59"}
{"abstract": "In this paper we concentrate on finding out multiples of primitive polynomials over GF(2). Given any primitive polynomial f(x) of degree d, we denote the number of t-nomial multiples (t < 2 d  - 1) with degree less than 2 d  - 1 as N d,t . We show that (t - 1)N d,t  = ( 2d-2  t-2 ) - N d,t-1  - t-1/t-2(2 d  - t + 1)N d,t-2 , with the initial conditions N d,2  = N d,1  = 0. Moreover, we show that the sum of the degree of all the t-nomial multiples of any primitive polynomial is t-1/t(2 d  - 1)N d,t . More interestingly we show that, given any primitive polynomial of degree d, the average degree t-1/t(2 d  - 1) of its t-nomial multiples with degree < 2 d  - 2 is equal to the average of maximum of all the distinct (t - 1) tuples from 1 to 2 d  - 2. In certain model of Linear Feedback Shift Register (LFSR) based cryptosystems, the security of the scheme is under threat if the connection polynomial corresponding to the LFSR has sparse multiples. We show here that given a primitive polynomial of degree d, it is almost guaranteed to get one t-nomial multiple with degree \u2264 2 d/t-1 +log 2 (t-1)+1.", "authors": ["Kishan Chand Gupta", "Subhamoy Maitra"], "n_citation": 50, "title": "Multiples of primitive polynomials over GF(2)", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "95e62742-03a0-4298-a034-7d303ef5d72d"}
{"abstract": "Recently, algebraic attacks were proposed to attack several cryptosystems, e.g. AES, LILI-128 and Toyocrypt. This paper extends the use of algebraic attacks to combiners with memory. A (k,l)-combiner consists of k parallel linear feedback shift registers (LFSRs), and the nonlinear filtering is done via a finite automaton with k input bits and l memory bits. It is shown that for (k, l)-combiners, nontrivial canceling relations of degree at most [k(l+1)/2] exist. This makes algebraic attacks possible. Also, a general method is presented to check for such relations with an even lower degree. This allows to show the invulnerability of certain (k,l)-combiners against this kind of algebraic attacks. On the other hand, this can also be used as a tool to find improved algebraic attacks. Inspired by this method, the E 0  keystream generator from the Bluetooth standard is analyzed. As it turns out, a secret key can be recovered by solving a system of linear equations with 2 23.07  unknowns. To our knowledge, this is the best published attack on the E 0  keystream generator yet.", "authors": ["Frederik Armknecht", "Matthias Krause"], "n_citation": 0, "title": "Algebraic attacks on combiners with memory", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "95ff0a75-eb93-42ca-b452-d7d5ed3f30f9"}
{"abstract": "The use of software component models has become popular during the last decade, in particular in the development of software for desktop applications and distributed information systems. However, such models have not been widely used in the domain of embedded real-time systems. There is a considerable amount of research on component models for embedded real-time systems, or even narrower application domains, which focuses on source code components and statically configured systems. This paper explores an alternative approach by laying the groundwork for a component model based on binary components and targeting the broader domain of embedded real-time systems. The work is inspired by component models for the desktop and information systems domains in the sense that a basic component model is extended with a set of services for the targeted application domain. A prototype tool for supporting these services is presented and its use illustrated by a control application.", "authors": ["Frank L\u00fcders", "Daniel Flemstr\u00f6m", "Anders Wall", "Ivica Crnkovic"], "n_citation": 0, "title": "A Prototype Tool for Software Component Services in Embedded Real-Time Systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "96f557d8-822d-48cf-9d50-9f6b6606bd42"}
{"abstract": "We propose two RSA-type cryptosystems using n-adic expansion, where n is the public key. These cryptosystems can have more than one block as a plaintext space, and the decrypting process is faster than any other multi-block RSA-type cryptosystem ever reported. Deciphering the entire plaintext of this system is as intractable as breaking the RSA cryptosystem or factoring. Even if a message is several times longer than a public key n, we can encrypt the message fast without repeatedly using the secret key cryptosystem.", "authors": ["Tsuyoshi Takagi"], "n_citation": 50, "title": "Fast RSA-type cryptosystems using N-adic expansion", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "972137ce-e336-4b21-b5dd-7cf060d73a73"}
{"abstract": "This paper describes the generation of data bursts using real IPv4 data as input and compares the performance of the three aggregation algorithms. The profiles of bursts are studied, in particular the mean packet delay per burst and burst inter-arrival time followed by mean burst size and mean number of packets per burst. Observations are made regarding the identification of relations between bursts and packets for the studied data traces and assessed the performance of the aggregation algorithms under study. The conclusions are generalized to IPv6 traffic.", "authors": ["Nuno M. Garcia", "Paulo P. Monteiro", "M\u00e1rio M. Freire"], "n_citation": 50, "title": "Burst assembly with real IPv4 data : Performance assessement of three assembly algorithms", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "97235234-3ced-4e62-a9cd-e98c28cac898"}
{"abstract": "Recent advances in Internet technologies, coupled with wide adoption of the web services paradigm and interoperability standards, makes the World Wide Web a popular vehicle for geo-spatial information distribution and online geo-processing. Web GIS is rapidly evolving and adapting to advances in Internet technologies. Web GlSes are predominantly designed under a thin-client / fat-server paradigm. This approach has several disadvantages. For example, as the number of users increases, the load on the server increases and system performance decreases. Recently the focus has been shifted towards client-side Web GISes, which are heavy-duty, stand-alone systems. We take an opposing approach and present a load balancing client/server Web-based spatial analysis system, UMN-MapServer, and evaluate its performance in a regional natural resource mapping and analysis (NRAMS) application which utilizes biweekly AVHRR imagery and several other raster and vector geo-spatial datasets. We also evaluate alternative approaches and assess the pros and cons of our design and implementation. UMN-MapServer also implements several open standards, such as, WMS, WCS, GML and WFS. In this paper, we also describe in detail the WMS, WCS,and GML extensions from the interoperability point of view, and discuss issues related to adoption of such standards.", "authors": ["Ranga Raju Vatsavai", "Shashi Shekhar", "Thomas E. Burk", "Stephen Lime"], "n_citation": 0, "title": "UMN-MapServer: A High-Performance, Interoperable, and Open Source Web Mapping and Geo-Spatial Analysis System", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9897ecbd-8ad7-4e42-b59f-11901d63f389"}
{"abstract": "The paper presents a fuzzy database management system, and a fuzzy method for dominant colour description of images, on which an image retrieval system is built. The paper shows the suitability of the fuzzy database management system for this kind of applications when the images are characterized by fuzzy data. The synergy of these two introduced components, improves traditional image retrieval systems in three aspects: natural and automatic image description, a natural and easy query language, and high performance in query resolution.", "authors": ["Carlos D. Barranco", "Juan Miguel Medina", "Jes\u00fas Chamorro Mart\u00ednez", "Jos\u00e9 M. Soto-Hidalgo"], "n_citation": 0, "title": "Using a fuzzy object-relational database for colour image retrieval", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "99b336e8-6403-4da3-a68a-664f32e8eef1"}
{"authors": ["Zhangfan Shen", "Chengqi Xue", "Jing Zhang", "Haiyan Wang", "Aaron Marcus", "Wentao Wang"], "n_citation": 0, "references": ["9efc6bd4-d96c-4ff0-87d1-66cd23948559", "b091a558-362c-40c0-bd1e-ae12c90999cd"], "title": "Research on the Style of Product Shape Based on NURBS Curve", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "9af9dfe6-81eb-4df0-83f0-cfb68bdbb43b"}
{"abstract": "In this paper, we propose a bag of system calls representation for intrusion detection of system call sequences and describe misuse detection results with widely used machine learning techniques on University of New Mexico (UNM) and MIT Lincoln Lab (MIT LL) system call sequences with the proposed representation. With the feature representation as input, we compare the performance of several machine learning techniques and show experimental results. The results show that the machine learning techniques on simple bag of system calls representation of system call sequences is effective and often perform better than those approaches that use foreign contiguous subsequences for detecting intrusive behaviors of compromised processes.", "authors": ["Dae-Ki Kang", "Doug Fuller", "Vasant Honavar"], "n_citation": 0, "title": "Learning classifiers for misuse detection using a bag of system calls representation", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9bacfc89-b499-4c48-972a-dd46ac6a2ec7"}
{"abstract": "A temporal question answering system must be able to deduce which qualitative temporal relation holds between two events, a reasoning task that is complicated by the fact that historical events tend to have a gradual beginning and ending. In this paper, we introduce an algebra of temporal relations that is well--suited to represent the qualitative temporal information we have at our disposal. We provide a practical algorithm for deducing new temporal knowledge, and show how this can be used to answer questions that require several pieces of qualitative and quantitative temporal information to be combined. Finally. We propose a heuristic technique to cope with inconsistencies that may arise when integrating qualitative and quantitative information.", "authors": ["Steven Schockaert", "David Ahn", "Martine Do Cock", "Etienne E. Kerre"], "n_citation": 0, "title": "Question answering with imperfect temporal information", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9c5cf282-989f-4cd6-a4e5-ca4aed5a1c58"}
{"abstract": "Emergency response systems play an important role in homeland security nowadays. Despite this, research in the design of emergency response systems is lacking. An effective design of emergency response system involves multi-disciplinary design considerations. On the basis of emergency response system requirement analysis, in this paper, we develop a set of supporting design concepts and strategic principles for an architecture for a coordinated multi-incident emergency response system.", "authors": ["Rui Chen", "Raj Sharman", "H. Raghav Rao", "Shambhu J. Upadhyaya"], "n_citation": 0, "title": "Design principles of coordinated multi-incident emergency response systems", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "9cd179b2-41cd-4800-95e3-52b99fc6e781"}
{"abstract": "We provide the secret Diffie-Hellman-Key which is requested by Kevin Mc-Curley's challenge of 1989. The DH-protocol in question has been carried out in (ZZ/pZZ) *  where p is a 129-digit prime of special form. Our method employed the Number Field Sieve. The linear algebra computation was done by the Lanczos algorithm.", "authors": ["Damian Weber", "Thomas F. Denny"], "n_citation": 63, "title": "The solution of McCurley's discrete log challenge", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "9d7e7d91-9270-40fe-a97b-a7cdeba65466"}
{"abstract": "Restoration techniques available in literature have not addressed their performance in terms of significant, multiple objective goals. Some of these methods have shown good performance for a single objective function. However, restoration must consider a number of objective functions. In this paper, we evaluate existing models and their performance in an attempt to verify their performance and efficacy based on literature. Our research has found not only inefficiency in some of these methods of restoration, but a general incompatibility. Consequently, this paper proposes eight objective functions that yield objective goals significant to the optimal design of a WDM (Wavelength Division Multiplexing) optical network. Each objective function model is presented and is examined by experimentation. Four proposed restoration algorithms are evaluated: KSDPR (k-Shortest Disjoint Path Restoration based on multiple uphill moves and heuristic rule), DCROS (Deep Conjectural Reinforced Optimal Search), RWWA (Random Walk-based Wavelength Assignment), and PTCI (Physical Topology Connectivity Increase). Numerical results obtained by experimental evaluation of KSDPR, DCROS, RWWA, and PTCI algorithms confirm that MTWM (objective function of Minimizing Total Wavelengths with Multi-objective goals) based on the DCROS algorithm is a technique for efficient restoration in WDM optical networks.", "authors": ["Sung Woo Tak", "Wonjun Lee"], "n_citation": 0, "title": "A Restoration Technique Incorporating Multi-objective Goals in WDM Optical Networks.", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "9dbccc3a-1699-4bba-b75b-1f1d43b09942"}
{"abstract": "Ubiquitous recommendation systems predict new items of interest for a user, based on predictive relationship discovered between the user and other participants in Ubiquitous Commerce. In this paper, optimal associative neighbor mining, using attributes, for the purpose of improving accuracy and performance in ubiquitous recommendation systems, is proposed. This optimal associative neighbor mining selects the associative users that have similar preferences by extracting the attributes that most affect preferences. The associative user pattern comprising 3-AUs (groups of associative users composed of 3-users), is grouped through the ARHP algorithm. The approach is empirically evaluated, for comparison with the nearest-neighbor model and k-means clustering, using the MovieLens datasets. This method can solve the large-scale dataset problem without deteriorating accuracy quality.", "authors": ["Kyung-Yong Jung", "Hee-Joung Hwang", "Un-Gu Kang"], "n_citation": 0, "title": "Optimal associative neighbor mining using attributes for ubiquitous recommendation systems", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "9f0163d0-e267-4304-a5e5-2460dc79ee23"}
{"abstract": "We describe a parallel algorithm for extending a small domain hash function to a very large domain hash function. Our construction can handle messages of any practical length and preserves the security properties of the basic hash function. The construction can be viewed as a parallel version of the well known Merkle-Damgard construction, which is a sequential construction. Our parallel algorithm provides a significant reduction in the computation time of the message digest, which is a basic operation in digital signatures.", "authors": ["Palash Sarkar", "Paul J. Schellenberg"], "n_citation": 0, "title": "A parallel algorithm for extending cryptographic hash functions", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "9f1dac9a-c0c8-48b4-8192-7b37d86ddd33"}
{"abstract": "In this paper we address the task of organising multi-agent systems in order to collectively solve problems. We base our approach on a logical model of rational agency comprising a few simple, but powerful, concepts. While many other researchers have tackled this problem using formal logic, the important aspect of the work described here is that the logical descriptions of the agents are directly executable using the Concurrent METATEM framework, allowing the execution of agents described in a combination of temporal, belief and ability logics. Here, we are particularly concerned with exploring some of the possible logical constraints that may be imposed upon these agents, and how these constraints affect the ability of the agents to come together to collectively solve problems.", "authors": ["Michael Fisher", "Chiara Ghidini", "Benjamin Hirsch"], "n_citation": 50, "title": "Organising logic-based agents", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "9fd40c20-d5b6-4787-a334-9acfe29768a6"}
{"abstract": "We address one of the most fundamental problems concerning the RSA cryptoscheme: Does the knowledge of the RSA public key/ secret key pair (e, d) yield the factorization of N = pq in polynomial time? It is well-known that there is a probabilistic polynomial time algorithm that on input (N, e, d) outputs the factors p and q. We present the first deterministic polynomial time algorithm that factors N provided that e, d < \u03a6(N) and that the factors p, q are of the same bit-size. Our approach is an application of Coppersmith's technique for finding small roots of bivariate integer polynomials.", "authors": ["Alexander May"], "n_citation": 0, "title": "Computing the RSA secret key is deterministic polynomial time equivalent to factoring", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a0eb4306-92aa-482d-81a4-dfc5e2919d44"}
{"abstract": "After many years, cryptography is coming to the Internet. Some protocols are in common use; more are being developed and deployed. The major issue has been one of cryptographic engineering: turning academic papers into a secure, implementable specification. But there is missing science as well, especially when it comes to efficient implementation techniques.", "authors": ["S. M. Bellovin"], "n_citation": 54, "title": "Cryptography and the Internet", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "a101459f-3ca5-406c-b073-496bd83f0398"}
{"abstract": "Small units like chip cards have the possibility of computing, storing and protecting data. Today such chip cards have limited computing power, then some cryptoprotocols are too slow. Some new chip cards with secure fast coprocessors are coming but are not very reliable at the moment and a little bit expensive for some applications. In banking applications there are few servers (ATM) relative to many small units: it is a better strategy to put the computing power into few large servers than into the not-very-often used cards. A possible solution is to use the computing power of the (insecure) server to help the chip card. But it remains an open question whether it is possible to accelerate significantly RSA signatures using an insecure server with the possibility of active attacks: that is, when the server returns false values to get some part of secret from the card. In this paper, we propose a new efficient protocol for accelerating RSA signatures, resistant against all known active and passive attacks. This protocol does not use expensive precomputations; the computation done by the card, the used RAM and the data transfers between the card and the server are small. With current chip cards it is thus possible to implement efficiently this protocol.", "authors": ["Philippe B\u00e9guin", "Jean-Jacques Quisquater"], "n_citation": 50, "title": "Fast server-aided RSA signatures secure against active attacks", "venue": "Lecture Notes in Computer Science", "year": 1995, "id": "a2be82f2-17f5-4db3-a08a-1277e3b3ed40"}
{"abstract": "In the bare public-key model (BPK in short), each verifier is assumed to have deposited a public key in a file that is accessible by all users at all times. In this model, introduced by Canetti et al. [STOC 2000], constant-round black-box concurrent and resettable zero knowledge is possible as opposed to the standard model for zero knowledge. As pointed out by Micali and Reyzin [Crypto 2001], the notion of soundness in this model is more subtle and complex than in the classical model and indeed four distinct notions have been introduced (from weakest to strongest): one-time, sequential, concurrent and resettable soundness. In this paper we present the first constant-round concurrently sound resettable zero-knowledge argument system in the bare public-key model for NP. More specifically, we present a 4-round protocol, which is optimal as far as the number of rounds is concerned. Our result solves the main open problem on resettable zero knowledge in the BPK model and improves the previous works of Micali and Reyzin [EuroCrypt 2001] and Zhao et al. [EuroCrypt 2003] since they achieved concurrent soundness in stronger models.", "authors": ["Giovanni Di Crescenzo", "Giuseppe Persiano", "Ivan Visconti"], "n_citation": 0, "title": "Constant-round resettable zero knowledge with concurrent soundness in the bare public-key model", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "a4b20738-fc34-4a32-99ad-d8b2defa16a0"}
{"abstract": "Location is a valuable information for services implemented in wireless networks. Location systems often use the infrastructure of already deployed cellular networks. Accordingly, location systems spend resources from the network they use. This paper proposes a middleware to reduce the consumption of network resources and optimize the location traffic being carried. This middleware, called MILCO (Middleware for Location Cost Optimization), selects the optimum location technique depending on the request, i.e. the location technique that fulfills the required quality of service (QoS) and minimizes the resource operating expense. In addition, MILCO takes advantage from ongoing and carried location processes to reduce the overall expenditure of resources. Results show that MILCO can reduce the location-process failures and improve the figures of latency for the location provisioning and resource usage in cellular networks such as UMTS.", "authors": ["Israel Martin-Escalona", "Francisco Barcel\u00f3"], "n_citation": 0, "title": "A middleware approach for reducing the network cost of location traffic in cellular networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a4f43da7-650a-4948-b0df-141bf4fd328f"}
{"abstract": "Designing architectures requires the balancing of multiple system quality objectives. In this paper, we present techniques that support the exploration of the quality properties of component-based architectures deployed on multiprocessor platforms. Special attention is paid to real-time properties and efficiency of resource use. The main steps of the process are (1) a simple way of modelling properties of software and hardware components, (2) from the component properties, a model of an execution architecture is composed and analyzed for system-level quality attributes, (3) for the composed system, selected execution scenarios are evaluated, (4) Pareto curves are used for making design trade-offs explicit. The process has been applied to several industrial systems. A Car Radio Navigation system is used to illustrate the method. For this system, we consider architectural alternatives, show their specification, and present their trade-off with respect to cost, performance and robustness.", "authors": ["E Egor Bondarev", "Michel R. V. Chaudron"], "n_citation": 50, "title": "A Process for Resolving Performance Trade-Offs in Component-Based Architectures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a678f7e2-6658-4ceb-a5e9-4cca86b85cf9"}
{"abstract": "Statistical language models can learn relationships between topics discussed in a document collection and persons, organizations and places mentioned in each document. We present a novel combination of statistical topic models and named-entity recognizers to jointly analyze entities mentioned (persons, organizations and places) and topics discussed in a collection of 330,000 New York Times news articles. We demonstrate an analytic framework which automatically extracts from a large collection: topics; topic trends; and topics that relate entities.", "authors": ["David Newman", "Chaitanya Chemudugunta", "Padhraic Smyth", "Mark Steyvers"], "n_citation": 0, "title": "Analyzing entities and topics in news articles using statistical topic models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a7b0b353-5484-4089-90c1-ba4c4e823d16"}
{"abstract": "We discuss a variety of roles for diagrams in helping with reasoning, focusing in particular on their role as physical models of states of affairs, much like an architectural model of a building or a 3-D molecular model of a chemical compound. We discuss the concept of a physical model for a logical sentence, and the role played by the causal structure of the physical medium in making the given sentence as well as a set of implied sentences true. This role of a diagram is consistent with a widely-held intuition that diagrams exploit the fact that 2-D space is an analog of the domain of discourse. One line of research in diagrammatic reasoning is that diagrams, rather then being models, are formal representations with specialized rules of inference that generate new diagrams. We reconcile these contrasting views by relating the usefulness of diagrammatic systems as formal representations to the fact that their rewrite rules take advantage of the diagrams' model-like character. When the physical model is prototypical, it supports the inference of certain other sentences for which it provides a model as well. We also informally discuss a proposal that diagrams and similar physical models help to explicate a certain sense of relevance in inference, an intuition that so-called Relevance Logics attempt to capture.", "authors": ["B. Chandrasekaran"], "n_citation": 50, "title": "Diagrams as physical models", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a8cd31a3-2d80-465e-aaf4-d0636863079d"}
{"authors": ["Bart Preneel", "Pc vanOorschot"], "n_citation": 50, "title": "MDx-MAC and Building Fast MACs from Hash Functions", "venue": "Lecture Notes in Computer Science", "year": 1995, "id": "a8eac919-5232-46c1-be5e-9494b1565325"}
{"abstract": "Following work of Stroud and Saeger, we investigate the formulation of the port of entry inspection algorithm problem as a problem of finding an optimal binary decision tree for an appropriate Boolean decision function. We report on an experimental analysis of the robustness of the conclusions of the Stroud-Saeger analysis and show that the optimal inspection strategy is remarkably insensitive to variations in the parameters needed to apply the Stroud-Saeger method.", "authors": ["Saket Anand", "David Madigan", "Richard J. Mammone", "Saumitr Pathak", "Fred S. Roberts"], "n_citation": 0, "title": "Experimental analysis of sequential decision making algorithms for port of entry inspection procedures", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "a9097816-9a75-4f34-99b6-1f2173a2a7fb"}
{"abstract": "We present a new protocol for efficient distributed computation modulo a shared secret. We further present a protocol to distributively generate a random shared prime or safe prime that is much more efficient than previously known methods. This allows one to distributively compute shared RSA keys, where the modulus is the product of two safe primes, much more efficiently than was previously known.", "authors": ["Joy Algesheimer", "Jan Camenisch", "Victor Shoup"], "n_citation": 0, "title": "Efficient computation modulo a shared secret with application to the generation of shared safe-prime products", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "a920e1a8-483c-49fd-bad6-1fa22b1958a8"}
{"abstract": "Log-polar images have been being used for pattern recognition and active vision tasks for some years. These images are obtained either from true retina-like sensors or from conventional cartesian images by software conversion. From the hardware perspective, the design of such log-polar retinae faces its own technological limitations. In the case of software remappers, however, their very flexibility has led to many researchers to use them with little or no justification of the choice of the particular log-polar layout. In this paper, a set of design criteria are proposed, and an approach to choose the parameters involved in the log-polar transform is described. This kind of design not only could be used in simulation software, but also could act as design guidelines for artificial hardware-built retinae.", "authors": ["V. Javier Traver", "Filiberto Pla"], "n_citation": 0, "title": "Designing the lattice for log-polar images", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "aa016334-b2ae-4e8a-9d09-82818d64ea30"}
{"abstract": "We use a mathematical morphology approach to compute the surface and curve skeletons of a 3D object. We focus on the behaviour of the surface skeleton, in particular the reversibility for the case when the skeleton is, and is not anchored to the set of centres of maximal balls. We elaborate on the difficulties to obtain a reversible surface skeleton that does not depend on the orientation of the original object with respect to the grid, and that has no jagged borders.", "authors": ["Stina Svensson", "Pieter P. Jonker"], "n_citation": 0, "title": "On the use of shape primitives for reversible surface skeletonization", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "aa069c20-f2cf-40a2-8ea5-471627225e64"}
{"abstract": "Previous work on energy efficient broadcast protocols for wireless ad hoc networks are based a commonly used physical layer model called Pathloss model which assume two nodes can communicate if and only if they exist within their transmission radius. In this paper, we analyze the effect of realistic physical layer on energy efficient broadcast protocols. We employ a more realistic log-normal shadowing model for physical layer and consider two link layer operating models: EER (end-to-end retransmission) and HHR (hop-by-hop retransmission). Networks with omni-antennas and directional antennas are dealt with separately. Based on above models, we analyze how to adjust actual transmission radius for transmission nodes and relay nodes to get the trade-off between maximizing probability of delivery and minimizing energy consumption. From our analysis based on shadowing model, we have derived the appropriate transmission range. The results presented in this paper are expected to improve the performance of broadcast protocols under realistic physical layer.", "authors": ["Hui Xu", "Manwoo Jeon", "Jinsung Cho", "Niu Yu", "S.Y. Lee"], "n_citation": 0, "title": "Effect of realistic physical layer on energy efficient broadcast protocols for wireless ad hoc networks", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aa352cb6-6499-4631-a7b8-a73ec4d98319"}
{"abstract": "Investments on multimedia technology enable us to store many more reflections of the real world in digital world as videos so that we carry a lot of information to the digital world directly. In order to store and efficiently query this information, a video database system (VDBS) is necessary. We propose a structural, event based and multimodal (SEBM) video data model which supports three different modalities that are visual, auditory and textual modalities for VDBSs and we can dissolve these three modalities within a single SEBM model. We answer the content-based, spatio-temporal and fuzzy queries of the user by using SEBM video data model more easily, since SEBM stores the video data as the way that user interprets the real world data. We follow divide and conquer technique when answering very complicated queries. We give the algorithms for querying on SEBM and try them on an implemented SEBM prototype system.", "authors": ["Hakan Oztarak", "Adnan Yazici"], "n_citation": 50, "title": "Flexible querying using structural and event based multimodal video data model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "aadea869-585e-4019-8809-a064fe1c9966"}
{"abstract": "In this study, students' expression of understanding of structure and function in three systems of the body through visual (drawn) and verbal (written and spoken) modes was probed. Those with good comprehension had high scores in both modes. Pedagogical practices must emphasise explicit use of drawings and words to link structure and function concepts. This can help students of lower ability to form an integrated mental model which will aid understanding and expression.", "authors": ["Sindhu Mathai", "Jayashree Ramadas"], "n_citation": 0, "title": "The visual and verbal as modes to express understanding of the human body", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ab66ef84-4a84-4bcf-83b1-0539d021038e"}
{"abstract": "In this paper, we examine role exchange in multiagenty. After defining the utility that reflects an agent's orientation toward role exchange, I.e., Role Exchange Value (REV) and the agent's Individual Utility Gain (IUG), we present two theorems and two corollaries that capture properties of proposed utility-based role exchange. Then, we provide an algorithm that predicts IUG for each agent in role exchange. The algorithm is implemented and the results are discussed.", "authors": ["Xin Zhang", "Henry Hexmoor"], "n_citation": 50, "title": "Utility-based role exchange", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "ad4846d9-06b7-4d77-a55f-c42f0ac2de06"}
{"abstract": "Verification of operating procedures (that is, specifications of manual control actions) by model checking has been discussed in [16]. The modelling language Promela and the model checker Spin were used in that report. In order to be able to apply model checking in a wider scope, modelling languages with graphical interface and verification tools used in industrial context are preferable (for example, to facilitate collaboration with process experts). In this paper, we discuss how to use SDL to model systems consisting of operating procedures and the controlled processes. Verification of procedures against correctness specifications is done by using the tool SDT. We conclude the paper with a short discussion of the integration of formal verification with the procedure design process.", "authors": ["Wenhui Zhang"], "n_citation": 0, "title": "Applying SDL specifications and tools to the verification of procedures", "venue": "Lecture Notes in Computer Science", "year": 2001, "id": "adba1dc6-3407-48df-86b8-16d67223bd0d"}
{"abstract": "This research initiative is an initial investigation into a novel approach for deriving indicators of deception from video-taped interaction. The team utilized two-dimensional spatial inputs extracted from video to construct a set of discrete and inter-relational features. The features for thirty-eight video interactions were then analyzed using discriminant analysis. Additionally, features were used to build a multivariate regression model. Through this exploratory study, the team established the validity of the approach, and identified a number of promising features, opening the door for further investigation.", "authors": ["Thomas O. Meservy", "Matthew L. Jensen", "John Kruse", "Judee K. Burgoon", "Jay F. Nunamaker"], "n_citation": 0, "title": "Automatic extraction of deceptive behavioral cues from video", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "adfd9221-6444-4d37-b7f6-b1ac4392f1dd"}
{"authors": ["Orr Dunkelman", "Gautham Sekar", "Bart Preneel"], "n_citation": 50, "title": "Improved Meet-in-the-Middle Attacks on Reduced-Round DES", "venue": "Lecture Notes in Computer Science", "year": 2007, "id": "ae22dfee-a92f-48ae-a3e7-f5bde765fbfd"}
{"abstract": "Data distortion is a critical component to preserve privacy in security-related data mining applications, such as in data mining-based terrorist analysis systems. We propose a sparsified Singular Value Decomposition (SVD) method for data distortion. We also put forth a few metrics to measure the difference between the distorted dataset and the original dataset. Our experimental results using synthetic and real world datasets show that the sparsified SVD method works well in preserving privacy as well as maintaining utility of the datasets.", "authors": ["Shuting Xu", "Jun Zhang", "Dianwei Han", "Jie Wang"], "n_citation": 0, "title": "Data distortion for privacy protection in a terrorist analysis system", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "ae4e81fe-2583-432d-b7ed-6141bb2aa1ae"}
{"abstract": "We present a functional model for the analysis of large volumes of detailed transactional data, accumulated over time. In our model. the data schema is an acyclic graph with a single root, and data analysis queries are formulated using paths starting at the root. The root models the objects of an application and the remaining nodes model attributes of the objects. Our objective is to use this model as a simple interface for the analyst to formulate queries, and then map the queries to a commercially available system for the actual evaluation.", "authors": ["Nicolas Spyratos"], "n_citation": 0, "title": "A functional model for data analysis", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "af711444-4bb2-4c6d-8593-713267c01948"}
{"abstract": "Each user accesses a Website with certain interest. The interest is associated with his navigation patterns. The interest navigation patterns represent different interest of the users. In this paper, hybrid Markov model is proposed for interest navigation pattern discovery. The novel model is better in prediction overlay rate and prediction correct rate than traditional Markov models. User group interest is also defined in this paper. The probability of user group interest navigation from one page to another is computed by navigation path characteristics and time characteristics. Compared with the previous ones, the results of the experiment show that the performance is improved efficiently by the hybrid Markov model.", "authors": ["Yijun Yu", "Huaizhong Lin", "Yimin Yu", "Chun Chen"], "n_citation": 0, "title": "Mining interest navigation patterns based on hybrid markov model", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "affc80ab-83b9-4981-8319-127722a67e72"}
{"abstract": "NTRU is an efficient patented public-key cryptosystem proposed in 1996 by Hoffstein, Pipher and Silverman. Although no devastating weakness of NTRU has been found, Jaulmes and Joux presented at Crypto '00 a simple chosen-ciphertext attack against NTRU as originally described. This led Hoffstein and Silverman to propose three encryption padding schemes more or less based on previous work by Fujisaki and Okamoto on strengthening encryption schemes. It was claimed that these three padding schemes made NTRU secure against adaptive chosen-ciphertext attacks (IND-CCA2) in the random oracle model. In this paper, we analyze and compare the three NTRU schemes obtained. It turns out that the first one is not even semantically secure (IND-CPA). The second and third ones can be proven IND-CCA2-secure in the random oracle model, under however rather unusual assumptions. They indeed require a partial-domain one-wayness of the NTRU one-way function which is likely to be a stronger assumption than the one-wayness of the NTRU one-way function. We propose several modifications to achieve IND-CCA2-security in the random oracle model under the original NTRU inversion assumption.", "authors": ["Phong Q. Nguyen", "David Pointcheval"], "n_citation": 53, "title": "Analysis and improvements of NTRU encryption paddings", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b20bc175-21dc-4222-b7a9-7f7382c2d686"}
{"abstract": "The presence of structure inside XML documents poses the hard challenge of providing flexible query matching methods for effective retrieval of results. In this paper we present an approach that faces this issue in a twofold fashion: 1) it exploits new approximations on data structure; 2) it provides a relevance ranking method that takes into account the degree of correctness and completeness of results with respect to a given query, as well as the degree of cohesion of data retrieved.", "authors": ["Paolo Ciaccia", "Wilma Penzo"], "n_citation": 50, "title": "Adding flexibility to structure similarity queries on XML data", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b39e174c-8678-4ae1-9516-911003c690cd"}
{"abstract": "The following presents a method for constructing taxonomies by utilizing the Latent Semantic Indexing (LSI) technique. The LSI technique enables representation of textual data in a vector space, facilitates access to all documents and terms by contextual queries, and allows for text comparisons. A taxonomy generator downloads collection of documents, creates document clusters, assigns titles to clusters, and organizes the clusters in a hierarchy. The nodes in the hierarchy are ordered from general to specific in the depth of the hierarchy, and from most similar to least similar in the breadth of the hierarchy. This method is capable of producing meaningful classifications in a short time.", "authors": ["Janusz Wnek"], "n_citation": 0, "title": "Lsi-based taxonomy generation : The taxonomist system", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "b53d7e3b-a851-4da8-9653-66a66848d36e"}
{"abstract": "We present in this paper some attempts to design a Machine Learning method to predict preference knowledge in a multi-agents context. This approach is applied to a corporate knowledge management system.", "authors": ["Adorjan Kiss", "Jo\u00ebl Quinqueton"], "n_citation": 0, "title": "Learning user preferences in multi-agent system", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b588b19e-53de-4244-b358-db716a55c0e4"}
{"abstract": "Long term synaptic changes induced by neural spike activity are believed to underlie learning and memory. Spike-driven long term synaptic plasticity has been investigated in simplified situations in which the patterns of asynchronous activity to be encoded were statistically independent. An extra regulatory mechanism is required to extend the learning capability to more complex and natural stimuli. This mechanism is provided by the effects of the action potentials that are believed to be responsible for spike-timing dependent plasticity. These effects, when combined with the dependence of synaptic plasticity on the post-synaptic depolarization, produce the learning rule needed for storing correlated patterns of asynchronous neuronal activity.", "authors": ["Stefano Fusi"], "n_citation": 50, "title": "Spike-driven synaptic plasticity for learning correlated patterns of asynchronous activity", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "b604692a-d6b5-473b-95f5-c9b4243ba65a"}
{"authors": ["David G. Novick", "Iv\u00e1n Gris", "Adriana Camacho", "Alex Rayon", "Timothy Gonzalez", "Masaaki Kurosu"], "n_citation": 0, "references": ["0c056242-0eac-462f-b8d9-ceb0cf94accd", "2f914ae5-15fd-493e-bb7b-3618102cec81", "3c0488ce-660f-4ca9-96f8-d9c0ce1c4d3a", "4387b2a5-96fd-416a-b402-da01400fb395", "58ffd3ab-8620-450c-83f0-de6addae5ed6", "5b43fbc6-fa04-443c-903c-b91a52cc99eb", "6601698c-1ada-4b2b-bae4-81e238a66cf0", "73021aa4-02f4-4062-922f-ef2fda567bfc", "858757eb-01f0-4e6f-b4e2-9efab4d59338", "870d9265-3030-4e96-8515-1bcece25a822", "8aac6558-5705-47a6-a8ff-151d4b2f0015", "f9765539-0c76-4a20-a8cc-d28717834ccf", "f9ba3721-b3bb-4e59-88e5-740c60e90872", "fe5664dc-1b9b-48a3-b527-4fa1f18ea3c5"], "title": "Bigger (Gesture) Isn\u2019t Always Better", "venue": "Lecture Notes in Computer Science", "year": 2017, "id": "b60cf411-d701-4028-be70-900bca942493"}
{"authors": ["Antoon Bosselaers", "Ren\u00e9 Govaerts", "Joos Vandewalle"], "n_citation": 163, "title": "Comparison of three modular reduction functions", "venue": "Lecture Notes in Computer Science", "year": 1993, "id": "b708be40-169c-4e95-96a9-e59f22ba38f6"}
{"abstract": "With the rapid growth of computing power, many concepts and tools of image analysis are becoming more and more popular in other data processing fields, such as image and video compression. Image segmentation, in particular, has a central role in the object-based video coding standard MPEG-4, as well as in various region-based coding schemes used for remote-sensing imagery. A region-based image description, however, is only useful if it has a limited representation cost, which calls for accurate and efficient tools for the description of region boundaries. A very promising approach relies on the extended boundary concept, first discussed in [6] and [7] and later used by Liow [5] to develop a contour tracing algorithm. In this work, we extend Liow's algorithm and introduce the corresponding reconstruction technique needed for coding purposes. In addition, we define an algebraic semi-group structure that allows us to formally prove the algorithm properties, to extend it to other boundary definitions, and to introduce a fast contour tracing algorithm which only requires a raster scan of the image.", "authors": ["Ciro D'Elia", "Giuseppe Scarpa"], "n_citation": 0, "title": "Contour-based shape representation for image compression and analysis", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "b95b0916-9006-47eb-8081-d4b0b8d944b9"}
{"abstract": "Since the September 11 th  terror attack at New York in 2001, the frequency of terror attacks around the world has been increasing and it draws more attention of the public. On January 20 of 2006, CNN reported that al Qaeda leader Osama bin Laden had released a tape claiming that a series of terror attacks were planned in US. These attacks and messages from terrorists are threatening everyone in the world. As an intelligence officer or a citizen in any countries, we are interested in the development of the terror attacks around us. We can easily extract hundreds or thousands of news stories of any terror attack incidents from newswires such as CNN.com but the volume of information is too large to capture the information we need. Information retrieval techniques such as Topic Detection and Tracking are able to organize the news stories as events within a topic of terror attack. However, they are incapable to present the complex evolution relationships between the events. We are interested to leam what the major events but also how they develop within the topic of a terror attack. It is beneficial to identify the starting and ending events, the seminal events and the evolution of these events. In this work, we propose to utilize the temporal relationship, event similarity, temporal proximity and document distributional proximity to identify the event evolution relationships between events in a terror attack incident. An event evolution graph is utilized to present the underlying structure of events for efficient browsing and extracting information. Case study and experiment are presented to illustrate and show the performance of our proposing technique.", "authors": ["Christopher C. Yang", "Xiaodong Shi", "Chih-Ping Wei"], "n_citation": 0, "title": "Tracing the event evolution of terror attacks from on-line news", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bb9ef1dc-28eb-4fb6-a405-4d11eb0459f7"}
{"abstract": "Broadcast Encryption schemes enable a center to broadcast encrypted programs so that only designated subsets of users can decrypt each program. The stateless variant of this problem provides each user with a fixed set of keys which is never updated. The best scheme published so far for this problem is the subset difference (SD) technique of Naor Naor and Lotspiech, in which each one of the n users is initially given O(log 2 (n)) symmetric encryption keys. This allows the broadcaster to define at a later stage any subset of up to r users as revoked, and to make the program accessible only to their complement by sending O(r) short messages before the encrypted program, and asking each user to perform an O(log(n)) computation. In this paper we describe the Layered Subset Difference (LSD) technique, which achieves the same goal with O(log 1+e (n)) keys, O(r) messages, and O(log(n)) computation. This reduces the number of keys given to each user by almost a square root factor without affecting the other parameters. In addition, we show how to use the same LSD keys in order to address any subset defined by a nested combination of inclusion and exclusion conditions with a number of messages which is proportional to the complexity of the description rather than to the size of the subset. The LSD scheme is truly practical, and makes it possible to broadcast an unlimited number of programs to 256,000,000 possible customers by giving each new customer a smart card with one kilobyte of tamper-resistant memory. It is then possible to address any subset defined by t nested inclusion and exclusion conditions by sending less than 4t short messages, and the scheme remains secure even if all the other users form an adversarial coalition.", "authors": ["Dani Halevy", "Adi Shamir"], "n_citation": 0, "title": "The LSD broadcast Encryption scheme", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "bd344fd1-063b-443c-a912-2dbb433403df"}
{"abstract": "This paper describes a method to accelerate the generation of shape primitives for N-dimensional images X N  These shape primitives can be used in conditions for topology preserving erosion or skeletonization in N-dimensional images. The method is based on the possibility to describe primitives for intrinsic dimensions N = N - 1 by quadratic equations of the form x N = n = N-1/\u03a3/n = 0 (a n x x  +b n x 2 n).", "authors": ["Pieter P. Jonker", "Stina Svensson"], "n_citation": 0, "title": "The generation of N dimensional shape primitives", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "bdc0da16-e01b-4d29-9866-961108dbab94"}
{"abstract": "This paper discusses the types of communicative signals that frequently appear in simple bar charts and how we exploit them as evidence in our system for inferring the intended message of an information graphic. Through a series of examples, we demonstrate the impact that various types of communicative signals, namely salience, captions and estimated perceptual task effort, have on the intended message inferred by our implemented system.", "authors": ["Stephanie Elzer", "Sandra Carberry", "Seniz Demir"], "n_citation": 50, "title": "Communicative signals as the key to automated understanding of simple bar charts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "bdd2214a-db5c-4df9-88b0-0508a73f85dc"}
{"abstract": "We evaluate constructions for building pseudo-random functions (PRFs) from pseudo-random permutations (PRPs). We present two constructions: a slower construction which preserves the security of the PRP and a faster construction which has less security. One application of our construction is to build a wider block cipher given a block cipher as a building tool. We do not require any additional constructions-e.g. pseudo-random generators-to create the wider block cipher. The security of the resulting cipher will be as strong as the original block cipher.", "authors": ["Chris Hall", "David Wagner", "John Kelsey", "Bruce Schneier"], "n_citation": 0, "title": "Building PRFs from PRPs", "venue": "Lecture Notes in Computer Science", "year": 1998, "id": "becf1e40-baa9-453b-9475-ff5fdf30699d"}
{"abstract": "This paper describes the development of the Conceptual Model of Counter Terrorist Operations or the CMCTO. The CMCTO is a top down decomposition of the functions that are performed in the Counter Terrorist Domain. The models first decomposes the domain into Functions directed toward terrorists; Functions directed toward victims; and, Functions of support. Each of these functions is further decomposed to varying levels. The paper also includes a comprehensive review of the literature and of the process used.", "authors": ["David Davis", "Allison Frendak-Blume", "Jennifer Wheeler", "Alexander E. Woodcock", "and Clarence Worrell"], "n_citation": 0, "title": "A conceptual model of counterterrorist operations", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "bf4b12da-a20b-4e29-a99b-6010c4bbf090"}
{"abstract": "Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software.", "authors": ["Raffi Khatchadourian", "Hidehiko Masuhara"], "n_citation": 0, "references": ["02e59455-2139-4697-97b3-c479947770c7", "13619122-e4d4-4ef8-8a9d-cdfddfdaed0d", "17997438-9d19-49dc-9f72-26c6a03f1d37", "31333ad6-6f42-458b-b09a-6948e7aadf26", "3fec9bdd-928b-4642-a0c8-85dd1b037ea2", "4061e35e-77bd-4c1e-b6ad-c8af83b193a7", "4317f382-5bf6-4e40-aaf4-a617a89ca624", "489a148d-dcdf-4fea-a275-13eb7abda8b2", "4b34969b-6e5c-4ebb-bdb7-1cb639521893", "4bcca392-59dc-4008-99e6-a32c79adddf3", "66e928ad-0e13-4dc1-8c8d-0020fc46d11a", "671b3152-2fd1-430e-8fe4-6e5e12c3cbdc", "6963bd98-a886-476c-b13a-c03486a284c3", "6c831a83-b234-4ce5-8ca4-6314b1d0aff0", "6fe233b0-39c1-42f1-bee7-ed0ffa3a4d45", "81334e23-ae68-44de-b55c-7fee2b9728fb", "a5a5871f-6b92-4801-a252-50ad5615e4e9", "a9bdadc6-a46f-4205-8538-90e4099c6b1e", "b1df836a-7aff-43fb-8726-4d1a71377320", "c0183ac9-2586-40b4-ad76-cb577472e3b1", "c28cf51b-79cf-4b24-9234-8b304f11e6ca", "c301f0f9-2ba4-47c1-ad37-949400934c5e", "d6b825c5-a14c-43c0-9b22-770f1db48715", "e90f35a9-fa51-450c-9cda-fead357901c4", "f7cc504f-ad67-426c-9cfb-59a1797fe536"], "title": "Automated refactoring of legacy Java software to default methods", "venue": "international conference on software engineering", "year": 2017, "id": "c00405c6-6433-4728-9d1e-c5a9ac4edc61"}
{"abstract": "In order to make image analysis methods more reliable it is important to analyse to what extend shape information is preserved during image digitization. Most existing approaches to this problem consider topology preservation and are restricted to ideal binary images. We extend these results in two ways. First, we characterize the set of binary images which can be correctly digitized by both regular and irregular sampling grids, such that not only topology is preserved but also the Hausdorff distance between the original image and the reconstruction is bounded. Second, we prove an analogous theorem for gray scale images that arise from blurring of binary images with a certain filter type. These results are steps towards a theory of shape digitization applicable to real optical systems.", "authors": ["Ullrich K\u00f6the", "Peer Stelldinger"], "n_citation": 0, "title": "Shape preserving digitization of ideal and blurred binary images", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "c0d74b8d-538d-4611-96c2-11dda1022487"}
{"abstract": "Managing crises requires collecting geographical intelligence and making spatial decisions through collaborative efforts among multiple, distributed agencies and task groups. Crisis management also requires close coordination among individuals and groups of individuals who need to collaboratively derive information from geospatial data and use that information in coordinated ways. However, geospatial information systems do not currently support group work and can not meet the information needs of crisis managers. This paper describes a group interface for geographical information system, featuring multimodal human input, conversational dialogues, and same-time, different place communications among teams.", "authors": ["Guoray Cai", "Alan M. MacEachren", "Isaac Brewer", "Michael D. McNeese", "Rajeev Sharma", "Sven Fuhrmann"], "n_citation": 0, "title": "Map-mediated GeoCollaborative crisis management", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c2439b6a-8e90-4df6-b56c-b30fecbb258f"}
{"abstract": "In distributed and mobile environments, the connections among the hosts on which a software system is running are often unstable. As a result of connectivity losses, the overall availability of the system decreases. The distribution of software components onto hardware nodes (i.e., deployment architecture) may be ill-suited for the given target hardware environment and may need to be altered to improve the software system's availability. The critical difficulty in achieving this task lies in the fact that determining a software system's deployment that will maximize its availability is an exponentially complex problem. In this paper, we present an automated, flexible, software architecture-based solution for disconnected operation that increases the availability of the system during disconnection. We provide a fast approximative solution for the exponentially complex redeployment problem, and assess its performance.", "authors": ["Marija Mikic-Rakic", "Nenad Medvidovic"], "n_citation": 50, "title": "Software architectural support for disconnected operation in highly distributed environments", "venue": "Lecture Notes in Computer Science", "year": 2004, "id": "c448d318-ca38-4934-af03-5e3fd7adcca1"}
{"authors": ["Bart Preneel"], "n_citation": 0, "title": "Cryptographic Hash Functions: Theory and Practice", "venue": "Lecture Notes in Computer Science", "year": 2010, "id": "c4a2225e-d7b5-4b54-b49e-0a2ac692631d"}
{"abstract": "Propositional Statecharts, described in [3], are a variation of David Harel's Statechart formalism [6] intended to enable both diagrammatic description of an agent interaction protocol, and interpretation as a theory in a dynamic logic. Here we provide an informal description of a diagrammatic extension to enable modular representation.", "authors": ["Hywel R. Dunn-Davies", "R. J. Cunningham", "Shamimabi Paurobally"], "n_citation": 0, "title": "Modularity and composition in propositional statecharts", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c51ca85a-9cc1-49f5-b30a-6b1c6f427c35"}
{"abstract": "A variant of the self-organizing maps algorithm is proposed in this paper for document organization and retrieval. Bigrams are used to encode the available documents and signed ranks are assigned to these bigrams according to their frequencies. A novel metric which is based on the Wilcoxon signed-rank test exploits these ranks in assessing the contextual similarity between documents. This metric replaces the Euclidean distance employed by the self-organizing maps algorithm in identifying the winner neuron. Experiments performed using both algorithms demonstrates a superior performance of the proposed variant against the self-organizing map algorithm regarding the average recall-precision curves.", "authors": ["Apostolos Georgakis", "Costas Kotropoulos", "Ioannis Pitas"], "n_citation": 0, "title": "A SOM variant based on the Wilcoxon test for document organization and retrieval", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c5a7de78-d1f9-44d1-961e-0ab29443a2f2"}
{"abstract": "Central Pattern generators (CPGs) are neural circuits that produce patterned signals to drive rhythmic behaviors in a robust and flexible manner. In this paper we analyze the triphasic rhythm of a well known CPG circuit using two different models of spiking-bursting neurons and several network topologies. By means of a measure of mutual information we calculate the degree of information exchange in the bursting activity between neurons. We discuss the precision and robustness of different network configurations.", "authors": ["Francisco de Borja Rodr\u00edguez", "Roberto Latorre", "Pablo Varona"], "n_citation": 50, "title": "Characterization of triphasic rhythms in Central Pattern generators (II): Burst information analysis", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "c6e160a2-befb-440e-bdab-4539ef939dde"}
{"abstract": "This paper examines the Gray Web Forums in Taiwan. We study their characteristics and develop an analysis framework for assisting investigations on forum communities. Based on the statistical data collected from online forums, we found that the relationship between a posting and its responses is highly correlated to the forum nature. In addition, hot threads extracted based on the proposed metric can be used to assist analysts in identifying illegal or inappropriate contents. Furthermore, members' roles and activities in a virtual ommunity can be identified by member level analysis.", "authors": ["Jau-Hwang Wang", "Tianjun Fu", "Hong-Ming Lin", "Hsinchun Chen"], "n_citation": 0, "title": "A framework for exploring gray web forums : Analysis of forum-based communities in Taiwan", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c7755d6c-06db-4c58-806a-cce6e7284155"}
{"abstract": "Context-awareness has become a necessity for adaptable intelligent applications and services. It is crucial for ubiquitous and pervasive computing because the context of a user or device serves as the source of information to adapt services. In this paper, we propose a modular context management system that is able to collect, transform, reason on and use context information to adapt services. By employing a component-based approach, we enable our infrastructure not only to support context-aware adaptation of services, but also to support adaptation of the context management system itself at deployment time and at runtime. This self-adaptation is based upon the service requirements and the current context of the device, such as the current resource usage or other devices in the neighborhood, resulting in an adaptive context management system for improved quality of service.", "authors": ["Davy Preuveneers", "Yolande Berbers"], "n_citation": 64, "title": "Adaptive context management using a component-based approach", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "c89cd081-7a81-405c-a547-2838862de322"}
{"abstract": "Urban multi-criteria optimized route guidance by considering unspecified site satisfaction, an extended type of urban multi-objective optimized route selection, called as both NP-Hard problems and one of the branches of multi-criteria shortest path problems (MSPP). It is not only suggests a route based on route guidance principles and optimized due to routing criteria but also passes through all unspecified site(s) such as gas stations, banks determined by drivers. By proposing a novel approach on the bases of route guidance navigation system principles, virus theory (viral infection and local/site infection) and by GIS and GA utilization, this paper is come up to rate of search improvement in urban multi-criteria optimized route guidance by considering unspecified site satisfaction on real network with multiple dependent criteria. Tests of route selection for a part of north-west of Tehran traffic network are conducted and the results show the efficiency of the algorithm and support our analyses.", "authors": ["Parham Pahlavani", "Farhad Samadzadegan", "M. R. Delavar"], "n_citation": 0, "title": "A GIS-Based Approach for Urban Multi-criteria Quasi Optimized Route Guidance by Considering Unspecified Site Satisfaction", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "c8a103b0-246f-4bb2-8c5e-f39e6731c982"}
{"abstract": "Cryptosystems based on the knapsack problem were among the first public key systems to be invented and for a while were considered quite promising. Basically all knapsack cryptosystems that have been proposed so far have been broken, mainly by means of lattice reduction techniques. However, a few knapsack-like cryptosystems have withstood cryptanalysis, among which the Chor-Rivest scheme [2] even if this is debatable (see [16]), and the Qu-Vanstone scheme proposed at the Dagstuhl'93 workshop [13] and published in [14]. The Qu-Vanstone scheme is a public key scheme based on group factorizations in the additive group of integers modulo n that generalizes Merkle-Hellman cryptosystems. In this paper, we present a novel use of lattice reduction, which is of independent interest, exploiting in a systematic manner the notion of an orthogonal lattice. Using the new technique, we successfully attack the Qu-Vanstone cryptosystem. Namely, we show how to recover the private key from the public key. The attack is based on a careful study of the so-called Merkle-Hellman transformation.", "authors": ["Phong Q. Nguyen", "Jacques Stern"], "n_citation": 0, "title": "Merkle-Hellman revisited: A cryptanalysis of the Qu-vanstone cryptosystem based on group factorizations", "venue": "Lecture Notes in Computer Science", "year": 1997, "id": "ca7659e0-ac70-4265-9e9e-a60907556622"}
{"abstract": "It is difficult to generate raster Digital Elevation Models (DEMs) from terrain mass point data sets too large to fit into memory, such as those obtained by LIDAR. We describe prototype tools for streaming DEM generation that use memory and disk I/O very efficiently. From 500 million bare-earth LIDAR double precision points (11.2 GB) our tool can, in just over an hour on a standard laptop with two hard drives, produce a 50,394 x 30,500 raster DEM with 20 foot post spacing in 16 bit binary BIL format (3 GB), using less than 100 MB of main memory and less than 300 MB of temporary disk space.", "authors": ["Martin Isenburg", "Yuanxin Liu", "Jonathan Richard Shewchuk", "Jack Snoeyink", "Tim Thirion"], "n_citation": 50, "title": "Generating Raster DEM from Mass Points Via TIN Streaming", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cb2d38c1-db72-48f2-ba6a-d31a8c9ac9ef"}
{"abstract": "The problem of reconstructing finite subsets of the integer lattice from X-rays has been studied in discrete mathematics and applied in several fields like image processing, data security, electron microscopy. In this paper we focus on the stability of the reconstruction problem for some lattice sets. First we show some theoretical bounds for additive sets, and a numerical experiment is made by using linear programming to deal with stability for convex sets.", "authors": ["Sara Brunetti", "Alain Daurat"], "n_citation": 0, "title": "Stability in discrete tomography: Linear programming, additivity and convexity", "venue": "Lecture Notes in Computer Science", "year": 2003, "id": "cbda0f48-1034-4606-a2ad-88970b19ffd5"}
{"abstract": "We present a method for constructing a teleological model of drawing of a physical device through analogical transfer of the teleological model of the same device in an almost identical drawing. A source case, in this method, contains both a 2-D vector-graphics line drawing of a physical device and a teleological model of the device called a Drawing-Shape-Structure-Behavior-Function (DSSBF) model that relates shapes and spatial relations in the drawing to specifications of the structure, behavior and function of the device. Given an almost identical target 2-D vector-graphics line drawing as input, we describe how an agent may align the two drawings, and transfer the relevant structural, behavioral and functional elements over to the target drawing. We also describe how the DSSBF model of the source drawing guides the alignment of the two drawings. The Archytas system implements this method in domain of kinematic devices that convert translational motion into rotational motion, such as a piston and crankshaft device.", "authors": ["Patrick W. Yaner", "Ashok K. Goel"], "n_citation": 0, "title": "From diagrams to models by analogical transfer", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cc153bd7-c6bc-4f48-a905-e9108e914bcc"}
{"abstract": "The notion of skeleton plays a major role in shape analysis. Some usually desirable characteristics of a skeleton are: sufficient for the reconstruction of the original object, centered, thin and homotopic. The Euclidean Medial Axis presents all these characteristics in a continuous framework. In the discrete case, the Exact Euclidean Medial Axis (MA) is also sufficient for reconstruction and centered. It no longer preserves homotopy but it can be combined with a homotopic thinning to generate homotopic skeletons. The thinness of the MA, however, may be discussed. In this paper we present the definition of the Exact Euclidean Medial Axis on Higher Resolution which has the same properties as the MA but with a better thinness characteristic, against the price of rising resolution. We provide an efficient algorithm to compute it.", "authors": ["Andr\u00e9 Vital Sa\u00fade", "Michel Couprie", "Roberto de Alencar Lotufo"], "n_citation": 0, "title": "Exact euclidean medial axis in higher resolution", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cd1259ba-29e6-4770-b8a0-17c83a7d9702"}
{"abstract": "The MIEL++ system integrates data expressed in two different formalisms: a relational database and an XML database. The XML database is filled with data semi-automatically retrieved from the Web, which have been semantically enriched according to the ontology used in the relational database. These data may be imprecise and represented as possibility distributions. The MIEL++ querying system scans the two databases simultaneously in a transparent way for the end-user. To scan the XML database, the MIEL query is translated into an XML tree query. In this paper, we propose to introduce flexibility into the query processing of the XML database, in order to take into account the imperfections due to the semantic enrichment of its data. This flexibility relies on fuzzy queries and query rewriting which consists in generating a set of approximate queries from an original query rising three transformation techniques: deletion, renaming and insertion of query nodes.", "authors": ["Patrice Buche", "Juliette Dibie -Barthelemy", "Fanny Wattez"], "n_citation": 50, "title": "Approximate querying of XML fuzzy data", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "ce944151-d9a7-4bc9-b490-e917e740ec3a"}
{"abstract": "We conducted eye-tracking studies of subjects solving the problem of finding shortest paths in a graph using a known procedure (Dijkstra's algorithm). The goal of these studies was to investigate how people reason about and solve graphically presented problems. First, we compared performance when the graphical display was animated to when the display was static. Second, we compared performance when the display was initially sparse, with detailed information being progressively revealed, to when the display presented all information simultaneously. Results suggest that while animation of the procedure or algorithm does not improve accuracy, animation coupled with progressively revealing objects of interest on the display does improve accuracy and other process measures of problem solving.", "authors": ["Daesub Yoon", "N. Hari Narayanan", "Soo-Cheol Lee", "Oh-Cheon Kwon"], "n_citation": 0, "title": "Exploring the effect of animation and progressive revealing on diagrammatic problem solving", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "cfedcd7e-02d5-41a6-8af5-6da21932f484"}
{"abstract": "The RSA public key cryptosystem is based on a single modular equation in one variable. A natural generalization of this approach is to consider systems of several modular equations in several variables. In this paper we consider Patarin's Hidden Field Equations (HFE) scheme, which is believed to be one of the strongest schemes of this type. We represent the published system of multivariate polynomials by a single univariate polynomial of a special form over an extension field, and use it to reduce the cryptanalytic problem to a system of cm 2  quadratic equations in m variables over the extension field, Finally, we develop a new relinearization method for solving such systems for any constant \u2208 > 0 in expected polynomial time. The new type of attack is quite general, and in a companion paper we use it to attack other multivariate algebraic schemes, such as the Dragon encryption and signature schemes. However, we would like to emphasize that the polynomal time complexities may be infeasibly large for some choices of the parameters, and thus some variants of these schemes may remain practically unbroken in spite of the new attack.", "authors": ["A. Kipuis", "Adi Shamir"], "n_citation": 0, "title": "Cryptanalysis of the HFE public key cryptosystem by relinearization", "venue": "Lecture Notes in Computer Science", "year": 1999, "id": "cff12d33-1efd-4762-9eb9-061245dbf3f1"}
{"abstract": "With recent advances in sensory and mobile computing technology, enormous amounts of data about moving objects are being collected. With such data, it becomes possible to automatically identify suspicious behavior in object movements. Anomaly detection in massive sets of moving objects has many important applications, especially in surveillance, law enforcement, and homeland security. Due to the sheer volume of spatiotemporal and non-spatial data (such as weather and object type) associated with moving objects, it is challenging to develop a method that can efficiently and effectively detect anomalies in complex scenarios. The problem is further complicated by the fact that anomalies may occur at various levels of abstraction and be associated with different time and location granularities. In this paper, we analyze the problem of anomaly detection in moving objects and propose an efficient and scalable classification method, Motion-Alert, which proceeds with the following three steps. 1. Object movement features, called motifs, are extracted from the object paths. Each path consists of a sequence of motif expressions; associated with the values related to time and location. 2. To discover anomalies in object movements, motif-based generalization is performed that clusters similar object movement fragments and generalizes the movements based on the associated motifs. 3. With motif-based generalization, objects are put into a multi-level feature space and are classified by a classifier that can handle high-dimensional feature spaces. We implemented the above method as one of the core components in our moving-object anomaly detection system, motion-alert. Our experiments show that the system is more accurate than traditional classification techniques.", "authors": ["Xiaolei Li", "Jiawei Han", "Sangkyum Kim"], "n_citation": 64, "title": "Motion-alert : Automatic anomaly detection in massive moving objects", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d02fb81a-c78b-4e1b-a0e4-c648bfd6d16d"}
{"abstract": "Infectious disease informatics is a subfield of security informatics that focuses on information analysis and management issues critical to the prevention, detection, and management of naturally occurring or terrorist-engineered infectious disease outbreaks. We have developed a research prototype called BioPortal which provides an integrated environment to support cross-jurisdictional and cross-species infectious disease information sharing, integration, analysis, and reporting. This paper reports a pilot study evaluating BioPortal's usability, user satisfaction, and potential impact on practice.", "authors": ["Paul Jen-Hwa Hu", "Daniel Zeng", "Hsinchun Chen", "Catherine A. Larson", "Wei Chang", "Chunju Tseng"], "n_citation": 0, "title": "Evaluating an infectious disease information sharing and analysis system", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d2511f28-6e3c-480b-aacd-7992ac83e865"}
{"abstract": "We introduce the notion of a dynamic accumulator. An accumulator scheme allows one to hash a large set of inputs into one short value, such that there is a short proof that a given input was incorporated into this value. A dynamic accumulator allows one to dynamically add and delete a value, such that the cost of an add or delete is independent of the number of accumulated values. We provide a construction of a dynamic accumulator and an efficient zero-knowledge proof of knowledge of an accumulated value. We prove their security under the strong RSA assumption. We then show that our construction of dynamic accumulators enables efficient revocation of anonymous credentials, and membership revocation for recent group signature and identity escrow schemes.", "authors": ["Jan Camenisch", "Anna Lysyanskaya"], "n_citation": 0, "title": "Dynamic accumulators and application to efficient revocation of anonymous credentials", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d3aa16fb-3266-422b-b3e8-57764b7742be"}
{"abstract": "Searchers' difficulty in formulating effective queries for their information needs is well known. Analysis of search session logs shows that users often pose short, vague queries and then struggle with revising them. Interactive query expansion (users selecting terms to add to their queries) dramatically improves effectiveness and satisfaction. Suggesting relevant candidate expansion terms based on the initial query enables users to satisfy their information needs faster. We find that suggesting query phrases other users have found it necessary to add for a given query (mined from session logs) dramatically improves the quality of suggestions over simply using cooccurrence. However, this exacerbates the sparseness problem faced when mining short queries that lack features. To mitigate this, we tag query phrases with higher level topical categories to mine more general rules, finding that this enables us to make suggestions for approximately 10% more queries while maintaining an acceptable false positive rate.", "authors": ["Eric C. Jensen", "Steven M. Beitzel", "Abdur Chowdhury", "Ophir Frieder"], "n_citation": 50, "title": "Query phrase suggestion from topically tagged session logs", "venue": "Lecture Notes in Computer Science", "year": 2006, "id": "d4204a19-13ad-4f94-bfcb-30fefc8c57e6"}
{"abstract": "The primary goal of this paper is to facilitate the rate-distortion control in compressed domain, without introducing a full decoding and re-encoding system in pixel domain. For this aim, the error propagation behavior over several frame-sequences due to DCT coefficients-drop is investigated on the basis of statistical and empirical properties. Then, such properties are used to develop a simple estimation model for the CD distortion accounting for the characteristics of the underlying coded-frame. Experimental results show that the proposed model allows us to effectively control rate-distortions into coded-frames over different kinds of video sequences.", "authors": ["Jin-Soo Kim", "Jae-Gon Kim"], "n_citation": 50, "title": "Efficient control for the distortion incurred by dropping DCT coefficients in compressed domain", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d5068005-53a5-4055-bf02-b9aafbbb9d4b"}
{"abstract": "We give the first construction of a practical threshold pseudo-random function. The protocol for evaluating the function is efficient enough that it can be used to replace random oracles in some protocols relying on such oracles. In particular, we show how to transform the efficient cryptographically secure Byzantine agreement protocol by Cachin, Kursawe and Shoup for the random oracle model into a cryptographically secure protocol for the complexity theoretic model without loosing efficiency or resilience, thereby constructing an efficient and optimally resilient Byzantine agreement protocol for the complexity theoretic model.", "authors": ["Jesper Buus Nielsen"], "n_citation": 0, "title": "A threshold pseudorandom function construction and its applications", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d5faf496-5d5b-4569-ad4e-85205c18c5ff"}
{"abstract": "We present a general treatment of all non-cryptographic (i.e., information-theoretically secure) linear verifiable-secret-sharing (VSS) and distributed-commitment (DC) schemes, based on an underlying secret sharing scheme, pairwise checks between players, complaints, and accusations of the dealer. VSS and DC are main building blocks for unconditional secure multi-party computation protocols. This general approach covers all known linear VSS and DC schemes. The main theorem states that the security of a scheme is equivalent to a pure linear-algebra condition on the linear mappings (e.g. described as matrices and vectors) describing the scheme. The security of all known schemes follows as corollaries whose proofs are pure linear-algebra arguments, in contrast to some hybrid arguments used in the literature. Our approach is demonstrated for the CDM DC scheme, which we generalize to be secure against mixed adversary settings (some curious and some dishonest players), and for the classical BGW VSS scheme, for which we show that some of the checks between players are superfluous, i.e., the scheme is not optimal. More generally, our approach, establishing the minimal conditions for security (and hence the common denominator of the known schemes), can lead to the design of more efficient VSS and DC schemes for general adversary structures.", "authors": ["Serge Fehr", "Ueli Maurer"], "n_citation": 0, "title": "Linear VSS and distributed commitments based on secret sharing and pairwise checks", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d692c1f4-5b50-48f7-9056-160c6b0a389a"}
{"abstract": "Using data that combines information from the Federal Aviation Administration, the RAND Corporation, and a newly developed database on global terrorist activity, we are able to examine trends in 1,101 attempted aerial hijackings that occurred around the world from 1931 to 2003. We have especially complete information for 828 hijackings that occurred before 1986. Using a rational choice theoretical framework, we employ econometric time-series methods to estimate the impact of several major counter hijacking interventions on the likelihood of differently motivated hijacking events and to model the predictors of successful hijackings. Some of the interventions examined use certainty-based strategies of target hardening to reduce the perceived likelihood of success while others focus on raising the perceived costs of hijacking by increasing the severity of punishment. We also assess which specific intervention strategies were most effective for deterring hijackers whose major purpose was terrorism related. We found support for the conclusion that new hijacking attempts were less likely to be undertaken when the certainty of apprehension was increased through metal detectors and law enforcement at passenger checkpoints. We also found that fewer hijackers attempted to divert airliners to Cuba once that country made it a crime to hijack flights. Our results support the contagion view that hijacking rates significantly increase after a series of hijackings closely-clustered in time. Finally, we found that policy interventions only significantly decrease the likelihood of non-terrorist-related hijackings.", "authors": ["Laura Dugan", "Gary LaFree", "Alex R. Piquero"], "n_citation": 50, "title": "Testing a rational choice model of airline hijackings", "venue": "Lecture Notes in Computer Science", "year": 2005, "id": "d6b2c97b-0255-4928-a079-b16afa0e391a"}
{"abstract": "In more than one decade, genomic research produced a huge amount of experimental data. Although these data are usually freely available on the World Wide Web, accessing them in a consistent and fruitful way is not always an easy task. One of the main causes of this problem can be recognized in the lack of user interfaces that are sufficiently flexible and, at the same time, highly interactive and cooperative and also semantically precise. This paper tackles this issues by presenting a semantics drive paradigm for formulating queries to genomic and protein databases. The paradigm is founded on knowledge-based reasoning capabilities, in order to provide the user with a semantics driven guide in the difficult task of building the intended query.", "authors": ["Paolo Bresciani", "Paolo Fontana"], "n_citation": 0, "title": "A knowledge-based query system for biological databases", "venue": "Lecture Notes in Computer Science", "year": 2002, "id": "d6e06467-84ec-4ae9-a28a-e9d4d8d9b73b"}
{"abstract": "Communication plays a fundamental role in multi-agents systems. One of the main issues in the design of agent interaction protocols is the verification that a given protocol implementation is conformant w.r.t. the abstract specification of it. In this work we tackle those aspects of the conformance verification issue, that regard the dependence/independence of conformance from the agent private state in the case of logic, individual agents, set in a multi-agent framework. We do this by working on a specific agent programming language, DyLOG, and by focussing on interaction protocol specifications described by AUML sequence diagrams. By showing how AUML sequence diagrams can be translated into regular grammars and, then, by interpreting the problem of conformance as a problem of language inclusion, we describe a method for automatically verifying a form of structural conformance; such a process is shown to be decidable and an upper bound of its complexity is given. We also give a set of properties that describes the influence of the agent private information on the conformance of its communication policies to protocol specifications.", "authors": ["Matte